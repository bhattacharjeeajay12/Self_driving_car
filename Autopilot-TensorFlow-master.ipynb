{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Problem Statment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem statement is to build a Self-Driving car. \n",
    "For learning and for the simplicity of training, we are using a small dataset of 2.3 GB. The dataset has been taken from : https://github.com/SullyChen/Autopilot-TensorFlow . Lot of thanks to Sully Chen for providing this dataset. \n",
    "\n",
    "In the dataset sully has provided dataset containg 45000 images with steering wheel angles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XHFnthirwlfn"
   },
   "outputs": [],
   "source": [
    "# Credits: https://github.com/SullyChen/Autopilot-TensorFlow\n",
    "# Research paper: End to End Learning for Self-Driving Cars by Nvidia. [https://arxiv.org/pdf/1604.07316.pdf]\n",
    "\n",
    "# NVidia dataset: 72 hrs of video => 72*60*60*30 = 7,776,000 images\n",
    "# Nvidia blog: https://devblogs.nvidia.com/deep-learning-self-driving-cars/\n",
    "\n",
    "\n",
    "# Our Dataset: https://github.com/SullyChen/Autopilot-TensorFlow [https://drive.google.com/file/d/0B-KJCaaF7elleG1RbzVPZWV4Tlk/view]\n",
    "# Size: 25 minutes = 25*60*30 = 45,000 images ~ 2.3 GB\n",
    "\n",
    "\n",
    "# If you want to try on a slightly large dataset: 70 minutes of data ~ 223GB\n",
    "# Refer: https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5\n",
    "# Format: Image, latitude, longitude, gear, brake, throttle, steering angles and speed\n",
    "\n",
    "\n",
    "\n",
    "# Additional Installations:\n",
    "# pip3 install h5py\n",
    "\n",
    "\n",
    "# AWS: https://aws.amazon.com/blogs/machine-learning/get-started-with-deep-learning-using-the-aws-deep-learning-ami/\n",
    "\n",
    "# Youtube:https://www.youtube.com/watch?v=qhUvQiKec2U\n",
    "# Further reading and extensions: https://medium.com/udacity/teaching-a-machine-to-steer-a-car-d73217f2492c\n",
    "# More data: https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bhatt\\Anaconda3\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import pi\n",
    "import cv2\n",
    "import scipy.misc\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n",
      "/physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(physical_devices[0].device_type)\n",
    "print(physical_devices[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D:\\Study\\Applied_AI\\SELF_DRIVING_CAR\\Autopilot-TensorFlow-master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Study\\\\Applied_AI\\\\SELF_DRIVING_CAR\\\\Autopilot-TensorFlow-master'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"driving_dataset\"\n",
    "DATA_FILE = os.path.join(DATA_FOLDER, \"data.txt\")\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "train_batch_pointer = 0\n",
    "test_batch_pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45406 45406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(DATA_FILE) as f:\n",
    "    for line in f:\n",
    "        image_name, angle = line.split()\n",
    "        \n",
    "        image_path = os.path.join(DATA_FOLDER, image_name)\n",
    "        x.append(image_path)\n",
    "        \n",
    "        angle_radians = float(angle) * (pi / 180)  #converting angle into radians\n",
    "        y.append(angle_radians)\n",
    "y = np.array(y)\n",
    "print(str(len(x))+\" \"+str(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36324, 36324, 9082, 9082)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_ratio = int(len(x) * 0.8)\n",
    "\n",
    "train_x = x[:split_ratio]\n",
    "train_y = y[:split_ratio]\n",
    "\n",
    "test_x = x[split_ratio:]\n",
    "test_y = y[split_ratio:]\n",
    "\n",
    "len(train_x), len(train_y), len(test_x), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (10, 7))\n",
    "plt.hist(train_y, bins = 50, histtype = \"step\")\n",
    "plt.hist(test_y, bins = 50, histtype = \"step\")\n",
    "plt.title(\"Steering Wheel angle in train and test\")\n",
    "plt.xlabel(\"Angle\")\n",
    "plt.ylabel(\"Bin count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainBatch(batch_size):\n",
    "    global train_batch_pointer\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for i in range(batch_size):\n",
    "        read_image = cv2.imread(train_x[(train_batch_pointer + i) % len(train_x)]) #here % len(train_x) is used to make sure that\n",
    "        #\"train_batch_pointer + i\" should not cross the number of train images. As soon as the value of \"train_batch_pointer\" is\n",
    "        #equal to number of train images then it will again start reading the train images from the beginning means from 0th\n",
    "        #index onwards.\n",
    "        read_image_road = read_image[-150:] #here, we are taking only the lower part of the images where there is a road in the\n",
    "        #image. As, we are concern only with the curves of the road to predict angles so therefore, we are discarding the upper\n",
    "        #part of the image. Hence, here -\"150\" is equivalent to the last 150 matrix pixels of the image.\n",
    "        read_image_resize = cv2.resize(read_image_road, (200, 66)) #After, resizing, each image will be of size (66, 200, 3). \n",
    "        #now since we have kept only the last 150 matrices in the image so the size of our image is now (150, 455, 3). \n",
    "        #Now 455/150 = 3.0303. Also 200/66 = 3.0303. Hence, here we are keeping the aspect ratio of images same.\n",
    "        read_image_final = read_image_resize/255.0  #here, we are normalizing the images\n",
    "        \n",
    "        x_result.append(read_image_final) #finally appending the image pixel matrix\n",
    "        \n",
    "        y_result.append(train_y[(train_batch_pointer + i) % len(train_y)]) #appending corresponding labels\n",
    "        \n",
    "    train_batch_pointer += batch_size\n",
    "        \n",
    "    return x_result, y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestBatch(batch_size):\n",
    "    global test_batch_pointer\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for i in range(batch_size):\n",
    "        read_image = cv2.imread(test_x[(test_batch_pointer + i) % len(test_x)]) #here % len(test_x) is used to make sure that\n",
    "        #\"test_batch_pointer + i\" should not cross the number of test images. As soon as the value of \"test_batch_pointer\" is\n",
    "        #equal to number of test images then it will again start reading the test images from the beginning means from 0th\n",
    "        #index onwards.\n",
    "        read_image_road = read_image[-150:] #here, we are taking only the lower part of the images where there is a road in the\n",
    "        #image. As, we are concern only with the curves of the road to predict angles so therefore, we are discarding the upper\n",
    "        #part of the image. Hence, here -\"150\" is equivalent to the last 150 matrix pixels of the image.\n",
    "        read_image_resize = cv2.resize(read_image_road, (200, 66)) #After, resizing, each image will be of size (66, 200, 3). \n",
    "        #now since we have kept only the last 150 matrices in the image so the size of our image is now (150, 455, 3). \n",
    "        #Now 455/150 = 3.0303. Also 200/66 = 3.0303. Hence, here we are keeping the aspect ratio of images same.\n",
    "        read_image_final = read_image_resize/255.0  #here, we are normalizing the images\n",
    "        \n",
    "        x_result.append(read_image_final) #finally appending the image pixel matrix\n",
    "        \n",
    "        y_result.append(test_y[(test_batch_pointer + i) % len(test_y)]) #appending corresponding labels\n",
    "        \n",
    "    test_batch_pointer += batch_size\n",
    "        \n",
    "    return x_result, y_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightVariable(shape):\n",
    "    initial = tf.truncated_normal(shape = shape, stddev = 0.1)\n",
    "    return tf.Variable(initial) \n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def convolution(previous_input, filter_input, strides):\n",
    "    return tf.nn.conv2d(previous_input, filter_input, strides = [1, strides, strides, 1], padding = \"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = tf.placeholder(tf.float32, shape = [None, 66, 200, 3], name = \"Plc_1\")\n",
    "y_true = tf.placeholder(tf.float32, name = \"Plc_2\")\n",
    "\n",
    "input_image = x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bhatt\\Anaconda3\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#Convolution Layers\n",
    "#First convolution layer\n",
    "W_Conv1 = weightVariable([5,5,3,24])\n",
    "B_Conv1 = bias_variable([24])\n",
    "Conv1 = tf.nn.relu(convolution(input_image, W_Conv1, 2) + B_Conv1)\n",
    "#strides = 2\n",
    "#Output size: 31*98*24\n",
    "\n",
    "#Second convolution layer\n",
    "W_Conv2 = weightVariable([5,5,24,36])\n",
    "B_Conv2 = bias_variable([36])\n",
    "Conv2 = tf.nn.relu(convolution(Conv1, W_Conv2, 2) + B_Conv2)\n",
    "#strides = 2\n",
    "#Output size: 14*47*36\n",
    "\n",
    "#Third convolution layer\n",
    "W_Conv3 = weightVariable([5,5,36,48])\n",
    "B_Conv3 = bias_variable([48])\n",
    "Conv3 = tf.nn.relu(convolution(Conv2, W_Conv3, 2) + B_Conv3)\n",
    "#strides = 2\n",
    "#Output size: 5*22*48\n",
    "\n",
    "#Fourth convolution layer\n",
    "W_Conv4 = weightVariable([3,3,48,64])\n",
    "B_Conv4 = bias_variable([64])\n",
    "Conv4 = tf.nn.relu(convolution(Conv3, W_Conv4, 1) + B_Conv4)\n",
    "#strides = 1\n",
    "#Output size: 3*20*64\n",
    "\n",
    "\n",
    "#Fifth convolution layer\n",
    "W_Conv5 = weightVariable([3,3,64,64])\n",
    "B_Conv5 = bias_variable([64])\n",
    "Conv5 = tf.nn.relu(convolution(Conv4, W_Conv5, 1) + B_Conv5)\n",
    "#strides = 1\n",
    "#Output size: 1*18*64\n",
    "\n",
    "#Fully-Connected Dense Layers\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "#First FC-Dense\n",
    "#Input = 1*18*64 = 1152\n",
    "W_FC1 = weightVariable([1152, 1164])\n",
    "B_FC1 = bias_variable([1164])\n",
    "FC1_Flatten = tf.reshape(Conv5, [-1, 1152]) #here, -1 indicates 1. It means that the shape of FC1_Flatten will be 1*1152\n",
    "Output_FC1 = tf.nn.relu(tf.matmul(FC1_Flatten, W_FC1) + B_FC1) #so, here shape of FC1_Flatten is 1*1152 and shape of W_FC1 will\n",
    "#be 1152*1164. Therefore, there will be a matrix multiplication of matrices: (1*1152) * (1152*1164) = (1*1164).\n",
    "Output_FC1_drop = tf.nn.dropout(Output_FC1, keep_prob)\n",
    "\n",
    "#Second FC-Dense\n",
    "#Input = 1*1164 = 1164\n",
    "W_FC2 = weightVariable([1164, 100])\n",
    "B_FC2 = bias_variable([100])\n",
    "Output_FC2 = tf.nn.relu(tf.matmul(Output_FC1_drop, W_FC2) + B_FC2) #so, here shape of Output_FC1_drop is 1*1164 and shape of \n",
    "#W_FC2 will be 1164*100. Therefore, there will be a matrix multiplication of matrices: (1*1164) * (1164*100) = (1*100).\n",
    "Output_FC2_drop = tf.nn.dropout(Output_FC2, keep_prob)\n",
    "\n",
    "#Third FC-Dense\n",
    "#Input = 1*100 = 100\n",
    "W_FC3 = weightVariable([100, 50])\n",
    "B_FC3 = bias_variable([50])\n",
    "Output_FC3 = tf.nn.relu(tf.matmul(Output_FC2_drop, W_FC3) + B_FC3) #so, here shape of Output_FC2_drop is 1*100 and shape of \n",
    "#W_FC3 will be 100*50. Therefore, there will be a matrix multiplication of matrices: (1*100) * (100*50) = (1*50).\n",
    "Output_FC3_drop = tf.nn.dropout(Output_FC3, keep_prob)\n",
    "\n",
    "#Fourth FC-Dense\n",
    "#Input = 1*50 = 50\n",
    "W_FC4 = weightVariable([50, 10])\n",
    "B_FC4 = bias_variable([10])\n",
    "Output_FC4 = tf.nn.relu(tf.matmul(Output_FC3_drop, W_FC4) + B_FC4) #so, here shape of Output_FC3_drop is 1*50 and shape of \n",
    "#W_FC4 will be 50*10. Therefore, there will be a matrix multiplication of matrices: (1*50) * (50*10) = (1*10).\n",
    "Output_FC4_drop = tf.nn.dropout(Output_FC4, keep_prob)\n",
    "\n",
    "#Final Output to one neuron with linear/identity function\n",
    "#Input = 1*10 = 10\n",
    "W_FC5 = weightVariable([10, 1])\n",
    "B_FC5 = bias_variable([1])\n",
    "y_predicted = tf.identity(tf.matmul(Output_FC4_drop, W_FC5) + B_FC5) #so, here shape of Output_FC4_drop is 1*10 and shape of \n",
    "#W_FC5 will be 10*1. Therefore, there will be a matrix multiplication of matrices: (1*10) * (10*1) = (1*1). Since, this is a \n",
    "#regression problem so we have applied identity fuction in the end. We can also apply \"atan\" function here. If computational\n",
    "#power is available then the model should be tested with both identity and atan functions. In the end, that function should be\n",
    "#considered which gives better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Study\\\\Applied_AI\\\\SELF_DRIVING_CAR\\\\Autopilot-TensorFlow-master'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 6.931443214416504, Test_Loss: 6.679807662963867 *\n",
      "Epoch: 1, Train_Loss: 6.5932488441467285, Test_Loss: 6.190629959106445 *\n",
      "Epoch: 1, Train_Loss: 18.928695678710938, Test_Loss: 6.221070289611816\n",
      "Epoch: 1, Train_Loss: 7.916269302368164, Test_Loss: 6.311333656311035\n",
      "Epoch: 1, Train_Loss: 8.736098289489746, Test_Loss: 6.222428798675537 *\n",
      "Epoch: 1, Train_Loss: 10.266275405883789, Test_Loss: 6.347701549530029\n",
      "Epoch: 1, Train_Loss: 6.303419589996338, Test_Loss: 6.5708489418029785\n",
      "Epoch: 1, Train_Loss: 6.311391353607178, Test_Loss: 7.498342037200928\n",
      "Epoch: 1, Train_Loss: 7.192412376403809, Test_Loss: 8.01220703125\n",
      "Epoch: 1, Train_Loss: 16.292089462280273, Test_Loss: 6.456736087799072 *\n",
      "Epoch: 1, Train_Loss: 6.715980052947998, Test_Loss: 6.158851146697998 *\n",
      "Epoch: 1, Train_Loss: 6.124484539031982, Test_Loss: 6.153463840484619 *\n",
      "Epoch: 1, Train_Loss: 10.860284805297852, Test_Loss: 6.158249378204346\n",
      "Epoch: 1, Train_Loss: 7.203709602355957, Test_Loss: 6.146958827972412 *\n",
      "Epoch: 1, Train_Loss: 6.185820579528809, Test_Loss: 6.183070182800293\n",
      "Epoch: 1, Train_Loss: 6.15267276763916, Test_Loss: 6.261601448059082\n",
      "Epoch: 1, Train_Loss: 6.1181159019470215, Test_Loss: 6.222155570983887 *\n",
      "Epoch: 1, Train_Loss: 6.156065940856934, Test_Loss: 6.145026683807373 *\n",
      "Epoch: 1, Train_Loss: 6.171975135803223, Test_Loss: 6.109511852264404 *\n",
      "Epoch: 1, Train_Loss: 6.141662120819092, Test_Loss: 6.399230003356934\n",
      "Epoch: 1, Train_Loss: 6.142180919647217, Test_Loss: 6.079220294952393 *\n",
      "Epoch: 1, Train_Loss: 6.117068767547607, Test_Loss: 6.148743629455566\n",
      "Epoch: 1, Train_Loss: 6.104958534240723, Test_Loss: 6.056068420410156 *\n",
      "Epoch: 1, Train_Loss: 6.064186096191406, Test_Loss: 6.044070243835449 *\n",
      "Epoch: 1, Train_Loss: 6.053714275360107, Test_Loss: 6.036402225494385 *\n",
      "Epoch: 1, Train_Loss: 6.076794624328613, Test_Loss: 6.028919696807861 *\n",
      "Epoch: 1, Train_Loss: 6.107814311981201, Test_Loss: 6.022097587585449 *\n",
      "Epoch: 1, Train_Loss: 6.067140102386475, Test_Loss: 10.99319076538086\n",
      "Epoch: 1, Train_Loss: 6.0327229499816895, Test_Loss: 6.987851619720459 *\n",
      "Epoch: 1, Train_Loss: 6.01749324798584, Test_Loss: 6.010209083557129 *\n",
      "Epoch: 1, Train_Loss: 6.002536296844482, Test_Loss: 6.0006608963012695 *\n",
      "Epoch: 1, Train_Loss: 5.993852138519287, Test_Loss: 5.993530750274658 *\n",
      "Epoch: 1, Train_Loss: 5.988445281982422, Test_Loss: 5.989876747131348 *\n",
      "Epoch: 1, Train_Loss: 5.98077392578125, Test_Loss: 5.983030319213867 *\n",
      "Epoch: 1, Train_Loss: 5.9773945808410645, Test_Loss: 5.982285976409912 *\n",
      "Epoch: 1, Train_Loss: 5.970302581787109, Test_Loss: 5.97306489944458 *\n",
      "Epoch: 1, Train_Loss: 5.96278715133667, Test_Loss: 5.968033790588379 *\n",
      "Epoch: 1, Train_Loss: 5.957311630249023, Test_Loss: 5.9695539474487305\n",
      "Epoch: 1, Train_Loss: 5.9520463943481445, Test_Loss: 5.962969779968262 *\n",
      "Epoch: 1, Train_Loss: 5.953821182250977, Test_Loss: 5.953335762023926 *\n",
      "Epoch: 1, Train_Loss: 5.963719367980957, Test_Loss: 5.9641432762146\n",
      "Epoch: 1, Train_Loss: 5.941863536834717, Test_Loss: 5.948332786560059 *\n",
      "Epoch: 1, Train_Loss: 5.946037292480469, Test_Loss: 5.941368103027344 *\n",
      "Epoch: 1, Train_Loss: 5.9272637367248535, Test_Loss: 5.9266743659973145 *\n",
      "Epoch: 1, Train_Loss: 15.049339294433594, Test_Loss: 5.929126262664795\n",
      "Epoch: 1, Train_Loss: 6.167094707489014, Test_Loss: 5.920319080352783 *\n",
      "Epoch: 1, Train_Loss: 5.915474891662598, Test_Loss: 5.913865566253662 *\n",
      "Epoch: 1, Train_Loss: 5.907345294952393, Test_Loss: 5.922252655029297\n",
      "Epoch: 1, Train_Loss: 5.908466815948486, Test_Loss: 5.910869121551514 *\n",
      "Epoch: 1, Train_Loss: 5.92286491394043, Test_Loss: 5.907786846160889 *\n",
      "Epoch: 1, Train_Loss: 5.904134750366211, Test_Loss: 5.90593957901001 *\n",
      "Epoch: 1, Train_Loss: 5.898983001708984, Test_Loss: 5.901653289794922 *\n",
      "Epoch: 1, Train_Loss: 6.070693492889404, Test_Loss: 5.893321990966797 *\n",
      "Epoch: 1, Train_Loss: 6.088475704193115, Test_Loss: 5.882059574127197 *\n",
      "Epoch: 1, Train_Loss: 6.054802417755127, Test_Loss: 5.8780951499938965 *\n",
      "Epoch: 1, Train_Loss: 5.8697733879089355, Test_Loss: 5.875472545623779 *\n",
      "Epoch: 1, Train_Loss: 5.970302581787109, Test_Loss: 5.869536876678467 *\n",
      "Epoch: 1, Train_Loss: 5.965175628662109, Test_Loss: 5.912147521972656\n",
      "Epoch: 1, Train_Loss: 5.986939430236816, Test_Loss: 6.151860237121582\n",
      "Epoch: 1, Train_Loss: 5.974697113037109, Test_Loss: 11.316690444946289\n",
      "Epoch: 1, Train_Loss: 5.961137294769287, Test_Loss: 5.857582092285156 *\n",
      "Epoch: 1, Train_Loss: 5.86763334274292, Test_Loss: 5.830672740936279 *\n",
      "Epoch: 1, Train_Loss: 5.837586879730225, Test_Loss: 5.839154243469238\n",
      "Epoch: 1, Train_Loss: 5.88527250289917, Test_Loss: 5.853397369384766\n",
      "Epoch: 1, Train_Loss: 5.830825328826904, Test_Loss: 5.852293014526367 *\n",
      "Epoch: 1, Train_Loss: 5.811226844787598, Test_Loss: 5.811363697052002 *\n",
      "Epoch: 1, Train_Loss: 5.80190896987915, Test_Loss: 5.966894626617432\n",
      "Epoch: 1, Train_Loss: 5.796208381652832, Test_Loss: 5.852248668670654 *\n",
      "Epoch: 1, Train_Loss: 5.958559513092041, Test_Loss: 5.788775444030762 *\n",
      "Epoch: 1, Train_Loss: 11.346290588378906, Test_Loss: 5.85630989074707\n",
      "Epoch: 1, Train_Loss: 5.793567180633545, Test_Loss: 5.778870105743408 *\n",
      "Epoch: 1, Train_Loss: 5.7882866859436035, Test_Loss: 5.8004655838012695\n",
      "Epoch: 1, Train_Loss: 5.789041519165039, Test_Loss: 5.795889377593994 *\n",
      "Epoch: 1, Train_Loss: 5.784690856933594, Test_Loss: 5.848677635192871\n",
      "Epoch: 1, Train_Loss: 5.773027420043945, Test_Loss: 5.8270158767700195 *\n",
      "Epoch: 1, Train_Loss: 5.75650691986084, Test_Loss: 5.927943229675293\n",
      "Epoch: 1, Train_Loss: 5.754798889160156, Test_Loss: 5.812458515167236 *\n",
      "Epoch: 1, Train_Loss: 5.766668796539307, Test_Loss: 5.751617908477783 *\n",
      "Epoch: 1, Train_Loss: 5.752182960510254, Test_Loss: 5.736941337585449 *\n",
      "Epoch: 1, Train_Loss: 5.738833427429199, Test_Loss: 5.729330062866211 *\n",
      "Epoch: 1, Train_Loss: 5.7356462478637695, Test_Loss: 5.724461078643799 *\n",
      "Epoch: 1, Train_Loss: 5.725451946258545, Test_Loss: 5.719028949737549 *\n",
      "Epoch: 1, Train_Loss: 5.739320755004883, Test_Loss: 5.7144622802734375 *\n",
      "Epoch: 1, Train_Loss: 5.710570335388184, Test_Loss: 5.707976341247559 *\n",
      "Epoch: 1, Train_Loss: 5.705290794372559, Test_Loss: 5.7068023681640625 *\n",
      "Epoch: 1, Train_Loss: 5.743391513824463, Test_Loss: 5.70088529586792 *\n",
      "Epoch: 1, Train_Loss: 5.769248962402344, Test_Loss: 5.700712203979492 *\n",
      "Epoch: 1, Train_Loss: 5.717635631561279, Test_Loss: 5.690821170806885 *\n",
      "Epoch: 1, Train_Loss: 5.685189247131348, Test_Loss: 5.685429096221924 *\n",
      "Epoch: 1, Train_Loss: 5.679787635803223, Test_Loss: 5.719697952270508\n",
      "Epoch: 1, Train_Loss: 5.743388652801514, Test_Loss: 5.6819586753845215 *\n",
      "Epoch: 1, Train_Loss: 5.758264541625977, Test_Loss: 5.968536853790283\n",
      "Epoch: 1, Train_Loss: 5.744843482971191, Test_Loss: 6.175609588623047\n",
      "Epoch: 1, Train_Loss: 5.713734149932861, Test_Loss: 5.816774845123291 *\n",
      "Epoch: 1, Train_Loss: 5.67405891418457, Test_Loss: 5.680996894836426 *\n",
      "Epoch: 1, Train_Loss: 5.739972114562988, Test_Loss: 5.6587324142456055 *\n",
      "Epoch: 1, Train_Loss: 5.73287296295166, Test_Loss: 5.667758941650391\n",
      "Epoch: 1, Train_Loss: 5.64884090423584, Test_Loss: 5.803551197052002\n",
      "Epoch: 1, Train_Loss: 5.737584114074707, Test_Loss: 6.798698425292969\n",
      "Epoch: 1, Train_Loss: 5.631763458251953, Test_Loss: 7.12050724029541\n",
      "Model saved at location save_new\\model.ckpt at epoch 1\n",
      "Epoch: 1, Train_Loss: 5.629606246948242, Test_Loss: 5.711994171142578 *\n",
      "Epoch: 1, Train_Loss: 5.619396686553955, Test_Loss: 5.686411380767822 *\n",
      "Epoch: 1, Train_Loss: 5.613891124725342, Test_Loss: 5.606386184692383 *\n",
      "Epoch: 1, Train_Loss: 5.6082763671875, Test_Loss: 5.616514205932617\n",
      "Epoch: 1, Train_Loss: 5.603630542755127, Test_Loss: 5.6027655601501465 *\n",
      "Epoch: 1, Train_Loss: 5.753034591674805, Test_Loss: 5.621362209320068\n",
      "Epoch: 1, Train_Loss: 10.432127952575684, Test_Loss: 5.663424491882324\n",
      "Epoch: 1, Train_Loss: 5.6465253829956055, Test_Loss: 5.62696647644043 *\n",
      "Epoch: 1, Train_Loss: 5.596709728240967, Test_Loss: 5.586844444274902 *\n",
      "Epoch: 1, Train_Loss: 5.602993011474609, Test_Loss: 5.6526875495910645\n",
      "Epoch: 1, Train_Loss: 5.566679954528809, Test_Loss: 5.941567420959473\n",
      "Epoch: 1, Train_Loss: 5.559058666229248, Test_Loss: 5.643731117248535 *\n",
      "Epoch: 1, Train_Loss: 5.556384563446045, Test_Loss: 5.644052505493164\n",
      "Epoch: 1, Train_Loss: 5.550950050354004, Test_Loss: 5.545988082885742 *\n",
      "Epoch: 1, Train_Loss: 5.544628620147705, Test_Loss: 5.540696144104004 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 5.5383806228637695, Test_Loss: 5.53547477722168 *\n",
      "Epoch: 1, Train_Loss: 5.645219326019287, Test_Loss: 5.530805587768555 *\n",
      "Epoch: 1, Train_Loss: 5.649725437164307, Test_Loss: 5.573405742645264\n",
      "Epoch: 1, Train_Loss: 5.653114318847656, Test_Loss: 10.947040557861328\n",
      "Epoch: 1, Train_Loss: 5.601764678955078, Test_Loss: 5.754220485687256 *\n",
      "Epoch: 1, Train_Loss: 5.508520603179932, Test_Loss: 5.5194268226623535 *\n",
      "Epoch: 1, Train_Loss: 5.6166276931762695, Test_Loss: 5.5073065757751465 *\n",
      "Epoch: 1, Train_Loss: 5.675382137298584, Test_Loss: 5.50648832321167 *\n",
      "Epoch: 1, Train_Loss: 5.655325412750244, Test_Loss: 5.501992702484131 *\n",
      "Epoch: 1, Train_Loss: 5.62756872177124, Test_Loss: 5.49200439453125 *\n",
      "Epoch: 1, Train_Loss: 5.491737365722656, Test_Loss: 5.492900371551514\n",
      "Epoch: 1, Train_Loss: 5.481453895568848, Test_Loss: 5.48583984375 *\n",
      "Epoch: 1, Train_Loss: 5.476634502410889, Test_Loss: 5.480947017669678 *\n",
      "Epoch: 1, Train_Loss: 5.468019485473633, Test_Loss: 5.478050708770752 *\n",
      "Epoch: 1, Train_Loss: 5.464747428894043, Test_Loss: 5.475710391998291 *\n",
      "Epoch: 1, Train_Loss: 5.458287239074707, Test_Loss: 5.465559959411621 *\n",
      "Epoch: 1, Train_Loss: 5.456223964691162, Test_Loss: 5.472056865692139\n",
      "Epoch: 1, Train_Loss: 5.453053951263428, Test_Loss: 5.459824562072754 *\n",
      "Epoch: 1, Train_Loss: 5.444299221038818, Test_Loss: 5.450677394866943 *\n",
      "Epoch: 1, Train_Loss: 5.442091941833496, Test_Loss: 5.4406280517578125 *\n",
      "Epoch: 1, Train_Loss: 5.569304466247559, Test_Loss: 5.441102027893066\n",
      "Epoch: 1, Train_Loss: 5.571539878845215, Test_Loss: 5.432732582092285 *\n",
      "Epoch: 1, Train_Loss: 5.583410739898682, Test_Loss: 5.426366329193115 *\n",
      "Epoch: 1, Train_Loss: 5.476640701293945, Test_Loss: 5.42587423324585 *\n",
      "Epoch: 1, Train_Loss: 5.596958637237549, Test_Loss: 5.415650844573975 *\n",
      "Epoch: 1, Train_Loss: 5.546061992645264, Test_Loss: 5.41344690322876 *\n",
      "Epoch: 1, Train_Loss: 5.505965232849121, Test_Loss: 5.41141414642334 *\n",
      "Epoch: 1, Train_Loss: 5.553116321563721, Test_Loss: 5.404280185699463 *\n",
      "Epoch: 1, Train_Loss: 5.733478546142578, Test_Loss: 5.398390293121338 *\n",
      "Epoch: 1, Train_Loss: 5.425481796264648, Test_Loss: 5.394891262054443 *\n",
      "Epoch: 1, Train_Loss: 5.40725040435791, Test_Loss: 5.387866973876953 *\n",
      "Epoch: 1, Train_Loss: 7.8691205978393555, Test_Loss: 5.384586334228516 *\n",
      "Epoch: 1, Train_Loss: 6.2625603675842285, Test_Loss: 5.386314392089844\n",
      "Epoch: 1, Train_Loss: 5.389883041381836, Test_Loss: 5.42962121963501\n",
      "Epoch: 1, Train_Loss: 5.401906490325928, Test_Loss: 6.796327590942383\n",
      "Epoch: 1, Train_Loss: 5.4008402824401855, Test_Loss: 9.658403396606445\n",
      "Epoch: 1, Train_Loss: 5.378127098083496, Test_Loss: 5.370867729187012 *\n",
      "Epoch: 1, Train_Loss: 5.351919174194336, Test_Loss: 5.354827404022217 *\n",
      "Epoch: 1, Train_Loss: 5.393233299255371, Test_Loss: 5.368577480316162\n",
      "Epoch: 1, Train_Loss: 5.510896682739258, Test_Loss: 5.372732162475586\n",
      "Epoch: 1, Train_Loss: 5.46560525894165, Test_Loss: 5.372824668884277\n",
      "Epoch: 1, Train_Loss: 5.453451156616211, Test_Loss: 5.3629608154296875 *\n",
      "Epoch: 1, Train_Loss: 5.4591193199157715, Test_Loss: 5.4992780685424805\n",
      "Epoch: 1, Train_Loss: 5.389191627502441, Test_Loss: 5.342763900756836 *\n",
      "Epoch: 1, Train_Loss: 5.375256538391113, Test_Loss: 5.3295369148254395 *\n",
      "Epoch: 1, Train_Loss: 5.329658508300781, Test_Loss: 5.367940425872803\n",
      "Epoch: 1, Train_Loss: 5.3198652267456055, Test_Loss: 5.309843063354492 *\n",
      "Epoch: 1, Train_Loss: 5.307916641235352, Test_Loss: 5.325058460235596\n",
      "Epoch: 1, Train_Loss: 5.297235012054443, Test_Loss: 5.340022087097168\n",
      "Epoch: 1, Train_Loss: 5.305558204650879, Test_Loss: 5.370038986206055\n",
      "Epoch: 1, Train_Loss: 5.357578754425049, Test_Loss: 5.382946968078613\n",
      "Epoch: 1, Train_Loss: 5.355260372161865, Test_Loss: 5.45053768157959\n",
      "Epoch: 1, Train_Loss: 5.284807205200195, Test_Loss: 5.308422088623047 *\n",
      "Epoch: 1, Train_Loss: 5.270304203033447, Test_Loss: 5.28559684753418 *\n",
      "Epoch: 1, Train_Loss: 5.26461935043335, Test_Loss: 5.272494316101074 *\n",
      "Epoch: 1, Train_Loss: 5.259897708892822, Test_Loss: 5.265990734100342 *\n",
      "Epoch: 1, Train_Loss: 5.255866050720215, Test_Loss: 5.262152194976807 *\n",
      "Epoch: 1, Train_Loss: 5.250988483428955, Test_Loss: 5.257315635681152 *\n",
      "Epoch: 1, Train_Loss: 5.2436089515686035, Test_Loss: 5.2540669441223145 *\n",
      "Epoch: 1, Train_Loss: 5.239923000335693, Test_Loss: 5.249656677246094 *\n",
      "Epoch: 1, Train_Loss: 5.23455810546875, Test_Loss: 5.253184795379639\n",
      "Epoch: 1, Train_Loss: 5.228950500488281, Test_Loss: 5.240615367889404 *\n",
      "Epoch: 1, Train_Loss: 5.226436614990234, Test_Loss: 5.240378379821777 *\n",
      "Epoch: 1, Train_Loss: 5.223753929138184, Test_Loss: 5.224916458129883 *\n",
      "Epoch: 1, Train_Loss: 5.217840671539307, Test_Loss: 5.230047225952148\n",
      "Epoch: 1, Train_Loss: 5.212156772613525, Test_Loss: 5.280340194702148\n",
      "Epoch: 1, Train_Loss: 5.215977668762207, Test_Loss: 5.2138471603393555 *\n",
      "Epoch: 1, Train_Loss: 5.201225280761719, Test_Loss: 5.654615879058838\n",
      "Epoch: 1, Train_Loss: 5.197640419006348, Test_Loss: 5.754915714263916\n",
      "Epoch: 1, Train_Loss: 5.192049503326416, Test_Loss: 5.362310409545898 *\n",
      "Epoch: 1, Train_Loss: 5.18834924697876, Test_Loss: 5.218191623687744 *\n",
      "Epoch: 1, Train_Loss: 5.191571235656738, Test_Loss: 5.203617572784424 *\n",
      "Epoch: 1, Train_Loss: 5.177713394165039, Test_Loss: 5.2045769691467285\n",
      "Epoch: 1, Train_Loss: 5.171709060668945, Test_Loss: 5.3815131187438965\n",
      "Epoch: 1, Train_Loss: 5.168774604797363, Test_Loss: 6.5112738609313965\n",
      "Epoch: 1, Train_Loss: 5.211003303527832, Test_Loss: 6.2981181144714355 *\n",
      "Epoch: 1, Train_Loss: 5.232977867126465, Test_Loss: 5.20401668548584 *\n",
      "Epoch: 1, Train_Loss: 5.183737754821777, Test_Loss: 5.228389739990234\n",
      "Epoch: 1, Train_Loss: 5.1621575355529785, Test_Loss: 5.14925479888916 *\n",
      "Epoch: 1, Train_Loss: 5.143629550933838, Test_Loss: 5.148806095123291 *\n",
      "Epoch: 1, Train_Loss: 5.182018280029297, Test_Loss: 5.145557880401611 *\n",
      "Epoch: 1, Train_Loss: 5.143742084503174, Test_Loss: 5.165257930755615\n",
      "Epoch: 1, Train_Loss: 5.138124942779541, Test_Loss: 5.200556755065918\n",
      "Epoch: 1, Train_Loss: 5.166597843170166, Test_Loss: 5.1355767250061035 *\n",
      "Epoch: 1, Train_Loss: 5.137866020202637, Test_Loss: 5.135430812835693 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 1\n",
      "Epoch: 1, Train_Loss: 5.259247779846191, Test_Loss: 5.2239909172058105\n",
      "Epoch: 1, Train_Loss: 5.214495658874512, Test_Loss: 5.491090297698975\n",
      "Epoch: 1, Train_Loss: 5.163731098175049, Test_Loss: 5.312302112579346 *\n",
      "Epoch: 1, Train_Loss: 5.115833282470703, Test_Loss: 5.112685203552246 *\n",
      "Epoch: 1, Train_Loss: 5.104039192199707, Test_Loss: 5.103987693786621 *\n",
      "Epoch: 1, Train_Loss: 5.11086368560791, Test_Loss: 5.0994744300842285 *\n",
      "Epoch: 1, Train_Loss: 5.087629795074463, Test_Loss: 5.095272064208984 *\n",
      "Epoch: 1, Train_Loss: 5.088606357574463, Test_Loss: 5.089728355407715 *\n",
      "Epoch: 1, Train_Loss: 5.097339153289795, Test_Loss: 5.468110084533691\n",
      "Epoch: 1, Train_Loss: 5.095808982849121, Test_Loss: 10.206897735595703\n",
      "Epoch: 1, Train_Loss: 5.160884380340576, Test_Loss: 5.128730773925781 *\n",
      "Epoch: 1, Train_Loss: 5.077322483062744, Test_Loss: 5.068609237670898 *\n",
      "Epoch: 1, Train_Loss: 5.142675876617432, Test_Loss: 5.0590643882751465 *\n",
      "Epoch: 1, Train_Loss: 5.069019317626953, Test_Loss: 5.059760570526123\n",
      "Epoch: 1, Train_Loss: 5.072107791900635, Test_Loss: 5.0529608726501465 *\n",
      "Epoch: 1, Train_Loss: 5.144696235656738, Test_Loss: 5.046103000640869 *\n",
      "Epoch: 1, Train_Loss: 5.250924587249756, Test_Loss: 5.043423652648926 *\n",
      "Epoch: 1, Train_Loss: 5.041733741760254, Test_Loss: 5.036693096160889 *\n",
      "Epoch: 1, Train_Loss: 5.058549404144287, Test_Loss: 5.032197952270508 *\n",
      "Epoch: 1, Train_Loss: 5.024458408355713, Test_Loss: 5.027797698974609 *\n",
      "Epoch: 1, Train_Loss: 5.020929336547852, Test_Loss: 5.025786876678467 *\n",
      "Epoch: 1, Train_Loss: 5.016662120819092, Test_Loss: 5.034852027893066\n",
      "Epoch: 1, Train_Loss: 5.010031223297119, Test_Loss: 5.035921573638916\n",
      "Epoch: 1, Train_Loss: 5.031398773193359, Test_Loss: 5.0181097984313965 *\n",
      "Epoch: 1, Train_Loss: 5.030430793762207, Test_Loss: 5.00300407409668 *\n",
      "Epoch: 1, Train_Loss: 5.024764537811279, Test_Loss: 4.997236728668213 *\n",
      "Epoch: 1, Train_Loss: 5.020791053771973, Test_Loss: 4.994252681732178 *\n",
      "Epoch: 1, Train_Loss: 5.0159382820129395, Test_Loss: 4.9889326095581055 *\n",
      "Epoch: 1, Train_Loss: 4.993940353393555, Test_Loss: 4.983449459075928 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 4.982192039489746, Test_Loss: 4.979316234588623 *\n",
      "Epoch: 1, Train_Loss: 4.973635196685791, Test_Loss: 4.972950458526611 *\n",
      "Epoch: 1, Train_Loss: 4.98661470413208, Test_Loss: 4.970463752746582 *\n",
      "Epoch: 1, Train_Loss: 4.9854512214660645, Test_Loss: 4.965851783752441 *\n",
      "Epoch: 1, Train_Loss: 4.982999801635742, Test_Loss: 4.960963726043701 *\n",
      "Epoch: 1, Train_Loss: 4.95667028427124, Test_Loss: 4.956366539001465 *\n",
      "Epoch: 1, Train_Loss: 5.019588947296143, Test_Loss: 4.9518022537231445 *\n",
      "Epoch: 1, Train_Loss: 5.006982803344727, Test_Loss: 4.946010589599609 *\n",
      "Epoch: 1, Train_Loss: 4.982626438140869, Test_Loss: 4.942715644836426 *\n",
      "Epoch: 1, Train_Loss: 4.940338134765625, Test_Loss: 4.9466352462768555\n",
      "Epoch: 1, Train_Loss: 4.955842018127441, Test_Loss: 4.990040302276611\n",
      "Epoch: 1, Train_Loss: 4.929582118988037, Test_Loss: 7.487386226654053\n",
      "Epoch: 1, Train_Loss: 4.943796634674072, Test_Loss: 7.9872846603393555\n",
      "Epoch: 1, Train_Loss: 4.928334712982178, Test_Loss: 4.924557685852051 *\n",
      "Epoch: 1, Train_Loss: 4.941492080688477, Test_Loss: 4.915736198425293 *\n",
      "Epoch: 1, Train_Loss: 6.266351222991943, Test_Loss: 4.947424411773682\n",
      "Epoch: 1, Train_Loss: 8.841446876525879, Test_Loss: 4.947163105010986 *\n",
      "Epoch: 1, Train_Loss: 5.087328910827637, Test_Loss: 4.938652038574219 *\n",
      "Epoch: 1, Train_Loss: 4.915619850158691, Test_Loss: 4.95549201965332\n",
      "Epoch: 1, Train_Loss: 4.905969619750977, Test_Loss: 5.041927337646484\n",
      "Epoch: 1, Train_Loss: 5.094010353088379, Test_Loss: 4.888396263122559 *\n",
      "Epoch: 1, Train_Loss: 4.956536769866943, Test_Loss: 4.913797855377197\n",
      "Epoch: 1, Train_Loss: 4.894093990325928, Test_Loss: 4.904610633850098 *\n",
      "Epoch: 1, Train_Loss: 4.872746467590332, Test_Loss: 4.884037017822266 *\n",
      "Epoch: 1, Train_Loss: 4.953221321105957, Test_Loss: 4.880490303039551 *\n",
      "Epoch: 1, Train_Loss: 4.881620407104492, Test_Loss: 4.934622764587402\n",
      "Epoch: 1, Train_Loss: 4.8728556632995605, Test_Loss: 4.931301116943359 *\n",
      "Epoch: 1, Train_Loss: 5.283697128295898, Test_Loss: 4.961309432983398\n",
      "Epoch: 1, Train_Loss: 6.330596446990967, Test_Loss: 4.990217685699463\n",
      "Epoch: 1, Train_Loss: 5.897999286651611, Test_Loss: 4.857978820800781 *\n",
      "Epoch: 1, Train_Loss: 4.964314937591553, Test_Loss: 4.8791728019714355\n",
      "Epoch: 1, Train_Loss: 5.078464984893799, Test_Loss: 4.878900527954102 *\n",
      "Epoch: 1, Train_Loss: 7.314699649810791, Test_Loss: 4.863430023193359 *\n",
      "Epoch: 1, Train_Loss: 5.811662197113037, Test_Loss: 4.851747989654541 *\n",
      "Epoch: 1, Train_Loss: 4.884703159332275, Test_Loss: 4.844263553619385 *\n",
      "Epoch: 1, Train_Loss: 4.853708744049072, Test_Loss: 4.837606906890869 *\n",
      "Epoch: 1, Train_Loss: 5.708876132965088, Test_Loss: 4.831693172454834 *\n",
      "Epoch: 1, Train_Loss: 6.520396709442139, Test_Loss: 4.839354515075684\n",
      "Epoch: 1, Train_Loss: 5.509757041931152, Test_Loss: 4.829816818237305 *\n",
      "Epoch: 1, Train_Loss: 4.830289363861084, Test_Loss: 4.8204193115234375 *\n",
      "Epoch: 1, Train_Loss: 4.811557769775391, Test_Loss: 4.800019264221191 *\n",
      "Epoch: 1, Train_Loss: 5.190078258514404, Test_Loss: 4.824662685394287\n",
      "Epoch: 1, Train_Loss: 5.142000675201416, Test_Loss: 4.870532989501953\n",
      "Epoch: 1, Train_Loss: 4.797177791595459, Test_Loss: 4.816138744354248 *\n",
      "Epoch: 1, Train_Loss: 4.829478740692139, Test_Loss: 5.3793416023254395\n",
      "Epoch: 1, Train_Loss: 4.897884845733643, Test_Loss: 5.328128337860107 *\n",
      "Epoch: 1, Train_Loss: 4.962498188018799, Test_Loss: 4.929452419281006 *\n",
      "Epoch: 1, Train_Loss: 4.846376419067383, Test_Loss: 4.806857585906982 *\n",
      "Epoch: 1, Train_Loss: 5.1789960861206055, Test_Loss: 4.782898426055908 *\n",
      "Epoch: 1, Train_Loss: 4.888399600982666, Test_Loss: 4.793239593505859\n",
      "Epoch: 1, Train_Loss: 4.82314920425415, Test_Loss: 5.075137138366699\n",
      "Epoch: 1, Train_Loss: 5.011853218078613, Test_Loss: 6.179172515869141\n",
      "Epoch: 1, Train_Loss: 5.0998454093933105, Test_Loss: 5.525479793548584 *\n",
      "Epoch: 1, Train_Loss: 5.230890274047852, Test_Loss: 4.817502021789551 *\n",
      "Epoch: 1, Train_Loss: 4.935144424438477, Test_Loss: 4.77878475189209 *\n",
      "Epoch: 1, Train_Loss: 4.830225944519043, Test_Loss: 4.736034393310547 *\n",
      "Epoch: 1, Train_Loss: 4.868413925170898, Test_Loss: 4.729033946990967 *\n",
      "Epoch: 1, Train_Loss: 4.802433013916016, Test_Loss: 4.731659412384033\n",
      "Epoch: 1, Train_Loss: 4.742554664611816, Test_Loss: 4.749753952026367\n",
      "Epoch: 1, Train_Loss: 4.715189456939697, Test_Loss: 4.777192115783691\n",
      "Epoch: 1, Train_Loss: 4.711690902709961, Test_Loss: 4.71145486831665 *\n",
      "Epoch: 1, Train_Loss: 4.712489604949951, Test_Loss: 4.752604007720947\n",
      "Epoch: 1, Train_Loss: 4.705310344696045, Test_Loss: 4.827972412109375\n",
      "Epoch: 1, Train_Loss: 4.735403537750244, Test_Loss: 5.058017730712891\n",
      "Epoch: 1, Train_Loss: 4.785490989685059, Test_Loss: 4.916335582733154 *\n",
      "Epoch: 1, Train_Loss: 4.800114631652832, Test_Loss: 4.706725597381592 *\n",
      "Epoch: 1, Train_Loss: 4.7798285484313965, Test_Loss: 4.697746276855469 *\n",
      "Epoch: 1, Train_Loss: 5.105276107788086, Test_Loss: 4.6935648918151855 *\n",
      "Epoch: 1, Train_Loss: 4.826488494873047, Test_Loss: 4.689149379730225 *\n",
      "Epoch: 1, Train_Loss: 4.685274124145508, Test_Loss: 4.68691873550415 *\n",
      "Epoch: 1, Train_Loss: 4.795208930969238, Test_Loss: 5.935810089111328\n",
      "Model saved at location save_new\\model.ckpt at epoch 1\n",
      "Epoch: 1, Train_Loss: 5.101396083831787, Test_Loss: 8.915552139282227\n",
      "Epoch: 1, Train_Loss: 5.210862159729004, Test_Loss: 4.674905776977539 *\n",
      "Epoch: 1, Train_Loss: 4.658158302307129, Test_Loss: 4.659518718719482 *\n",
      "Epoch: 1, Train_Loss: 4.654918670654297, Test_Loss: 4.655472755432129 *\n",
      "Epoch: 1, Train_Loss: 5.242428779602051, Test_Loss: 4.655229091644287 *\n",
      "Epoch: 1, Train_Loss: 5.3681793212890625, Test_Loss: 4.647466659545898 *\n",
      "Epoch: 1, Train_Loss: 4.8100996017456055, Test_Loss: 4.64313268661499 *\n",
      "Epoch: 1, Train_Loss: 4.670095443725586, Test_Loss: 4.641485691070557 *\n",
      "Epoch: 1, Train_Loss: 4.643841743469238, Test_Loss: 4.635328769683838 *\n",
      "Epoch: 1, Train_Loss: 5.349355220794678, Test_Loss: 4.632641315460205 *\n",
      "Epoch: 1, Train_Loss: 6.263516902923584, Test_Loss: 4.625263214111328 *\n",
      "Epoch: 1, Train_Loss: 4.726991653442383, Test_Loss: 4.625888824462891\n",
      "Epoch: 1, Train_Loss: 4.640830039978027, Test_Loss: 4.642189979553223\n",
      "Epoch: 1, Train_Loss: 4.6173577308654785, Test_Loss: 4.636387825012207 *\n",
      "Epoch: 1, Train_Loss: 4.610933303833008, Test_Loss: 4.615626335144043 *\n",
      "Epoch: 1, Train_Loss: 5.057247161865234, Test_Loss: 4.6039605140686035 *\n",
      "Epoch: 1, Train_Loss: 4.635791301727295, Test_Loss: 4.600031852722168 *\n",
      "Epoch: 1, Train_Loss: 4.728404521942139, Test_Loss: 4.596208095550537 *\n",
      "Epoch: 1, Train_Loss: 4.859212875366211, Test_Loss: 4.592740535736084 *\n",
      "Epoch: 1, Train_Loss: 4.596884727478027, Test_Loss: 4.589161396026611 *\n",
      "Epoch: 1, Train_Loss: 4.680765151977539, Test_Loss: 4.583338260650635 *\n",
      "Epoch: 1, Train_Loss: 4.684360980987549, Test_Loss: 4.578746795654297 *\n",
      "Epoch: 1, Train_Loss: 4.926473140716553, Test_Loss: 4.577904224395752 *\n",
      "Epoch: 1, Train_Loss: 4.602506637573242, Test_Loss: 4.571979999542236 *\n",
      "Epoch: 1, Train_Loss: 4.7435431480407715, Test_Loss: 4.568593978881836 *\n",
      "Epoch: 1, Train_Loss: 4.89487361907959, Test_Loss: 4.564239501953125 *\n",
      "Epoch: 1, Train_Loss: 4.678888320922852, Test_Loss: 4.560076713562012 *\n",
      "Epoch: 1, Train_Loss: 4.605836391448975, Test_Loss: 4.555572986602783 *\n",
      "Epoch: 1, Train_Loss: 4.556581974029541, Test_Loss: 4.551923751831055 *\n",
      "Epoch: 1, Train_Loss: 4.5637359619140625, Test_Loss: 4.576981067657471\n",
      "Epoch: 1, Train_Loss: 4.5761613845825195, Test_Loss: 4.582862377166748\n",
      "Epoch: 1, Train_Loss: 5.044539928436279, Test_Loss: 8.048337936401367\n",
      "Epoch: 1, Train_Loss: 4.991453647613525, Test_Loss: 6.539350509643555 *\n",
      "Epoch: 1, Train_Loss: 5.042764663696289, Test_Loss: 4.532706260681152 *\n",
      "Epoch: 1, Train_Loss: 5.403395652770996, Test_Loss: 4.530852794647217 *\n",
      "Epoch: 1, Train_Loss: 4.687417030334473, Test_Loss: 4.571319580078125\n",
      "Epoch: 1, Train_Loss: 5.161637306213379, Test_Loss: 4.5751471519470215\n",
      "Epoch: 1, Train_Loss: 4.736495018005371, Test_Loss: 4.53906774520874 *\n",
      "Epoch: 1, Train_Loss: 4.517520904541016, Test_Loss: 4.590155124664307\n",
      "Epoch: 1, Train_Loss: 4.522697925567627, Test_Loss: 4.619041919708252\n",
      "Epoch: 1, Train_Loss: 4.582809925079346, Test_Loss: 4.503505229949951 *\n",
      "Epoch: 1, Train_Loss: 4.659566402435303, Test_Loss: 4.538496494293213\n",
      "Epoch: 1, Train_Loss: 5.491822719573975, Test_Loss: 4.512264728546143 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 4.8135151863098145, Test_Loss: 4.505256652832031 *\n",
      "Epoch: 1, Train_Loss: 6.60598087310791, Test_Loss: 4.49284553527832 *\n",
      "Epoch: 1, Train_Loss: 5.006101608276367, Test_Loss: 4.577648162841797\n",
      "Epoch: 1, Train_Loss: 5.428182125091553, Test_Loss: 4.530070781707764 *\n",
      "Epoch: 1, Train_Loss: 4.503808975219727, Test_Loss: 4.6012091636657715\n",
      "Epoch: 1, Train_Loss: 4.475434303283691, Test_Loss: 4.579659461975098 *\n",
      "Epoch: 1, Train_Loss: 4.859423637390137, Test_Loss: 4.48668098449707 *\n",
      "Epoch: 1, Train_Loss: 5.887051582336426, Test_Loss: 4.474666118621826 *\n",
      "Epoch: 1, Train_Loss: 4.769997596740723, Test_Loss: 4.4713287353515625 *\n",
      "Epoch: 1, Train_Loss: 4.584727764129639, Test_Loss: 4.4628143310546875 *\n",
      "Epoch: 1, Train_Loss: 4.456618785858154, Test_Loss: 4.461325645446777 *\n",
      "Epoch: 1, Train_Loss: 4.566864490509033, Test_Loss: 4.4587297439575195 *\n",
      "Epoch: 1, Train_Loss: 4.789705276489258, Test_Loss: 4.453181266784668 *\n",
      "Epoch: 1, Train_Loss: 4.767153739929199, Test_Loss: 4.445653438568115 *\n",
      "Epoch: 1, Train_Loss: 5.775828838348389, Test_Loss: 4.451385974884033\n",
      "Epoch: 1, Train_Loss: 4.967957973480225, Test_Loss: 4.4461798667907715 *\n",
      "Epoch: 1, Train_Loss: 4.434090614318848, Test_Loss: 4.438026428222656 *\n",
      "Epoch: 1, Train_Loss: 4.440896987915039, Test_Loss: 4.431856155395508 *\n",
      "Epoch: 1, Train_Loss: 4.447666168212891, Test_Loss: 4.454946994781494\n",
      "Epoch: 1, Train_Loss: 4.451683044433594, Test_Loss: 4.465607643127441\n",
      "Epoch: 1, Train_Loss: 4.429703235626221, Test_Loss: 4.517534255981445\n",
      "Epoch: 1, Train_Loss: 4.430176258087158, Test_Loss: 4.952932834625244\n",
      "Epoch: 1, Train_Loss: 7.142157554626465, Test_Loss: 4.812033176422119 *\n",
      "Epoch: 1, Train_Loss: 19.225831985473633, Test_Loss: 4.515660285949707 *\n",
      "Epoch: 1, Train_Loss: 4.781048774719238, Test_Loss: 4.427515506744385 *\n",
      "Epoch: 1, Train_Loss: 7.637914657592773, Test_Loss: 4.408952236175537 *\n",
      "Epoch: 1, Train_Loss: 5.008890628814697, Test_Loss: 4.455860614776611\n",
      "Epoch: 1, Train_Loss: 4.4375319480896, Test_Loss: 4.913290500640869\n",
      "Epoch: 1, Train_Loss: 4.4395670890808105, Test_Loss: 5.837002754211426\n",
      "Epoch: 1, Train_Loss: 14.240327835083008, Test_Loss: 4.942110061645508 *\n",
      "Epoch: 1, Train_Loss: 7.191642761230469, Test_Loss: 4.469824314117432 *\n",
      "Epoch: 1, Train_Loss: 4.4003376960754395, Test_Loss: 4.3818206787109375 *\n",
      "Epoch: 1, Train_Loss: 6.28806209564209, Test_Loss: 4.377900123596191 *\n",
      "Epoch: 1, Train_Loss: 8.911317825317383, Test_Loss: 4.371924877166748 *\n",
      "Epoch: 1, Train_Loss: 4.381863117218018, Test_Loss: 4.3708086013793945 *\n",
      "Epoch: 1, Train_Loss: 4.366847515106201, Test_Loss: 4.410143852233887\n",
      "Epoch: 1, Train_Loss: 4.358215808868408, Test_Loss: 4.434023380279541\n",
      "Epoch: 1, Train_Loss: 4.350619792938232, Test_Loss: 4.358274459838867 *\n",
      "Epoch: 1, Train_Loss: 4.351853847503662, Test_Loss: 4.409914970397949\n",
      "Epoch: 1, Train_Loss: 4.348206043243408, Test_Loss: 4.536605358123779\n",
      "Epoch: 1, Train_Loss: 4.3461737632751465, Test_Loss: 4.561389923095703\n",
      "Epoch: 1, Train_Loss: 4.341765880584717, Test_Loss: 4.5057172775268555 *\n",
      "Epoch: 1, Train_Loss: 4.342452049255371, Test_Loss: 4.332413673400879 *\n",
      "Epoch: 1, Train_Loss: 4.335390090942383, Test_Loss: 4.327531814575195 *\n",
      "Epoch: 1, Train_Loss: 4.333272457122803, Test_Loss: 4.323897838592529 *\n",
      "Epoch: 1, Train_Loss: 4.327646255493164, Test_Loss: 4.320152759552002 *\n",
      "Epoch: 1, Train_Loss: 4.3734917640686035, Test_Loss: 4.322721004486084\n",
      "Epoch: 1, Train_Loss: 4.364043712615967, Test_Loss: 6.980958938598633\n",
      "Epoch: 1, Train_Loss: 4.322530269622803, Test_Loss: 7.471418380737305\n",
      "Epoch: 1, Train_Loss: 4.314233779907227, Test_Loss: 4.312995910644531 *\n",
      "Epoch: 1, Train_Loss: 4.3057169914245605, Test_Loss: 4.305323600769043 *\n",
      "Epoch: 1, Train_Loss: 4.297457218170166, Test_Loss: 4.2986273765563965 *\n",
      "Epoch: 1, Train_Loss: 4.2950239181518555, Test_Loss: 4.2953410148620605 *\n",
      "Epoch: 1, Train_Loss: 4.2923407554626465, Test_Loss: 4.294830322265625 *\n",
      "Epoch: 1, Train_Loss: 4.290866374969482, Test_Loss: 4.296291351318359\n",
      "Epoch: 1, Train_Loss: 4.28787088394165, Test_Loss: 4.293919563293457 *\n",
      "Epoch: 1, Train_Loss: 4.279972076416016, Test_Loss: 4.293374538421631 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 1\n",
      "Epoch: 1, Train_Loss: 4.277095794677734, Test_Loss: 4.2899580001831055 *\n",
      "Epoch: 1, Train_Loss: 4.274196147918701, Test_Loss: 4.281547546386719 *\n",
      "Epoch: 1, Train_Loss: 4.269073486328125, Test_Loss: 4.2809014320373535 *\n",
      "Epoch: 1, Train_Loss: 4.284433364868164, Test_Loss: 4.28168249130249\n",
      "Epoch: 1, Train_Loss: 4.278730392456055, Test_Loss: 4.275264739990234 *\n",
      "Epoch: 1, Train_Loss: 4.2692060470581055, Test_Loss: 4.268248081207275 *\n",
      "Epoch: 1, Train_Loss: 4.257674694061279, Test_Loss: 4.259188175201416 *\n",
      "Epoch: 1, Train_Loss: 11.954983711242676, Test_Loss: 4.259787559509277\n",
      "Epoch: 1, Train_Loss: 5.8951096534729, Test_Loss: 4.254307746887207 *\n",
      "Epoch: 1, Train_Loss: 4.259984016418457, Test_Loss: 4.252870559692383 *\n",
      "Epoch: 1, Train_Loss: 4.2458319664001465, Test_Loss: 4.253218650817871\n",
      "Epoch: 1, Train_Loss: 4.245296955108643, Test_Loss: 4.24119234085083 *\n",
      "Epoch: 1, Train_Loss: 4.2621893882751465, Test_Loss: 4.241175174713135 *\n",
      "Epoch: 1, Train_Loss: 4.266631603240967, Test_Loss: 4.245849132537842\n",
      "Epoch: 1, Train_Loss: 4.23504638671875, Test_Loss: 4.2367143630981445 *\n",
      "Epoch: 1, Train_Loss: 4.324032306671143, Test_Loss: 4.236610412597656 *\n",
      "Epoch: 1, Train_Loss: 4.458016872406006, Test_Loss: 4.229762077331543 *\n",
      "Epoch: 1, Train_Loss: 4.425934791564941, Test_Loss: 4.227425575256348 *\n",
      "Epoch: 1, Train_Loss: 4.253737926483154, Test_Loss: 4.224730491638184 *\n",
      "Epoch: 1, Train_Loss: 4.294803142547607, Test_Loss: 4.22053337097168 *\n",
      "Epoch: 1, Train_Loss: 4.325327396392822, Test_Loss: 4.268664360046387\n",
      "Epoch: 1, Train_Loss: 4.337392807006836, Test_Loss: 4.228078365325928 *\n",
      "Epoch: 1, Train_Loss: 4.358913898468018, Test_Loss: 9.217910766601562\n",
      "Epoch: 1, Train_Loss: 4.315764427185059, Test_Loss: 5.037082195281982 *\n",
      "Epoch: 1, Train_Loss: 4.2662672996521, Test_Loss: 4.199730396270752 *\n",
      "Epoch: 1, Train_Loss: 4.196018218994141, Test_Loss: 4.20186185836792\n",
      "Epoch: 1, Train_Loss: 4.259716510772705, Test_Loss: 4.215697288513184\n",
      "Epoch: 1, Train_Loss: 4.206173419952393, Test_Loss: 4.222856044769287\n",
      "Epoch: 1, Train_Loss: 4.188131332397461, Test_Loss: 4.183060169219971 *\n",
      "Epoch: 1, Train_Loss: 4.177433490753174, Test_Loss: 4.30360746383667\n",
      "Epoch: 1, Train_Loss: 4.174009323120117, Test_Loss: 4.2823405265808105 *\n",
      "Epoch: 1, Train_Loss: 4.2056660652160645, Test_Loss: 4.1691999435424805 *\n",
      "Epoch: 1, Train_Loss: 9.714555740356445, Test_Loss: 4.237460613250732\n",
      "Epoch: 1, Train_Loss: 4.319364547729492, Test_Loss: 4.1645002365112305 *\n",
      "Epoch: 1, Train_Loss: 4.174929618835449, Test_Loss: 4.185128688812256\n",
      "Epoch: 1, Train_Loss: 4.173928260803223, Test_Loss: 4.159182071685791 *\n",
      "Epoch: 1, Train_Loss: 4.175204277038574, Test_Loss: 4.244015693664551\n",
      "Epoch: 1, Train_Loss: 4.1689252853393555, Test_Loss: 4.181794166564941 *\n",
      "Epoch: 1, Train_Loss: 4.152462005615234, Test_Loss: 4.333128452301025\n",
      "Epoch: 1, Train_Loss: 4.151288032531738, Test_Loss: 4.25447416305542 *\n",
      "Epoch: 1, Train_Loss: 4.158390998840332, Test_Loss: 4.146658897399902 *\n",
      "Epoch: 1, Train_Loss: 4.156317234039307, Test_Loss: 4.133371829986572 *\n",
      "Epoch: 1, Train_Loss: 4.139464378356934, Test_Loss: 4.130377769470215 *\n",
      "Epoch: 1, Train_Loss: 4.1362409591674805, Test_Loss: 4.126144886016846 *\n",
      "Epoch: 1, Train_Loss: 4.132602214813232, Test_Loss: 4.12253475189209 *\n",
      "Epoch: 1, Train_Loss: 4.148649215698242, Test_Loss: 4.119454860687256 *\n",
      "Epoch: 1, Train_Loss: 4.118261814117432, Test_Loss: 4.1158647537231445 *\n",
      "Epoch: 1, Train_Loss: 4.1177754402160645, Test_Loss: 4.111574172973633 *\n",
      "Epoch: 1, Train_Loss: 4.137688636779785, Test_Loss: 4.116580009460449\n",
      "Epoch: 1, Train_Loss: 4.180545330047607, Test_Loss: 4.111922264099121 *\n",
      "Epoch: 1, Train_Loss: 4.151726722717285, Test_Loss: 4.105900764465332 *\n",
      "Epoch: 1, Train_Loss: 4.102454662322998, Test_Loss: 4.106786727905273\n",
      "Epoch: 1, Train_Loss: 4.101893424987793, Test_Loss: 4.131364345550537\n",
      "Epoch: 1, Train_Loss: 4.142345428466797, Test_Loss: 4.113216876983643 *\n",
      "Epoch: 2, Train_Loss: 4.18614387512207, Test_Loss: 4.283931732177734 *\n",
      "Epoch: 2, Train_Loss: 4.164559841156006, Test_Loss: 4.577036380767822\n",
      "Epoch: 2, Train_Loss: 4.153388977050781, Test_Loss: 4.332691192626953 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 4.101891040802002, Test_Loss: 4.1461334228515625 *\n",
      "Epoch: 2, Train_Loss: 4.1714186668396, Test_Loss: 4.093815326690674 *\n",
      "Epoch: 2, Train_Loss: 4.154326915740967, Test_Loss: 4.0876784324646 *\n",
      "Epoch: 2, Train_Loss: 4.105301380157471, Test_Loss: 4.1978044509887695\n",
      "Epoch: 2, Train_Loss: 4.154993534088135, Test_Loss: 4.887609958648682\n",
      "Epoch: 2, Train_Loss: 4.092782497406006, Test_Loss: 5.668546199798584\n",
      "Epoch: 2, Train_Loss: 4.07569694519043, Test_Loss: 4.424839496612549 *\n",
      "Epoch: 2, Train_Loss: 4.062168598175049, Test_Loss: 4.132983684539795 *\n",
      "Epoch: 2, Train_Loss: 4.0580668449401855, Test_Loss: 4.053072929382324 *\n",
      "Epoch: 2, Train_Loss: 4.054207801818848, Test_Loss: 4.062377452850342\n",
      "Epoch: 2, Train_Loss: 4.051242828369141, Test_Loss: 4.054753303527832 *\n",
      "Epoch: 2, Train_Loss: 4.045616626739502, Test_Loss: 4.057008266448975\n",
      "Epoch: 2, Train_Loss: 8.571471214294434, Test_Loss: 4.105648994445801\n",
      "Epoch: 2, Train_Loss: 4.574383735656738, Test_Loss: 4.106281757354736\n",
      "Epoch: 2, Train_Loss: 4.040825843811035, Test_Loss: 4.045388221740723 *\n",
      "Epoch: 2, Train_Loss: 4.068007469177246, Test_Loss: 4.1071577072143555\n",
      "Epoch: 2, Train_Loss: 4.034780025482178, Test_Loss: 4.3665947914123535\n",
      "Epoch: 2, Train_Loss: 4.021823406219482, Test_Loss: 4.0939412117004395 *\n",
      "Epoch: 2, Train_Loss: 4.021745681762695, Test_Loss: 4.1658148765563965\n",
      "Epoch: 2, Train_Loss: 4.017439842224121, Test_Loss: 4.013108253479004 *\n",
      "Epoch: 2, Train_Loss: 4.0130157470703125, Test_Loss: 4.009793758392334 *\n",
      "Epoch: 2, Train_Loss: 4.0088958740234375, Test_Loss: 4.006453037261963 *\n",
      "Epoch: 2, Train_Loss: 4.081101894378662, Test_Loss: 4.003273010253906 *\n",
      "Epoch: 2, Train_Loss: 4.133147239685059, Test_Loss: 4.005496978759766\n",
      "Epoch: 2, Train_Loss: 4.13810920715332, Test_Loss: 8.10773754119873\n",
      "Epoch: 2, Train_Loss: 4.129321575164795, Test_Loss: 5.634124755859375 *\n",
      "Epoch: 2, Train_Loss: 3.994837760925293, Test_Loss: 3.993870973587036 *\n",
      "Epoch: 2, Train_Loss: 4.027069568634033, Test_Loss: 3.9868195056915283 *\n",
      "Epoch: 2, Train_Loss: 4.145966529846191, Test_Loss: 3.981876850128174 *\n",
      "Epoch: 2, Train_Loss: 4.134230613708496, Test_Loss: 3.9815456867218018 *\n",
      "Epoch: 2, Train_Loss: 4.144877910614014, Test_Loss: 3.9781837463378906 *\n",
      "Epoch: 2, Train_Loss: 3.9862020015716553, Test_Loss: 3.981079339981079\n",
      "Epoch: 2, Train_Loss: 3.970491409301758, Test_Loss: 3.9740700721740723 *\n",
      "Epoch: 2, Train_Loss: 3.97064208984375, Test_Loss: 3.974463939666748\n",
      "Epoch: 2, Train_Loss: 3.9622273445129395, Test_Loss: 3.972175359725952 *\n",
      "Epoch: 2, Train_Loss: 3.9604976177215576, Test_Loss: 3.968074083328247 *\n",
      "Epoch: 2, Train_Loss: 3.9565017223358154, Test_Loss: 3.963742971420288 *\n",
      "Epoch: 2, Train_Loss: 3.9542884826660156, Test_Loss: 3.973041534423828\n",
      "Epoch: 2, Train_Loss: 3.954380989074707, Test_Loss: 3.9631359577178955 *\n",
      "Epoch: 2, Train_Loss: 3.9480295181274414, Test_Loss: 3.95346736907959 *\n",
      "Epoch: 2, Train_Loss: 3.944347858428955, Test_Loss: 3.944988250732422 *\n",
      "Epoch: 2, Train_Loss: 4.049561977386475, Test_Loss: 3.947103977203369\n",
      "Epoch: 2, Train_Loss: 4.086633682250977, Test_Loss: 3.9408652782440186 *\n",
      "Epoch: 2, Train_Loss: 4.095531463623047, Test_Loss: 3.9369728565216064 *\n",
      "Epoch: 2, Train_Loss: 3.983515739440918, Test_Loss: 3.938777208328247\n",
      "Epoch: 2, Train_Loss: 4.108396530151367, Test_Loss: 3.9283268451690674 *\n",
      "Epoch: 2, Train_Loss: 4.0993170738220215, Test_Loss: 3.9272515773773193 *\n",
      "Epoch: 2, Train_Loss: 3.9540836811065674, Test_Loss: 3.9304909706115723\n",
      "Epoch: 2, Train_Loss: 4.108867645263672, Test_Loss: 3.922879934310913 *\n",
      "Epoch: 2, Train_Loss: 4.081863880157471, Test_Loss: 3.9209132194519043 *\n",
      "Epoch: 2, Train_Loss: 4.154764175415039, Test_Loss: 3.915282964706421 *\n",
      "Epoch: 2, Train_Loss: 3.9338080883026123, Test_Loss: 3.913384437561035 *\n",
      "Epoch: 2, Train_Loss: 5.40473747253418, Test_Loss: 3.9102671146392822 *\n",
      "Epoch: 2, Train_Loss: 5.7813191413879395, Test_Loss: 3.9095304012298584 *\n",
      "Epoch: 2, Train_Loss: 3.9320297241210938, Test_Loss: 3.9545271396636963\n",
      "Epoch: 2, Train_Loss: 3.9300241470336914, Test_Loss: 3.9539952278137207 *\n",
      "Epoch: 2, Train_Loss: 3.930262565612793, Test_Loss: 9.511509895324707\n",
      "Epoch: 2, Train_Loss: 3.9202728271484375, Test_Loss: 3.97558331489563 *\n",
      "Epoch: 2, Train_Loss: 3.8871042728424072, Test_Loss: 3.8890480995178223 *\n",
      "Epoch: 2, Train_Loss: 3.9033846855163574, Test_Loss: 3.8985533714294434\n",
      "Epoch: 2, Train_Loss: 4.045598030090332, Test_Loss: 3.9188432693481445\n",
      "Epoch: 2, Train_Loss: 4.000735759735107, Test_Loss: 3.918771266937256 *\n",
      "Epoch: 2, Train_Loss: 3.9978749752044678, Test_Loss: 3.8752281665802 *\n",
      "Epoch: 2, Train_Loss: 3.987935781478882, Test_Loss: 4.015623569488525\n",
      "Epoch: 2, Train_Loss: 3.9545931816101074, Test_Loss: 3.9383726119995117 *\n",
      "Epoch: 2, Train_Loss: 3.9028353691101074, Test_Loss: 3.861649513244629 *\n",
      "Epoch: 2, Train_Loss: 3.885565996170044, Test_Loss: 3.92091703414917\n",
      "Epoch: 2, Train_Loss: 3.8691844940185547, Test_Loss: 3.8608076572418213 *\n",
      "Epoch: 2, Train_Loss: 3.8689568042755127, Test_Loss: 3.8729450702667236\n",
      "Epoch: 2, Train_Loss: 3.853630542755127, Test_Loss: 3.8701484203338623 *\n",
      "Epoch: 2, Train_Loss: 3.8455774784088135, Test_Loss: 3.955881357192993\n",
      "Epoch: 2, Train_Loss: 3.8952887058258057, Test_Loss: 3.8888566493988037 *\n",
      "Epoch: 2, Train_Loss: 3.903296709060669, Test_Loss: 3.994389057159424\n",
      "Epoch: 2, Train_Loss: 3.8596527576446533, Test_Loss: 3.9010517597198486 *\n",
      "Epoch: 2, Train_Loss: 3.831380844116211, Test_Loss: 3.859696388244629 *\n",
      "Epoch: 2, Train_Loss: 3.8283419609069824, Test_Loss: 3.8413658142089844 *\n",
      "Epoch: 2, Train_Loss: 3.8248186111450195, Test_Loss: 3.834132194519043 *\n",
      "Epoch: 2, Train_Loss: 3.82177472114563, Test_Loss: 3.831815481185913 *\n",
      "Epoch: 2, Train_Loss: 3.81890606880188, Test_Loss: 3.8309357166290283 *\n",
      "Epoch: 2, Train_Loss: 3.816045045852661, Test_Loss: 3.8288469314575195 *\n",
      "Epoch: 2, Train_Loss: 3.8127553462982178, Test_Loss: 3.8252434730529785 *\n",
      "Epoch: 2, Train_Loss: 3.809640884399414, Test_Loss: 3.8210434913635254 *\n",
      "Epoch: 2, Train_Loss: 3.805751323699951, Test_Loss: 3.8269383907318115\n",
      "Epoch: 2, Train_Loss: 3.80281662940979, Test_Loss: 3.8304028511047363\n",
      "Epoch: 2, Train_Loss: 3.8062446117401123, Test_Loss: 3.806164026260376 *\n",
      "Epoch: 2, Train_Loss: 3.804168939590454, Test_Loss: 3.8103859424591064\n",
      "Epoch: 2, Train_Loss: 3.7988948822021484, Test_Loss: 3.859149932861328\n",
      "Epoch: 2, Train_Loss: 3.8056435585021973, Test_Loss: 3.811645269393921 *\n",
      "Epoch: 2, Train_Loss: 3.788469076156616, Test_Loss: 4.081956386566162\n",
      "Epoch: 2, Train_Loss: 3.7856123447418213, Test_Loss: 4.361055374145508\n",
      "Epoch: 2, Train_Loss: 3.7819759845733643, Test_Loss: 4.0051655769348145 *\n",
      "Epoch: 2, Train_Loss: 3.777827501296997, Test_Loss: 3.846349000930786 *\n",
      "Epoch: 2, Train_Loss: 3.7858641147613525, Test_Loss: 3.8060214519500732 *\n",
      "Epoch: 2, Train_Loss: 3.7746706008911133, Test_Loss: 3.788098096847534 *\n",
      "Epoch: 2, Train_Loss: 3.7690305709838867, Test_Loss: 3.9004104137420654\n",
      "Epoch: 2, Train_Loss: 3.7676069736480713, Test_Loss: 4.72672176361084\n",
      "Epoch: 2, Train_Loss: 3.7831737995147705, Test_Loss: 5.262570381164551\n",
      "Model saved at location save_new\\model.ckpt at epoch 2\n",
      "Epoch: 2, Train_Loss: 3.859435558319092, Test_Loss: 3.9051594734191895 *\n",
      "Epoch: 2, Train_Loss: 3.7802746295928955, Test_Loss: 3.8517866134643555 *\n",
      "Epoch: 2, Train_Loss: 3.778045177459717, Test_Loss: 3.75384259223938 *\n",
      "Epoch: 2, Train_Loss: 3.750596523284912, Test_Loss: 3.7619616985321045\n",
      "Epoch: 2, Train_Loss: 3.77994441986084, Test_Loss: 3.7544164657592773 *\n",
      "Epoch: 2, Train_Loss: 3.7595646381378174, Test_Loss: 3.764270782470703\n",
      "Epoch: 2, Train_Loss: 3.7449584007263184, Test_Loss: 3.798959732055664\n",
      "Epoch: 2, Train_Loss: 3.774768114089966, Test_Loss: 3.789295196533203 *\n",
      "Epoch: 2, Train_Loss: 3.769214153289795, Test_Loss: 3.7452926635742188 *\n",
      "Epoch: 2, Train_Loss: 3.8524227142333984, Test_Loss: 3.8296396732330322\n",
      "Epoch: 2, Train_Loss: 3.840890645980835, Test_Loss: 4.11753511428833\n",
      "Epoch: 2, Train_Loss: 3.807701587677002, Test_Loss: 3.7968270778656006 *\n",
      "Epoch: 2, Train_Loss: 3.7579424381256104, Test_Loss: 3.8575730323791504\n",
      "Epoch: 2, Train_Loss: 3.7213501930236816, Test_Loss: 3.7219855785369873 *\n",
      "Epoch: 2, Train_Loss: 3.748119592666626, Test_Loss: 3.719043016433716 *\n",
      "Epoch: 2, Train_Loss: 3.7138803005218506, Test_Loss: 3.7161865234375 *\n",
      "Epoch: 2, Train_Loss: 3.7198808193206787, Test_Loss: 3.7134642601013184 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 3.7188665866851807, Test_Loss: 3.721297264099121\n",
      "Epoch: 2, Train_Loss: 3.716754674911499, Test_Loss: 8.864075660705566\n",
      "Epoch: 2, Train_Loss: 3.789020299911499, Test_Loss: 4.187892913818359 *\n",
      "Epoch: 2, Train_Loss: 3.704362630844116, Test_Loss: 3.704685688018799 *\n",
      "Epoch: 2, Train_Loss: 3.7940027713775635, Test_Loss: 3.6948204040527344 *\n",
      "Epoch: 2, Train_Loss: 3.700807571411133, Test_Loss: 3.693056583404541 *\n",
      "Epoch: 2, Train_Loss: 3.7166030406951904, Test_Loss: 3.6938438415527344\n",
      "Epoch: 2, Train_Loss: 3.7188117504119873, Test_Loss: 3.6869752407073975 *\n",
      "Epoch: 2, Train_Loss: 3.958653211593628, Test_Loss: 3.68892765045166\n",
      "Epoch: 2, Train_Loss: 3.6900668144226074, Test_Loss: 3.6831657886505127 *\n",
      "Epoch: 2, Train_Loss: 3.7022712230682373, Test_Loss: 3.680928945541382 *\n",
      "Epoch: 2, Train_Loss: 3.6741998195648193, Test_Loss: 3.6804230213165283 *\n",
      "Epoch: 2, Train_Loss: 3.6722259521484375, Test_Loss: 3.6801908016204834 *\n",
      "Epoch: 2, Train_Loss: 3.6703197956085205, Test_Loss: 3.67456316947937 *\n",
      "Epoch: 2, Train_Loss: 3.66485333442688, Test_Loss: 3.689699411392212\n",
      "Epoch: 2, Train_Loss: 3.6789464950561523, Test_Loss: 3.675151824951172 *\n",
      "Epoch: 2, Train_Loss: 3.686615467071533, Test_Loss: 3.6620497703552246 *\n",
      "Epoch: 2, Train_Loss: 3.688624858856201, Test_Loss: 3.6562397480010986 *\n",
      "Epoch: 2, Train_Loss: 3.678997039794922, Test_Loss: 3.6572585105895996\n",
      "Epoch: 2, Train_Loss: 3.6765809059143066, Test_Loss: 3.6517887115478516 *\n",
      "Epoch: 2, Train_Loss: 3.6697802543640137, Test_Loss: 3.6470272541046143 *\n",
      "Epoch: 2, Train_Loss: 3.6485366821289062, Test_Loss: 3.648505449295044\n",
      "Epoch: 2, Train_Loss: 3.640688180923462, Test_Loss: 3.6407198905944824 *\n",
      "Epoch: 2, Train_Loss: 3.6506195068359375, Test_Loss: 3.6393861770629883 *\n",
      "Epoch: 2, Train_Loss: 3.6503655910491943, Test_Loss: 3.639397621154785\n",
      "Epoch: 2, Train_Loss: 3.6581366062164307, Test_Loss: 3.634831666946411 *\n",
      "Epoch: 2, Train_Loss: 3.627687931060791, Test_Loss: 3.6303505897521973 *\n",
      "Epoch: 2, Train_Loss: 3.6805591583251953, Test_Loss: 3.6281590461730957 *\n",
      "Epoch: 2, Train_Loss: 3.690793752670288, Test_Loss: 3.6249654293060303 *\n",
      "Epoch: 2, Train_Loss: 3.677297592163086, Test_Loss: 3.6228344440460205 *\n",
      "Epoch: 2, Train_Loss: 3.6158719062805176, Test_Loss: 3.6240930557250977\n",
      "Epoch: 2, Train_Loss: 3.640087604522705, Test_Loss: 3.6743133068084717\n",
      "Epoch: 2, Train_Loss: 3.611875057220459, Test_Loss: 4.3224897384643555\n",
      "Epoch: 2, Train_Loss: 3.6318094730377197, Test_Loss: 8.529531478881836\n",
      "Epoch: 2, Train_Loss: 3.6060876846313477, Test_Loss: 3.6177897453308105 *\n",
      "Epoch: 2, Train_Loss: 3.6229336261749268, Test_Loss: 3.6028761863708496 *\n",
      "Epoch: 2, Train_Loss: 3.7240445613861084, Test_Loss: 3.6277918815612793\n",
      "Epoch: 2, Train_Loss: 7.052667617797852, Test_Loss: 3.640195846557617\n",
      "Epoch: 2, Train_Loss: 5.514545440673828, Test_Loss: 3.6402058601379395\n",
      "Epoch: 2, Train_Loss: 3.61189341545105, Test_Loss: 3.6008851528167725 *\n",
      "Epoch: 2, Train_Loss: 3.5890607833862305, Test_Loss: 3.748138189315796\n",
      "Epoch: 2, Train_Loss: 3.744511365890503, Test_Loss: 3.6198718547821045 *\n",
      "Epoch: 2, Train_Loss: 3.7040553092956543, Test_Loss: 3.5831544399261475 *\n",
      "Epoch: 2, Train_Loss: 3.602870464324951, Test_Loss: 3.6316680908203125\n",
      "Epoch: 2, Train_Loss: 3.5764029026031494, Test_Loss: 3.581481695175171 *\n",
      "Epoch: 2, Train_Loss: 3.6440553665161133, Test_Loss: 3.589672803878784\n",
      "Epoch: 2, Train_Loss: 3.6042776107788086, Test_Loss: 3.6137399673461914\n",
      "Epoch: 2, Train_Loss: 3.577211856842041, Test_Loss: 3.6645050048828125\n",
      "Epoch: 2, Train_Loss: 3.738377809524536, Test_Loss: 3.637394428253174 *\n",
      "Epoch: 2, Train_Loss: 4.963197708129883, Test_Loss: 3.7017035484313965\n",
      "Epoch: 2, Train_Loss: 4.969086170196533, Test_Loss: 3.5940749645233154 *\n",
      "Epoch: 2, Train_Loss: 3.6995863914489746, Test_Loss: 3.589118480682373 *\n",
      "Epoch: 2, Train_Loss: 3.6146817207336426, Test_Loss: 3.5712242126464844 *\n",
      "Epoch: 2, Train_Loss: 5.5656046867370605, Test_Loss: 3.562610149383545 *\n",
      "Epoch: 2, Train_Loss: 4.984900951385498, Test_Loss: 3.559164524078369 *\n",
      "Epoch: 2, Train_Loss: 3.586113452911377, Test_Loss: 3.5560598373413086 *\n",
      "Epoch: 2, Train_Loss: 3.571448564529419, Test_Loss: 3.552212715148926 *\n",
      "Epoch: 2, Train_Loss: 3.965808391571045, Test_Loss: 3.5491626262664795 *\n",
      "Epoch: 2, Train_Loss: 5.364904403686523, Test_Loss: 3.5523483753204346\n",
      "Epoch: 2, Train_Loss: 4.658383369445801, Test_Loss: 3.5481464862823486 *\n",
      "Epoch: 2, Train_Loss: 3.550835371017456, Test_Loss: 3.550319194793701\n",
      "Epoch: 2, Train_Loss: 3.554180145263672, Test_Loss: 3.5279197692871094 *\n",
      "Epoch: 2, Train_Loss: 3.6916680335998535, Test_Loss: 3.5411717891693115\n",
      "Epoch: 2, Train_Loss: 4.077294826507568, Test_Loss: 3.596489906311035\n",
      "Epoch: 2, Train_Loss: 3.5277719497680664, Test_Loss: 3.5282881259918213 *\n",
      "Epoch: 2, Train_Loss: 3.560959815979004, Test_Loss: 3.93388032913208\n",
      "Epoch: 2, Train_Loss: 3.579009532928467, Test_Loss: 4.121181488037109\n",
      "Epoch: 2, Train_Loss: 3.659914255142212, Test_Loss: 3.714353084564209 *\n",
      "Epoch: 2, Train_Loss: 3.6112966537475586, Test_Loss: 3.551420211791992 *\n",
      "Epoch: 2, Train_Loss: 3.8880035877227783, Test_Loss: 3.534330368041992 *\n",
      "Epoch: 2, Train_Loss: 3.7119956016540527, Test_Loss: 3.5132126808166504 *\n",
      "Epoch: 2, Train_Loss: 3.5587990283966064, Test_Loss: 3.6340630054473877\n",
      "Epoch: 2, Train_Loss: 3.7052395343780518, Test_Loss: 4.620086669921875\n",
      "Epoch: 2, Train_Loss: 3.761293888092041, Test_Loss: 4.728205680847168\n",
      "Epoch: 2, Train_Loss: 3.9877564907073975, Test_Loss: 3.544452428817749 *\n",
      "Epoch: 2, Train_Loss: 3.7615461349487305, Test_Loss: 3.5743181705474854\n",
      "Epoch: 2, Train_Loss: 3.542083263397217, Test_Loss: 3.4820215702056885 *\n",
      "Epoch: 2, Train_Loss: 3.6269724369049072, Test_Loss: 3.4866933822631836\n",
      "Epoch: 2, Train_Loss: 3.5994489192962646, Test_Loss: 3.4833829402923584 *\n",
      "Epoch: 2, Train_Loss: 3.487487554550171, Test_Loss: 3.49698805809021\n",
      "Epoch: 2, Train_Loss: 3.4807674884796143, Test_Loss: 3.530780792236328\n",
      "Epoch: 2, Train_Loss: 3.4675393104553223, Test_Loss: 3.490973949432373 *\n",
      "Epoch: 2, Train_Loss: 3.470966339111328, Test_Loss: 3.4742472171783447 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 2\n",
      "Epoch: 2, Train_Loss: 3.4659719467163086, Test_Loss: 3.567441463470459\n",
      "Epoch: 2, Train_Loss: 3.4773528575897217, Test_Loss: 3.8520708084106445\n",
      "Epoch: 2, Train_Loss: 3.5609753131866455, Test_Loss: 3.627063751220703 *\n",
      "Epoch: 2, Train_Loss: 3.5540812015533447, Test_Loss: 3.501828193664551 *\n",
      "Epoch: 2, Train_Loss: 3.581066846847534, Test_Loss: 3.4564266204833984 *\n",
      "Epoch: 2, Train_Loss: 3.617164373397827, Test_Loss: 3.4536054134368896 *\n",
      "Epoch: 2, Train_Loss: 3.8185925483703613, Test_Loss: 3.450706720352173 *\n",
      "Epoch: 2, Train_Loss: 3.45330548286438, Test_Loss: 3.448173761367798 *\n",
      "Epoch: 2, Train_Loss: 3.504804849624634, Test_Loss: 3.5858728885650635\n",
      "Epoch: 2, Train_Loss: 3.78623104095459, Test_Loss: 8.769784927368164\n",
      "Epoch: 2, Train_Loss: 4.080517292022705, Test_Loss: 3.560579299926758 *\n",
      "Epoch: 2, Train_Loss: 3.5130116939544678, Test_Loss: 3.4372446537017822 *\n",
      "Epoch: 2, Train_Loss: 3.428685426712036, Test_Loss: 3.428497552871704 *\n",
      "Epoch: 2, Train_Loss: 3.8608908653259277, Test_Loss: 3.4281952381134033 *\n",
      "Epoch: 2, Train_Loss: 4.104928016662598, Test_Loss: 3.42722487449646 *\n",
      "Epoch: 2, Train_Loss: 3.717593193054199, Test_Loss: 3.4207897186279297 *\n",
      "Epoch: 2, Train_Loss: 3.443964719772339, Test_Loss: 3.4254534244537354\n",
      "Epoch: 2, Train_Loss: 3.4282796382904053, Test_Loss: 3.419771671295166 *\n",
      "Epoch: 2, Train_Loss: 3.6948070526123047, Test_Loss: 3.417320966720581 *\n",
      "Epoch: 2, Train_Loss: 5.147858142852783, Test_Loss: 3.415355682373047 *\n",
      "Epoch: 2, Train_Loss: 3.878021478652954, Test_Loss: 3.4158167839050293\n",
      "Epoch: 2, Train_Loss: 3.4239795207977295, Test_Loss: 3.415095567703247 *\n",
      "Epoch: 2, Train_Loss: 3.420707941055298, Test_Loss: 3.4212846755981445\n",
      "Epoch: 2, Train_Loss: 3.4010305404663086, Test_Loss: 3.4104950428009033 *\n",
      "Epoch: 2, Train_Loss: 3.7251334190368652, Test_Loss: 3.3979904651641846 *\n",
      "Epoch: 2, Train_Loss: 3.5201590061187744, Test_Loss: 3.3933794498443604 *\n",
      "Epoch: 2, Train_Loss: 3.452183485031128, Test_Loss: 3.3925068378448486 *\n",
      "Epoch: 2, Train_Loss: 3.747417449951172, Test_Loss: 3.3897550106048584 *\n",
      "Epoch: 2, Train_Loss: 3.3954734802246094, Test_Loss: 3.3857362270355225 *\n",
      "Epoch: 2, Train_Loss: 3.445082187652588, Test_Loss: 3.3852756023406982 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 3.4476828575134277, Test_Loss: 3.379822254180908 *\n",
      "Epoch: 2, Train_Loss: 3.740043878555298, Test_Loss: 3.3803505897521973\n",
      "Epoch: 2, Train_Loss: 3.418680191040039, Test_Loss: 3.378939390182495 *\n",
      "Epoch: 2, Train_Loss: 3.4804365634918213, Test_Loss: 3.375203847885132 *\n",
      "Epoch: 2, Train_Loss: 3.591430902481079, Test_Loss: 3.3707969188690186 *\n",
      "Epoch: 2, Train_Loss: 3.5793373584747314, Test_Loss: 3.3690996170043945 *\n",
      "Epoch: 2, Train_Loss: 3.388547897338867, Test_Loss: 3.3652186393737793 *\n",
      "Epoch: 2, Train_Loss: 3.3914949893951416, Test_Loss: 3.3637797832489014 *\n",
      "Epoch: 2, Train_Loss: 3.3632216453552246, Test_Loss: 3.368835687637329\n",
      "Epoch: 2, Train_Loss: 3.390892267227173, Test_Loss: 3.4156692028045654\n",
      "Epoch: 2, Train_Loss: 3.5096023082733154, Test_Loss: 5.228188514709473\n",
      "Epoch: 2, Train_Loss: 3.992682456970215, Test_Loss: 6.83483362197876\n",
      "Epoch: 2, Train_Loss: 3.6177022457122803, Test_Loss: 3.349370241165161 *\n",
      "Epoch: 2, Train_Loss: 4.198832035064697, Test_Loss: 3.342971086502075 *\n",
      "Epoch: 2, Train_Loss: 3.7296409606933594, Test_Loss: 3.3803913593292236\n",
      "Epoch: 2, Train_Loss: 3.722684860229492, Test_Loss: 3.373950481414795 *\n",
      "Epoch: 2, Train_Loss: 3.5659143924713135, Test_Loss: 3.390225410461426\n",
      "Epoch: 2, Train_Loss: 3.3498480319976807, Test_Loss: 3.3682703971862793 *\n",
      "Epoch: 2, Train_Loss: 3.3432815074920654, Test_Loss: 3.436218500137329\n",
      "Epoch: 2, Train_Loss: 3.3777053356170654, Test_Loss: 3.3351669311523438 *\n",
      "Epoch: 2, Train_Loss: 3.4735443592071533, Test_Loss: 3.340660333633423\n",
      "Epoch: 2, Train_Loss: 3.9770431518554688, Test_Loss: 3.346282720565796\n",
      "Epoch: 2, Train_Loss: 3.505496025085449, Test_Loss: 3.3732056617736816\n",
      "Epoch: 2, Train_Loss: 5.381935119628906, Test_Loss: 3.327200412750244 *\n",
      "Epoch: 2, Train_Loss: 3.9409613609313965, Test_Loss: 3.3779211044311523\n",
      "Epoch: 2, Train_Loss: 4.148174285888672, Test_Loss: 3.381692886352539\n",
      "Epoch: 2, Train_Loss: 3.424809455871582, Test_Loss: 3.3903911113739014\n",
      "Epoch: 2, Train_Loss: 3.314873456954956, Test_Loss: 3.333083152770996 *\n",
      "Epoch: 2, Train_Loss: 3.494133472442627, Test_Loss: 3.355473041534424\n",
      "Epoch: 2, Train_Loss: 4.600241661071777, Test_Loss: 3.3226706981658936 *\n",
      "Epoch: 2, Train_Loss: 4.094444274902344, Test_Loss: 3.3053653240203857 *\n",
      "Epoch: 2, Train_Loss: 3.3563194274902344, Test_Loss: 3.298591136932373 *\n",
      "Epoch: 2, Train_Loss: 3.3232882022857666, Test_Loss: 3.2968428134918213 *\n",
      "Epoch: 2, Train_Loss: 3.370293378829956, Test_Loss: 3.295804023742676 *\n",
      "Epoch: 2, Train_Loss: 3.650939464569092, Test_Loss: 3.291567325592041 *\n",
      "Epoch: 2, Train_Loss: 3.49922251701355, Test_Loss: 3.2886147499084473 *\n",
      "Epoch: 2, Train_Loss: 4.38994836807251, Test_Loss: 3.2931854724884033\n",
      "Epoch: 2, Train_Loss: 4.023277759552002, Test_Loss: 3.2862510681152344 *\n",
      "Epoch: 2, Train_Loss: 3.3610777854919434, Test_Loss: 3.2859044075012207 *\n",
      "Epoch: 2, Train_Loss: 3.2972304821014404, Test_Loss: 3.283491611480713 *\n",
      "Epoch: 2, Train_Loss: 3.3076868057250977, Test_Loss: 3.284938335418701\n",
      "Epoch: 2, Train_Loss: 3.3107001781463623, Test_Loss: 3.3198180198669434\n",
      "Epoch: 2, Train_Loss: 3.3043341636657715, Test_Loss: 3.279431104660034 *\n",
      "Epoch: 2, Train_Loss: 3.2765703201293945, Test_Loss: 3.6963603496551514\n",
      "Epoch: 2, Train_Loss: 3.2815682888031006, Test_Loss: 3.7368340492248535\n",
      "Epoch: 2, Train_Loss: 20.57480812072754, Test_Loss: 3.3892085552215576 *\n",
      "Epoch: 2, Train_Loss: 3.2703189849853516, Test_Loss: 3.277235269546509 *\n",
      "Epoch: 2, Train_Loss: 6.121092319488525, Test_Loss: 3.2737693786621094 *\n",
      "Epoch: 2, Train_Loss: 4.678492546081543, Test_Loss: 3.276801586151123\n",
      "Epoch: 2, Train_Loss: 3.2630677223205566, Test_Loss: 3.4521002769470215\n",
      "Epoch: 2, Train_Loss: 3.3167924880981445, Test_Loss: 4.44495153427124\n",
      "Epoch: 2, Train_Loss: 9.751049041748047, Test_Loss: 4.061026096343994 *\n",
      "Epoch: 2, Train_Loss: 9.081116676330566, Test_Loss: 3.28981876373291 *\n",
      "Epoch: 2, Train_Loss: 3.264842987060547, Test_Loss: 3.2846033573150635 *\n",
      "Epoch: 2, Train_Loss: 3.38214373588562, Test_Loss: 3.2467072010040283 *\n",
      "Epoch: 2, Train_Loss: 9.435420989990234, Test_Loss: 3.252143383026123\n",
      "Epoch: 2, Train_Loss: 3.251441478729248, Test_Loss: 3.248762369155884 *\n",
      "Epoch: 2, Train_Loss: 3.2531752586364746, Test_Loss: 3.271862506866455\n",
      "Epoch: 2, Train_Loss: 3.2305736541748047, Test_Loss: 3.341778516769409\n",
      "Epoch: 2, Train_Loss: 3.2270443439483643, Test_Loss: 3.2369821071624756 *\n",
      "Epoch: 2, Train_Loss: 3.231405258178711, Test_Loss: 3.2537550926208496\n",
      "Epoch: 2, Train_Loss: 3.2324767112731934, Test_Loss: 3.3324029445648193\n",
      "Epoch: 2, Train_Loss: 3.2310855388641357, Test_Loss: 3.5367164611816406\n",
      "Epoch: 2, Train_Loss: 3.2299509048461914, Test_Loss: 3.379356861114502 *\n",
      "Epoch: 2, Train_Loss: 3.229015350341797, Test_Loss: 3.2184348106384277 *\n",
      "Epoch: 2, Train_Loss: 3.2251245975494385, Test_Loss: 3.2105937004089355 *\n",
      "Epoch: 2, Train_Loss: 3.2246384620666504, Test_Loss: 3.2083911895751953 *\n",
      "Epoch: 2, Train_Loss: 3.2153711318969727, Test_Loss: 3.205716371536255 *\n",
      "Epoch: 2, Train_Loss: 3.2623751163482666, Test_Loss: 3.2035934925079346 *\n",
      "Epoch: 2, Train_Loss: 3.270549774169922, Test_Loss: 3.9637653827667236\n",
      "Model saved at location save_new\\model.ckpt at epoch 2\n",
      "Epoch: 2, Train_Loss: 3.2180702686309814, Test_Loss: 8.303502082824707\n",
      "Epoch: 2, Train_Loss: 3.2088847160339355, Test_Loss: 3.244163990020752 *\n",
      "Epoch: 2, Train_Loss: 3.199840545654297, Test_Loss: 3.2014503479003906 *\n",
      "Epoch: 2, Train_Loss: 3.191852569580078, Test_Loss: 3.1936681270599365 *\n",
      "Epoch: 2, Train_Loss: 3.1898746490478516, Test_Loss: 3.189267873764038 *\n",
      "Epoch: 2, Train_Loss: 3.187534809112549, Test_Loss: 3.1932270526885986\n",
      "Epoch: 2, Train_Loss: 3.1856629848480225, Test_Loss: 3.189857006072998 *\n",
      "Epoch: 2, Train_Loss: 3.1859261989593506, Test_Loss: 3.2048065662384033\n",
      "Epoch: 2, Train_Loss: 3.179527997970581, Test_Loss: 3.194361925125122 *\n",
      "Epoch: 2, Train_Loss: 3.177339553833008, Test_Loss: 3.196685791015625\n",
      "Epoch: 2, Train_Loss: 3.1758298873901367, Test_Loss: 3.189380168914795 *\n",
      "Epoch: 2, Train_Loss: 3.172762393951416, Test_Loss: 3.192049026489258\n",
      "Epoch: 2, Train_Loss: 3.1806700229644775, Test_Loss: 3.177588939666748 *\n",
      "Epoch: 2, Train_Loss: 3.1877875328063965, Test_Loss: 3.1757802963256836 *\n",
      "Epoch: 2, Train_Loss: 3.1770167350769043, Test_Loss: 3.179485321044922\n",
      "Epoch: 2, Train_Loss: 3.1701300144195557, Test_Loss: 3.1736650466918945 *\n",
      "Epoch: 2, Train_Loss: 6.760588645935059, Test_Loss: 3.169534206390381 *\n",
      "Epoch: 2, Train_Loss: 8.822470664978027, Test_Loss: 3.168884515762329 *\n",
      "Epoch: 2, Train_Loss: 3.178504467010498, Test_Loss: 3.1694839000701904\n",
      "Epoch: 2, Train_Loss: 3.15971040725708, Test_Loss: 3.165480136871338 *\n",
      "Epoch: 2, Train_Loss: 3.1661934852600098, Test_Loss: 3.1658413410186768\n",
      "Epoch: 2, Train_Loss: 3.1688528060913086, Test_Loss: 3.162436008453369 *\n",
      "Epoch: 2, Train_Loss: 3.175865411758423, Test_Loss: 3.1669530868530273\n",
      "Epoch: 2, Train_Loss: 3.1551156044006348, Test_Loss: 3.1620123386383057 *\n",
      "Epoch: 2, Train_Loss: 3.1911706924438477, Test_Loss: 3.161357879638672 *\n",
      "Epoch: 2, Train_Loss: 3.4021365642547607, Test_Loss: 3.156179189682007 *\n",
      "Epoch: 2, Train_Loss: 3.3441686630249023, Test_Loss: 3.155276298522949 *\n",
      "Epoch: 2, Train_Loss: 3.223768711090088, Test_Loss: 3.150390148162842 *\n",
      "Epoch: 2, Train_Loss: 3.1941752433776855, Test_Loss: 3.149639368057251 *\n",
      "Epoch: 2, Train_Loss: 3.2666335105895996, Test_Loss: 3.173448324203491\n",
      "Epoch: 2, Train_Loss: 3.25370192527771, Test_Loss: 3.1852126121520996\n",
      "Epoch: 2, Train_Loss: 3.2935853004455566, Test_Loss: 6.320863246917725\n",
      "Epoch: 2, Train_Loss: 3.257176399230957, Test_Loss: 5.816142559051514 *\n",
      "Epoch: 2, Train_Loss: 3.2217798233032227, Test_Loss: 3.13346791267395 *\n",
      "Epoch: 2, Train_Loss: 3.120241641998291, Test_Loss: 3.1274631023406982 *\n",
      "Epoch: 2, Train_Loss: 3.177678108215332, Test_Loss: 3.142822265625\n",
      "Epoch: 2, Train_Loss: 3.1532812118530273, Test_Loss: 3.1425957679748535 *\n",
      "Epoch: 2, Train_Loss: 3.1265203952789307, Test_Loss: 3.135286808013916 *\n",
      "Epoch: 2, Train_Loss: 3.1152491569519043, Test_Loss: 3.1977572441101074\n",
      "Epoch: 2, Train_Loss: 3.113672971725464, Test_Loss: 3.2667019367218018\n",
      "Epoch: 2, Train_Loss: 3.1132237911224365, Test_Loss: 3.1093459129333496 *\n",
      "Epoch: 2, Train_Loss: 6.5850067138671875, Test_Loss: 3.15102481842041\n",
      "Epoch: 2, Train_Loss: 5.347412109375, Test_Loss: 3.129094362258911 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 3.114222526550293, Test_Loss: 3.1146721839904785 *\n",
      "Epoch: 2, Train_Loss: 3.118837833404541, Test_Loss: 3.115180015563965\n",
      "Epoch: 2, Train_Loss: 3.121002435684204, Test_Loss: 3.153829574584961\n",
      "Epoch: 2, Train_Loss: 3.113842010498047, Test_Loss: 3.139803409576416 *\n",
      "Epoch: 2, Train_Loss: 3.1013007164001465, Test_Loss: 3.238271474838257\n",
      "Epoch: 2, Train_Loss: 3.0989975929260254, Test_Loss: 3.2510547637939453\n",
      "Epoch: 2, Train_Loss: 3.100302219390869, Test_Loss: 3.0939276218414307 *\n",
      "Epoch: 2, Train_Loss: 3.1091091632843018, Test_Loss: 3.086738348007202 *\n",
      "Epoch: 2, Train_Loss: 3.093245267868042, Test_Loss: 3.0809226036071777 *\n",
      "Epoch: 2, Train_Loss: 3.0876665115356445, Test_Loss: 3.0773839950561523 *\n",
      "Epoch: 2, Train_Loss: 3.0847675800323486, Test_Loss: 3.0754570960998535 *\n",
      "Epoch: 2, Train_Loss: 3.0986344814300537, Test_Loss: 3.073528289794922 *\n",
      "Epoch: 2, Train_Loss: 3.079390048980713, Test_Loss: 3.0710408687591553 *\n",
      "Epoch: 2, Train_Loss: 3.073795795440674, Test_Loss: 3.0683183670043945 *\n",
      "Epoch: 2, Train_Loss: 3.074814558029175, Test_Loss: 3.0729424953460693\n",
      "Epoch: 2, Train_Loss: 3.1367173194885254, Test_Loss: 3.0678961277008057 *\n",
      "Epoch: 2, Train_Loss: 3.123061418533325, Test_Loss: 3.0661168098449707 *\n",
      "Epoch: 2, Train_Loss: 3.063655138015747, Test_Loss: 3.067958354949951\n",
      "Epoch: 2, Train_Loss: 3.06554913520813, Test_Loss: 3.0710978507995605\n",
      "Epoch: 2, Train_Loss: 3.0841140747070312, Test_Loss: 3.0931010246276855\n",
      "Epoch: 2, Train_Loss: 3.150117874145508, Test_Loss: 3.095578193664551\n",
      "Epoch: 2, Train_Loss: 3.1149027347564697, Test_Loss: 3.521801710128784\n",
      "Epoch: 2, Train_Loss: 3.125521659851074, Test_Loss: 3.4452617168426514 *\n",
      "Epoch: 2, Train_Loss: 3.080361843109131, Test_Loss: 3.1379878520965576 *\n",
      "Epoch: 2, Train_Loss: 3.128566265106201, Test_Loss: 3.0580852031707764 *\n",
      "Epoch: 2, Train_Loss: 3.1151351928710938, Test_Loss: 3.0560142993927 *\n",
      "Epoch: 2, Train_Loss: 3.095371961593628, Test_Loss: 3.1146063804626465\n",
      "Epoch: 2, Train_Loss: 3.109910488128662, Test_Loss: 3.48715877532959\n",
      "Epoch: 2, Train_Loss: 3.088285446166992, Test_Loss: 4.5792436599731445\n",
      "Epoch: 2, Train_Loss: 3.0502214431762695, Test_Loss: 3.7917728424072266 *\n",
      "Epoch: 2, Train_Loss: 3.0350148677825928, Test_Loss: 3.0911693572998047 *\n",
      "Epoch: 2, Train_Loss: 3.0329813957214355, Test_Loss: 3.045405387878418 *\n",
      "Epoch: 2, Train_Loss: 3.029057264328003, Test_Loss: 3.0379176139831543 *\n",
      "Epoch: 2, Train_Loss: 3.0261147022247314, Test_Loss: 3.0318706035614014 *\n",
      "Epoch: 2, Train_Loss: 3.0253703594207764, Test_Loss: 3.0309524536132812 *\n",
      "Epoch: 2, Train_Loss: 6.377284049987793, Test_Loss: 3.063901901245117\n",
      "Epoch: 2, Train_Loss: 4.718027114868164, Test_Loss: 3.1023290157318115\n",
      "Epoch: 2, Train_Loss: 3.015477180480957, Test_Loss: 3.0159571170806885 *\n",
      "Epoch: 2, Train_Loss: 3.0415592193603516, Test_Loss: 3.0631144046783447\n",
      "Epoch: 2, Train_Loss: 3.021646022796631, Test_Loss: 3.1300578117370605\n",
      "Epoch: 2, Train_Loss: 3.0058164596557617, Test_Loss: 3.3466522693634033\n",
      "Epoch: 2, Train_Loss: 3.005647659301758, Test_Loss: 3.1997172832489014 *\n",
      "Epoch: 2, Train_Loss: 3.0019564628601074, Test_Loss: 3.010094165802002 *\n",
      "Epoch: 2, Train_Loss: 2.9997060298919678, Test_Loss: 3.00437593460083 *\n",
      "Epoch: 2, Train_Loss: 2.996615171432495, Test_Loss: 3.002758026123047 *\n",
      "Epoch: 2, Train_Loss: 3.024789571762085, Test_Loss: 3.0013434886932373 *\n",
      "Epoch: 2, Train_Loss: 3.093698740005493, Test_Loss: 3.0080442428588867\n",
      "Epoch: 2, Train_Loss: 3.080606698989868, Test_Loss: 4.785026550292969\n",
      "Epoch: 2, Train_Loss: 3.0941100120544434, Test_Loss: 6.724615097045898\n",
      "Epoch: 2, Train_Loss: 3.009429454803467, Test_Loss: 2.997659206390381 *\n",
      "Epoch: 2, Train_Loss: 2.9976577758789062, Test_Loss: 2.986203670501709 *\n",
      "Epoch: 2, Train_Loss: 3.1828272342681885, Test_Loss: 2.9842376708984375 *\n",
      "Epoch: 2, Train_Loss: 3.195537805557251, Test_Loss: 2.988312005996704\n",
      "Epoch: 2, Train_Loss: 3.176677942276001, Test_Loss: 2.9795081615448 *\n",
      "Epoch: 2, Train_Loss: 3.031463146209717, Test_Loss: 2.9788899421691895 *\n",
      "Epoch: 2, Train_Loss: 2.971459150314331, Test_Loss: 2.977391242980957 *\n",
      "Epoch: 2, Train_Loss: 2.97080135345459, Test_Loss: 2.9755382537841797 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 2\n",
      "Epoch: 2, Train_Loss: 2.9701924324035645, Test_Loss: 2.975306749343872 *\n",
      "Epoch: 2, Train_Loss: 2.971158504486084, Test_Loss: 2.9692673683166504 *\n",
      "Epoch: 2, Train_Loss: 2.9690957069396973, Test_Loss: 2.9721343517303467\n",
      "Epoch: 2, Train_Loss: 2.9637274742126465, Test_Loss: 2.980849504470825\n",
      "Epoch: 2, Train_Loss: 2.961679458618164, Test_Loss: 2.9762468338012695 *\n",
      "Epoch: 2, Train_Loss: 2.95776629447937, Test_Loss: 2.9659109115600586 *\n",
      "Epoch: 2, Train_Loss: 2.959141254425049, Test_Loss: 2.957009792327881 *\n",
      "Epoch: 2, Train_Loss: 3.026951551437378, Test_Loss: 2.9572010040283203\n",
      "Epoch: 2, Train_Loss: 3.124844789505005, Test_Loss: 2.953883171081543 *\n",
      "Epoch: 2, Train_Loss: 3.1088550090789795, Test_Loss: 2.9542887210845947\n",
      "Epoch: 2, Train_Loss: 3.0359654426574707, Test_Loss: 2.953059434890747 *\n",
      "Epoch: 2, Train_Loss: 3.0886306762695312, Test_Loss: 2.9471654891967773 *\n",
      "Epoch: 2, Train_Loss: 3.1306490898132324, Test_Loss: 2.946645736694336 *\n",
      "Epoch: 2, Train_Loss: 2.9570438861846924, Test_Loss: 2.9495999813079834\n",
      "Epoch: 2, Train_Loss: 3.115565061569214, Test_Loss: 2.9430675506591797 *\n",
      "Epoch: 2, Train_Loss: 3.0735366344451904, Test_Loss: 2.9432778358459473\n",
      "Epoch: 2, Train_Loss: 3.2305965423583984, Test_Loss: 2.9388985633850098 *\n",
      "Epoch: 2, Train_Loss: 2.9465343952178955, Test_Loss: 2.9382104873657227 *\n",
      "Epoch: 2, Train_Loss: 3.479649066925049, Test_Loss: 2.935840368270874 *\n",
      "Epoch: 2, Train_Loss: 5.711101531982422, Test_Loss: 2.9331893920898438 *\n",
      "Epoch: 2, Train_Loss: 2.9760451316833496, Test_Loss: 2.9770824909210205\n",
      "Epoch: 2, Train_Loss: 2.961880683898926, Test_Loss: 2.9514036178588867 *\n",
      "Epoch: 2, Train_Loss: 2.9652082920074463, Test_Loss: 7.081007957458496\n",
      "Epoch: 2, Train_Loss: 2.9630990028381348, Test_Loss: 4.442831039428711 *\n",
      "Epoch: 2, Train_Loss: 2.9187498092651367, Test_Loss: 2.919799566268921 *\n",
      "Epoch: 2, Train_Loss: 2.9211294651031494, Test_Loss: 2.923038959503174\n",
      "Epoch: 2, Train_Loss: 3.0436341762542725, Test_Loss: 2.9527411460876465\n",
      "Epoch: 2, Train_Loss: 3.050107479095459, Test_Loss: 2.9621970653533936\n",
      "Epoch: 2, Train_Loss: 3.038207769393921, Test_Loss: 2.9195456504821777 *\n",
      "Epoch: 2, Train_Loss: 3.0048835277557373, Test_Loss: 3.0012996196746826\n",
      "Epoch: 2, Train_Loss: 3.0042264461517334, Test_Loss: 3.0103960037231445\n",
      "Epoch: 2, Train_Loss: 2.936264753341675, Test_Loss: 2.9017653465270996 *\n",
      "Epoch: 2, Train_Loss: 2.9369075298309326, Test_Loss: 2.945342540740967\n",
      "Epoch: 2, Train_Loss: 2.9063758850097656, Test_Loss: 2.910691976547241 *\n",
      "Epoch: 2, Train_Loss: 2.9183013439178467, Test_Loss: 2.909343719482422 *\n",
      "Epoch: 2, Train_Loss: 2.9009816646575928, Test_Loss: 2.896124839782715 *\n",
      "Epoch: 2, Train_Loss: 2.8888890743255615, Test_Loss: 3.0014114379882812\n",
      "Epoch: 2, Train_Loss: 2.935321092605591, Test_Loss: 2.931849956512451 *\n",
      "Epoch: 2, Train_Loss: 2.949892520904541, Test_Loss: 3.018874406814575\n",
      "Epoch: 2, Train_Loss: 2.9235875606536865, Test_Loss: 2.9787211418151855 *\n",
      "Epoch: 2, Train_Loss: 2.8804986476898193, Test_Loss: 2.9059181213378906 *\n",
      "Epoch: 2, Train_Loss: 2.8790957927703857, Test_Loss: 2.8918087482452393 *\n",
      "Epoch: 2, Train_Loss: 2.876642942428589, Test_Loss: 2.889967918395996 *\n",
      "Epoch: 2, Train_Loss: 2.8742032051086426, Test_Loss: 2.8870582580566406 *\n",
      "Epoch: 2, Train_Loss: 2.8730502128601074, Test_Loss: 2.8874118328094482\n",
      "Epoch: 2, Train_Loss: 2.8695082664489746, Test_Loss: 2.8858585357666016 *\n",
      "Epoch: 2, Train_Loss: 2.86757230758667, Test_Loss: 2.883913993835449 *\n",
      "Epoch: 2, Train_Loss: 2.866173267364502, Test_Loss: 2.8786635398864746 *\n",
      "Epoch: 2, Train_Loss: 2.8632447719573975, Test_Loss: 2.889188289642334\n",
      "Epoch: 2, Train_Loss: 2.861253261566162, Test_Loss: 2.879964590072632 *\n",
      "Epoch: 2, Train_Loss: 2.864220380783081, Test_Loss: 2.87137508392334 *\n",
      "Epoch: 2, Train_Loss: 2.8644230365753174, Test_Loss: 2.8665096759796143 *\n",
      "Epoch: 2, Train_Loss: 2.861180305480957, Test_Loss: 2.90246319770813\n",
      "Epoch: 2, Train_Loss: 2.869107484817505, Test_Loss: 2.895610809326172 *\n",
      "Epoch: 3, Train_Loss: 2.852299928665161, Test_Loss: 3.0188004970550537 *\n",
      "Epoch: 3, Train_Loss: 2.851510763168335, Test_Loss: 3.415408134460449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_Loss: 2.8470523357391357, Test_Loss: 3.2158546447753906 *\n",
      "Epoch: 3, Train_Loss: 2.8444552421569824, Test_Loss: 2.955400228500366 *\n",
      "Epoch: 3, Train_Loss: 2.855131149291992, Test_Loss: 2.8766942024230957 *\n",
      "Epoch: 3, Train_Loss: 2.849830150604248, Test_Loss: 2.853146553039551 *\n",
      "Epoch: 3, Train_Loss: 2.838733434677124, Test_Loss: 2.9149863719940186\n",
      "Epoch: 3, Train_Loss: 2.836557626724243, Test_Loss: 3.4321961402893066\n",
      "Epoch: 3, Train_Loss: 2.841890335083008, Test_Loss: 4.269993305206299\n",
      "Epoch: 3, Train_Loss: 2.928969144821167, Test_Loss: 3.2825305461883545 *\n",
      "Epoch: 3, Train_Loss: 2.8525924682617188, Test_Loss: 2.929119110107422 *\n",
      "Epoch: 3, Train_Loss: 2.8686811923980713, Test_Loss: 2.834641218185425 *\n",
      "Epoch: 3, Train_Loss: 2.8270421028137207, Test_Loss: 2.833867311477661 *\n",
      "Epoch: 3, Train_Loss: 2.8418333530426025, Test_Loss: 2.827814817428589 *\n",
      "Epoch: 3, Train_Loss: 2.8641090393066406, Test_Loss: 2.831967830657959\n",
      "Epoch: 3, Train_Loss: 2.8211874961853027, Test_Loss: 2.860701084136963\n",
      "Epoch: 3, Train_Loss: 2.8422117233276367, Test_Loss: 2.875098943710327\n",
      "Epoch: 3, Train_Loss: 2.852201223373413, Test_Loss: 2.8210413455963135 *\n",
      "Epoch: 3, Train_Loss: 2.8950588703155518, Test_Loss: 2.8996760845184326\n",
      "Epoch: 3, Train_Loss: 2.9292635917663574, Test_Loss: 3.0973973274230957\n",
      "Epoch: 3, Train_Loss: 2.8985378742218018, Test_Loss: 2.9813640117645264 *\n",
      "Epoch: 3, Train_Loss: 2.8504109382629395, Test_Loss: 2.9964728355407715\n",
      "Epoch: 3, Train_Loss: 2.807884931564331, Test_Loss: 2.8119349479675293 *\n",
      "Epoch: 3, Train_Loss: 2.8323628902435303, Test_Loss: 2.810293436050415 *\n",
      "Epoch: 3, Train_Loss: 2.802319288253784, Test_Loss: 2.808211326599121 *\n",
      "Epoch: 3, Train_Loss: 2.8072574138641357, Test_Loss: 2.8065185546875 *\n",
      "Epoch: 3, Train_Loss: 2.807814836502075, Test_Loss: 2.814143657684326\n",
      "Epoch: 3, Train_Loss: 2.8093371391296387, Test_Loss: 5.974056243896484\n",
      "Epoch: 3, Train_Loss: 2.8707845211029053, Test_Loss: 5.160491943359375 *\n",
      "Epoch: 3, Train_Loss: 2.8116793632507324, Test_Loss: 2.799438238143921 *\n",
      "Epoch: 3, Train_Loss: 2.8742198944091797, Test_Loss: 2.7913224697113037 *\n",
      "Epoch: 3, Train_Loss: 2.799219846725464, Test_Loss: 2.7885420322418213 *\n",
      "Epoch: 3, Train_Loss: 2.813800811767578, Test_Loss: 2.7927584648132324\n",
      "Epoch: 3, Train_Loss: 2.790299415588379, Test_Loss: 2.7847533226013184 *\n",
      "Epoch: 3, Train_Loss: 3.0236847400665283, Test_Loss: 2.786486864089966\n",
      "Epoch: 3, Train_Loss: 2.8605408668518066, Test_Loss: 2.78159499168396 *\n",
      "Epoch: 3, Train_Loss: 2.79118013381958, Test_Loss: 2.7830443382263184\n",
      "Epoch: 3, Train_Loss: 2.7920563220977783, Test_Loss: 2.781690835952759 *\n",
      "Epoch: 3, Train_Loss: 2.773195743560791, Test_Loss: 2.7766988277435303 *\n",
      "Epoch: 3, Train_Loss: 2.7724802494049072, Test_Loss: 2.7798309326171875\n",
      "Epoch: 3, Train_Loss: 2.7699384689331055, Test_Loss: 2.7929773330688477\n",
      "Epoch: 3, Train_Loss: 2.7749998569488525, Test_Loss: 2.7850701808929443 *\n",
      "Epoch: 3, Train_Loss: 2.786968946456909, Test_Loss: 2.7690160274505615 *\n",
      "Epoch: 3, Train_Loss: 2.7949366569519043, Test_Loss: 2.7629384994506836 *\n",
      "Epoch: 3, Train_Loss: 2.7788734436035156, Test_Loss: 2.7643990516662598\n",
      "Epoch: 3, Train_Loss: 2.7812318801879883, Test_Loss: 2.760223150253296 *\n",
      "Epoch: 3, Train_Loss: 2.7836432456970215, Test_Loss: 2.7590105533599854 *\n",
      "Epoch: 3, Train_Loss: 2.7578587532043457, Test_Loss: 2.7593576908111572\n",
      "Epoch: 3, Train_Loss: 2.7546911239624023, Test_Loss: 2.7531228065490723 *\n",
      "Epoch: 3, Train_Loss: 2.753795623779297, Test_Loss: 2.7520103454589844 *\n",
      "Epoch: 3, Train_Loss: 2.7744815349578857, Test_Loss: 2.7552261352539062\n",
      "Epoch: 3, Train_Loss: 2.776449203491211, Test_Loss: 2.7490508556365967 *\n",
      "Epoch: 3, Train_Loss: 2.747769832611084, Test_Loss: 2.748856782913208 *\n",
      "Epoch: 3, Train_Loss: 2.7805216312408447, Test_Loss: 2.744370460510254 *\n",
      "Epoch: 3, Train_Loss: 2.810546398162842, Test_Loss: 2.7434422969818115 *\n",
      "Epoch: 3, Train_Loss: 2.8107712268829346, Test_Loss: 2.7421698570251465 *\n",
      "Epoch: 3, Train_Loss: 2.736495018005371, Test_Loss: 2.74084210395813 *\n",
      "Epoch: 3, Train_Loss: 2.75754451751709, Test_Loss: 2.7804417610168457\n",
      "Epoch: 3, Train_Loss: 2.7392382621765137, Test_Loss: 2.7613143920898438 *\n",
      "Epoch: 3, Train_Loss: 2.75459623336792, Test_Loss: 8.022695541381836\n",
      "Epoch: 3, Train_Loss: 2.7317521572113037, Test_Loss: 3.059849739074707 *\n",
      "Epoch: 3, Train_Loss: 2.750342845916748, Test_Loss: 2.7274556159973145 *\n",
      "Epoch: 3, Train_Loss: 2.7852375507354736, Test_Loss: 2.742877960205078\n",
      "Epoch: 3, Train_Loss: 5.087523460388184, Test_Loss: 2.774041175842285\n",
      "Epoch: 3, Train_Loss: 5.786971092224121, Test_Loss: 2.77673077583313\n",
      "Epoch: 3, Train_Loss: 2.740600347518921, Test_Loss: 2.7234556674957275 *\n",
      "Epoch: 3, Train_Loss: 2.7173166275024414, Test_Loss: 2.8335325717926025\n",
      "Epoch: 3, Train_Loss: 2.818169116973877, Test_Loss: 2.7909271717071533 *\n",
      "Epoch: 3, Train_Loss: 2.8689253330230713, Test_Loss: 2.712257146835327 *\n",
      "Epoch: 3, Train_Loss: 2.7378175258636475, Test_Loss: 2.759993553161621\n",
      "Epoch: 3, Train_Loss: 2.7120108604431152, Test_Loss: 2.7180488109588623 *\n",
      "Epoch: 3, Train_Loss: 2.741150140762329, Test_Loss: 2.720200538635254\n",
      "Epoch: 3, Train_Loss: 2.7552621364593506, Test_Loss: 2.7145400047302246 *\n",
      "Epoch: 3, Train_Loss: 2.7141242027282715, Test_Loss: 2.8308944702148438\n",
      "Epoch: 3, Train_Loss: 2.7146944999694824, Test_Loss: 2.7357325553894043 *\n",
      "Epoch: 3, Train_Loss: 3.8560292720794678, Test_Loss: 2.8155508041381836\n",
      "Epoch: 3, Train_Loss: 4.115218162536621, Test_Loss: 2.744065523147583 *\n",
      "Epoch: 3, Train_Loss: 2.9793007373809814, Test_Loss: 2.7432467937469482 *\n",
      "Epoch: 3, Train_Loss: 2.776376962661743, Test_Loss: 2.720285177230835 *\n",
      "Epoch: 3, Train_Loss: 4.111883163452148, Test_Loss: 2.716398000717163 *\n",
      "Epoch: 3, Train_Loss: 4.708868026733398, Test_Loss: 2.7172117233276367\n",
      "Epoch: 3, Train_Loss: 2.7797248363494873, Test_Loss: 2.7165908813476562 *\n",
      "Epoch: 3, Train_Loss: 2.732417345046997, Test_Loss: 2.7136480808258057 *\n",
      "Epoch: 3, Train_Loss: 2.810556650161743, Test_Loss: 2.7059223651885986 *\n",
      "Epoch: 3, Train_Loss: 4.287287712097168, Test_Loss: 2.6935691833496094 *\n",
      "Epoch: 3, Train_Loss: 3.9692845344543457, Test_Loss: 2.704890251159668\n",
      "Epoch: 3, Train_Loss: 2.696045398712158, Test_Loss: 2.6939306259155273 *\n",
      "Epoch: 3, Train_Loss: 2.6920206546783447, Test_Loss: 2.6810779571533203 *\n",
      "Epoch: 3, Train_Loss: 2.7043797969818115, Test_Loss: 2.682325839996338\n",
      "Epoch: 3, Train_Loss: 3.484882354736328, Test_Loss: 2.7240376472473145\n",
      "Epoch: 3, Train_Loss: 2.724856376647949, Test_Loss: 2.6895713806152344 *\n",
      "Epoch: 3, Train_Loss: 2.741776466369629, Test_Loss: 2.9358432292938232\n",
      "Epoch: 3, Train_Loss: 2.698249578475952, Test_Loss: 3.263813018798828\n",
      "Epoch: 3, Train_Loss: 2.8404548168182373, Test_Loss: 2.957324266433716 *\n",
      "Epoch: 3, Train_Loss: 2.8025949001312256, Test_Loss: 2.7675442695617676 *\n",
      "Epoch: 3, Train_Loss: 2.895015239715576, Test_Loss: 2.6939423084259033 *\n",
      "Epoch: 3, Train_Loss: 2.9401473999023438, Test_Loss: 2.6637136936187744 *\n",
      "Epoch: 3, Train_Loss: 2.718148946762085, Test_Loss: 2.731279134750366\n",
      "Epoch: 3, Train_Loss: 2.7872374057769775, Test_Loss: 3.3422460556030273\n",
      "Epoch: 3, Train_Loss: 2.871347665786743, Test_Loss: 3.878462791442871\n",
      "Model saved at location save_new\\model.ckpt at epoch 3\n",
      "Epoch: 3, Train_Loss: 3.1112942695617676, Test_Loss: 2.8739261627197266 *\n",
      "Epoch: 3, Train_Loss: 2.9928982257843018, Test_Loss: 2.7399215698242188 *\n",
      "Epoch: 3, Train_Loss: 2.671037435531616, Test_Loss: 2.647747755050659 *\n",
      "Epoch: 3, Train_Loss: 2.7632205486297607, Test_Loss: 2.6571006774902344\n",
      "Epoch: 3, Train_Loss: 2.797766923904419, Test_Loss: 2.649982213973999 *\n",
      "Epoch: 3, Train_Loss: 2.650635004043579, Test_Loss: 2.659654378890991\n",
      "Epoch: 3, Train_Loss: 2.6569111347198486, Test_Loss: 2.6770153045654297\n",
      "Epoch: 3, Train_Loss: 2.637453079223633, Test_Loss: 2.6781277656555176\n",
      "Epoch: 3, Train_Loss: 2.6359126567840576, Test_Loss: 2.641667127609253 *\n",
      "Epoch: 3, Train_Loss: 2.6392626762390137, Test_Loss: 2.7386763095855713\n",
      "Epoch: 3, Train_Loss: 2.6386802196502686, Test_Loss: 2.9986376762390137\n",
      "Epoch: 3, Train_Loss: 2.7330358028411865, Test_Loss: 2.713498592376709 *\n",
      "Epoch: 3, Train_Loss: 2.701256513595581, Test_Loss: 2.817960500717163\n",
      "Epoch: 3, Train_Loss: 2.764815092086792, Test_Loss: 2.6435210704803467 *\n",
      "Epoch: 3, Train_Loss: 2.7336337566375732, Test_Loss: 2.642921209335327 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_Loss: 3.061316728591919, Test_Loss: 2.641331672668457 *\n",
      "Epoch: 3, Train_Loss: 2.626039743423462, Test_Loss: 2.638814926147461 *\n",
      "Epoch: 3, Train_Loss: 2.6491739749908447, Test_Loss: 2.651198387145996\n",
      "Epoch: 3, Train_Loss: 2.8586266040802, Test_Loss: 7.104144096374512\n",
      "Epoch: 3, Train_Loss: 3.14123272895813, Test_Loss: 3.59574294090271 *\n",
      "Epoch: 3, Train_Loss: 2.830264091491699, Test_Loss: 2.618483781814575 *\n",
      "Epoch: 3, Train_Loss: 2.612889051437378, Test_Loss: 2.611654758453369 *\n",
      "Epoch: 3, Train_Loss: 2.8837993144989014, Test_Loss: 2.609264373779297 *\n",
      "Epoch: 3, Train_Loss: 3.227222442626953, Test_Loss: 2.613096237182617\n",
      "Epoch: 3, Train_Loss: 3.0801453590393066, Test_Loss: 2.607771873474121 *\n",
      "Epoch: 3, Train_Loss: 2.6353516578674316, Test_Loss: 2.614748239517212\n",
      "Epoch: 3, Train_Loss: 2.6142656803131104, Test_Loss: 2.6066973209381104 *\n",
      "Epoch: 3, Train_Loss: 2.6403238773345947, Test_Loss: 2.6084418296813965\n",
      "Epoch: 3, Train_Loss: 3.866658926010132, Test_Loss: 2.609251022338867\n",
      "Epoch: 3, Train_Loss: 3.331345796585083, Test_Loss: 2.6090919971466064 *\n",
      "Epoch: 3, Train_Loss: 2.6004674434661865, Test_Loss: 2.5999066829681396 *\n",
      "Epoch: 3, Train_Loss: 2.6178958415985107, Test_Loss: 2.609380006790161\n",
      "Epoch: 3, Train_Loss: 2.593607187271118, Test_Loss: 2.603262424468994 *\n",
      "Epoch: 3, Train_Loss: 2.7483174800872803, Test_Loss: 2.590663433074951 *\n",
      "Epoch: 3, Train_Loss: 2.936310052871704, Test_Loss: 2.588136672973633 *\n",
      "Epoch: 3, Train_Loss: 2.5939533710479736, Test_Loss: 2.5876195430755615 *\n",
      "Epoch: 3, Train_Loss: 2.8361244201660156, Test_Loss: 2.589435577392578\n",
      "Epoch: 3, Train_Loss: 2.5978662967681885, Test_Loss: 2.5837631225585938 *\n",
      "Epoch: 3, Train_Loss: 2.6399760246276855, Test_Loss: 2.5818779468536377 *\n",
      "Epoch: 3, Train_Loss: 2.668478012084961, Test_Loss: 2.5796306133270264 *\n",
      "Epoch: 3, Train_Loss: 2.8895351886749268, Test_Loss: 2.580894708633423\n",
      "Epoch: 3, Train_Loss: 2.6796298027038574, Test_Loss: 2.5842971801757812\n",
      "Epoch: 3, Train_Loss: 2.6158673763275146, Test_Loss: 2.5826709270477295 *\n",
      "Epoch: 3, Train_Loss: 2.6382317543029785, Test_Loss: 2.576528787612915 *\n",
      "Epoch: 3, Train_Loss: 2.7040374279022217, Test_Loss: 2.57430362701416 *\n",
      "Epoch: 3, Train_Loss: 2.582202672958374, Test_Loss: 2.5768015384674072\n",
      "Epoch: 3, Train_Loss: 2.6056060791015625, Test_Loss: 2.5734341144561768 *\n",
      "Epoch: 3, Train_Loss: 2.5947208404541016, Test_Loss: 2.5746750831604004\n",
      "Epoch: 3, Train_Loss: 2.6031947135925293, Test_Loss: 2.632018566131592\n",
      "Epoch: 3, Train_Loss: 2.604612112045288, Test_Loss: 2.7411110401153564\n",
      "Epoch: 3, Train_Loss: 2.9878690242767334, Test_Loss: 7.762657165527344\n",
      "Epoch: 3, Train_Loss: 2.7355008125305176, Test_Loss: 2.5655574798583984 *\n",
      "Epoch: 3, Train_Loss: 3.2577078342437744, Test_Loss: 2.5624234676361084 *\n",
      "Epoch: 3, Train_Loss: 3.0134263038635254, Test_Loss: 2.5848135948181152\n",
      "Epoch: 3, Train_Loss: 2.7526485919952393, Test_Loss: 2.570483684539795 *\n",
      "Epoch: 3, Train_Loss: 2.731015205383301, Test_Loss: 2.5909430980682373\n",
      "Epoch: 3, Train_Loss: 2.6202340126037598, Test_Loss: 2.5568771362304688 *\n",
      "Epoch: 3, Train_Loss: 2.5512585639953613, Test_Loss: 2.618189573287964\n",
      "Epoch: 3, Train_Loss: 2.5489351749420166, Test_Loss: 2.582826852798462 *\n",
      "Epoch: 3, Train_Loss: 2.6244819164276123, Test_Loss: 2.560103178024292 *\n",
      "Epoch: 3, Train_Loss: 2.924894332885742, Test_Loss: 2.5757834911346436\n",
      "Epoch: 3, Train_Loss: 2.937765121459961, Test_Loss: 2.6748270988464355\n",
      "Epoch: 3, Train_Loss: 3.8101673126220703, Test_Loss: 2.5894699096679688 *\n",
      "Epoch: 3, Train_Loss: 3.610650062561035, Test_Loss: 2.6264469623565674\n",
      "Epoch: 3, Train_Loss: 3.079155445098877, Test_Loss: 2.5442965030670166 *\n",
      "Epoch: 3, Train_Loss: 2.8251819610595703, Test_Loss: 2.6458921432495117\n",
      "Epoch: 3, Train_Loss: 2.553227663040161, Test_Loss: 2.5577642917633057 *\n",
      "Epoch: 3, Train_Loss: 2.6351864337921143, Test_Loss: 2.6605937480926514\n",
      "Epoch: 3, Train_Loss: 3.286198139190674, Test_Loss: 2.6303088665008545 *\n",
      "Epoch: 3, Train_Loss: 3.809568405151367, Test_Loss: 2.54870867729187 *\n",
      "Epoch: 3, Train_Loss: 2.600006341934204, Test_Loss: 2.533492088317871 *\n",
      "Epoch: 3, Train_Loss: 2.583491563796997, Test_Loss: 2.553331136703491\n",
      "Epoch: 3, Train_Loss: 2.614924907684326, Test_Loss: 2.5536091327667236\n",
      "Epoch: 3, Train_Loss: 2.860814094543457, Test_Loss: 2.5530033111572266 *\n",
      "Epoch: 3, Train_Loss: 2.7034246921539307, Test_Loss: 2.5294911861419678 *\n",
      "Epoch: 3, Train_Loss: 3.187267303466797, Test_Loss: 2.5316829681396484\n",
      "Epoch: 3, Train_Loss: 3.119462728500366, Test_Loss: 2.5171408653259277 *\n",
      "Epoch: 3, Train_Loss: 2.7775654792785645, Test_Loss: 2.51484751701355 *\n",
      "Epoch: 3, Train_Loss: 2.528700113296509, Test_Loss: 2.5566112995147705\n",
      "Epoch: 3, Train_Loss: 2.5173535346984863, Test_Loss: 2.527040719985962 *\n",
      "Epoch: 3, Train_Loss: 2.508000612258911, Test_Loss: 2.5111451148986816 *\n",
      "Epoch: 3, Train_Loss: 2.5519180297851562, Test_Loss: 2.5421700477600098\n",
      "Epoch: 3, Train_Loss: 2.5121777057647705, Test_Loss: 2.746349334716797\n",
      "Epoch: 3, Train_Loss: 2.5204243659973145, Test_Loss: 2.9474127292633057\n",
      "Epoch: 3, Train_Loss: 19.729793548583984, Test_Loss: 2.6835968494415283 *\n",
      "Epoch: 3, Train_Loss: 2.50873064994812, Test_Loss: 2.54907488822937 *\n",
      "Epoch: 3, Train_Loss: 4.488734722137451, Test_Loss: 2.5149643421173096 *\n",
      "Epoch: 3, Train_Loss: 4.406060695648193, Test_Loss: 2.498809814453125 *\n",
      "Epoch: 3, Train_Loss: 2.501523971557617, Test_Loss: 2.5197322368621826\n",
      "Epoch: 3, Train_Loss: 2.5334134101867676, Test_Loss: 2.991344451904297\n",
      "Epoch: 3, Train_Loss: 6.305558204650879, Test_Loss: 3.128211498260498\n",
      "Epoch: 3, Train_Loss: 10.667914390563965, Test_Loss: 2.582406997680664 *\n",
      "Epoch: 3, Train_Loss: 2.542994499206543, Test_Loss: 2.527785539627075 *\n",
      "Epoch: 3, Train_Loss: 2.507601261138916, Test_Loss: 2.5063040256500244 *\n",
      "Epoch: 3, Train_Loss: 8.535444259643555, Test_Loss: 2.5293972492218018\n",
      "Epoch: 3, Train_Loss: 2.524088144302368, Test_Loss: 2.531723976135254\n",
      "Epoch: 3, Train_Loss: 2.5150134563446045, Test_Loss: 2.572497606277466\n",
      "Epoch: 3, Train_Loss: 2.4891061782836914, Test_Loss: 2.6300947666168213\n",
      "Epoch: 3, Train_Loss: 2.482576370239258, Test_Loss: 2.5801773071289062 *\n",
      "Epoch: 3, Train_Loss: 2.5028486251831055, Test_Loss: 2.5469000339508057 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 3\n",
      "Epoch: 3, Train_Loss: 2.5136160850524902, Test_Loss: 2.5162322521209717 *\n",
      "Epoch: 3, Train_Loss: 2.4997811317443848, Test_Loss: 2.8134512901306152\n",
      "Epoch: 3, Train_Loss: 2.4815993309020996, Test_Loss: 2.54533052444458 *\n",
      "Epoch: 3, Train_Loss: 2.474464178085327, Test_Loss: 2.576906442642212\n",
      "Epoch: 3, Train_Loss: 2.4733781814575195, Test_Loss: 2.4696269035339355 *\n",
      "Epoch: 3, Train_Loss: 2.470595359802246, Test_Loss: 2.4697859287261963\n",
      "Epoch: 3, Train_Loss: 2.4639599323272705, Test_Loss: 2.469982862472534\n",
      "Epoch: 3, Train_Loss: 2.4819321632385254, Test_Loss: 2.471285820007324\n",
      "Epoch: 3, Train_Loss: 2.4953255653381348, Test_Loss: 2.5007119178771973\n",
      "Epoch: 3, Train_Loss: 2.467367649078369, Test_Loss: 7.761929512023926\n",
      "Epoch: 3, Train_Loss: 2.45902943611145, Test_Loss: 2.6832275390625 *\n",
      "Epoch: 3, Train_Loss: 2.456847906112671, Test_Loss: 2.465587854385376 *\n",
      "Epoch: 3, Train_Loss: 2.4552996158599854, Test_Loss: 2.456676721572876 *\n",
      "Epoch: 3, Train_Loss: 2.4540674686431885, Test_Loss: 2.4587066173553467\n",
      "Epoch: 3, Train_Loss: 2.450061559677124, Test_Loss: 2.4612834453582764\n",
      "Epoch: 3, Train_Loss: 2.4484236240386963, Test_Loss: 2.4522931575775146 *\n",
      "Epoch: 3, Train_Loss: 2.4480626583099365, Test_Loss: 2.4527950286865234\n",
      "Epoch: 3, Train_Loss: 2.4456162452697754, Test_Loss: 2.4488377571105957 *\n",
      "Epoch: 3, Train_Loss: 2.443786382675171, Test_Loss: 2.447136878967285 *\n",
      "Epoch: 3, Train_Loss: 2.4422736167907715, Test_Loss: 2.4485747814178467\n",
      "Epoch: 3, Train_Loss: 2.4431068897247314, Test_Loss: 2.4500608444213867\n",
      "Epoch: 3, Train_Loss: 2.459348440170288, Test_Loss: 2.449770927429199 *\n",
      "Epoch: 3, Train_Loss: 2.464569091796875, Test_Loss: 2.458209753036499\n",
      "Epoch: 3, Train_Loss: 2.4448468685150146, Test_Loss: 2.449437141418457 *\n",
      "Epoch: 3, Train_Loss: 2.447018623352051, Test_Loss: 2.4436838626861572 *\n",
      "Epoch: 3, Train_Loss: 3.412480592727661, Test_Loss: 2.437709331512451 *\n",
      "Epoch: 3, Train_Loss: 10.779596328735352, Test_Loss: 2.4424188137054443\n",
      "Epoch: 3, Train_Loss: 2.494940757751465, Test_Loss: 2.4379167556762695 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_Loss: 2.4335508346557617, Test_Loss: 2.4349589347839355 *\n",
      "Epoch: 3, Train_Loss: 2.4334542751312256, Test_Loss: 2.4395053386688232\n",
      "Epoch: 3, Train_Loss: 2.448383331298828, Test_Loss: 2.4318156242370605 *\n",
      "Epoch: 3, Train_Loss: 2.4686038494110107, Test_Loss: 2.434278964996338\n",
      "Epoch: 3, Train_Loss: 2.4382519721984863, Test_Loss: 2.436457872390747\n",
      "Epoch: 3, Train_Loss: 2.4351518154144287, Test_Loss: 2.432281255722046 *\n",
      "Epoch: 3, Train_Loss: 2.6314287185668945, Test_Loss: 2.4291832447052 *\n",
      "Epoch: 3, Train_Loss: 2.61942720413208, Test_Loss: 2.4295573234558105\n",
      "Epoch: 3, Train_Loss: 2.543729782104492, Test_Loss: 2.4257590770721436 *\n",
      "Epoch: 3, Train_Loss: 2.445669174194336, Test_Loss: 2.4260764122009277\n",
      "Epoch: 3, Train_Loss: 2.539857864379883, Test_Loss: 2.4320051670074463\n",
      "Epoch: 3, Train_Loss: 2.5185110569000244, Test_Loss: 2.4769790172576904\n",
      "Epoch: 3, Train_Loss: 2.564500331878662, Test_Loss: 3.711503744125366\n",
      "Epoch: 3, Train_Loss: 2.54567289352417, Test_Loss: 6.88027286529541\n",
      "Epoch: 3, Train_Loss: 2.5276501178741455, Test_Loss: 2.425508975982666 *\n",
      "Epoch: 3, Train_Loss: 2.415799140930176, Test_Loss: 2.412177085876465 *\n",
      "Epoch: 3, Train_Loss: 2.449059009552002, Test_Loss: 2.4331765174865723\n",
      "Epoch: 3, Train_Loss: 2.4629735946655273, Test_Loss: 2.44161057472229\n",
      "Epoch: 3, Train_Loss: 2.4224960803985596, Test_Loss: 2.4429261684417725\n",
      "Epoch: 3, Train_Loss: 2.405230760574341, Test_Loss: 2.4295222759246826 *\n",
      "Epoch: 3, Train_Loss: 2.4021310806274414, Test_Loss: 2.5710766315460205\n",
      "Epoch: 3, Train_Loss: 2.4004194736480713, Test_Loss: 2.4247117042541504 *\n",
      "Epoch: 3, Train_Loss: 3.9021172523498535, Test_Loss: 2.40919828414917 *\n",
      "Epoch: 3, Train_Loss: 6.696468353271484, Test_Loss: 2.4544413089752197\n",
      "Epoch: 3, Train_Loss: 2.406440258026123, Test_Loss: 2.399200916290283 *\n",
      "Epoch: 3, Train_Loss: 2.408879041671753, Test_Loss: 2.4168074131011963\n",
      "Epoch: 3, Train_Loss: 2.4104907512664795, Test_Loss: 2.436256170272827\n",
      "Epoch: 3, Train_Loss: 2.4064652919769287, Test_Loss: 2.472559690475464\n",
      "Epoch: 3, Train_Loss: 2.3990392684936523, Test_Loss: 2.4815633296966553\n",
      "Epoch: 3, Train_Loss: 2.3930087089538574, Test_Loss: 2.5527753829956055\n",
      "Epoch: 3, Train_Loss: 2.3950560092926025, Test_Loss: 2.421715497970581 *\n",
      "Epoch: 3, Train_Loss: 2.4202537536621094, Test_Loss: 2.3991775512695312 *\n",
      "Epoch: 3, Train_Loss: 2.3983659744262695, Test_Loss: 2.387449026107788 *\n",
      "Epoch: 3, Train_Loss: 2.3873908519744873, Test_Loss: 2.3836536407470703 *\n",
      "Epoch: 3, Train_Loss: 2.3861873149871826, Test_Loss: 2.3824760913848877 *\n",
      "Epoch: 3, Train_Loss: 2.3831627368927, Test_Loss: 2.3809902667999268 *\n",
      "Epoch: 3, Train_Loss: 2.3988795280456543, Test_Loss: 2.3794984817504883 *\n",
      "Epoch: 3, Train_Loss: 2.379624128341675, Test_Loss: 2.3779187202453613 *\n",
      "Epoch: 3, Train_Loss: 2.3757450580596924, Test_Loss: 2.3851547241210938\n",
      "Epoch: 3, Train_Loss: 2.43208646774292, Test_Loss: 2.378352165222168 *\n",
      "Epoch: 3, Train_Loss: 2.4395291805267334, Test_Loss: 2.3818044662475586\n",
      "Epoch: 3, Train_Loss: 2.3783364295959473, Test_Loss: 2.372467041015625 *\n",
      "Epoch: 3, Train_Loss: 2.371933937072754, Test_Loss: 2.37860369682312\n",
      "Epoch: 3, Train_Loss: 2.37817645072937, Test_Loss: 2.423557996749878\n",
      "Epoch: 3, Train_Loss: 2.4705522060394287, Test_Loss: 2.371218681335449 *\n",
      "Epoch: 3, Train_Loss: 2.4444503784179688, Test_Loss: 2.7605526447296143\n",
      "Epoch: 3, Train_Loss: 2.458097457885742, Test_Loss: 2.8664138317108154\n",
      "Epoch: 3, Train_Loss: 2.408322811126709, Test_Loss: 2.5036816596984863 *\n",
      "Epoch: 3, Train_Loss: 2.4189586639404297, Test_Loss: 2.376830577850342 *\n",
      "Epoch: 3, Train_Loss: 2.4281139373779297, Test_Loss: 2.3738837242126465 *\n",
      "Epoch: 3, Train_Loss: 2.427980899810791, Test_Loss: 2.3920905590057373\n",
      "Epoch: 3, Train_Loss: 2.3613381385803223, Test_Loss: 2.5758109092712402\n",
      "Epoch: 3, Train_Loss: 2.460578680038452, Test_Loss: 3.725311279296875\n",
      "Epoch: 3, Train_Loss: 2.369363784790039, Test_Loss: 3.5878348350524902 *\n",
      "Epoch: 3, Train_Loss: 2.352212429046631, Test_Loss: 2.3940391540527344 *\n",
      "Epoch: 3, Train_Loss: 2.354574203491211, Test_Loss: 2.411421060562134\n",
      "Epoch: 3, Train_Loss: 2.352729082107544, Test_Loss: 2.3493402004241943 *\n",
      "Epoch: 3, Train_Loss: 2.351088285446167, Test_Loss: 2.3559536933898926\n",
      "Epoch: 3, Train_Loss: 2.34879994392395, Test_Loss: 2.3503615856170654 *\n",
      "Epoch: 3, Train_Loss: 3.8809797763824463, Test_Loss: 2.380199432373047\n",
      "Epoch: 3, Train_Loss: 5.827790260314941, Test_Loss: 2.4247894287109375\n",
      "Epoch: 3, Train_Loss: 2.3397059440612793, Test_Loss: 2.357475996017456 *\n",
      "Epoch: 3, Train_Loss: 2.3666396141052246, Test_Loss: 2.35366153717041 *\n",
      "Epoch: 3, Train_Loss: 2.361111640930176, Test_Loss: 2.432813882827759\n",
      "Epoch: 3, Train_Loss: 2.335249185562134, Test_Loss: 2.7060370445251465\n",
      "Epoch: 3, Train_Loss: 2.3351848125457764, Test_Loss: 2.510512113571167 *\n",
      "Epoch: 3, Train_Loss: 2.3336496353149414, Test_Loss: 2.3394148349761963 *\n",
      "Epoch: 3, Train_Loss: 2.332700729370117, Test_Loss: 2.3299124240875244 *\n",
      "Epoch: 3, Train_Loss: 2.330075740814209, Test_Loss: 2.3284990787506104 *\n",
      "Epoch: 3, Train_Loss: 2.3390772342681885, Test_Loss: 2.326918125152588 *\n",
      "Epoch: 3, Train_Loss: 2.4624195098876953, Test_Loss: 2.3251397609710693 *\n",
      "Epoch: 3, Train_Loss: 2.446216106414795, Test_Loss: 2.6686148643493652\n",
      "Model saved at location save_new\\model.ckpt at epoch 3\n",
      "Epoch: 3, Train_Loss: 2.468416452407837, Test_Loss: 7.641334056854248\n",
      "Epoch: 3, Train_Loss: 2.391279697418213, Test_Loss: 2.398341178894043 *\n",
      "Epoch: 3, Train_Loss: 2.3213915824890137, Test_Loss: 2.3241055011749268 *\n",
      "Epoch: 3, Train_Loss: 2.449399471282959, Test_Loss: 2.316948175430298 *\n",
      "Epoch: 3, Train_Loss: 2.483889579772949, Test_Loss: 2.316707134246826 *\n",
      "Epoch: 3, Train_Loss: 2.471679210662842, Test_Loss: 2.3177390098571777\n",
      "Epoch: 3, Train_Loss: 2.406151294708252, Test_Loss: 2.3136842250823975 *\n",
      "Epoch: 3, Train_Loss: 2.314861297607422, Test_Loss: 2.3207321166992188\n",
      "Epoch: 3, Train_Loss: 2.313556671142578, Test_Loss: 2.316941738128662 *\n",
      "Epoch: 3, Train_Loss: 2.30908465385437, Test_Loss: 2.3152575492858887 *\n",
      "Epoch: 3, Train_Loss: 2.3073995113372803, Test_Loss: 2.3137426376342773 *\n",
      "Epoch: 3, Train_Loss: 2.3078463077545166, Test_Loss: 2.3148436546325684\n",
      "Epoch: 3, Train_Loss: 2.3042635917663574, Test_Loss: 2.316875457763672\n",
      "Epoch: 3, Train_Loss: 2.30648136138916, Test_Loss: 2.3211722373962402\n",
      "Epoch: 3, Train_Loss: 2.3040428161621094, Test_Loss: 2.3126537799835205 *\n",
      "Epoch: 3, Train_Loss: 2.3005430698394775, Test_Loss: 2.305170774459839 *\n",
      "Epoch: 3, Train_Loss: 2.3296279907226562, Test_Loss: 2.3005928993225098 *\n",
      "Epoch: 3, Train_Loss: 2.459277868270874, Test_Loss: 2.3020682334899902\n",
      "Epoch: 3, Train_Loss: 2.4420325756073, Test_Loss: 2.2996912002563477 *\n",
      "Epoch: 3, Train_Loss: 2.418272018432617, Test_Loss: 2.2971208095550537 *\n",
      "Epoch: 3, Train_Loss: 2.403506278991699, Test_Loss: 2.298283815383911\n",
      "Epoch: 3, Train_Loss: 2.4769973754882812, Test_Loss: 2.293877363204956 *\n",
      "Epoch: 3, Train_Loss: 2.362945795059204, Test_Loss: 2.295278310775757\n",
      "Epoch: 3, Train_Loss: 2.44142484664917, Test_Loss: 2.2939021587371826 *\n",
      "Epoch: 3, Train_Loss: 2.4258008003234863, Test_Loss: 2.292053461074829 *\n",
      "Epoch: 3, Train_Loss: 2.612151622772217, Test_Loss: 2.2903225421905518 *\n",
      "Epoch: 3, Train_Loss: 2.2945504188537598, Test_Loss: 2.2896604537963867 *\n",
      "Epoch: 3, Train_Loss: 2.3300416469573975, Test_Loss: 2.285994291305542 *\n",
      "Epoch: 3, Train_Loss: 5.3871846199035645, Test_Loss: 2.2871973514556885\n",
      "Epoch: 3, Train_Loss: 2.552288055419922, Test_Loss: 2.295534610748291\n",
      "Epoch: 3, Train_Loss: 2.307884931564331, Test_Loss: 2.339176654815674\n",
      "Epoch: 3, Train_Loss: 2.320732593536377, Test_Loss: 4.780862808227539\n",
      "Epoch: 3, Train_Loss: 2.3183658123016357, Test_Loss: 5.510929107666016\n",
      "Epoch: 3, Train_Loss: 2.2849020957946777, Test_Loss: 2.28408145904541 *\n",
      "Epoch: 3, Train_Loss: 2.2792656421661377, Test_Loss: 2.2778499126434326 *\n",
      "Epoch: 3, Train_Loss: 2.3648104667663574, Test_Loss: 2.3035829067230225\n",
      "Epoch: 3, Train_Loss: 2.431830406188965, Test_Loss: 2.3077754974365234\n",
      "Epoch: 3, Train_Loss: 2.415299892425537, Test_Loss: 2.303433418273926 *\n",
      "Epoch: 3, Train_Loss: 2.3823509216308594, Test_Loss: 2.3272829055786133\n",
      "Epoch: 3, Train_Loss: 2.4049863815307617, Test_Loss: 2.4237775802612305\n",
      "Epoch: 3, Train_Loss: 2.318333387374878, Test_Loss: 2.268923282623291 *\n",
      "Epoch: 3, Train_Loss: 2.332292079925537, Test_Loss: 2.2982070446014404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_Loss: 2.265209436416626, Test_Loss: 2.296295642852783 *\n",
      "Epoch: 3, Train_Loss: 2.2729055881500244, Test_Loss: 2.2710347175598145 *\n",
      "Epoch: 3, Train_Loss: 2.264381170272827, Test_Loss: 2.2769315242767334\n",
      "Epoch: 3, Train_Loss: 2.260406255722046, Test_Loss: 2.318798780441284\n",
      "Epoch: 3, Train_Loss: 2.294583797454834, Test_Loss: 2.3255221843719482\n",
      "Epoch: 3, Train_Loss: 2.345966339111328, Test_Loss: 2.369379758834839\n",
      "Epoch: 3, Train_Loss: 2.3333933353424072, Test_Loss: 2.4244494438171387\n",
      "Epoch: 3, Train_Loss: 2.2527859210968018, Test_Loss: 2.2621495723724365 *\n",
      "Epoch: 3, Train_Loss: 2.2533578872680664, Test_Loss: 2.263068437576294\n",
      "Epoch: 3, Train_Loss: 2.251055955886841, Test_Loss: 2.2543253898620605 *\n",
      "Epoch: 3, Train_Loss: 2.2499043941497803, Test_Loss: 2.251180648803711 *\n",
      "Epoch: 3, Train_Loss: 2.248640537261963, Test_Loss: 2.249847650527954 *\n",
      "Epoch: 3, Train_Loss: 2.245997667312622, Test_Loss: 2.24845027923584 *\n",
      "Epoch: 3, Train_Loss: 2.242955207824707, Test_Loss: 2.247081995010376 *\n",
      "Epoch: 3, Train_Loss: 2.2437868118286133, Test_Loss: 2.245631694793701 *\n",
      "Epoch: 3, Train_Loss: 2.2405850887298584, Test_Loss: 2.253952741622925\n",
      "Epoch: 3, Train_Loss: 2.2396159172058105, Test_Loss: 2.2478883266448975 *\n",
      "Epoch: 3, Train_Loss: 2.240215539932251, Test_Loss: 2.247429847717285 *\n",
      "Epoch: 3, Train_Loss: 2.23569917678833, Test_Loss: 2.240262269973755 *\n",
      "Epoch: 3, Train_Loss: 2.234814167022705, Test_Loss: 2.252939462661743\n",
      "Epoch: 3, Train_Loss: 2.2354049682617188, Test_Loss: 2.289033889770508\n",
      "Epoch: 3, Train_Loss: 2.235440731048584, Test_Loss: 2.2512683868408203 *\n",
      "Epoch: 3, Train_Loss: 2.229963779449463, Test_Loss: 2.7127432823181152\n",
      "Epoch: 3, Train_Loss: 2.2319302558898926, Test_Loss: 2.6864542961120605 *\n",
      "Epoch: 3, Train_Loss: 2.2288730144500732, Test_Loss: 2.3401665687561035 *\n",
      "Epoch: 3, Train_Loss: 2.227886915206909, Test_Loss: 2.2453677654266357 *\n",
      "Epoch: 3, Train_Loss: 2.2271616458892822, Test_Loss: 2.2399747371673584 *\n",
      "Epoch: 3, Train_Loss: 2.2251479625701904, Test_Loss: 2.281717538833618\n",
      "Epoch: 3, Train_Loss: 2.222440481185913, Test_Loss: 2.582801342010498\n",
      "Epoch: 3, Train_Loss: 2.2231175899505615, Test_Loss: 3.7722744941711426\n",
      "Epoch: 3, Train_Loss: 2.319509506225586, Test_Loss: 3.1246535778045654 *\n",
      "Epoch: 3, Train_Loss: 2.2585887908935547, Test_Loss: 2.274998188018799 *\n",
      "Epoch: 3, Train_Loss: 2.2427124977111816, Test_Loss: 2.2497479915618896 *\n",
      "Epoch: 3, Train_Loss: 2.221040725708008, Test_Loss: 2.222358465194702 *\n",
      "Epoch: 3, Train_Loss: 2.2163643836975098, Test_Loss: 2.220679521560669 *\n",
      "Epoch: 3, Train_Loss: 2.251072645187378, Test_Loss: 2.2204396724700928 *\n",
      "Epoch: 3, Train_Loss: 2.212797164916992, Test_Loss: 2.2573204040527344\n",
      "Epoch: 3, Train_Loss: 2.2374374866485596, Test_Loss: 2.2981772422790527\n",
      "Epoch: 3, Train_Loss: 2.2658894062042236, Test_Loss: 2.2116048336029053 *\n",
      "Epoch: 3, Train_Loss: 2.231635808944702, Test_Loss: 2.244964838027954\n",
      "Epoch: 3, Train_Loss: 2.3925955295562744, Test_Loss: 2.330721855163574\n",
      "Epoch: 3, Train_Loss: 2.317124843597412, Test_Loss: 2.5314249992370605\n",
      "Epoch: 3, Train_Loss: 2.2585692405700684, Test_Loss: 2.3822999000549316 *\n",
      "Epoch: 3, Train_Loss: 2.2155792713165283, Test_Loss: 2.205277681350708 *\n",
      "Epoch: 3, Train_Loss: 2.2342402935028076, Test_Loss: 2.2002713680267334 *\n",
      "Epoch: 3, Train_Loss: 2.20491623878479, Test_Loss: 2.1989448070526123 *\n",
      "Epoch: 3, Train_Loss: 2.2099483013153076, Test_Loss: 2.1975176334381104 *\n",
      "Epoch: 3, Train_Loss: 2.2025747299194336, Test_Loss: 2.197348117828369 *\n",
      "Epoch: 3, Train_Loss: 2.2003891468048096, Test_Loss: 3.3879330158233643\n",
      "Epoch: 3, Train_Loss: 2.2260282039642334, Test_Loss: 6.716756820678711\n",
      "Epoch: 3, Train_Loss: 2.2417585849761963, Test_Loss: 2.2070112228393555 *\n",
      "Epoch: 3, Train_Loss: 2.2526021003723145, Test_Loss: 2.191861152648926 *\n",
      "Epoch: 3, Train_Loss: 2.2557594776153564, Test_Loss: 2.188351631164551 *\n",
      "Epoch: 3, Train_Loss: 2.2048842906951904, Test_Loss: 2.188258409500122 *\n",
      "Epoch: 3, Train_Loss: 2.189265489578247, Test_Loss: 2.1877596378326416 *\n",
      "Epoch: 3, Train_Loss: 2.325559616088867, Test_Loss: 2.1862876415252686 *\n",
      "Epoch: 3, Train_Loss: 2.354099750518799, Test_Loss: 2.191455602645874\n",
      "Epoch: 3, Train_Loss: 2.180025815963745, Test_Loss: 2.1880905628204346 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 3\n",
      "Epoch: 3, Train_Loss: 2.205460548400879, Test_Loss: 2.188174247741699\n",
      "Epoch: 3, Train_Loss: 2.1828911304473877, Test_Loss: 2.1827616691589355 *\n",
      "Epoch: 3, Train_Loss: 2.1815595626831055, Test_Loss: 2.1855666637420654\n",
      "Epoch: 3, Train_Loss: 2.1811509132385254, Test_Loss: 2.1939895153045654\n",
      "Epoch: 3, Train_Loss: 2.18023419380188, Test_Loss: 2.1930131912231445 *\n",
      "Epoch: 3, Train_Loss: 2.2083466053009033, Test_Loss: 2.1826207637786865 *\n",
      "Epoch: 3, Train_Loss: 2.2164950370788574, Test_Loss: 2.1751859188079834 *\n",
      "Epoch: 3, Train_Loss: 2.2025725841522217, Test_Loss: 2.1735897064208984 *\n",
      "Epoch: 3, Train_Loss: 2.2065391540527344, Test_Loss: 2.1726295948028564 *\n",
      "Epoch: 3, Train_Loss: 2.213301658630371, Test_Loss: 2.171976327896118 *\n",
      "Epoch: 3, Train_Loss: 2.1711766719818115, Test_Loss: 2.1710987091064453 *\n",
      "Epoch: 3, Train_Loss: 2.1729533672332764, Test_Loss: 2.1681323051452637 *\n",
      "Epoch: 3, Train_Loss: 2.1683905124664307, Test_Loss: 2.1667237281799316 *\n",
      "Epoch: 3, Train_Loss: 2.1789379119873047, Test_Loss: 2.1683578491210938\n",
      "Epoch: 3, Train_Loss: 2.179385185241699, Test_Loss: 2.164560317993164 *\n",
      "Epoch: 3, Train_Loss: 2.1684725284576416, Test_Loss: 2.1650240421295166\n",
      "Epoch: 3, Train_Loss: 2.18404221534729, Test_Loss: 2.163161277770996 *\n",
      "Epoch: 3, Train_Loss: 2.251582622528076, Test_Loss: 2.16214656829834 *\n",
      "Epoch: 3, Train_Loss: 2.2410902976989746, Test_Loss: 2.159363269805908 *\n",
      "Epoch: 3, Train_Loss: 2.1834096908569336, Test_Loss: 2.159618616104126\n",
      "Epoch: 3, Train_Loss: 2.1618611812591553, Test_Loss: 2.1900739669799805\n",
      "Epoch: 3, Train_Loss: 2.1636078357696533, Test_Loss: 2.1923158168792725\n",
      "Epoch: 3, Train_Loss: 2.1709792613983154, Test_Loss: 5.669358253479004\n",
      "Epoch: 3, Train_Loss: 2.1691181659698486, Test_Loss: 4.362462043762207 *\n",
      "Epoch: 3, Train_Loss: 2.167003870010376, Test_Loss: 2.1539812088012695 *\n",
      "Epoch: 3, Train_Loss: 2.189713954925537, Test_Loss: 2.1519625186920166 *\n",
      "Epoch: 3, Train_Loss: 4.46372652053833, Test_Loss: 2.18300724029541\n",
      "Epoch: 3, Train_Loss: 5.270140647888184, Test_Loss: 2.1916451454162598\n",
      "Epoch: 3, Train_Loss: 2.162616491317749, Test_Loss: 2.1625239849090576 *\n",
      "Epoch: 3, Train_Loss: 2.156938076019287, Test_Loss: 2.2285943031311035\n",
      "Epoch: 3, Train_Loss: 2.2126054763793945, Test_Loss: 2.2730588912963867\n",
      "Epoch: 3, Train_Loss: 2.3753886222839355, Test_Loss: 2.1398181915283203 *\n",
      "Epoch: 3, Train_Loss: 2.186107635498047, Test_Loss: 2.186948776245117\n",
      "Epoch: 3, Train_Loss: 2.1530401706695557, Test_Loss: 2.1566567420959473 *\n",
      "Epoch: 3, Train_Loss: 2.151843309402466, Test_Loss: 2.151676893234253 *\n",
      "Epoch: 3, Train_Loss: 2.244185447692871, Test_Loss: 2.144648313522339 *\n",
      "Epoch: 3, Train_Loss: 2.142350912094116, Test_Loss: 2.2159841060638428\n",
      "Epoch: 3, Train_Loss: 2.1397364139556885, Test_Loss: 2.1817855834960938 *\n",
      "Epoch: 3, Train_Loss: 3.1091082096099854, Test_Loss: 2.2734787464141846\n",
      "Epoch: 3, Train_Loss: 3.696572780609131, Test_Loss: 2.270392417907715 *\n",
      "Epoch: 3, Train_Loss: 2.8257999420166016, Test_Loss: 2.1400749683380127 *\n",
      "Epoch: 3, Train_Loss: 2.280749797821045, Test_Loss: 2.1359410285949707 *\n",
      "Epoch: 3, Train_Loss: 2.918677806854248, Test_Loss: 2.1365854740142822\n",
      "Epoch: 3, Train_Loss: 4.368538856506348, Test_Loss: 2.131795883178711 *\n",
      "Epoch: 3, Train_Loss: 2.448594808578491, Test_Loss: 2.1299619674682617 *\n",
      "Epoch: 3, Train_Loss: 2.142826557159424, Test_Loss: 2.127689838409424 *\n",
      "Epoch: 3, Train_Loss: 2.1296730041503906, Test_Loss: 2.1246559619903564 *\n",
      "Epoch: 3, Train_Loss: 3.6775217056274414, Test_Loss: 2.1217288970947266 *\n",
      "Epoch: 3, Train_Loss: 3.763371229171753, Test_Loss: 2.1318612098693848\n",
      "Epoch: 3, Train_Loss: 2.2916877269744873, Test_Loss: 2.1280698776245117 *\n",
      "Epoch: 3, Train_Loss: 2.137761354446411, Test_Loss: 2.1220178604125977 *\n",
      "Epoch: 3, Train_Loss: 2.1137330532073975, Test_Loss: 2.118605136871338 *\n",
      "Epoch: 3, Train_Loss: 2.7548699378967285, Test_Loss: 2.1394832134246826\n",
      "Epoch: 3, Train_Loss: 2.2387495040893555, Test_Loss: 2.138684034347534 *\n",
      "Epoch: 4, Train_Loss: 2.138735771179199, Test_Loss: 2.187974691390991 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_Loss: 2.136301040649414, Test_Loss: 2.635755777359009\n",
      "Epoch: 4, Train_Loss: 2.233294725418091, Test_Loss: 2.503710985183716 *\n",
      "Epoch: 4, Train_Loss: 2.1924517154693604, Test_Loss: 2.211167097091675 *\n",
      "Epoch: 4, Train_Loss: 2.275733232498169, Test_Loss: 2.1099281311035156 *\n",
      "Epoch: 4, Train_Loss: 2.5451154708862305, Test_Loss: 2.1220450401306152\n",
      "Epoch: 4, Train_Loss: 2.2224483489990234, Test_Loss: 2.1489179134368896\n",
      "Epoch: 4, Train_Loss: 2.1694247722625732, Test_Loss: 2.5163044929504395\n",
      "Epoch: 4, Train_Loss: 2.3048200607299805, Test_Loss: 3.3419578075408936\n",
      "Epoch: 4, Train_Loss: 2.3538949489593506, Test_Loss: 2.678896427154541 *\n",
      "Epoch: 4, Train_Loss: 2.3301808834075928, Test_Loss: 2.1357109546661377 *\n",
      "Epoch: 4, Train_Loss: 2.155473232269287, Test_Loss: 2.1226584911346436 *\n",
      "Epoch: 4, Train_Loss: 2.247326612472534, Test_Loss: 2.1529994010925293\n",
      "Epoch: 4, Train_Loss: 2.334362268447876, Test_Loss: 2.1612346172332764\n",
      "Epoch: 4, Train_Loss: 2.146552801132202, Test_Loss: 2.1838502883911133\n",
      "Epoch: 4, Train_Loss: 2.1402111053466797, Test_Loss: 2.117985248565674 *\n",
      "Epoch: 4, Train_Loss: 2.10667085647583, Test_Loss: 2.190800428390503\n",
      "Epoch: 4, Train_Loss: 2.098834991455078, Test_Loss: 2.09322452545166 *\n",
      "Epoch: 4, Train_Loss: 2.090373992919922, Test_Loss: 2.147369384765625\n",
      "Epoch: 4, Train_Loss: 2.093686819076538, Test_Loss: 2.2584049701690674\n",
      "Epoch: 4, Train_Loss: 2.179647445678711, Test_Loss: 2.3781728744506836\n",
      "Epoch: 4, Train_Loss: 2.13271427154541, Test_Loss: 2.297424793243408 *\n",
      "Epoch: 4, Train_Loss: 2.179692506790161, Test_Loss: 2.082369565963745 *\n",
      "Epoch: 4, Train_Loss: 2.188178777694702, Test_Loss: 2.0809757709503174 *\n",
      "Epoch: 4, Train_Loss: 2.581926107406616, Test_Loss: 2.080961227416992 *\n",
      "Epoch: 4, Train_Loss: 2.0829548835754395, Test_Loss: 2.0802133083343506 *\n",
      "Epoch: 4, Train_Loss: 2.109609603881836, Test_Loss: 2.0979762077331543\n",
      "Epoch: 4, Train_Loss: 2.243098020553589, Test_Loss: 4.338695526123047\n",
      "Epoch: 4, Train_Loss: 2.5616021156311035, Test_Loss: 5.177586555480957\n",
      "Epoch: 4, Train_Loss: 2.391427516937256, Test_Loss: 2.07987117767334 *\n",
      "Epoch: 4, Train_Loss: 2.07000470161438, Test_Loss: 2.0714051723480225 *\n",
      "Epoch: 4, Train_Loss: 2.1519298553466797, Test_Loss: 2.0697021484375 *\n",
      "Epoch: 4, Train_Loss: 2.7385177612304688, Test_Loss: 2.0781636238098145\n",
      "Epoch: 4, Train_Loss: 2.6769320964813232, Test_Loss: 2.0675148963928223 *\n",
      "Epoch: 4, Train_Loss: 2.1374173164367676, Test_Loss: 2.0702261924743652\n",
      "Epoch: 4, Train_Loss: 2.084625482559204, Test_Loss: 2.0667641162872314 *\n",
      "Epoch: 4, Train_Loss: 2.0704774856567383, Test_Loss: 2.0669426918029785\n",
      "Epoch: 4, Train_Loss: 2.9938790798187256, Test_Loss: 2.0683815479278564\n",
      "Epoch: 4, Train_Loss: 3.017944574356079, Test_Loss: 2.064406394958496 *\n",
      "Epoch: 4, Train_Loss: 2.074709177017212, Test_Loss: 2.067966938018799\n",
      "Epoch: 4, Train_Loss: 2.0763463973999023, Test_Loss: 2.074692487716675\n",
      "Epoch: 4, Train_Loss: 2.057412624359131, Test_Loss: 2.0684595108032227 *\n",
      "Epoch: 4, Train_Loss: 2.0881664752960205, Test_Loss: 2.061211109161377 *\n",
      "Epoch: 4, Train_Loss: 2.575854778289795, Test_Loss: 2.056483030319214 *\n",
      "Epoch: 4, Train_Loss: 2.0582993030548096, Test_Loss: 2.055722951889038 *\n",
      "Epoch: 4, Train_Loss: 2.227644681930542, Test_Loss: 2.057218313217163\n",
      "Epoch: 4, Train_Loss: 2.1575582027435303, Test_Loss: 2.0541419982910156 *\n",
      "Epoch: 4, Train_Loss: 2.0714216232299805, Test_Loss: 2.05182147026062 *\n",
      "Epoch: 4, Train_Loss: 2.109450578689575, Test_Loss: 2.0491809844970703 *\n",
      "Epoch: 4, Train_Loss: 2.254755735397339, Test_Loss: 2.0508670806884766\n",
      "Epoch: 4, Train_Loss: 2.2508628368377686, Test_Loss: 2.0713446140289307\n",
      "Epoch: 4, Train_Loss: 2.058396577835083, Test_Loss: 2.0572314262390137 *\n",
      "Epoch: 4, Train_Loss: 2.081343173980713, Test_Loss: 2.056492567062378 *\n",
      "Epoch: 4, Train_Loss: 2.137908458709717, Test_Loss: 2.0474796295166016 *\n",
      "Epoch: 4, Train_Loss: 2.0857441425323486, Test_Loss: 2.0551698207855225\n",
      "Epoch: 4, Train_Loss: 2.1058974266052246, Test_Loss: 2.0564708709716797\n",
      "Epoch: 4, Train_Loss: 2.0463531017303467, Test_Loss: 2.048922061920166 *\n",
      "Epoch: 4, Train_Loss: 2.064319610595703, Test_Loss: 2.1351327896118164\n",
      "Epoch: 4, Train_Loss: 2.0501010417938232, Test_Loss: 2.090162754058838 *\n",
      "Epoch: 4, Train_Loss: 2.452932119369507, Test_Loss: 6.737853050231934\n",
      "Epoch: 4, Train_Loss: 2.291593551635742, Test_Loss: 2.8504738807678223 *\n",
      "Epoch: 4, Train_Loss: 2.5753438472747803, Test_Loss: 2.0482397079467773 *\n",
      "Epoch: 4, Train_Loss: 2.4841222763061523, Test_Loss: 2.059899091720581\n",
      "Epoch: 4, Train_Loss: 2.238433837890625, Test_Loss: 2.0569002628326416 *\n",
      "Epoch: 4, Train_Loss: 2.3230903148651123, Test_Loss: 2.04423189163208 *\n",
      "Epoch: 4, Train_Loss: 2.104691743850708, Test_Loss: 2.035809278488159 *\n",
      "Epoch: 4, Train_Loss: 2.035121440887451, Test_Loss: 2.096043825149536\n",
      "Epoch: 4, Train_Loss: 2.0367181301116943, Test_Loss: 2.071640968322754 *\n",
      "Epoch: 4, Train_Loss: 2.0681354999542236, Test_Loss: 2.0280163288116455 *\n",
      "Epoch: 4, Train_Loss: 2.3101792335510254, Test_Loss: 2.0445289611816406\n",
      "Epoch: 4, Train_Loss: 2.5199496746063232, Test_Loss: 2.0911357402801514\n",
      "Epoch: 4, Train_Loss: 2.6764087677001953, Test_Loss: 2.0558502674102783 *\n",
      "Epoch: 4, Train_Loss: 3.444377899169922, Test_Loss: 2.048206090927124 *\n",
      "Epoch: 4, Train_Loss: 2.4236702919006348, Test_Loss: 2.0341286659240723 *\n",
      "Epoch: 4, Train_Loss: 2.3085241317749023, Test_Loss: 2.1098999977111816\n",
      "Epoch: 4, Train_Loss: 2.053783416748047, Test_Loss: 2.029104709625244 *\n",
      "Epoch: 4, Train_Loss: 2.0566210746765137, Test_Loss: 2.0735056400299072\n",
      "Epoch: 4, Train_Loss: 2.5347604751586914, Test_Loss: 2.1826705932617188\n",
      "Epoch: 4, Train_Loss: 3.516608953475952, Test_Loss: 2.021791458129883 *\n",
      "Epoch: 4, Train_Loss: 2.1442556381225586, Test_Loss: 2.03756046295166\n",
      "Epoch: 4, Train_Loss: 2.0991499423980713, Test_Loss: 2.0512852668762207\n",
      "Epoch: 4, Train_Loss: 2.075371742248535, Test_Loss: 2.0429766178131104 *\n",
      "Epoch: 4, Train_Loss: 2.158583879470825, Test_Loss: 2.036257028579712 *\n",
      "Epoch: 4, Train_Loss: 2.2919652462005615, Test_Loss: 2.0343427658081055 *\n",
      "Epoch: 4, Train_Loss: 2.500112295150757, Test_Loss: 2.049823522567749\n",
      "Epoch: 4, Train_Loss: 2.690699577331543, Test_Loss: 2.033837080001831 *\n",
      "Epoch: 4, Train_Loss: 2.3793134689331055, Test_Loss: 2.0412416458129883\n",
      "Epoch: 4, Train_Loss: 2.0383894443511963, Test_Loss: 2.081451892852783\n",
      "Epoch: 4, Train_Loss: 2.013606071472168, Test_Loss: 2.1260602474212646\n",
      "Epoch: 4, Train_Loss: 2.007408857345581, Test_Loss: 2.02146053314209 *\n",
      "Epoch: 4, Train_Loss: 2.0975217819213867, Test_Loss: 2.0661301612854004\n",
      "Epoch: 4, Train_Loss: 2.0041143894195557, Test_Loss: 2.1168372631073\n",
      "Epoch: 4, Train_Loss: 2.0257728099823, Test_Loss: 2.2240147590637207\n",
      "Epoch: 4, Train_Loss: 17.805269241333008, Test_Loss: 2.1463375091552734 *\n",
      "Epoch: 4, Train_Loss: 3.0432896614074707, Test_Loss: 2.052070379257202 *\n",
      "Epoch: 4, Train_Loss: 3.1408748626708984, Test_Loss: 1.9940093755722046 *\n",
      "Epoch: 4, Train_Loss: 4.438905715942383, Test_Loss: 2.0071914196014404\n",
      "Epoch: 4, Train_Loss: 2.039092779159546, Test_Loss: 2.02300763130188\n",
      "Epoch: 4, Train_Loss: 2.065263271331787, Test_Loss: 2.3096749782562256\n",
      "Epoch: 4, Train_Loss: 3.3495430946350098, Test_Loss: 2.586719512939453\n",
      "Model saved at location save_new\\model.ckpt at epoch 4\n",
      "Epoch: 4, Train_Loss: 11.706604957580566, Test_Loss: 2.276897668838501 *\n",
      "Epoch: 4, Train_Loss: 2.376162052154541, Test_Loss: 2.029141426086426 *\n",
      "Epoch: 4, Train_Loss: 2.0236973762512207, Test_Loss: 2.0479297637939453\n",
      "Epoch: 4, Train_Loss: 7.067935466766357, Test_Loss: 2.1062774658203125\n",
      "Epoch: 4, Train_Loss: 2.69256591796875, Test_Loss: 2.1372382640838623\n",
      "Epoch: 4, Train_Loss: 2.0522959232330322, Test_Loss: 2.1718170642852783\n",
      "Epoch: 4, Train_Loss: 2.008108139038086, Test_Loss: 2.1881766319274902\n",
      "Epoch: 4, Train_Loss: 1.999729037284851, Test_Loss: 2.283947706222534\n",
      "Epoch: 4, Train_Loss: 2.021327495574951, Test_Loss: 2.1512861251831055 *\n",
      "Epoch: 4, Train_Loss: 2.0339207649230957, Test_Loss: 2.051546573638916 *\n",
      "Epoch: 4, Train_Loss: 2.029245376586914, Test_Loss: 2.363161563873291\n",
      "Epoch: 4, Train_Loss: 1.9910985231399536, Test_Loss: 2.015486478805542 *\n",
      "Epoch: 4, Train_Loss: 1.979461908340454, Test_Loss: 2.0818471908569336\n",
      "Epoch: 4, Train_Loss: 1.986245036125183, Test_Loss: 1.9907766580581665 *\n",
      "Epoch: 4, Train_Loss: 1.983563780784607, Test_Loss: 1.9790318012237549 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_Loss: 1.9750542640686035, Test_Loss: 1.9696027040481567 *\n",
      "Epoch: 4, Train_Loss: 1.9931682348251343, Test_Loss: 1.9684571027755737 *\n",
      "Epoch: 4, Train_Loss: 2.021427631378174, Test_Loss: 1.9751145839691162\n",
      "Epoch: 4, Train_Loss: 1.9981926679611206, Test_Loss: 5.976499080657959\n",
      "Epoch: 4, Train_Loss: 1.9732354879379272, Test_Loss: 3.7684671878814697 *\n",
      "Epoch: 4, Train_Loss: 1.9690942764282227, Test_Loss: 1.9854031801223755 *\n",
      "Epoch: 4, Train_Loss: 1.9645811319351196, Test_Loss: 1.9739339351654053 *\n",
      "Epoch: 4, Train_Loss: 1.9610698223114014, Test_Loss: 1.9733790159225464 *\n",
      "Epoch: 4, Train_Loss: 1.9614466428756714, Test_Loss: 1.9643206596374512 *\n",
      "Epoch: 4, Train_Loss: 1.9610087871551514, Test_Loss: 1.9664883613586426\n",
      "Epoch: 4, Train_Loss: 1.963388204574585, Test_Loss: 1.9733844995498657\n",
      "Epoch: 4, Train_Loss: 1.9590883255004883, Test_Loss: 1.9631437063217163 *\n",
      "Epoch: 4, Train_Loss: 1.9567800760269165, Test_Loss: 1.9652904272079468\n",
      "Epoch: 4, Train_Loss: 1.9558913707733154, Test_Loss: 1.966363787651062\n",
      "Epoch: 4, Train_Loss: 1.9553033113479614, Test_Loss: 1.9659613370895386 *\n",
      "Epoch: 4, Train_Loss: 1.9605039358139038, Test_Loss: 1.9632610082626343 *\n",
      "Epoch: 4, Train_Loss: 1.9789443016052246, Test_Loss: 1.9659669399261475\n",
      "Epoch: 4, Train_Loss: 1.9643100500106812, Test_Loss: 1.9595493078231812 *\n",
      "Epoch: 4, Train_Loss: 1.9626610279083252, Test_Loss: 1.9643126726150513\n",
      "Epoch: 4, Train_Loss: 1.9611670970916748, Test_Loss: 1.9515714645385742 *\n",
      "Epoch: 4, Train_Loss: 11.17483139038086, Test_Loss: 1.9587430953979492\n",
      "Epoch: 4, Train_Loss: 2.15439772605896, Test_Loss: 1.9539096355438232 *\n",
      "Epoch: 4, Train_Loss: 1.9540690183639526, Test_Loss: 1.9525091648101807 *\n",
      "Epoch: 4, Train_Loss: 1.951184630393982, Test_Loss: 1.9576224088668823\n",
      "Epoch: 4, Train_Loss: 1.9584001302719116, Test_Loss: 1.9524641036987305 *\n",
      "Epoch: 4, Train_Loss: 1.9670610427856445, Test_Loss: 1.9625213146209717\n",
      "Epoch: 4, Train_Loss: 1.9551496505737305, Test_Loss: 1.9789507389068604\n",
      "Epoch: 4, Train_Loss: 1.9641093015670776, Test_Loss: 1.9543801546096802 *\n",
      "Epoch: 4, Train_Loss: 2.1018993854522705, Test_Loss: 1.9534810781478882 *\n",
      "Epoch: 4, Train_Loss: 2.1280980110168457, Test_Loss: 1.9467387199401855 *\n",
      "Epoch: 4, Train_Loss: 2.0937695503234863, Test_Loss: 1.9523754119873047\n",
      "Epoch: 4, Train_Loss: 1.9461915493011475, Test_Loss: 1.9479773044586182 *\n",
      "Epoch: 4, Train_Loss: 2.043875217437744, Test_Loss: 1.9470494985580444 *\n",
      "Epoch: 4, Train_Loss: 2.035208225250244, Test_Loss: 1.9955883026123047\n",
      "Epoch: 4, Train_Loss: 2.075817108154297, Test_Loss: 1.9816814661026 *\n",
      "Epoch: 4, Train_Loss: 2.065713405609131, Test_Loss: 7.5494561195373535\n",
      "Epoch: 4, Train_Loss: 2.0564844608306885, Test_Loss: 2.0498270988464355 *\n",
      "Epoch: 4, Train_Loss: 1.9646321535110474, Test_Loss: 1.9351487159729004 *\n",
      "Epoch: 4, Train_Loss: 1.953432559967041, Test_Loss: 1.9469568729400635\n",
      "Epoch: 4, Train_Loss: 1.9997632503509521, Test_Loss: 1.9686022996902466\n",
      "Epoch: 4, Train_Loss: 1.951745629310608, Test_Loss: 1.9723989963531494\n",
      "Epoch: 4, Train_Loss: 1.9329606294631958, Test_Loss: 1.9305065870285034 *\n",
      "Epoch: 4, Train_Loss: 1.928598165512085, Test_Loss: 2.0680408477783203\n",
      "Epoch: 4, Train_Loss: 1.9270683526992798, Test_Loss: 2.002657651901245 *\n",
      "Epoch: 4, Train_Loss: 2.1632275581359863, Test_Loss: 1.9241819381713867 *\n",
      "Epoch: 4, Train_Loss: 7.471540451049805, Test_Loss: 1.9929639101028442\n",
      "Epoch: 4, Train_Loss: 1.937029242515564, Test_Loss: 1.9247289896011353 *\n",
      "Epoch: 4, Train_Loss: 1.9366135597229004, Test_Loss: 1.9460153579711914\n",
      "Epoch: 4, Train_Loss: 1.938151240348816, Test_Loss: 1.9364221096038818 *\n",
      "Epoch: 4, Train_Loss: 1.9362435340881348, Test_Loss: 2.018446207046509\n",
      "Epoch: 4, Train_Loss: 1.9309080839157104, Test_Loss: 1.9626492261886597 *\n",
      "Epoch: 4, Train_Loss: 1.9214290380477905, Test_Loss: 2.0860307216644287\n",
      "Epoch: 4, Train_Loss: 1.9231990575790405, Test_Loss: 1.998649001121521 *\n",
      "Epoch: 4, Train_Loss: 1.9489861726760864, Test_Loss: 1.9298301935195923 *\n",
      "Epoch: 4, Train_Loss: 1.9334311485290527, Test_Loss: 1.9181207418441772 *\n",
      "Epoch: 4, Train_Loss: 1.9202640056610107, Test_Loss: 1.9148204326629639 *\n",
      "Epoch: 4, Train_Loss: 1.919013261795044, Test_Loss: 1.9140108823776245 *\n",
      "Epoch: 4, Train_Loss: 1.9158380031585693, Test_Loss: 1.913177251815796 *\n",
      "Epoch: 4, Train_Loss: 1.9337193965911865, Test_Loss: 1.9122802019119263 *\n",
      "Epoch: 4, Train_Loss: 1.911558747291565, Test_Loss: 1.9112186431884766 *\n",
      "Epoch: 4, Train_Loss: 1.9095040559768677, Test_Loss: 1.911034345626831 *\n",
      "Epoch: 4, Train_Loss: 1.9500417709350586, Test_Loss: 1.9167156219482422\n",
      "Epoch: 4, Train_Loss: 1.973436713218689, Test_Loss: 1.918542504310608\n",
      "Epoch: 4, Train_Loss: 1.9285775423049927, Test_Loss: 1.9084633588790894 *\n",
      "Epoch: 4, Train_Loss: 1.906232237815857, Test_Loss: 1.9107245206832886\n",
      "Epoch: 4, Train_Loss: 1.9061020612716675, Test_Loss: 1.9499118328094482\n",
      "Epoch: 4, Train_Loss: 1.9860570430755615, Test_Loss: 1.9184893369674683 *\n",
      "Epoch: 4, Train_Loss: 1.994797706604004, Test_Loss: 2.1532480716705322\n",
      "Epoch: 4, Train_Loss: 1.9910966157913208, Test_Loss: 2.428745746612549\n",
      "Epoch: 4, Train_Loss: 1.9383373260498047, Test_Loss: 2.1010324954986572 *\n",
      "Epoch: 4, Train_Loss: 1.9302308559417725, Test_Loss: 1.952802300453186 *\n",
      "Epoch: 4, Train_Loss: 1.9783878326416016, Test_Loss: 1.9155051708221436 *\n",
      "Epoch: 4, Train_Loss: 1.9780808687210083, Test_Loss: 1.9155726432800293\n",
      "Epoch: 4, Train_Loss: 1.9060174226760864, Test_Loss: 2.0368599891662598\n",
      "Epoch: 4, Train_Loss: 2.012483596801758, Test_Loss: 2.871420383453369\n",
      "Epoch: 4, Train_Loss: 1.9021351337432861, Test_Loss: 3.4462480545043945\n",
      "Epoch: 4, Train_Loss: 1.9017655849456787, Test_Loss: 2.0625174045562744 *\n",
      "Epoch: 4, Train_Loss: 1.895097017288208, Test_Loss: 1.9663934707641602 *\n",
      "Epoch: 4, Train_Loss: 1.8938782215118408, Test_Loss: 1.889697790145874 *\n",
      "Epoch: 4, Train_Loss: 1.8928121328353882, Test_Loss: 1.9008574485778809\n",
      "Epoch: 4, Train_Loss: 1.892171859741211, Test_Loss: 1.8938443660736084 *\n",
      "Epoch: 4, Train_Loss: 2.1459879875183105, Test_Loss: 1.9082980155944824\n",
      "Epoch: 4, Train_Loss: 6.6046142578125, Test_Loss: 1.9467967748641968\n",
      "Epoch: 4, Train_Loss: 1.920959711074829, Test_Loss: 1.9392387866973877 *\n",
      "Epoch: 4, Train_Loss: 1.9038512706756592, Test_Loss: 1.8941707611083984 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 4\n",
      "Epoch: 4, Train_Loss: 1.9111580848693848, Test_Loss: 1.9698035717010498\n",
      "Epoch: 4, Train_Loss: 1.8823531866073608, Test_Loss: 2.260847568511963\n",
      "Epoch: 4, Train_Loss: 1.880071759223938, Test_Loss: 1.9428373575210571 *\n",
      "Epoch: 4, Train_Loss: 1.8812519311904907, Test_Loss: 2.010493278503418\n",
      "Epoch: 4, Train_Loss: 1.8798638582229614, Test_Loss: 1.8778295516967773 *\n",
      "Epoch: 4, Train_Loss: 1.877868413925171, Test_Loss: 1.8767629861831665 *\n",
      "Epoch: 4, Train_Loss: 1.876319408416748, Test_Loss: 1.8757433891296387 *\n",
      "Epoch: 4, Train_Loss: 1.988472580909729, Test_Loss: 1.8745089769363403 *\n",
      "Epoch: 4, Train_Loss: 1.990513801574707, Test_Loss: 1.8815205097198486\n",
      "Epoch: 4, Train_Loss: 2.013720989227295, Test_Loss: 7.004180431365967\n",
      "Epoch: 4, Train_Loss: 1.966604471206665, Test_Loss: 2.4546003341674805 *\n",
      "Epoch: 4, Train_Loss: 1.8712078332901, Test_Loss: 1.8753706216812134 *\n",
      "Epoch: 4, Train_Loss: 1.9643888473510742, Test_Loss: 1.8680957555770874 *\n",
      "Epoch: 4, Train_Loss: 2.0308263301849365, Test_Loss: 1.867118239402771 *\n",
      "Epoch: 4, Train_Loss: 2.028012990951538, Test_Loss: 1.8703348636627197\n",
      "Epoch: 4, Train_Loss: 1.9996885061264038, Test_Loss: 1.866217851638794 *\n",
      "Epoch: 4, Train_Loss: 1.8712773323059082, Test_Loss: 1.8724502325057983\n",
      "Epoch: 4, Train_Loss: 1.8651328086853027, Test_Loss: 1.8681514263153076 *\n",
      "Epoch: 4, Train_Loss: 1.863830804824829, Test_Loss: 1.8678503036499023 *\n",
      "Epoch: 4, Train_Loss: 1.8619918823242188, Test_Loss: 1.8698289394378662\n",
      "Epoch: 4, Train_Loss: 1.8629790544509888, Test_Loss: 1.8719205856323242\n",
      "Epoch: 4, Train_Loss: 1.8599865436553955, Test_Loss: 1.8648245334625244 *\n",
      "Epoch: 4, Train_Loss: 1.8606101274490356, Test_Loss: 1.8734618425369263\n",
      "Epoch: 4, Train_Loss: 1.860356092453003, Test_Loss: 1.868087649345398 *\n",
      "Epoch: 4, Train_Loss: 1.8572471141815186, Test_Loss: 1.8645353317260742 *\n",
      "Epoch: 4, Train_Loss: 1.8629298210144043, Test_Loss: 1.8564231395721436 *\n",
      "Epoch: 4, Train_Loss: 2.0027291774749756, Test_Loss: 1.8616516590118408\n",
      "Epoch: 4, Train_Loss: 2.0053915977478027, Test_Loss: 1.8567503690719604 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_Loss: 2.014763593673706, Test_Loss: 1.8541933298110962 *\n",
      "Epoch: 4, Train_Loss: 1.911895513534546, Test_Loss: 1.8594087362289429\n",
      "Epoch: 4, Train_Loss: 2.026811361312866, Test_Loss: 1.852895975112915 *\n",
      "Epoch: 4, Train_Loss: 1.9748902320861816, Test_Loss: 1.8568205833435059\n",
      "Epoch: 4, Train_Loss: 1.9563642740249634, Test_Loss: 1.860227108001709\n",
      "Epoch: 4, Train_Loss: 1.9956849813461304, Test_Loss: 1.8528821468353271 *\n",
      "Epoch: 4, Train_Loss: 2.1939892768859863, Test_Loss: 1.851060152053833 *\n",
      "Epoch: 4, Train_Loss: 1.8604353666305542, Test_Loss: 1.8519618511199951\n",
      "Epoch: 4, Train_Loss: 1.8658846616744995, Test_Loss: 1.849025011062622 *\n",
      "Epoch: 4, Train_Loss: 4.479223251342773, Test_Loss: 1.8489010334014893 *\n",
      "Epoch: 4, Train_Loss: 2.5941431522369385, Test_Loss: 1.8523077964782715\n",
      "Epoch: 4, Train_Loss: 1.8652845621109009, Test_Loss: 1.9044520854949951\n",
      "Epoch: 4, Train_Loss: 1.87956964969635, Test_Loss: 2.445075511932373\n",
      "Epoch: 4, Train_Loss: 1.8835971355438232, Test_Loss: 6.915834426879883\n",
      "Epoch: 4, Train_Loss: 1.8624873161315918, Test_Loss: 1.8543373346328735 *\n",
      "Epoch: 4, Train_Loss: 1.841286540031433, Test_Loss: 1.8396919965744019 *\n",
      "Epoch: 4, Train_Loss: 1.8904366493225098, Test_Loss: 1.8606585264205933\n",
      "Epoch: 4, Train_Loss: 1.99954354763031, Test_Loss: 1.8773674964904785\n",
      "Epoch: 4, Train_Loss: 1.9631165266036987, Test_Loss: 1.8808245658874512\n",
      "Epoch: 4, Train_Loss: 1.9429783821105957, Test_Loss: 1.8424659967422485 *\n",
      "Epoch: 4, Train_Loss: 1.9503281116485596, Test_Loss: 1.9788607358932495\n",
      "Epoch: 4, Train_Loss: 1.884310245513916, Test_Loss: 1.8698676824569702 *\n",
      "Epoch: 4, Train_Loss: 1.8728291988372803, Test_Loss: 1.8313229084014893 *\n",
      "Epoch: 4, Train_Loss: 1.8418760299682617, Test_Loss: 1.8781079053878784\n",
      "Epoch: 4, Train_Loss: 1.8566410541534424, Test_Loss: 1.8375880718231201 *\n",
      "Epoch: 4, Train_Loss: 1.8456693887710571, Test_Loss: 1.8408154249191284\n",
      "Epoch: 4, Train_Loss: 1.8259302377700806, Test_Loss: 1.871338963508606\n",
      "Epoch: 4, Train_Loss: 1.835838794708252, Test_Loss: 1.931720495223999\n",
      "Epoch: 4, Train_Loss: 1.8861560821533203, Test_Loss: 1.886467695236206 *\n",
      "Epoch: 4, Train_Loss: 1.8915437459945679, Test_Loss: 1.9513601064682007\n",
      "Epoch: 4, Train_Loss: 1.8276172876358032, Test_Loss: 1.8610153198242188 *\n",
      "Epoch: 4, Train_Loss: 1.820371389389038, Test_Loss: 1.8513129949569702 *\n",
      "Epoch: 4, Train_Loss: 1.8186293840408325, Test_Loss: 1.8365445137023926 *\n",
      "Epoch: 4, Train_Loss: 1.8176665306091309, Test_Loss: 1.8319929838180542 *\n",
      "Epoch: 4, Train_Loss: 1.817037582397461, Test_Loss: 1.8320354223251343\n",
      "Epoch: 4, Train_Loss: 1.8165994882583618, Test_Loss: 1.831943392753601 *\n",
      "Epoch: 4, Train_Loss: 1.8144639730453491, Test_Loss: 1.831246256828308 *\n",
      "Epoch: 4, Train_Loss: 1.8141758441925049, Test_Loss: 1.8299179077148438 *\n",
      "Epoch: 4, Train_Loss: 1.8129714727401733, Test_Loss: 1.837141752243042\n",
      "Epoch: 4, Train_Loss: 1.811558485031128, Test_Loss: 1.8318952322006226 *\n",
      "Epoch: 4, Train_Loss: 1.8115521669387817, Test_Loss: 1.8357638120651245\n",
      "Epoch: 4, Train_Loss: 1.818093180656433, Test_Loss: 1.8145759105682373 *\n",
      "Epoch: 4, Train_Loss: 1.8152930736541748, Test_Loss: 1.8270431756973267\n",
      "Epoch: 4, Train_Loss: 1.8140356540679932, Test_Loss: 1.8822389841079712\n",
      "Epoch: 4, Train_Loss: 1.8194583654403687, Test_Loss: 1.8206303119659424 *\n",
      "Epoch: 4, Train_Loss: 1.8085194826126099, Test_Loss: 2.1945748329162598\n",
      "Epoch: 4, Train_Loss: 1.8052687644958496, Test_Loss: 2.3866701126098633\n",
      "Epoch: 4, Train_Loss: 1.8041670322418213, Test_Loss: 2.0000243186950684 *\n",
      "Epoch: 4, Train_Loss: 1.8077170848846436, Test_Loss: 1.8448870182037354 *\n",
      "Epoch: 4, Train_Loss: 1.8141478300094604, Test_Loss: 1.8311724662780762 *\n",
      "Epoch: 4, Train_Loss: 1.8021931648254395, Test_Loss: 1.8209172487258911 *\n",
      "Epoch: 4, Train_Loss: 1.8002227544784546, Test_Loss: 1.9368497133255005\n",
      "Epoch: 4, Train_Loss: 1.8005530834197998, Test_Loss: 2.923936367034912\n",
      "Epoch: 4, Train_Loss: 1.8498542308807373, Test_Loss: 3.0952749252319336\n",
      "Epoch: 4, Train_Loss: 1.869987964630127, Test_Loss: 1.8614965677261353 *\n",
      "Epoch: 4, Train_Loss: 1.8325623273849487, Test_Loss: 1.8824131488800049\n",
      "Epoch: 4, Train_Loss: 1.8087584972381592, Test_Loss: 1.7966399192810059 *\n",
      "Epoch: 4, Train_Loss: 1.7960195541381836, Test_Loss: 1.8051033020019531\n",
      "Epoch: 4, Train_Loss: 1.8366808891296387, Test_Loss: 1.801769495010376 *\n",
      "Epoch: 4, Train_Loss: 1.800784707069397, Test_Loss: 1.8199243545532227\n",
      "Epoch: 4, Train_Loss: 1.803383708000183, Test_Loss: 1.8572757244110107\n",
      "Epoch: 4, Train_Loss: 1.8338388204574585, Test_Loss: 1.8211349248886108 *\n",
      "Epoch: 4, Train_Loss: 1.8071234226226807, Test_Loss: 1.8010621070861816 *\n",
      "Epoch: 4, Train_Loss: 1.9475892782211304, Test_Loss: 1.888569951057434\n",
      "Epoch: 4, Train_Loss: 1.9024558067321777, Test_Loss: 2.1807680130004883\n",
      "Epoch: 4, Train_Loss: 1.8500019311904907, Test_Loss: 1.9338821172714233 *\n",
      "Epoch: 4, Train_Loss: 1.8060842752456665, Test_Loss: 1.847058653831482 *\n",
      "Epoch: 4, Train_Loss: 1.7974332571029663, Test_Loss: 1.7889560461044312 *\n",
      "Epoch: 4, Train_Loss: 1.808154821395874, Test_Loss: 1.7880557775497437 *\n",
      "Epoch: 4, Train_Loss: 1.787492036819458, Test_Loss: 1.787240743637085 *\n",
      "Epoch: 4, Train_Loss: 1.7924795150756836, Test_Loss: 1.7863633632659912 *\n",
      "Epoch: 4, Train_Loss: 1.794579267501831, Test_Loss: 1.8969762325286865\n",
      "Model saved at location save_new\\model.ckpt at epoch 4\n",
      "Epoch: 4, Train_Loss: 1.7980626821517944, Test_Loss: 7.169369697570801\n",
      "Epoch: 4, Train_Loss: 1.860620379447937, Test_Loss: 1.9221630096435547 *\n",
      "Epoch: 4, Train_Loss: 1.7970219850540161, Test_Loss: 1.7851835489273071 *\n",
      "Epoch: 4, Train_Loss: 1.8647867441177368, Test_Loss: 1.7782723903656006 *\n",
      "Epoch: 4, Train_Loss: 1.7928011417388916, Test_Loss: 1.7797561883926392\n",
      "Epoch: 4, Train_Loss: 1.7929185628890991, Test_Loss: 1.7811177968978882\n",
      "Epoch: 4, Train_Loss: 1.888474464416504, Test_Loss: 1.7764571905136108 *\n",
      "Epoch: 4, Train_Loss: 1.975223422050476, Test_Loss: 1.7795518636703491\n",
      "Epoch: 4, Train_Loss: 1.7781801223754883, Test_Loss: 1.7771430015563965 *\n",
      "Epoch: 4, Train_Loss: 1.800998330116272, Test_Loss: 1.7758015394210815 *\n",
      "Epoch: 4, Train_Loss: 1.7724699974060059, Test_Loss: 1.7759504318237305\n",
      "Epoch: 4, Train_Loss: 1.7721694707870483, Test_Loss: 1.777618169784546\n",
      "Epoch: 4, Train_Loss: 1.7714133262634277, Test_Loss: 1.7820055484771729\n",
      "Epoch: 4, Train_Loss: 1.769139289855957, Test_Loss: 1.7889703512191772\n",
      "Epoch: 4, Train_Loss: 1.792905569076538, Test_Loss: 1.7800081968307495 *\n",
      "Epoch: 4, Train_Loss: 1.7957098484039307, Test_Loss: 1.7696890830993652 *\n",
      "Epoch: 4, Train_Loss: 1.7922155857086182, Test_Loss: 1.7662031650543213 *\n",
      "Epoch: 4, Train_Loss: 1.7859022617340088, Test_Loss: 1.7695302963256836\n",
      "Epoch: 4, Train_Loss: 1.7953498363494873, Test_Loss: 1.7657471895217896 *\n",
      "Epoch: 4, Train_Loss: 1.773775339126587, Test_Loss: 1.7634998559951782 *\n",
      "Epoch: 4, Train_Loss: 1.7664141654968262, Test_Loss: 1.766109824180603\n",
      "Epoch: 4, Train_Loss: 1.7618955373764038, Test_Loss: 1.761810541152954 *\n",
      "Epoch: 4, Train_Loss: 1.7777397632598877, Test_Loss: 1.7641432285308838\n",
      "Epoch: 4, Train_Loss: 1.7801685333251953, Test_Loss: 1.7645246982574463\n",
      "Epoch: 4, Train_Loss: 1.7781453132629395, Test_Loss: 1.7607817649841309 *\n",
      "Epoch: 4, Train_Loss: 1.7613557577133179, Test_Loss: 1.7591946125030518 *\n",
      "Epoch: 4, Train_Loss: 1.8298320770263672, Test_Loss: 1.7607308626174927\n",
      "Epoch: 4, Train_Loss: 1.8201985359191895, Test_Loss: 1.7568302154541016 *\n",
      "Epoch: 4, Train_Loss: 1.7935302257537842, Test_Loss: 1.7574450969696045\n",
      "Epoch: 4, Train_Loss: 1.7584413290023804, Test_Loss: 1.7626361846923828\n",
      "Epoch: 4, Train_Loss: 1.7762107849121094, Test_Loss: 1.8121954202651978\n",
      "Epoch: 4, Train_Loss: 1.7558842897415161, Test_Loss: 3.5356225967407227\n",
      "Epoch: 4, Train_Loss: 1.7731597423553467, Test_Loss: 5.557910919189453\n",
      "Epoch: 4, Train_Loss: 1.7624685764312744, Test_Loss: 1.7579900026321411 *\n",
      "Epoch: 4, Train_Loss: 1.7781606912612915, Test_Loss: 1.7497353553771973 *\n",
      "Epoch: 4, Train_Loss: 3.354775905609131, Test_Loss: 1.7866336107254028\n",
      "Epoch: 4, Train_Loss: 5.541438102722168, Test_Loss: 1.7942055463790894\n",
      "Epoch: 4, Train_Loss: 1.8293774127960205, Test_Loss: 1.7932190895080566 *\n",
      "Epoch: 4, Train_Loss: 1.765269160270691, Test_Loss: 1.7812614440917969 *\n",
      "Epoch: 4, Train_Loss: 1.7652013301849365, Test_Loss: 1.887404203414917\n",
      "Epoch: 4, Train_Loss: 1.9543461799621582, Test_Loss: 1.7522882223129272 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_Loss: 1.8038533926010132, Test_Loss: 1.7589991092681885\n",
      "Epoch: 4, Train_Loss: 1.7576247453689575, Test_Loss: 1.778749704360962\n",
      "Epoch: 4, Train_Loss: 1.7413098812103271, Test_Loss: 1.7499526739120483 *\n",
      "Epoch: 4, Train_Loss: 1.8219435214996338, Test_Loss: 1.7532916069030762\n",
      "Epoch: 4, Train_Loss: 1.7554904222488403, Test_Loss: 1.8030046224594116\n",
      "Epoch: 4, Train_Loss: 1.7519679069519043, Test_Loss: 1.8331341743469238\n",
      "Epoch: 4, Train_Loss: 2.238393545150757, Test_Loss: 1.8189432621002197 *\n",
      "Epoch: 4, Train_Loss: 3.2039928436279297, Test_Loss: 1.8660805225372314\n",
      "Epoch: 4, Train_Loss: 2.7425310611724854, Test_Loss: 1.7534599304199219 *\n",
      "Epoch: 4, Train_Loss: 1.8790428638458252, Test_Loss: 1.7683945894241333\n",
      "Epoch: 4, Train_Loss: 2.021089792251587, Test_Loss: 1.7559213638305664 *\n",
      "Epoch: 4, Train_Loss: 4.109771251678467, Test_Loss: 1.7515984773635864 *\n",
      "Epoch: 4, Train_Loss: 2.581252098083496, Test_Loss: 1.7513436079025269 *\n",
      "Epoch: 4, Train_Loss: 1.7775198221206665, Test_Loss: 1.7500767707824707 *\n",
      "Epoch: 4, Train_Loss: 1.7563024759292603, Test_Loss: 1.7479233741760254 *\n",
      "Epoch: 4, Train_Loss: 2.685914993286133, Test_Loss: 1.747153639793396 *\n",
      "Epoch: 4, Train_Loss: 3.3417770862579346, Test_Loss: 1.7606600522994995\n",
      "Epoch: 4, Train_Loss: 2.3426742553710938, Test_Loss: 1.7478694915771484 *\n",
      "Epoch: 4, Train_Loss: 1.7404588460922241, Test_Loss: 1.7444602251052856 *\n",
      "Epoch: 4, Train_Loss: 1.735151767730713, Test_Loss: 1.7263107299804688 *\n",
      "Epoch: 4, Train_Loss: 2.187973737716675, Test_Loss: 1.7474035024642944\n",
      "Epoch: 4, Train_Loss: 2.073420524597168, Test_Loss: 1.7784732580184937\n",
      "Epoch: 4, Train_Loss: 1.7416069507598877, Test_Loss: 1.733670949935913 *\n",
      "Epoch: 4, Train_Loss: 1.7810118198394775, Test_Loss: 2.2311131954193115\n",
      "Epoch: 4, Train_Loss: 1.8614574670791626, Test_Loss: 2.276005268096924\n",
      "Epoch: 4, Train_Loss: 1.8193438053131104, Test_Loss: 1.888792872428894 *\n",
      "Epoch: 4, Train_Loss: 1.7970263957977295, Test_Loss: 1.7376469373703003 *\n",
      "Epoch: 4, Train_Loss: 2.159956455230713, Test_Loss: 1.7412397861480713\n",
      "Epoch: 4, Train_Loss: 1.8617043495178223, Test_Loss: 1.7370288372039795 *\n",
      "Epoch: 4, Train_Loss: 1.7856056690216064, Test_Loss: 1.8940620422363281\n",
      "Epoch: 4, Train_Loss: 1.967651128768921, Test_Loss: 2.8679065704345703\n",
      "Epoch: 4, Train_Loss: 1.9501351118087769, Test_Loss: 2.612929344177246 *\n",
      "Epoch: 4, Train_Loss: 1.9643369913101196, Test_Loss: 1.7768300771713257 *\n",
      "Epoch: 4, Train_Loss: 1.8711131811141968, Test_Loss: 1.758962869644165 *\n",
      "Epoch: 4, Train_Loss: 1.8007056713104248, Test_Loss: 1.7425284385681152 *\n",
      "Epoch: 4, Train_Loss: 1.97113037109375, Test_Loss: 1.7528975009918213\n",
      "Epoch: 4, Train_Loss: 1.8531910181045532, Test_Loss: 1.8250905275344849\n",
      "Epoch: 4, Train_Loss: 1.748521089553833, Test_Loss: 1.7510082721710205 *\n",
      "Epoch: 4, Train_Loss: 1.7464712858200073, Test_Loss: 1.7987784147262573\n",
      "Epoch: 4, Train_Loss: 1.7491823434829712, Test_Loss: 1.7099881172180176 *\n",
      "Epoch: 4, Train_Loss: 1.7121069431304932, Test_Loss: 1.7329751253128052\n",
      "Epoch: 4, Train_Loss: 1.7155053615570068, Test_Loss: 1.8206416368484497\n",
      "Epoch: 4, Train_Loss: 1.7493129968643188, Test_Loss: 2.0835936069488525\n",
      "Epoch: 4, Train_Loss: 1.7756754159927368, Test_Loss: 1.946999430656433 *\n",
      "Epoch: 4, Train_Loss: 1.7873255014419556, Test_Loss: 1.7162349224090576 *\n",
      "Epoch: 4, Train_Loss: 1.841360092163086, Test_Loss: 1.70749032497406 *\n",
      "Epoch: 4, Train_Loss: 2.10372257232666, Test_Loss: 1.7066465616226196 *\n",
      "Epoch: 4, Train_Loss: 1.8067892789840698, Test_Loss: 1.7059141397476196 *\n",
      "Epoch: 4, Train_Loss: 1.7182409763336182, Test_Loss: 1.7097817659378052\n",
      "Epoch: 4, Train_Loss: 1.800938367843628, Test_Loss: 2.2908449172973633\n",
      "Epoch: 4, Train_Loss: 2.149533987045288, Test_Loss: 6.603466033935547\n",
      "Epoch: 4, Train_Loss: 2.1797943115234375, Test_Loss: 1.7350451946258545 *\n",
      "Epoch: 4, Train_Loss: 1.695763111114502, Test_Loss: 1.7005482912063599 *\n",
      "Epoch: 4, Train_Loss: 1.6982710361480713, Test_Loss: 1.6953967809677124 *\n",
      "Epoch: 4, Train_Loss: 2.2719473838806152, Test_Loss: 1.6994580030441284\n",
      "Epoch: 4, Train_Loss: 2.289280414581299, Test_Loss: 1.6956760883331299 *\n",
      "Epoch: 4, Train_Loss: 1.8214036226272583, Test_Loss: 1.6943613290786743 *\n",
      "Epoch: 4, Train_Loss: 1.716010332107544, Test_Loss: 1.706080436706543\n",
      "Epoch: 4, Train_Loss: 1.7029246091842651, Test_Loss: 1.692370057106018 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 4\n",
      "Epoch: 4, Train_Loss: 2.3709158897399902, Test_Loss: 1.7015012502670288\n",
      "Epoch: 4, Train_Loss: 3.0721354484558105, Test_Loss: 1.7057620286941528\n",
      "Epoch: 4, Train_Loss: 1.8141952753067017, Test_Loss: 1.7263891696929932\n",
      "Epoch: 4, Train_Loss: 1.7180719375610352, Test_Loss: 1.6905916929244995 *\n",
      "Epoch: 4, Train_Loss: 1.6886478662490845, Test_Loss: 1.6934748888015747\n",
      "Epoch: 4, Train_Loss: 1.6923539638519287, Test_Loss: 1.7159059047698975\n",
      "Epoch: 4, Train_Loss: 2.1472084522247314, Test_Loss: 1.6868767738342285 *\n",
      "Epoch: 4, Train_Loss: 1.7060999870300293, Test_Loss: 1.6876276731491089\n",
      "Epoch: 4, Train_Loss: 1.793350338935852, Test_Loss: 1.6978435516357422\n",
      "Epoch: 4, Train_Loss: 2.0102827548980713, Test_Loss: 1.6960259675979614 *\n",
      "Epoch: 4, Train_Loss: 1.701023817062378, Test_Loss: 1.6817145347595215 *\n",
      "Epoch: 4, Train_Loss: 1.6993749141693115, Test_Loss: 1.6984686851501465\n",
      "Epoch: 4, Train_Loss: 1.7847628593444824, Test_Loss: 1.6924667358398438 *\n",
      "Epoch: 4, Train_Loss: 1.9536490440368652, Test_Loss: 1.7180339097976685\n",
      "Epoch: 4, Train_Loss: 1.7010029554367065, Test_Loss: 1.7087211608886719 *\n",
      "Epoch: 4, Train_Loss: 1.7516062259674072, Test_Loss: 1.6924909353256226 *\n",
      "Epoch: 4, Train_Loss: 1.770235300064087, Test_Loss: 1.6832119226455688 *\n",
      "Epoch: 4, Train_Loss: 1.786464810371399, Test_Loss: 1.6944175958633423\n",
      "Epoch: 4, Train_Loss: 1.7430044412612915, Test_Loss: 1.68514883518219 *\n",
      "Epoch: 4, Train_Loss: 1.6770124435424805, Test_Loss: 1.6822808980941772 *\n",
      "Epoch: 4, Train_Loss: 1.7003878355026245, Test_Loss: 1.7222723960876465\n",
      "Epoch: 4, Train_Loss: 1.691518783569336, Test_Loss: 1.7369608879089355\n",
      "Epoch: 4, Train_Loss: 2.0573160648345947, Test_Loss: 4.502346038818359\n",
      "Epoch: 4, Train_Loss: 1.9800529479980469, Test_Loss: 4.122442245483398 *\n",
      "Epoch: 4, Train_Loss: 2.099001169204712, Test_Loss: 1.6852948665618896 *\n",
      "Epoch: 4, Train_Loss: 2.305647373199463, Test_Loss: 1.699703335762024\n",
      "Epoch: 4, Train_Loss: 1.8790684938430786, Test_Loss: 1.7247209548950195\n",
      "Epoch: 4, Train_Loss: 1.9641648530960083, Test_Loss: 1.6679238080978394 *\n",
      "Epoch: 4, Train_Loss: 1.764194130897522, Test_Loss: 1.6902525424957275\n",
      "Epoch: 4, Train_Loss: 1.6779049634933472, Test_Loss: 1.7197680473327637\n",
      "Epoch: 4, Train_Loss: 1.6705461740493774, Test_Loss: 1.7108628749847412 *\n",
      "Epoch: 4, Train_Loss: 1.685280442237854, Test_Loss: 1.6980007886886597 *\n",
      "Epoch: 4, Train_Loss: 1.8833105564117432, Test_Loss: 1.7184216976165771\n",
      "Epoch: 4, Train_Loss: 2.2089786529541016, Test_Loss: 1.7734014987945557\n",
      "Epoch: 4, Train_Loss: 2.0102035999298096, Test_Loss: 1.8093717098236084\n",
      "Epoch: 4, Train_Loss: 3.4703269004821777, Test_Loss: 1.7475459575653076 *\n",
      "Epoch: 4, Train_Loss: 2.0788655281066895, Test_Loss: 1.7237237691879272 *\n",
      "Epoch: 4, Train_Loss: 2.190429449081421, Test_Loss: 1.7252432107925415\n",
      "Epoch: 4, Train_Loss: 1.7280082702636719, Test_Loss: 1.7126340866088867 *\n",
      "Epoch: 4, Train_Loss: 1.6659247875213623, Test_Loss: 1.6722240447998047 *\n",
      "Epoch: 4, Train_Loss: 2.001224994659424, Test_Loss: 1.9126527309417725\n",
      "Epoch: 4, Train_Loss: 2.883800506591797, Test_Loss: 1.7022548913955688 *\n",
      "Epoch: 4, Train_Loss: 2.1077094078063965, Test_Loss: 1.6772555112838745 *\n",
      "Epoch: 4, Train_Loss: 1.8134294748306274, Test_Loss: 1.7005952596664429\n",
      "Epoch: 4, Train_Loss: 1.743675708770752, Test_Loss: 1.703256607055664\n",
      "Epoch: 4, Train_Loss: 1.7328698635101318, Test_Loss: 1.6766870021820068 *\n",
      "Epoch: 4, Train_Loss: 2.0337066650390625, Test_Loss: 1.6846106052398682\n",
      "Epoch: 4, Train_Loss: 1.9481948614120483, Test_Loss: 1.6975845098495483\n",
      "Epoch: 4, Train_Loss: 2.487492799758911, Test_Loss: 1.6984285116195679\n",
      "Epoch: 4, Train_Loss: 1.9464025497436523, Test_Loss: 1.6818854808807373 *\n",
      "Epoch: 4, Train_Loss: 1.706662893295288, Test_Loss: 1.6909615993499756\n",
      "Epoch: 4, Train_Loss: 1.6563040018081665, Test_Loss: 1.7525230646133423\n",
      "Epoch: 4, Train_Loss: 1.6572670936584473, Test_Loss: 1.685227394104004 *\n",
      "Epoch: 4, Train_Loss: 1.693772315979004, Test_Loss: 1.6659172773361206 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 1.6792988777160645, Test_Loss: 1.7312129735946655 *\n",
      "Epoch: 5, Train_Loss: 1.6767290830612183, Test_Loss: 1.877880573272705\n",
      "Epoch: 5, Train_Loss: 6.662769794464111, Test_Loss: 1.9002004861831665\n",
      "Epoch: 5, Train_Loss: 13.651662826538086, Test_Loss: 1.7273069620132446 *\n",
      "Epoch: 5, Train_Loss: 2.1770710945129395, Test_Loss: 1.646237850189209 *\n",
      "Epoch: 5, Train_Loss: 4.499354839324951, Test_Loss: 1.6690289974212646\n",
      "Epoch: 5, Train_Loss: 2.1105434894561768, Test_Loss: 1.6534886360168457 *\n",
      "Epoch: 5, Train_Loss: 1.7209786176681519, Test_Loss: 1.7806297540664673\n",
      "Epoch: 5, Train_Loss: 1.741137146949768, Test_Loss: 2.2799196243286133\n",
      "Epoch: 5, Train_Loss: 10.814035415649414, Test_Loss: 2.037412405014038 *\n",
      "Epoch: 5, Train_Loss: 3.5891427993774414, Test_Loss: 1.702134132385254 *\n",
      "Epoch: 5, Train_Loss: 1.699114441871643, Test_Loss: 1.6849168539047241 *\n",
      "Epoch: 5, Train_Loss: 3.866565704345703, Test_Loss: 1.7554361820220947\n",
      "Epoch: 5, Train_Loss: 5.347507953643799, Test_Loss: 1.7806280851364136\n",
      "Epoch: 5, Train_Loss: 1.7307629585266113, Test_Loss: 1.8754467964172363\n",
      "Epoch: 5, Train_Loss: 1.6392793655395508, Test_Loss: 1.6773351430892944 *\n",
      "Epoch: 5, Train_Loss: 1.6395820379257202, Test_Loss: 1.8869706392288208\n",
      "Epoch: 5, Train_Loss: 1.6334478855133057, Test_Loss: 1.640239953994751 *\n",
      "Epoch: 5, Train_Loss: 1.6337367296218872, Test_Loss: 1.7142395973205566\n",
      "Epoch: 5, Train_Loss: 1.6276397705078125, Test_Loss: 1.9077503681182861\n",
      "Epoch: 5, Train_Loss: 1.6302403211593628, Test_Loss: 1.8315303325653076 *\n",
      "Epoch: 5, Train_Loss: 1.6296221017837524, Test_Loss: 1.7929377555847168 *\n",
      "Epoch: 5, Train_Loss: 1.6349135637283325, Test_Loss: 1.6367824077606201 *\n",
      "Epoch: 5, Train_Loss: 1.6488704681396484, Test_Loss: 1.6529037952423096\n",
      "Epoch: 5, Train_Loss: 1.631779432296753, Test_Loss: 1.647522211074829 *\n",
      "Epoch: 5, Train_Loss: 1.6408069133758545, Test_Loss: 1.6427611112594604 *\n",
      "Epoch: 5, Train_Loss: 1.6829161643981934, Test_Loss: 1.6235311031341553 *\n",
      "Epoch: 5, Train_Loss: 1.6716831922531128, Test_Loss: 3.4918642044067383\n",
      "Epoch: 5, Train_Loss: 1.6307518482208252, Test_Loss: 6.198727607727051\n",
      "Epoch: 5, Train_Loss: 1.62575364112854, Test_Loss: 1.7247040271759033 *\n",
      "Epoch: 5, Train_Loss: 1.6201533079147339, Test_Loss: 1.7158647775650024 *\n",
      "Epoch: 5, Train_Loss: 1.6146087646484375, Test_Loss: 1.7080243825912476 *\n",
      "Epoch: 5, Train_Loss: 1.6149203777313232, Test_Loss: 1.6403944492340088 *\n",
      "Epoch: 5, Train_Loss: 1.614788293838501, Test_Loss: 1.7530901432037354\n",
      "Epoch: 5, Train_Loss: 1.6164473295211792, Test_Loss: 1.7729787826538086\n",
      "Epoch: 5, Train_Loss: 1.6163923740386963, Test_Loss: 1.719624400138855 *\n",
      "Epoch: 5, Train_Loss: 1.6116530895233154, Test_Loss: 1.6462706327438354 *\n",
      "Epoch: 5, Train_Loss: 1.6114857196807861, Test_Loss: 1.7085946798324585\n",
      "Epoch: 5, Train_Loss: 1.6109566688537598, Test_Loss: 1.7139991521835327\n",
      "Epoch: 5, Train_Loss: 1.6106597185134888, Test_Loss: 1.7284421920776367\n",
      "Epoch: 5, Train_Loss: 1.626842975616455, Test_Loss: 1.679213047027588 *\n",
      "Epoch: 5, Train_Loss: 1.6282333135604858, Test_Loss: 1.74945867061615\n",
      "Epoch: 5, Train_Loss: 1.620478868484497, Test_Loss: 1.7668876647949219\n",
      "Epoch: 5, Train_Loss: 1.61106538772583, Test_Loss: 1.6295363903045654 *\n",
      "Epoch: 5, Train_Loss: 9.379571914672852, Test_Loss: 1.6713066101074219\n",
      "Epoch: 5, Train_Loss: 2.607227325439453, Test_Loss: 1.687608003616333\n",
      "Epoch: 5, Train_Loss: 1.6977722644805908, Test_Loss: 1.8017216920852661\n",
      "Epoch: 5, Train_Loss: 1.781200885772705, Test_Loss: 1.706295371055603 *\n",
      "Epoch: 5, Train_Loss: 1.769063115119934, Test_Loss: 1.7622860670089722\n",
      "Epoch: 5, Train_Loss: 1.6132351160049438, Test_Loss: 1.7546665668487549 *\n",
      "Epoch: 5, Train_Loss: 1.6120513677597046, Test_Loss: 1.780653715133667\n",
      "Epoch: 5, Train_Loss: 1.6697582006454468, Test_Loss: 1.6952818632125854 *\n",
      "Epoch: 5, Train_Loss: 1.731955885887146, Test_Loss: 1.6176419258117676 *\n",
      "Epoch: 5, Train_Loss: 1.8090729713439941, Test_Loss: 1.611711859703064 *\n",
      "Epoch: 5, Train_Loss: 1.7803676128387451, Test_Loss: 1.6118489503860474\n",
      "Epoch: 5, Train_Loss: 1.6235512495040894, Test_Loss: 1.6059269905090332 *\n",
      "Epoch: 5, Train_Loss: 1.6771920919418335, Test_Loss: 1.605561375617981 *\n",
      "Epoch: 5, Train_Loss: 1.7108118534088135, Test_Loss: 1.651403546333313\n",
      "Epoch: 5, Train_Loss: 1.7242294549942017, Test_Loss: 1.6231971979141235 *\n",
      "Epoch: 5, Train_Loss: 1.741666555404663, Test_Loss: 5.667033672332764\n",
      "Epoch: 5, Train_Loss: 1.7116416692733765, Test_Loss: 3.2758939266204834 *\n",
      "Epoch: 5, Train_Loss: 1.6552519798278809, Test_Loss: 1.5985722541809082 *\n",
      "Epoch: 5, Train_Loss: 1.6017733812332153, Test_Loss: 1.600569486618042\n",
      "Epoch: 5, Train_Loss: 1.6730092763900757, Test_Loss: 1.6264249086380005\n",
      "Epoch: 5, Train_Loss: 1.6174336671829224, Test_Loss: 1.6402268409729004\n",
      "Epoch: 5, Train_Loss: 1.5976943969726562, Test_Loss: 1.601496696472168 *\n",
      "Epoch: 5, Train_Loss: 1.590275526046753, Test_Loss: 1.6880996227264404\n",
      "Epoch: 5, Train_Loss: 1.5898668766021729, Test_Loss: 1.7094244956970215\n",
      "Epoch: 5, Train_Loss: 1.638445258140564, Test_Loss: 1.5883740186691284 *\n",
      "Epoch: 5, Train_Loss: 7.274154186248779, Test_Loss: 1.6436469554901123\n",
      "Epoch: 5, Train_Loss: 1.6575090885162354, Test_Loss: 1.6004149913787842 *\n",
      "Epoch: 5, Train_Loss: 1.5983561277389526, Test_Loss: 1.6044647693634033\n",
      "Epoch: 5, Train_Loss: 1.6008589267730713, Test_Loss: 1.592002272605896 *\n",
      "Epoch: 5, Train_Loss: 1.6018767356872559, Test_Loss: 1.6746079921722412\n",
      "Epoch: 5, Train_Loss: 1.5991398096084595, Test_Loss: 1.6245946884155273 *\n",
      "Epoch: 5, Train_Loss: 1.5868545770645142, Test_Loss: 1.7387083768844604\n",
      "Epoch: 5, Train_Loss: 1.5881417989730835, Test_Loss: 1.710557460784912 *\n",
      "Epoch: 5, Train_Loss: 1.6092853546142578, Test_Loss: 1.5931298732757568 *\n",
      "Epoch: 5, Train_Loss: 1.6082485914230347, Test_Loss: 1.583907961845398 *\n",
      "Epoch: 5, Train_Loss: 1.585099220275879, Test_Loss: 1.5834479331970215 *\n",
      "Epoch: 5, Train_Loss: 1.5845402479171753, Test_Loss: 1.5811971426010132 *\n",
      "Epoch: 5, Train_Loss: 1.5837161540985107, Test_Loss: 1.5805069208145142 *\n",
      "Epoch: 5, Train_Loss: 1.601647973060608, Test_Loss: 1.5799084901809692 *\n",
      "Epoch: 5, Train_Loss: 1.5774048566818237, Test_Loss: 1.5790674686431885 *\n",
      "Epoch: 5, Train_Loss: 1.5784211158752441, Test_Loss: 1.5775436162948608 *\n",
      "Epoch: 5, Train_Loss: 1.6024298667907715, Test_Loss: 1.587130069732666\n",
      "Epoch: 5, Train_Loss: 1.6389614343643188, Test_Loss: 1.5830804109573364 *\n",
      "Epoch: 5, Train_Loss: 1.6134732961654663, Test_Loss: 1.5798603296279907 *\n",
      "Epoch: 5, Train_Loss: 1.573712944984436, Test_Loss: 1.578964114189148 *\n",
      "Epoch: 5, Train_Loss: 1.5752507448196411, Test_Loss: 1.6049754619598389\n",
      "Epoch: 5, Train_Loss: 1.6307339668273926, Test_Loss: 1.6066211462020874\n",
      "Epoch: 5, Train_Loss: 1.6780215501785278, Test_Loss: 1.7058051824569702\n",
      "Epoch: 5, Train_Loss: 1.661370038986206, Test_Loss: 2.0830814838409424\n",
      "Epoch: 5, Train_Loss: 1.648181676864624, Test_Loss: 1.9031283855438232 *\n",
      "Epoch: 5, Train_Loss: 1.5871162414550781, Test_Loss: 1.6543420553207397 *\n",
      "Epoch: 5, Train_Loss: 1.6562929153442383, Test_Loss: 1.5890130996704102 *\n",
      "Epoch: 5, Train_Loss: 1.6412731409072876, Test_Loss: 1.57876455783844 *\n",
      "Epoch: 5, Train_Loss: 1.5941206216812134, Test_Loss: 1.6670297384262085\n",
      "Epoch: 5, Train_Loss: 1.6477519273757935, Test_Loss: 2.21396541595459\n",
      "Epoch: 5, Train_Loss: 1.5962692499160767, Test_Loss: 3.1422057151794434\n",
      "Model saved at location save_new\\model.ckpt at epoch 5\n",
      "Epoch: 5, Train_Loss: 1.5788825750350952, Test_Loss: 2.087171792984009 *\n",
      "Epoch: 5, Train_Loss: 1.5673364400863647, Test_Loss: 1.6456397771835327 *\n",
      "Epoch: 5, Train_Loss: 1.5664141178131104, Test_Loss: 1.5663105249404907 *\n",
      "Epoch: 5, Train_Loss: 1.5655442476272583, Test_Loss: 1.5702545642852783\n",
      "Epoch: 5, Train_Loss: 1.5647869110107422, Test_Loss: 1.5656661987304688 *\n",
      "Epoch: 5, Train_Loss: 1.5628079175949097, Test_Loss: 1.5678424835205078\n",
      "Epoch: 5, Train_Loss: 6.153256893157959, Test_Loss: 1.6135016679763794\n",
      "Epoch: 5, Train_Loss: 1.964174747467041, Test_Loss: 1.6279475688934326\n",
      "Epoch: 5, Train_Loss: 1.5650250911712646, Test_Loss: 1.5647640228271484 *\n",
      "Epoch: 5, Train_Loss: 1.590293526649475, Test_Loss: 1.627591848373413\n",
      "Epoch: 5, Train_Loss: 1.5624613761901855, Test_Loss: 1.810896396636963\n",
      "Epoch: 5, Train_Loss: 1.5542292594909668, Test_Loss: 1.7326061725616455 *\n",
      "Epoch: 5, Train_Loss: 1.5559849739074707, Test_Loss: 1.7209500074386597 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 1.5545282363891602, Test_Loss: 1.5534369945526123 *\n",
      "Epoch: 5, Train_Loss: 1.553135871887207, Test_Loss: 1.5524001121520996 *\n",
      "Epoch: 5, Train_Loss: 1.5512847900390625, Test_Loss: 1.5517120361328125 *\n",
      "Epoch: 5, Train_Loss: 1.627807855606079, Test_Loss: 1.5509467124938965 *\n",
      "Epoch: 5, Train_Loss: 1.6769264936447144, Test_Loss: 1.555158019065857\n",
      "Epoch: 5, Train_Loss: 1.6852269172668457, Test_Loss: 4.6529059410095215\n",
      "Epoch: 5, Train_Loss: 1.6756548881530762, Test_Loss: 4.14491081237793 *\n",
      "Epoch: 5, Train_Loss: 1.5491899251937866, Test_Loss: 1.552704095840454 *\n",
      "Epoch: 5, Train_Loss: 1.595977783203125, Test_Loss: 1.5470659732818604 *\n",
      "Epoch: 5, Train_Loss: 1.7162736654281616, Test_Loss: 1.5445963144302368 *\n",
      "Epoch: 5, Train_Loss: 1.7068334817886353, Test_Loss: 1.5474516153335571\n",
      "Epoch: 5, Train_Loss: 1.7197102308273315, Test_Loss: 1.5445019006729126 *\n",
      "Epoch: 5, Train_Loss: 1.551375389099121, Test_Loss: 1.5482443571090698\n",
      "Epoch: 5, Train_Loss: 1.5419713258743286, Test_Loss: 1.5455163717269897 *\n",
      "Epoch: 5, Train_Loss: 1.5439456701278687, Test_Loss: 1.5481971502304077\n",
      "Epoch: 5, Train_Loss: 1.5404046773910522, Test_Loss: 1.5473430156707764 *\n",
      "Epoch: 5, Train_Loss: 1.541480302810669, Test_Loss: 1.5433684587478638 *\n",
      "Epoch: 5, Train_Loss: 1.5400182008743286, Test_Loss: 1.5472995042800903\n",
      "Epoch: 5, Train_Loss: 1.539482593536377, Test_Loss: 1.5612133741378784\n",
      "Epoch: 5, Train_Loss: 1.5404185056686401, Test_Loss: 1.5559759140014648 *\n",
      "Epoch: 5, Train_Loss: 1.5377531051635742, Test_Loss: 1.5420808792114258 *\n",
      "Epoch: 5, Train_Loss: 1.5382343530654907, Test_Loss: 1.5364545583724976 *\n",
      "Epoch: 5, Train_Loss: 1.6574808359146118, Test_Loss: 1.5392460823059082\n",
      "Epoch: 5, Train_Loss: 1.687778353691101, Test_Loss: 1.5367032289505005 *\n",
      "Epoch: 5, Train_Loss: 1.7013928890228271, Test_Loss: 1.536087155342102 *\n",
      "Epoch: 5, Train_Loss: 1.5815926790237427, Test_Loss: 1.5391100645065308\n",
      "Epoch: 5, Train_Loss: 1.7071176767349243, Test_Loss: 1.5320147275924683 *\n",
      "Epoch: 5, Train_Loss: 1.6948562860488892, Test_Loss: 1.5332292318344116\n",
      "Epoch: 5, Train_Loss: 1.5718625783920288, Test_Loss: 1.5374565124511719\n",
      "Epoch: 5, Train_Loss: 1.7059916257858276, Test_Loss: 1.5325607061386108 *\n",
      "Epoch: 5, Train_Loss: 1.7028530836105347, Test_Loss: 1.534123420715332\n",
      "Epoch: 5, Train_Loss: 1.7461599111557007, Test_Loss: 1.5311763286590576 *\n",
      "Epoch: 5, Train_Loss: 1.551783800125122, Test_Loss: 1.5311708450317383 *\n",
      "Epoch: 5, Train_Loss: 3.203402519226074, Test_Loss: 1.5309572219848633 *\n",
      "Epoch: 5, Train_Loss: 3.218113899230957, Test_Loss: 1.5312554836273193\n",
      "Epoch: 5, Train_Loss: 1.5570591688156128, Test_Loss: 1.573545217514038\n",
      "Epoch: 5, Train_Loss: 1.562943458557129, Test_Loss: 1.5511748790740967 *\n",
      "Epoch: 5, Train_Loss: 1.5665940046310425, Test_Loss: 6.791904449462891\n",
      "Epoch: 5, Train_Loss: 1.5566362142562866, Test_Loss: 1.9667332172393799 *\n",
      "Epoch: 5, Train_Loss: 1.5244591236114502, Test_Loss: 1.525949478149414 *\n",
      "Epoch: 5, Train_Loss: 1.5466387271881104, Test_Loss: 1.5364270210266113\n",
      "Epoch: 5, Train_Loss: 1.6866732835769653, Test_Loss: 1.563814640045166\n",
      "Epoch: 5, Train_Loss: 1.6438000202178955, Test_Loss: 1.5714858770370483\n",
      "Epoch: 5, Train_Loss: 1.6516194343566895, Test_Loss: 1.5238014459609985 *\n",
      "Epoch: 5, Train_Loss: 1.6494255065917969, Test_Loss: 1.6419188976287842\n",
      "Epoch: 5, Train_Loss: 1.613661289215088, Test_Loss: 1.6099557876586914 *\n",
      "Epoch: 5, Train_Loss: 1.5704039335250854, Test_Loss: 1.5175755023956299 *\n",
      "Epoch: 5, Train_Loss: 1.5486615896224976, Test_Loss: 1.5842130184173584\n",
      "Epoch: 5, Train_Loss: 1.5263274908065796, Test_Loss: 1.5189481973648071 *\n",
      "Epoch: 5, Train_Loss: 1.526119351387024, Test_Loss: 1.5384154319763184\n",
      "Epoch: 5, Train_Loss: 1.5183547735214233, Test_Loss: 1.5199103355407715 *\n",
      "Epoch: 5, Train_Loss: 1.5160096883773804, Test_Loss: 1.6309646368026733\n",
      "Epoch: 5, Train_Loss: 1.5827703475952148, Test_Loss: 1.5475993156433105 *\n",
      "Epoch: 5, Train_Loss: 1.592529058456421, Test_Loss: 1.6803090572357178\n",
      "Epoch: 5, Train_Loss: 1.5392082929611206, Test_Loss: 1.6055126190185547 *\n",
      "Epoch: 5, Train_Loss: 1.5100256204605103, Test_Loss: 1.526661992073059 *\n",
      "Epoch: 5, Train_Loss: 1.5093690156936646, Test_Loss: 1.515430212020874 *\n",
      "Epoch: 5, Train_Loss: 1.508396863937378, Test_Loss: 1.5146911144256592 *\n",
      "Epoch: 5, Train_Loss: 1.506968379020691, Test_Loss: 1.5133253335952759 *\n",
      "Epoch: 5, Train_Loss: 1.508057951927185, Test_Loss: 1.5126440525054932 *\n",
      "Epoch: 5, Train_Loss: 1.5044569969177246, Test_Loss: 1.51235830783844 *\n",
      "Epoch: 5, Train_Loss: 1.504231572151184, Test_Loss: 1.511444330215454 *\n",
      "Epoch: 5, Train_Loss: 1.5040911436080933, Test_Loss: 1.5100862979888916 *\n",
      "Epoch: 5, Train_Loss: 1.5024116039276123, Test_Loss: 1.5195332765579224\n",
      "Epoch: 5, Train_Loss: 1.5018702745437622, Test_Loss: 1.5229361057281494\n",
      "Epoch: 5, Train_Loss: 1.5067089796066284, Test_Loss: 1.5102719068527222 *\n",
      "Epoch: 5, Train_Loss: 1.5061076879501343, Test_Loss: 1.5121855735778809\n",
      "Epoch: 5, Train_Loss: 1.5033049583435059, Test_Loss: 1.5614858865737915\n",
      "Epoch: 5, Train_Loss: 1.51163911819458, Test_Loss: 1.5239349603652954 *\n",
      "Epoch: 5, Train_Loss: 1.4992040395736694, Test_Loss: 1.7376900911331177\n",
      "Epoch: 5, Train_Loss: 1.4992142915725708, Test_Loss: 2.0455851554870605\n",
      "Epoch: 5, Train_Loss: 1.497874140739441, Test_Loss: 1.7464717626571655 *\n",
      "Epoch: 5, Train_Loss: 1.4965012073516846, Test_Loss: 1.5737391710281372 *\n",
      "Epoch: 5, Train_Loss: 1.5042717456817627, Test_Loss: 1.5256203413009644 *\n",
      "Epoch: 5, Train_Loss: 1.4973325729370117, Test_Loss: 1.5096765756607056 *\n",
      "Epoch: 5, Train_Loss: 1.4951345920562744, Test_Loss: 1.614933729171753\n",
      "Epoch: 5, Train_Loss: 1.496180534362793, Test_Loss: 2.327211380004883\n",
      "Epoch: 5, Train_Loss: 1.5179331302642822, Test_Loss: 3.0361452102661133\n",
      "Epoch: 5, Train_Loss: 1.596555471420288, Test_Loss: 1.765542984008789 *\n",
      "Epoch: 5, Train_Loss: 1.5142956972122192, Test_Loss: 1.5849422216415405 *\n",
      "Epoch: 5, Train_Loss: 1.5121829509735107, Test_Loss: 1.4920759201049805 *\n",
      "Epoch: 5, Train_Loss: 1.4919590950012207, Test_Loss: 1.5017379522323608\n",
      "Epoch: 5, Train_Loss: 1.521525263786316, Test_Loss: 1.4961738586425781 *\n",
      "Epoch: 5, Train_Loss: 1.5010732412338257, Test_Loss: 1.5059927701950073\n",
      "Epoch: 5, Train_Loss: 1.4947625398635864, Test_Loss: 1.5473988056182861\n",
      "Epoch: 5, Train_Loss: 1.531935691833496, Test_Loss: 1.5484635829925537\n",
      "Epoch: 5, Train_Loss: 1.5218597650527954, Test_Loss: 1.4989135265350342 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 5\n",
      "Epoch: 5, Train_Loss: 1.618367314338684, Test_Loss: 1.5767470598220825\n",
      "Epoch: 5, Train_Loss: 1.6080291271209717, Test_Loss: 1.8504869937896729\n",
      "Epoch: 5, Train_Loss: 1.5734001398086548, Test_Loss: 1.5559710264205933 *\n",
      "Epoch: 5, Train_Loss: 1.5196057558059692, Test_Loss: 1.6346497535705566\n",
      "Epoch: 5, Train_Loss: 1.484853982925415, Test_Loss: 1.4854010343551636 *\n",
      "Epoch: 5, Train_Loss: 1.5196350812911987, Test_Loss: 1.4846856594085693 *\n",
      "Epoch: 5, Train_Loss: 1.4838688373565674, Test_Loss: 1.4841015338897705 *\n",
      "Epoch: 5, Train_Loss: 1.4938066005706787, Test_Loss: 1.4833370447158813 *\n",
      "Epoch: 5, Train_Loss: 1.4901142120361328, Test_Loss: 1.4895342588424683\n",
      "Epoch: 5, Train_Loss: 1.4908255338668823, Test_Loss: 5.989454746246338\n",
      "Epoch: 5, Train_Loss: 1.5586780309677124, Test_Loss: 2.6482739448547363 *\n",
      "Epoch: 5, Train_Loss: 1.4884281158447266, Test_Loss: 1.4843723773956299 *\n",
      "Epoch: 5, Train_Loss: 1.5800869464874268, Test_Loss: 1.4774785041809082 *\n",
      "Epoch: 5, Train_Loss: 1.4871002435684204, Test_Loss: 1.4768332242965698 *\n",
      "Epoch: 5, Train_Loss: 1.4986180067062378, Test_Loss: 1.4802323579788208\n",
      "Epoch: 5, Train_Loss: 1.5222784280776978, Test_Loss: 1.4770396947860718 *\n",
      "Epoch: 5, Train_Loss: 1.7354180812835693, Test_Loss: 1.4806978702545166\n",
      "Epoch: 5, Train_Loss: 1.4803481101989746, Test_Loss: 1.476708173751831 *\n",
      "Epoch: 5, Train_Loss: 1.4974039793014526, Test_Loss: 1.4784070253372192\n",
      "Epoch: 5, Train_Loss: 1.4745076894760132, Test_Loss: 1.4793283939361572\n",
      "Epoch: 5, Train_Loss: 1.4749466180801392, Test_Loss: 1.4790620803833008 *\n",
      "Epoch: 5, Train_Loss: 1.4749600887298584, Test_Loss: 1.4774481058120728 *\n",
      "Epoch: 5, Train_Loss: 1.4719209671020508, Test_Loss: 1.4959737062454224\n",
      "Epoch: 5, Train_Loss: 1.4909443855285645, Test_Loss: 1.4851125478744507 *\n",
      "Epoch: 5, Train_Loss: 1.4990535974502563, Test_Loss: 1.4736860990524292 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 1.5036213397979736, Test_Loss: 1.4684944152832031 *\n",
      "Epoch: 5, Train_Loss: 1.4944939613342285, Test_Loss: 1.4720052480697632\n",
      "Epoch: 5, Train_Loss: 1.495813012123108, Test_Loss: 1.4690762758255005 *\n",
      "Epoch: 5, Train_Loss: 1.489456057548523, Test_Loss: 1.4668205976486206 *\n",
      "Epoch: 5, Train_Loss: 1.4710863828659058, Test_Loss: 1.470099925994873\n",
      "Epoch: 5, Train_Loss: 1.4654284715652466, Test_Loss: 1.4641900062561035 *\n",
      "Epoch: 5, Train_Loss: 1.4773322343826294, Test_Loss: 1.4653326272964478\n",
      "Epoch: 5, Train_Loss: 1.478757619857788, Test_Loss: 1.4687379598617554\n",
      "Epoch: 5, Train_Loss: 1.4887163639068604, Test_Loss: 1.4656041860580444 *\n",
      "Epoch: 5, Train_Loss: 1.4614289999008179, Test_Loss: 1.464977741241455 *\n",
      "Epoch: 5, Train_Loss: 1.5247368812561035, Test_Loss: 1.463118553161621 *\n",
      "Epoch: 5, Train_Loss: 1.533786654472351, Test_Loss: 1.462929606437683 *\n",
      "Epoch: 5, Train_Loss: 1.5159800052642822, Test_Loss: 1.4625244140625 *\n",
      "Epoch: 5, Train_Loss: 1.4587311744689941, Test_Loss: 1.4645816087722778\n",
      "Epoch: 5, Train_Loss: 1.4847825765609741, Test_Loss: 1.5146517753601074\n",
      "Epoch: 5, Train_Loss: 1.459362268447876, Test_Loss: 1.6009328365325928\n",
      "Epoch: 5, Train_Loss: 1.482940912246704, Test_Loss: 6.892674446105957\n",
      "Epoch: 5, Train_Loss: 1.4579297304153442, Test_Loss: 1.484706997871399 *\n",
      "Epoch: 5, Train_Loss: 1.4766919612884521, Test_Loss: 1.4562971591949463 *\n",
      "Epoch: 5, Train_Loss: 1.6780190467834473, Test_Loss: 1.4766250848770142\n",
      "Epoch: 5, Train_Loss: 5.151360034942627, Test_Loss: 1.5045207738876343\n",
      "Epoch: 5, Train_Loss: 3.023475170135498, Test_Loss: 1.5068103075027466\n",
      "Epoch: 5, Train_Loss: 1.4761111736297607, Test_Loss: 1.4557231664657593 *\n",
      "Epoch: 5, Train_Loss: 1.455197811126709, Test_Loss: 1.5868409872055054\n",
      "Epoch: 5, Train_Loss: 1.6233890056610107, Test_Loss: 1.5071057081222534 *\n",
      "Epoch: 5, Train_Loss: 1.5649856328964233, Test_Loss: 1.4493722915649414 *\n",
      "Epoch: 5, Train_Loss: 1.4712704420089722, Test_Loss: 1.5063903331756592\n",
      "Epoch: 5, Train_Loss: 1.4488165378570557, Test_Loss: 1.455539345741272 *\n",
      "Epoch: 5, Train_Loss: 1.5145549774169922, Test_Loss: 1.4647865295410156\n",
      "Epoch: 5, Train_Loss: 1.474049687385559, Test_Loss: 1.4790078401565552\n",
      "Epoch: 5, Train_Loss: 1.4606188535690308, Test_Loss: 1.5698415040969849\n",
      "Epoch: 5, Train_Loss: 1.6440281867980957, Test_Loss: 1.4920625686645508 *\n",
      "Epoch: 5, Train_Loss: 2.8227362632751465, Test_Loss: 1.5702884197235107\n",
      "Epoch: 5, Train_Loss: 2.7249293327331543, Test_Loss: 1.490164041519165 *\n",
      "Epoch: 5, Train_Loss: 1.5403045415878296, Test_Loss: 1.4891188144683838 *\n",
      "Epoch: 5, Train_Loss: 1.5095582008361816, Test_Loss: 1.4776058197021484 *\n",
      "Epoch: 5, Train_Loss: 3.7061898708343506, Test_Loss: 1.4765316247940063 *\n",
      "Epoch: 5, Train_Loss: 2.896138906478882, Test_Loss: 1.4763076305389404 *\n",
      "Epoch: 5, Train_Loss: 1.5067318677902222, Test_Loss: 1.47303307056427 *\n",
      "Epoch: 5, Train_Loss: 1.4890955686569214, Test_Loss: 1.4691656827926636 *\n",
      "Epoch: 5, Train_Loss: 1.8941235542297363, Test_Loss: 1.4647337198257446 *\n",
      "Epoch: 5, Train_Loss: 3.1737730503082275, Test_Loss: 1.4680932760238647\n",
      "Epoch: 5, Train_Loss: 2.4089016914367676, Test_Loss: 1.4744799137115479\n",
      "Epoch: 5, Train_Loss: 1.4444396495819092, Test_Loss: 1.4875627756118774\n",
      "Epoch: 5, Train_Loss: 1.4512858390808105, Test_Loss: 1.4455597400665283 *\n",
      "Epoch: 5, Train_Loss: 1.6744862794876099, Test_Loss: 1.4677393436431885\n",
      "Epoch: 5, Train_Loss: 1.983903408050537, Test_Loss: 1.5460660457611084\n",
      "Epoch: 5, Train_Loss: 1.4473377466201782, Test_Loss: 1.4563437700271606 *\n",
      "Epoch: 5, Train_Loss: 1.4836037158966064, Test_Loss: 1.8048999309539795\n",
      "Epoch: 5, Train_Loss: 1.51688551902771, Test_Loss: 2.0869500637054443\n",
      "Epoch: 5, Train_Loss: 1.5939123630523682, Test_Loss: 1.6901880502700806 *\n",
      "Epoch: 5, Train_Loss: 1.540282130241394, Test_Loss: 1.503522515296936 *\n",
      "Epoch: 5, Train_Loss: 1.7996902465820312, Test_Loss: 1.4717373847961426 *\n",
      "Epoch: 5, Train_Loss: 1.6299479007720947, Test_Loss: 1.4369754791259766 *\n",
      "Epoch: 5, Train_Loss: 1.4856021404266357, Test_Loss: 1.537395715713501\n",
      "Epoch: 5, Train_Loss: 1.6733020544052124, Test_Loss: 2.351198673248291\n",
      "Epoch: 5, Train_Loss: 1.724149227142334, Test_Loss: 2.7454066276550293\n",
      "Epoch: 5, Train_Loss: 1.9491088390350342, Test_Loss: 1.5215444564819336 *\n",
      "Epoch: 5, Train_Loss: 1.7023299932479858, Test_Loss: 1.531972885131836\n",
      "Epoch: 5, Train_Loss: 1.4845823049545288, Test_Loss: 1.4252758026123047 *\n",
      "Epoch: 5, Train_Loss: 1.561174988746643, Test_Loss: 1.4288949966430664\n",
      "Epoch: 5, Train_Loss: 1.5281368494033813, Test_Loss: 1.4307305812835693\n",
      "Epoch: 5, Train_Loss: 1.437839150428772, Test_Loss: 1.438175082206726\n",
      "Epoch: 5, Train_Loss: 1.4284309148788452, Test_Loss: 1.4596712589263916\n",
      "Epoch: 5, Train_Loss: 1.4211784601211548, Test_Loss: 1.4523855447769165 *\n",
      "Epoch: 5, Train_Loss: 1.4242784976959229, Test_Loss: 1.425980806350708 *\n",
      "Epoch: 5, Train_Loss: 1.4229220151901245, Test_Loss: 1.5343329906463623\n",
      "Epoch: 5, Train_Loss: 1.4336177110671997, Test_Loss: 1.8292367458343506\n",
      "Epoch: 5, Train_Loss: 1.5039877891540527, Test_Loss: 1.5308202505111694 *\n",
      "Epoch: 5, Train_Loss: 1.5028830766677856, Test_Loss: 1.5628467798233032\n",
      "Epoch: 5, Train_Loss: 1.534554362297058, Test_Loss: 1.4304101467132568 *\n",
      "Epoch: 5, Train_Loss: 1.6273794174194336, Test_Loss: 1.4296650886535645 *\n",
      "Epoch: 5, Train_Loss: 1.7826085090637207, Test_Loss: 1.4284372329711914 *\n",
      "Epoch: 5, Train_Loss: 1.4307613372802734, Test_Loss: 1.4268498420715332 *\n",
      "Epoch: 5, Train_Loss: 1.4806437492370605, Test_Loss: 1.449934959411621\n",
      "Model saved at location save_new\\model.ckpt at epoch 5\n",
      "Epoch: 5, Train_Loss: 1.7738615274429321, Test_Loss: 6.638453483581543\n",
      "Epoch: 5, Train_Loss: 2.042496681213379, Test_Loss: 1.680646538734436 *\n",
      "Epoch: 5, Train_Loss: 1.4608938694000244, Test_Loss: 1.4229172468185425 *\n",
      "Epoch: 5, Train_Loss: 1.4120970964431763, Test_Loss: 1.4146699905395508 *\n",
      "Epoch: 5, Train_Loss: 1.8904078006744385, Test_Loss: 1.417739748954773\n",
      "Epoch: 5, Train_Loss: 2.1526286602020264, Test_Loss: 1.4211417436599731\n",
      "Epoch: 5, Train_Loss: 1.6895678043365479, Test_Loss: 1.4127898216247559 *\n",
      "Epoch: 5, Train_Loss: 1.4403870105743408, Test_Loss: 1.4133118391036987\n",
      "Epoch: 5, Train_Loss: 1.4277797937393188, Test_Loss: 1.4103939533233643 *\n",
      "Epoch: 5, Train_Loss: 1.7334390878677368, Test_Loss: 1.4099273681640625 *\n",
      "Epoch: 5, Train_Loss: 3.077968120574951, Test_Loss: 1.4113050699234009\n",
      "Epoch: 5, Train_Loss: 1.790628433227539, Test_Loss: 1.4126213788986206\n",
      "Epoch: 5, Train_Loss: 1.4288536310195923, Test_Loss: 1.4183118343353271\n",
      "Epoch: 5, Train_Loss: 1.4154189825057983, Test_Loss: 1.433752417564392\n",
      "Epoch: 5, Train_Loss: 1.4070885181427002, Test_Loss: 1.4187079668045044 *\n",
      "Epoch: 5, Train_Loss: 1.7956597805023193, Test_Loss: 1.4068466424942017 *\n",
      "Epoch: 5, Train_Loss: 1.5069838762283325, Test_Loss: 1.4029786586761475 *\n",
      "Epoch: 5, Train_Loss: 1.4596258401870728, Test_Loss: 1.4069890975952148\n",
      "Epoch: 5, Train_Loss: 1.7552253007888794, Test_Loss: 1.404058575630188 *\n",
      "Epoch: 5, Train_Loss: 1.4125967025756836, Test_Loss: 1.4012110233306885 *\n",
      "Epoch: 5, Train_Loss: 1.4671932458877563, Test_Loss: 1.4020988941192627\n",
      "Epoch: 5, Train_Loss: 1.4829131364822388, Test_Loss: 1.3996163606643677 *\n",
      "Epoch: 5, Train_Loss: 1.7936649322509766, Test_Loss: 1.4029231071472168\n",
      "Epoch: 5, Train_Loss: 1.4544702768325806, Test_Loss: 1.4043515920639038\n",
      "Epoch: 5, Train_Loss: 1.5056637525558472, Test_Loss: 1.4007419347763062 *\n",
      "Epoch: 5, Train_Loss: 1.6347140073776245, Test_Loss: 1.399150013923645 *\n",
      "Epoch: 5, Train_Loss: 1.5931683778762817, Test_Loss: 1.3983733654022217 *\n",
      "Epoch: 5, Train_Loss: 1.4287325143814087, Test_Loss: 1.3977679014205933 *\n",
      "Epoch: 5, Train_Loss: 1.4200130701065063, Test_Loss: 1.3980181217193604\n",
      "Epoch: 5, Train_Loss: 1.4029043912887573, Test_Loss: 1.4002315998077393\n",
      "Epoch: 5, Train_Loss: 1.4330583810806274, Test_Loss: 1.4548264741897583\n",
      "Epoch: 5, Train_Loss: 1.5949825048446655, Test_Loss: 2.4397823810577393\n",
      "Epoch: 5, Train_Loss: 2.0246684551239014, Test_Loss: 5.592970848083496\n",
      "Epoch: 5, Train_Loss: 1.7084347009658813, Test_Loss: 1.3951008319854736 *\n",
      "Epoch: 5, Train_Loss: 2.2780637741088867, Test_Loss: 1.391036033630371 *\n",
      "Epoch: 5, Train_Loss: 1.7362515926361084, Test_Loss: 1.4354528188705444\n",
      "Epoch: 5, Train_Loss: 1.8312511444091797, Test_Loss: 1.434312343597412 *\n",
      "Epoch: 5, Train_Loss: 1.6265326738357544, Test_Loss: 1.4698173999786377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 1.404109001159668, Test_Loss: 1.4008357524871826 *\n",
      "Epoch: 5, Train_Loss: 1.3973499536514282, Test_Loss: 1.4547792673110962\n",
      "Epoch: 5, Train_Loss: 1.430747151374817, Test_Loss: 1.4043587446212769 *\n",
      "Epoch: 5, Train_Loss: 1.5309700965881348, Test_Loss: 1.397297978401184 *\n",
      "Epoch: 5, Train_Loss: 2.122018337249756, Test_Loss: 1.4081859588623047\n",
      "Epoch: 5, Train_Loss: 1.5725886821746826, Test_Loss: 1.4489021301269531\n",
      "Epoch: 5, Train_Loss: 3.3744444847106934, Test_Loss: 1.391403317451477 *\n",
      "Epoch: 5, Train_Loss: 1.982861042022705, Test_Loss: 1.4469903707504272\n",
      "Epoch: 5, Train_Loss: 2.40432071685791, Test_Loss: 1.4364756345748901 *\n",
      "Epoch: 5, Train_Loss: 1.5089573860168457, Test_Loss: 1.4379549026489258\n",
      "Epoch: 5, Train_Loss: 1.3923298120498657, Test_Loss: 1.431817889213562 *\n",
      "Epoch: 5, Train_Loss: 1.5367088317871094, Test_Loss: 1.4173500537872314 *\n",
      "Epoch: 5, Train_Loss: 2.568769931793213, Test_Loss: 1.4108161926269531 *\n",
      "Epoch: 5, Train_Loss: 2.086381435394287, Test_Loss: 1.3807389736175537 *\n",
      "Epoch: 5, Train_Loss: 1.4670300483703613, Test_Loss: 1.3851919174194336\n",
      "Epoch: 5, Train_Loss: 1.4069511890411377, Test_Loss: 1.38420569896698 *\n",
      "Epoch: 5, Train_Loss: 1.4401464462280273, Test_Loss: 1.3819270133972168 *\n",
      "Epoch: 5, Train_Loss: 1.664564609527588, Test_Loss: 1.385837197303772\n",
      "Epoch: 5, Train_Loss: 1.658933162689209, Test_Loss: 1.3909832239151\n",
      "Epoch: 5, Train_Loss: 2.4567008018493652, Test_Loss: 1.3889080286026 *\n",
      "Epoch: 5, Train_Loss: 1.9259164333343506, Test_Loss: 1.381310224533081 *\n",
      "Epoch: 5, Train_Loss: 1.4754319190979004, Test_Loss: 1.3759464025497437 *\n",
      "Epoch: 5, Train_Loss: 1.406546711921692, Test_Loss: 1.4018131494522095\n",
      "Epoch: 5, Train_Loss: 1.4015705585479736, Test_Loss: 1.3750771284103394 *\n",
      "Epoch: 5, Train_Loss: 1.4038006067276, Test_Loss: 1.396157145500183\n",
      "Epoch: 5, Train_Loss: 1.4174479246139526, Test_Loss: 1.4021550416946411\n",
      "Epoch: 5, Train_Loss: 1.3866937160491943, Test_Loss: 1.6613951921463013\n",
      "Epoch: 5, Train_Loss: 1.392928123474121, Test_Loss: 1.7759429216384888\n",
      "Epoch: 5, Train_Loss: 18.561365127563477, Test_Loss: 1.4893441200256348 *\n",
      "Epoch: 5, Train_Loss: 1.3848769664764404, Test_Loss: 1.3871369361877441 *\n",
      "Epoch: 5, Train_Loss: 4.231987476348877, Test_Loss: 1.3888194561004639\n",
      "Epoch: 5, Train_Loss: 2.591064929962158, Test_Loss: 1.380760908126831 *\n",
      "Epoch: 5, Train_Loss: 1.3958691358566284, Test_Loss: 1.437361717224121\n",
      "Epoch: 5, Train_Loss: 1.4577609300613403, Test_Loss: 2.097222328186035\n",
      "Epoch: 5, Train_Loss: 7.933356285095215, Test_Loss: 2.037029504776001 *\n",
      "Epoch: 5, Train_Loss: 6.082004070281982, Test_Loss: 1.4339057207107544 *\n",
      "Epoch: 5, Train_Loss: 1.4106431007385254, Test_Loss: 1.4037010669708252 *\n",
      "Epoch: 5, Train_Loss: 1.6209065914154053, Test_Loss: 1.4107472896575928\n",
      "Epoch: 5, Train_Loss: 6.908623218536377, Test_Loss: 1.4438503980636597\n",
      "Epoch: 5, Train_Loss: 1.5155972242355347, Test_Loss: 1.5010528564453125\n",
      "Epoch: 5, Train_Loss: 1.42660653591156, Test_Loss: 1.4614325761795044 *\n",
      "Epoch: 5, Train_Loss: 1.3719584941864014, Test_Loss: 1.6531375646591187\n",
      "Epoch: 5, Train_Loss: 1.3790303468704224, Test_Loss: 1.5173954963684082 *\n",
      "Epoch: 5, Train_Loss: 1.4102516174316406, Test_Loss: 1.5811245441436768\n",
      "Epoch: 5, Train_Loss: 1.3868560791015625, Test_Loss: 1.4585440158843994 *\n",
      "Epoch: 5, Train_Loss: 1.4042530059814453, Test_Loss: 1.6597421169281006\n",
      "Epoch: 5, Train_Loss: 1.3740026950836182, Test_Loss: 1.4435087442398071 *\n",
      "Epoch: 5, Train_Loss: 1.3849583864212036, Test_Loss: 1.4821993112564087\n",
      "Epoch: 5, Train_Loss: 1.4231306314468384, Test_Loss: 1.4693961143493652 *\n",
      "Epoch: 5, Train_Loss: 1.40761137008667, Test_Loss: 1.442398190498352 *\n",
      "Epoch: 5, Train_Loss: 1.3979490995407104, Test_Loss: 1.4167072772979736 *\n",
      "Epoch: 5, Train_Loss: 1.425632357597351, Test_Loss: 1.3836655616760254 *\n",
      "Epoch: 5, Train_Loss: 1.465975046157837, Test_Loss: 1.6524217128753662\n",
      "Epoch: 5, Train_Loss: 1.367918848991394, Test_Loss: 7.223226547241211\n",
      "Epoch: 5, Train_Loss: 1.3807326555252075, Test_Loss: 1.5379793643951416 *\n",
      "Epoch: 5, Train_Loss: 1.3761610984802246, Test_Loss: 1.4164519309997559 *\n",
      "Epoch: 5, Train_Loss: 1.3673090934753418, Test_Loss: 1.4056130647659302 *\n",
      "Epoch: 5, Train_Loss: 1.3597118854522705, Test_Loss: 1.3779397010803223 *\n",
      "Epoch: 5, Train_Loss: 1.3543299436569214, Test_Loss: 1.4030851125717163\n",
      "Epoch: 5, Train_Loss: 1.3511452674865723, Test_Loss: 1.42256498336792\n",
      "Epoch: 5, Train_Loss: 1.3528417348861694, Test_Loss: 1.4628825187683105\n",
      "Epoch: 5, Train_Loss: 1.3498433828353882, Test_Loss: 1.3892319202423096 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 5\n",
      "Epoch: 5, Train_Loss: 1.347985029220581, Test_Loss: 1.414933681488037\n",
      "Epoch: 5, Train_Loss: 1.347650170326233, Test_Loss: 1.4208437204360962\n",
      "Epoch: 5, Train_Loss: 1.3494611978530884, Test_Loss: 1.442704677581787\n",
      "Epoch: 5, Train_Loss: 1.3522131443023682, Test_Loss: 1.3672189712524414 *\n",
      "Epoch: 5, Train_Loss: 1.3967212438583374, Test_Loss: 1.365468144416809 *\n",
      "Epoch: 5, Train_Loss: 1.3619906902313232, Test_Loss: 1.4083384275436401\n",
      "Epoch: 5, Train_Loss: 1.3825736045837402, Test_Loss: 1.38679838180542 *\n",
      "Epoch: 5, Train_Loss: 5.632072448730469, Test_Loss: 1.3856141567230225 *\n",
      "Epoch: 5, Train_Loss: 5.972491264343262, Test_Loss: 1.3979700803756714\n",
      "Epoch: 5, Train_Loss: 1.354807734489441, Test_Loss: 1.442204236984253\n",
      "Epoch: 5, Train_Loss: 1.3758102655410767, Test_Loss: 1.4007868766784668 *\n",
      "Epoch: 5, Train_Loss: 1.4152802228927612, Test_Loss: 1.4044026136398315\n",
      "Epoch: 5, Train_Loss: 1.3582947254180908, Test_Loss: 1.4116758108139038\n",
      "Epoch: 5, Train_Loss: 1.3431415557861328, Test_Loss: 1.4767683744430542\n",
      "Epoch: 5, Train_Loss: 1.4055309295654297, Test_Loss: 1.4747480154037476 *\n",
      "Epoch: 5, Train_Loss: 1.5152554512023926, Test_Loss: 1.45364511013031 *\n",
      "Epoch: 5, Train_Loss: 1.6770176887512207, Test_Loss: 1.4105314016342163 *\n",
      "Epoch: 5, Train_Loss: 1.5054162740707397, Test_Loss: 1.406903624534607 *\n",
      "Epoch: 5, Train_Loss: 1.397359013557434, Test_Loss: 1.4016953706741333 *\n",
      "Epoch: 5, Train_Loss: 1.402784824371338, Test_Loss: 1.4046189785003662\n",
      "Epoch: 5, Train_Loss: 1.4940986633300781, Test_Loss: 1.427238941192627\n",
      "Epoch: 5, Train_Loss: 1.4904636144638062, Test_Loss: 1.4239261150360107 *\n",
      "Epoch: 5, Train_Loss: 1.4758278131484985, Test_Loss: 3.882455348968506\n",
      "Epoch: 5, Train_Loss: 1.4122116565704346, Test_Loss: 4.786393642425537\n",
      "Epoch: 5, Train_Loss: 1.3858330249786377, Test_Loss: 1.350927710533142 *\n",
      "Epoch: 5, Train_Loss: 1.3373453617095947, Test_Loss: 1.3392215967178345 *\n",
      "Epoch: 5, Train_Loss: 1.3911927938461304, Test_Loss: 1.3505021333694458\n",
      "Epoch: 5, Train_Loss: 1.378630518913269, Test_Loss: 1.3411078453063965 *\n",
      "Epoch: 5, Train_Loss: 1.3495430946350098, Test_Loss: 1.358026146888733\n",
      "Epoch: 5, Train_Loss: 1.3352762460708618, Test_Loss: 1.3953423500061035\n",
      "Epoch: 5, Train_Loss: 1.339429497718811, Test_Loss: 1.4960137605667114\n",
      "Epoch: 5, Train_Loss: 1.3441318273544312, Test_Loss: 1.3363702297210693 *\n",
      "Epoch: 5, Train_Loss: 5.075801849365234, Test_Loss: 1.3577262163162231\n",
      "Epoch: 5, Train_Loss: 3.208970069885254, Test_Loss: 1.3591175079345703\n",
      "Epoch: 5, Train_Loss: 1.336045265197754, Test_Loss: 1.3404476642608643 *\n",
      "Epoch: 5, Train_Loss: 1.372094988822937, Test_Loss: 1.343734622001648\n",
      "Epoch: 5, Train_Loss: 1.3681966066360474, Test_Loss: 1.3682944774627686\n",
      "Epoch: 5, Train_Loss: 1.3504247665405273, Test_Loss: 1.3644767999649048 *\n",
      "Epoch: 5, Train_Loss: 1.3385865688323975, Test_Loss: 1.4426383972167969\n",
      "Epoch: 5, Train_Loss: 1.3501629829406738, Test_Loss: 1.4661260843276978\n",
      "Epoch: 5, Train_Loss: 1.3385639190673828, Test_Loss: 1.3373247385025024 *\n",
      "Epoch: 5, Train_Loss: 1.3343123197555542, Test_Loss: 1.3339381217956543 *\n",
      "Epoch: 5, Train_Loss: 1.360214352607727, Test_Loss: 1.3238853216171265 *\n",
      "Epoch: 5, Train_Loss: 1.3414138555526733, Test_Loss: 1.3232667446136475 *\n",
      "Epoch: 5, Train_Loss: 1.331613302230835, Test_Loss: 1.3226146697998047 *\n",
      "Epoch: 5, Train_Loss: 1.3471277952194214, Test_Loss: 1.3233187198638916\n",
      "Epoch: 5, Train_Loss: 1.325827717781067, Test_Loss: 1.322600245475769 *\n",
      "Epoch: 5, Train_Loss: 1.3268933296203613, Test_Loss: 1.3206077814102173 *\n",
      "Epoch: 5, Train_Loss: 1.327919602394104, Test_Loss: 1.3230392932891846\n",
      "Epoch: 5, Train_Loss: 1.368883490562439, Test_Loss: 1.3202694654464722 *\n",
      "Epoch: 5, Train_Loss: 1.3572206497192383, Test_Loss: 1.3207123279571533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 1.3213962316513062, Test_Loss: 1.3367161750793457\n",
      "Epoch: 5, Train_Loss: 1.3243318796157837, Test_Loss: 1.3216018676757812 *\n",
      "Epoch: 5, Train_Loss: 1.3535990715026855, Test_Loss: 1.3402891159057617\n",
      "Epoch: 6, Train_Loss: 1.3858228921890259, Test_Loss: 1.341217279434204 *\n",
      "Epoch: 6, Train_Loss: 1.3513442277908325, Test_Loss: 1.6936979293823242\n",
      "Epoch: 6, Train_Loss: 1.356937050819397, Test_Loss: 1.6781388521194458 *\n",
      "Epoch: 6, Train_Loss: 1.3577419519424438, Test_Loss: 1.40636146068573 *\n",
      "Epoch: 6, Train_Loss: 1.4019067287445068, Test_Loss: 1.3223633766174316 *\n",
      "Epoch: 6, Train_Loss: 1.371917486190796, Test_Loss: 1.3325821161270142\n",
      "Epoch: 6, Train_Loss: 1.3730512857437134, Test_Loss: 1.3590532541275024\n",
      "Epoch: 6, Train_Loss: 1.4502873420715332, Test_Loss: 1.5457671880722046\n",
      "Epoch: 6, Train_Loss: 1.356346607208252, Test_Loss: 2.446218490600586\n",
      "Epoch: 6, Train_Loss: 1.337014079093933, Test_Loss: 2.0163772106170654 *\n",
      "Epoch: 6, Train_Loss: 1.314887285232544, Test_Loss: 1.3572578430175781 *\n",
      "Epoch: 6, Train_Loss: 1.3129310607910156, Test_Loss: 1.3395798206329346 *\n",
      "Epoch: 6, Train_Loss: 1.3108617067337036, Test_Loss: 1.3236132860183716 *\n",
      "Epoch: 6, Train_Loss: 1.3091050386428833, Test_Loss: 1.3250514268875122\n",
      "Epoch: 6, Train_Loss: 1.3167227506637573, Test_Loss: 1.3301763534545898\n",
      "Epoch: 6, Train_Loss: 4.96144962310791, Test_Loss: 1.3269307613372803 *\n",
      "Epoch: 6, Train_Loss: 2.6851422786712646, Test_Loss: 1.39492666721344\n",
      "Epoch: 6, Train_Loss: 1.3078714609146118, Test_Loss: 1.307441234588623 *\n",
      "Epoch: 6, Train_Loss: 1.3238571882247925, Test_Loss: 1.3430246114730835\n",
      "Epoch: 6, Train_Loss: 1.3107707500457764, Test_Loss: 1.4336198568344116\n",
      "Epoch: 6, Train_Loss: 1.3036799430847168, Test_Loss: 1.6434869766235352\n",
      "Epoch: 6, Train_Loss: 1.3035060167312622, Test_Loss: 1.5309842824935913 *\n",
      "Epoch: 6, Train_Loss: 1.3025555610656738, Test_Loss: 1.3225923776626587 *\n",
      "Epoch: 6, Train_Loss: 1.301343321800232, Test_Loss: 1.3109526634216309 *\n",
      "Epoch: 6, Train_Loss: 1.301111102104187, Test_Loss: 1.3101097345352173 *\n",
      "Epoch: 6, Train_Loss: 1.3244682550430298, Test_Loss: 1.3093242645263672 *\n",
      "Epoch: 6, Train_Loss: 1.3661561012268066, Test_Loss: 1.3165737390518188\n",
      "Epoch: 6, Train_Loss: 1.3633533716201782, Test_Loss: 2.2924726009368896\n",
      "Epoch: 6, Train_Loss: 1.3828165531158447, Test_Loss: 5.719328880310059\n",
      "Epoch: 6, Train_Loss: 1.3147417306900024, Test_Loss: 1.3178924322128296 *\n",
      "Epoch: 6, Train_Loss: 1.3180934190750122, Test_Loss: 1.3021535873413086 *\n",
      "Epoch: 6, Train_Loss: 1.4809361696243286, Test_Loss: 1.2995582818984985 *\n",
      "Epoch: 6, Train_Loss: 1.4879359006881714, Test_Loss: 1.3014534711837769\n",
      "Epoch: 6, Train_Loss: 1.4810420274734497, Test_Loss: 1.3007615804672241 *\n",
      "Epoch: 6, Train_Loss: 1.3428624868392944, Test_Loss: 1.300990343093872\n",
      "Epoch: 6, Train_Loss: 1.2953803539276123, Test_Loss: 1.3102976083755493\n",
      "Epoch: 6, Train_Loss: 1.295228362083435, Test_Loss: 1.299936056137085 *\n",
      "Epoch: 6, Train_Loss: 1.2977228164672852, Test_Loss: 1.3048537969589233\n",
      "Epoch: 6, Train_Loss: 1.2994693517684937, Test_Loss: 1.3054163455963135\n",
      "Epoch: 6, Train_Loss: 1.2964212894439697, Test_Loss: 1.3140912055969238\n",
      "Epoch: 6, Train_Loss: 1.2948565483093262, Test_Loss: 1.2989782094955444 *\n",
      "Epoch: 6, Train_Loss: 1.2945469617843628, Test_Loss: 1.2947641611099243 *\n",
      "Epoch: 6, Train_Loss: 1.2928663492202759, Test_Loss: 1.307741403579712\n",
      "Epoch: 6, Train_Loss: 1.2999577522277832, Test_Loss: 1.2958426475524902 *\n",
      "Epoch: 6, Train_Loss: 1.3773844242095947, Test_Loss: 1.2988038063049316\n",
      "Epoch: 6, Train_Loss: 1.477166771888733, Test_Loss: 1.2980936765670776 *\n",
      "Epoch: 6, Train_Loss: 1.4663431644439697, Test_Loss: 1.3066807985305786\n",
      "Epoch: 6, Train_Loss: 1.3662800788879395, Test_Loss: 1.2966656684875488 *\n",
      "Epoch: 6, Train_Loss: 1.4442150592803955, Test_Loss: 1.29867684841156\n",
      "Epoch: 6, Train_Loss: 1.4842888116836548, Test_Loss: 1.3012627363204956\n",
      "Epoch: 6, Train_Loss: 1.3097889423370361, Test_Loss: 1.3142846822738647\n",
      "Epoch: 6, Train_Loss: 1.5083277225494385, Test_Loss: 1.3068581819534302 *\n",
      "Epoch: 6, Train_Loss: 1.4513455629348755, Test_Loss: 1.299452304840088 *\n",
      "Epoch: 6, Train_Loss: 1.5777502059936523, Test_Loss: 1.29375159740448 *\n",
      "Epoch: 6, Train_Loss: 1.302329659461975, Test_Loss: 1.2970565557479858\n",
      "Epoch: 6, Train_Loss: 1.977525234222412, Test_Loss: 1.291877031326294 *\n",
      "Epoch: 6, Train_Loss: 3.90351939201355, Test_Loss: 1.291646122932434 *\n",
      "Epoch: 6, Train_Loss: 1.3284218311309814, Test_Loss: 1.3255380392074585\n",
      "Epoch: 6, Train_Loss: 1.3192335367202759, Test_Loss: 1.3273591995239258\n",
      "Epoch: 6, Train_Loss: 1.318885087966919, Test_Loss: 4.681056499481201\n",
      "Epoch: 6, Train_Loss: 1.3201603889465332, Test_Loss: 3.602208375930786 *\n",
      "Epoch: 6, Train_Loss: 1.283196210861206, Test_Loss: 1.2860900163650513 *\n",
      "Epoch: 6, Train_Loss: 1.288548469543457, Test_Loss: 1.2845160961151123 *\n",
      "Epoch: 6, Train_Loss: 1.420211672782898, Test_Loss: 1.3173441886901855\n",
      "Epoch: 6, Train_Loss: 1.4204351902008057, Test_Loss: 1.3220722675323486\n",
      "Epoch: 6, Train_Loss: 1.4057807922363281, Test_Loss: 1.301952838897705 *\n",
      "Epoch: 6, Train_Loss: 1.3704736232757568, Test_Loss: 1.3583941459655762\n",
      "Epoch: 6, Train_Loss: 1.362512230873108, Test_Loss: 1.4053541421890259\n",
      "Epoch: 6, Train_Loss: 1.316611647605896, Test_Loss: 1.2779607772827148 *\n",
      "Epoch: 6, Train_Loss: 1.3124706745147705, Test_Loss: 1.3164609670639038\n",
      "Epoch: 6, Train_Loss: 1.2836896181106567, Test_Loss: 1.2949670553207397 *\n",
      "Epoch: 6, Train_Loss: 1.292885661125183, Test_Loss: 1.2895638942718506 *\n",
      "Epoch: 6, Train_Loss: 1.282219648361206, Test_Loss: 1.2830133438110352 *\n",
      "Epoch: 6, Train_Loss: 1.2728191614151, Test_Loss: 1.3537932634353638\n",
      "Epoch: 6, Train_Loss: 1.3148553371429443, Test_Loss: 1.3260854482650757 *\n",
      "Epoch: 6, Train_Loss: 1.340799331665039, Test_Loss: 1.3888720273971558\n",
      "Epoch: 6, Train_Loss: 1.313691258430481, Test_Loss: 1.3755033016204834 *\n",
      "Epoch: 6, Train_Loss: 1.2701196670532227, Test_Loss: 1.2915304899215698 *\n",
      "Epoch: 6, Train_Loss: 1.2702101469039917, Test_Loss: 1.2806187868118286 *\n",
      "Epoch: 6, Train_Loss: 1.2691346406936646, Test_Loss: 1.2764859199523926 *\n",
      "Epoch: 6, Train_Loss: 1.2683054208755493, Test_Loss: 1.2746236324310303 *\n",
      "Epoch: 6, Train_Loss: 1.2687761783599854, Test_Loss: 1.274541974067688 *\n",
      "Epoch: 6, Train_Loss: 1.267421841621399, Test_Loss: 1.2753204107284546\n",
      "Epoch: 6, Train_Loss: 1.2676857709884644, Test_Loss: 1.2745169401168823 *\n",
      "Epoch: 6, Train_Loss: 1.2667244672775269, Test_Loss: 1.2715520858764648 *\n",
      "Epoch: 6, Train_Loss: 1.26543128490448, Test_Loss: 1.280846118927002\n",
      "Epoch: 6, Train_Loss: 1.26885986328125, Test_Loss: 1.2763214111328125 *\n",
      "Epoch: 6, Train_Loss: 1.2750728130340576, Test_Loss: 1.275539755821228 *\n",
      "Epoch: 6, Train_Loss: 1.2769206762313843, Test_Loss: 1.2718653678894043 *\n",
      "Epoch: 6, Train_Loss: 1.2744412422180176, Test_Loss: 1.2906655073165894\n",
      "Epoch: 6, Train_Loss: 1.2818084955215454, Test_Loss: 1.3078088760375977\n",
      "Epoch: 6, Train_Loss: 1.2659039497375488, Test_Loss: 1.3388890027999878\n",
      "Epoch: 6, Train_Loss: 1.27129065990448, Test_Loss: 1.7728632688522339\n",
      "Epoch: 6, Train_Loss: 1.2610708475112915, Test_Loss: 1.65260648727417 *\n",
      "Epoch: 6, Train_Loss: 1.2612296342849731, Test_Loss: 1.3650243282318115 *\n",
      "Epoch: 6, Train_Loss: 1.2758831977844238, Test_Loss: 1.281982183456421 *\n",
      "Epoch: 6, Train_Loss: 1.2746902704238892, Test_Loss: 1.276328682899475 *\n",
      "Epoch: 6, Train_Loss: 1.259464144706726, Test_Loss: 1.3110253810882568\n",
      "Epoch: 6, Train_Loss: 1.257878303527832, Test_Loss: 1.6579457521438599\n",
      "Epoch: 6, Train_Loss: 1.2649765014648438, Test_Loss: 2.488056182861328\n",
      "Model saved at location save_new\\model.ckpt at epoch 6\n",
      "Epoch: 6, Train_Loss: 1.3473232984542847, Test_Loss: 1.802502155303955 *\n",
      "Epoch: 6, Train_Loss: 1.2743593454360962, Test_Loss: 1.337756633758545 *\n",
      "Epoch: 6, Train_Loss: 1.290019154548645, Test_Loss: 1.2701534032821655 *\n",
      "Epoch: 6, Train_Loss: 1.2561702728271484, Test_Loss: 1.2669119834899902 *\n",
      "Epoch: 6, Train_Loss: 1.277464509010315, Test_Loss: 1.261095643043518 *\n",
      "Epoch: 6, Train_Loss: 1.292651653289795, Test_Loss: 1.2643756866455078\n",
      "Epoch: 6, Train_Loss: 1.2568601369857788, Test_Loss: 1.292595624923706\n",
      "Epoch: 6, Train_Loss: 1.2715657949447632, Test_Loss: 1.3230715990066528\n",
      "Epoch: 6, Train_Loss: 1.2923338413238525, Test_Loss: 1.2559374570846558 *\n",
      "Epoch: 6, Train_Loss: 1.3439102172851562, Test_Loss: 1.314866065979004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_Loss: 1.3675280809402466, Test_Loss: 1.4069069623947144\n",
      "Epoch: 6, Train_Loss: 1.3476144075393677, Test_Loss: 1.5510470867156982\n",
      "Epoch: 6, Train_Loss: 1.2983176708221436, Test_Loss: 1.4427552223205566 *\n",
      "Epoch: 6, Train_Loss: 1.2527084350585938, Test_Loss: 1.2565046548843384 *\n",
      "Epoch: 6, Train_Loss: 1.2833302021026611, Test_Loss: 1.2530368566513062 *\n",
      "Epoch: 6, Train_Loss: 1.2507822513580322, Test_Loss: 1.252454161643982 *\n",
      "Epoch: 6, Train_Loss: 1.260404109954834, Test_Loss: 1.251832365989685 *\n",
      "Epoch: 6, Train_Loss: 1.2550925016403198, Test_Loss: 1.2596466541290283\n",
      "Epoch: 6, Train_Loss: 1.2564440965652466, Test_Loss: 3.4694623947143555\n",
      "Epoch: 6, Train_Loss: 1.323449730873108, Test_Loss: 4.605767250061035\n",
      "Epoch: 6, Train_Loss: 1.2591623067855835, Test_Loss: 1.2553796768188477 *\n",
      "Epoch: 6, Train_Loss: 1.335556149482727, Test_Loss: 1.2478753328323364 *\n",
      "Epoch: 6, Train_Loss: 1.253861427307129, Test_Loss: 1.2454792261123657 *\n",
      "Epoch: 6, Train_Loss: 1.2674590349197388, Test_Loss: 1.2484711408615112\n",
      "Epoch: 6, Train_Loss: 1.2499244213104248, Test_Loss: 1.2465283870697021 *\n",
      "Epoch: 6, Train_Loss: 1.5022441148757935, Test_Loss: 1.2499994039535522\n",
      "Epoch: 6, Train_Loss: 1.3048878908157349, Test_Loss: 1.2489460706710815 *\n",
      "Epoch: 6, Train_Loss: 1.2598286867141724, Test_Loss: 1.2487603425979614 *\n",
      "Epoch: 6, Train_Loss: 1.254565954208374, Test_Loss: 1.2500638961791992\n",
      "Epoch: 6, Train_Loss: 1.2420639991760254, Test_Loss: 1.24701726436615 *\n",
      "Epoch: 6, Train_Loss: 1.242598533630371, Test_Loss: 1.2508963346481323\n",
      "Epoch: 6, Train_Loss: 1.2430492639541626, Test_Loss: 1.2547926902770996\n",
      "Epoch: 6, Train_Loss: 1.2497690916061401, Test_Loss: 1.2495309114456177 *\n",
      "Epoch: 6, Train_Loss: 1.255741000175476, Test_Loss: 1.2491929531097412 *\n",
      "Epoch: 6, Train_Loss: 1.2695226669311523, Test_Loss: 1.2395232915878296 *\n",
      "Epoch: 6, Train_Loss: 1.2478607892990112, Test_Loss: 1.243198037147522\n",
      "Epoch: 6, Train_Loss: 1.2537751197814941, Test_Loss: 1.2396745681762695 *\n",
      "Epoch: 6, Train_Loss: 1.26617431640625, Test_Loss: 1.241757869720459\n",
      "Epoch: 6, Train_Loss: 1.2414058446884155, Test_Loss: 1.241353988647461 *\n",
      "Epoch: 6, Train_Loss: 1.2380039691925049, Test_Loss: 1.2377828359603882 *\n",
      "Epoch: 6, Train_Loss: 1.2411882877349854, Test_Loss: 1.2391483783721924\n",
      "Epoch: 6, Train_Loss: 1.2539483308792114, Test_Loss: 1.2444607019424438\n",
      "Epoch: 6, Train_Loss: 1.2625049352645874, Test_Loss: 1.2390413284301758 *\n",
      "Epoch: 6, Train_Loss: 1.23545503616333, Test_Loss: 1.2397311925888062\n",
      "Epoch: 6, Train_Loss: 1.2780842781066895, Test_Loss: 1.2361910343170166 *\n",
      "Epoch: 6, Train_Loss: 1.3141671419143677, Test_Loss: 1.2379873991012573\n",
      "Epoch: 6, Train_Loss: 1.29430091381073, Test_Loss: 1.236607313156128 *\n",
      "Epoch: 6, Train_Loss: 1.2311044931411743, Test_Loss: 1.2359215021133423 *\n",
      "Epoch: 6, Train_Loss: 1.2539260387420654, Test_Loss: 1.2823154926300049\n",
      "Epoch: 6, Train_Loss: 1.2361438274383545, Test_Loss: 1.2598196268081665 *\n",
      "Epoch: 6, Train_Loss: 1.2538875341415405, Test_Loss: 5.802034378051758\n",
      "Epoch: 6, Train_Loss: 1.2317485809326172, Test_Loss: 2.2933099269866943 *\n",
      "Epoch: 6, Train_Loss: 1.2498793601989746, Test_Loss: 1.2295161485671997 *\n",
      "Epoch: 6, Train_Loss: 1.284576177597046, Test_Loss: 1.2405729293823242\n",
      "Epoch: 6, Train_Loss: 3.623399257659912, Test_Loss: 1.2708934545516968\n",
      "Epoch: 6, Train_Loss: 4.221529483795166, Test_Loss: 1.281276822090149\n",
      "Epoch: 6, Train_Loss: 1.2530993223190308, Test_Loss: 1.2357447147369385 *\n",
      "Epoch: 6, Train_Loss: 1.2275532484054565, Test_Loss: 1.3220210075378418\n",
      "Epoch: 6, Train_Loss: 1.358053207397461, Test_Loss: 1.3163347244262695 *\n",
      "Epoch: 6, Train_Loss: 1.3841923475265503, Test_Loss: 1.2252126932144165 *\n",
      "Epoch: 6, Train_Loss: 1.2439826726913452, Test_Loss: 1.2643251419067383\n",
      "Epoch: 6, Train_Loss: 1.2245593070983887, Test_Loss: 1.236736536026001 *\n",
      "Epoch: 6, Train_Loss: 1.2568966150283813, Test_Loss: 1.234639048576355 *\n",
      "Epoch: 6, Train_Loss: 1.2581833600997925, Test_Loss: 1.2254928350448608 *\n",
      "Epoch: 6, Train_Loss: 1.2328083515167236, Test_Loss: 1.3236373662948608\n",
      "Epoch: 6, Train_Loss: 1.2497830390930176, Test_Loss: 1.2595205307006836 *\n",
      "Epoch: 6, Train_Loss: 2.400362014770508, Test_Loss: 1.32674241065979\n",
      "Epoch: 6, Train_Loss: 2.6543633937835693, Test_Loss: 1.2743638753890991 *\n",
      "Epoch: 6, Train_Loss: 1.4364292621612549, Test_Loss: 1.2689138650894165 *\n",
      "Epoch: 6, Train_Loss: 1.29438316822052, Test_Loss: 1.238448143005371 *\n",
      "Epoch: 6, Train_Loss: 2.7614214420318604, Test_Loss: 1.233413815498352 *\n",
      "Epoch: 6, Train_Loss: 3.125246047973633, Test_Loss: 1.234391212463379\n",
      "Epoch: 6, Train_Loss: 1.2874120473861694, Test_Loss: 1.2313772439956665 *\n",
      "Epoch: 6, Train_Loss: 1.2643617391586304, Test_Loss: 1.2382546663284302\n",
      "Epoch: 6, Train_Loss: 1.376218318939209, Test_Loss: 1.2337106466293335 *\n",
      "Epoch: 6, Train_Loss: 2.7110424041748047, Test_Loss: 1.2196388244628906 *\n",
      "Epoch: 6, Train_Loss: 2.4417362213134766, Test_Loss: 1.235694169998169\n",
      "Epoch: 6, Train_Loss: 1.2210105657577515, Test_Loss: 1.225840449333191 *\n",
      "Epoch: 6, Train_Loss: 1.2263096570968628, Test_Loss: 1.2206696271896362 *\n",
      "Epoch: 6, Train_Loss: 1.267281174659729, Test_Loss: 1.2233830690383911\n",
      "Epoch: 6, Train_Loss: 2.1148931980133057, Test_Loss: 1.2523362636566162\n",
      "Epoch: 6, Train_Loss: 1.2863770723342896, Test_Loss: 1.2347296476364136 *\n",
      "Epoch: 6, Train_Loss: 1.3190333843231201, Test_Loss: 1.3927761316299438\n",
      "Epoch: 6, Train_Loss: 1.2828627824783325, Test_Loss: 1.7066290378570557\n",
      "Epoch: 6, Train_Loss: 1.3595162630081177, Test_Loss: 1.5086181163787842 *\n",
      "Epoch: 6, Train_Loss: 1.3222228288650513, Test_Loss: 1.3076808452606201 *\n",
      "Epoch: 6, Train_Loss: 1.4399149417877197, Test_Loss: 1.2254009246826172 *\n",
      "Epoch: 6, Train_Loss: 1.4714984893798828, Test_Loss: 1.2235335111618042 *\n",
      "Epoch: 6, Train_Loss: 1.2542074918746948, Test_Loss: 1.2521597146987915\n",
      "Epoch: 6, Train_Loss: 1.3327361345291138, Test_Loss: 1.664322018623352\n",
      "Epoch: 6, Train_Loss: 1.3931139707565308, Test_Loss: 2.2098522186279297\n",
      "Epoch: 6, Train_Loss: 1.5965455770492554, Test_Loss: 1.5088744163513184 *\n",
      "Epoch: 6, Train_Loss: 1.5004945993423462, Test_Loss: 1.283918023109436 *\n",
      "Epoch: 6, Train_Loss: 1.2313179969787598, Test_Loss: 1.211810827255249 *\n",
      "Epoch: 6, Train_Loss: 1.2940497398376465, Test_Loss: 1.2229291200637817\n",
      "Epoch: 6, Train_Loss: 1.3581970930099487, Test_Loss: 1.2176934480667114 *\n",
      "Epoch: 6, Train_Loss: 1.2174586057662964, Test_Loss: 1.2345832586288452\n",
      "Epoch: 6, Train_Loss: 1.2138277292251587, Test_Loss: 1.219228982925415 *\n",
      "Epoch: 6, Train_Loss: 1.2040494680404663, Test_Loss: 1.2457412481307983\n",
      "Epoch: 6, Train_Loss: 1.205368161201477, Test_Loss: 1.2089663743972778 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 6\n",
      "Epoch: 6, Train_Loss: 1.2049760818481445, Test_Loss: 1.3097634315490723\n",
      "Epoch: 6, Train_Loss: 1.2088733911514282, Test_Loss: 1.5231983661651611\n",
      "Epoch: 6, Train_Loss: 1.2636765241622925, Test_Loss: 1.3498634099960327 *\n",
      "Epoch: 6, Train_Loss: 1.2270700931549072, Test_Loss: 1.433261513710022\n",
      "Epoch: 6, Train_Loss: 1.312692403793335, Test_Loss: 1.2052916288375854 *\n",
      "Epoch: 6, Train_Loss: 1.308024287223816, Test_Loss: 1.199554204940796 *\n",
      "Epoch: 6, Train_Loss: 1.565943956375122, Test_Loss: 1.198737382888794 *\n",
      "Epoch: 6, Train_Loss: 1.2129075527191162, Test_Loss: 1.1981620788574219 *\n",
      "Epoch: 6, Train_Loss: 1.232009768486023, Test_Loss: 1.2428431510925293\n",
      "Epoch: 6, Train_Loss: 1.4572675228118896, Test_Loss: 4.8202667236328125\n",
      "Epoch: 6, Train_Loss: 1.6733635663986206, Test_Loss: 3.0800461769104004 *\n",
      "Epoch: 6, Train_Loss: 1.3946501016616821, Test_Loss: 1.204315185546875 *\n",
      "Epoch: 6, Train_Loss: 1.2099707126617432, Test_Loss: 1.20656156539917\n",
      "Epoch: 6, Train_Loss: 1.4395780563354492, Test_Loss: 1.2024950981140137 *\n",
      "Epoch: 6, Train_Loss: 1.7006053924560547, Test_Loss: 1.195630669593811 *\n",
      "Epoch: 6, Train_Loss: 1.5187946557998657, Test_Loss: 1.2189826965332031\n",
      "Epoch: 6, Train_Loss: 1.2098784446716309, Test_Loss: 1.2363907098770142\n",
      "Epoch: 6, Train_Loss: 1.2054791450500488, Test_Loss: 1.2132232189178467 *\n",
      "Epoch: 6, Train_Loss: 1.2428065538406372, Test_Loss: 1.217632532119751\n",
      "Epoch: 6, Train_Loss: 2.4262959957122803, Test_Loss: 1.231304407119751\n",
      "Epoch: 6, Train_Loss: 1.8423261642456055, Test_Loss: 1.2393417358398438\n",
      "Epoch: 6, Train_Loss: 1.2089356184005737, Test_Loss: 1.2292437553405762 *\n",
      "Epoch: 6, Train_Loss: 1.217166543006897, Test_Loss: 1.1909080743789673 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_Loss: 1.1945340633392334, Test_Loss: 1.1975280046463013\n",
      "Epoch: 6, Train_Loss: 1.3516584634780884, Test_Loss: 1.21467924118042\n",
      "Epoch: 6, Train_Loss: 1.4887380599975586, Test_Loss: 1.1942716836929321 *\n",
      "Epoch: 6, Train_Loss: 1.1930811405181885, Test_Loss: 1.2179425954818726\n",
      "Epoch: 6, Train_Loss: 1.5491771697998047, Test_Loss: 1.206298589706421 *\n",
      "Epoch: 6, Train_Loss: 1.217646598815918, Test_Loss: 1.210693359375\n",
      "Epoch: 6, Train_Loss: 1.201236605644226, Test_Loss: 1.1964571475982666 *\n",
      "Epoch: 6, Train_Loss: 1.235966682434082, Test_Loss: 1.210679531097412\n",
      "Epoch: 6, Train_Loss: 1.3790132999420166, Test_Loss: 1.2216094732284546\n",
      "Epoch: 6, Train_Loss: 1.2536134719848633, Test_Loss: 1.23726224899292\n",
      "Epoch: 6, Train_Loss: 1.2532601356506348, Test_Loss: 1.2235004901885986 *\n",
      "Epoch: 6, Train_Loss: 1.2004356384277344, Test_Loss: 1.210255742073059 *\n",
      "Epoch: 6, Train_Loss: 1.308319330215454, Test_Loss: 1.1958814859390259 *\n",
      "Epoch: 6, Train_Loss: 1.2132422924041748, Test_Loss: 1.208215594291687\n",
      "Epoch: 6, Train_Loss: 1.232393741607666, Test_Loss: 1.2069857120513916 *\n",
      "Epoch: 6, Train_Loss: 1.1973838806152344, Test_Loss: 1.204298496246338 *\n",
      "Epoch: 6, Train_Loss: 1.207051157951355, Test_Loss: 1.2792799472808838\n",
      "Epoch: 6, Train_Loss: 1.2440783977508545, Test_Loss: 1.2309445142745972 *\n",
      "Epoch: 6, Train_Loss: 1.5281178951263428, Test_Loss: 6.824357032775879\n",
      "Epoch: 6, Train_Loss: 1.3539468050003052, Test_Loss: 1.2802932262420654 *\n",
      "Epoch: 6, Train_Loss: 1.7133703231811523, Test_Loss: 1.2354766130447388 *\n",
      "Epoch: 6, Train_Loss: 1.5726165771484375, Test_Loss: 1.2565016746520996\n",
      "Epoch: 6, Train_Loss: 1.3537461757659912, Test_Loss: 1.1873600482940674 *\n",
      "Epoch: 6, Train_Loss: 1.3046067953109741, Test_Loss: 1.2137157917022705\n",
      "Epoch: 6, Train_Loss: 1.2411067485809326, Test_Loss: 1.189736008644104 *\n",
      "Epoch: 6, Train_Loss: 1.1977943181991577, Test_Loss: 1.2019314765930176\n",
      "Epoch: 6, Train_Loss: 1.1840368509292603, Test_Loss: 1.216130018234253\n",
      "Epoch: 6, Train_Loss: 1.2444454431533813, Test_Loss: 1.2133874893188477 *\n",
      "Epoch: 6, Train_Loss: 1.5428186655044556, Test_Loss: 1.2156291007995605\n",
      "Epoch: 6, Train_Loss: 1.5575246810913086, Test_Loss: 1.3304511308670044\n",
      "Epoch: 6, Train_Loss: 2.3825197219848633, Test_Loss: 1.2591336965560913 *\n",
      "Epoch: 6, Train_Loss: 2.0864338874816895, Test_Loss: 1.2646238803863525\n",
      "Epoch: 6, Train_Loss: 1.640407919883728, Test_Loss: 1.1744775772094727 *\n",
      "Epoch: 6, Train_Loss: 1.4457299709320068, Test_Loss: 1.284813642501831\n",
      "Epoch: 6, Train_Loss: 1.1922643184661865, Test_Loss: 1.1955662965774536 *\n",
      "Epoch: 6, Train_Loss: 1.2479846477508545, Test_Loss: 1.281115174293518\n",
      "Epoch: 6, Train_Loss: 1.7574684619903564, Test_Loss: 1.3519341945648193\n",
      "Epoch: 6, Train_Loss: 2.1975784301757812, Test_Loss: 1.1866604089736938 *\n",
      "Epoch: 6, Train_Loss: 1.2472809553146362, Test_Loss: 1.2198498249053955\n",
      "Epoch: 6, Train_Loss: 1.2565784454345703, Test_Loss: 1.2370257377624512\n",
      "Epoch: 6, Train_Loss: 1.3111746311187744, Test_Loss: 1.2446556091308594\n",
      "Epoch: 6, Train_Loss: 1.4491300582885742, Test_Loss: 1.2202959060668945 *\n",
      "Epoch: 6, Train_Loss: 1.3660489320755005, Test_Loss: 1.2298051118850708\n",
      "Epoch: 6, Train_Loss: 1.6908109188079834, Test_Loss: 1.255550503730774\n",
      "Epoch: 6, Train_Loss: 1.5074442625045776, Test_Loss: 1.214796543121338 *\n",
      "Epoch: 6, Train_Loss: 1.4730381965637207, Test_Loss: 1.218550205230713\n",
      "Epoch: 6, Train_Loss: 1.198289394378662, Test_Loss: 1.2629497051239014\n",
      "Epoch: 6, Train_Loss: 1.1795003414154053, Test_Loss: 1.2637025117874146\n",
      "Epoch: 6, Train_Loss: 1.1825064420700073, Test_Loss: 1.180197834968567 *\n",
      "Epoch: 6, Train_Loss: 1.2572747468948364, Test_Loss: 1.2506999969482422\n",
      "Epoch: 6, Train_Loss: 1.196966528892517, Test_Loss: 1.2824937105178833\n",
      "Epoch: 6, Train_Loss: 1.2079356908798218, Test_Loss: 1.3591305017471313\n",
      "Epoch: 6, Train_Loss: 17.53778839111328, Test_Loss: 1.239374041557312 *\n",
      "Epoch: 6, Train_Loss: 1.2070376873016357, Test_Loss: 1.199406623840332 *\n",
      "Epoch: 6, Train_Loss: 2.9836597442626953, Test_Loss: 1.1704809665679932 *\n",
      "Epoch: 6, Train_Loss: 2.7948107719421387, Test_Loss: 1.1837910413742065\n",
      "Epoch: 6, Train_Loss: 1.1963733434677124, Test_Loss: 1.229408860206604\n",
      "Epoch: 6, Train_Loss: 1.302945852279663, Test_Loss: 1.4959440231323242\n",
      "Epoch: 6, Train_Loss: 4.633947849273682, Test_Loss: 1.6414005756378174\n",
      "Epoch: 6, Train_Loss: 7.712566375732422, Test_Loss: 1.3653709888458252 *\n",
      "Epoch: 6, Train_Loss: 1.3061695098876953, Test_Loss: 1.1981419324874878 *\n",
      "Epoch: 6, Train_Loss: 1.259735345840454, Test_Loss: 1.2307283878326416\n",
      "Epoch: 6, Train_Loss: 6.338540554046631, Test_Loss: 1.4271812438964844\n",
      "Epoch: 6, Train_Loss: 1.3527705669403076, Test_Loss: 1.4599580764770508\n",
      "Epoch: 6, Train_Loss: 1.2700121402740479, Test_Loss: 1.5453864336013794\n",
      "Epoch: 6, Train_Loss: 1.1769239902496338, Test_Loss: 1.3973783254623413 *\n",
      "Epoch: 6, Train_Loss: 1.1700574159622192, Test_Loss: 1.4092762470245361\n",
      "Epoch: 6, Train_Loss: 1.2273603677749634, Test_Loss: 1.46030855178833\n",
      "Epoch: 6, Train_Loss: 1.180321455001831, Test_Loss: 1.2677110433578491 *\n",
      "Epoch: 6, Train_Loss: 1.1925042867660522, Test_Loss: 1.6344029903411865\n",
      "Epoch: 6, Train_Loss: 1.1570584774017334, Test_Loss: 1.208093523979187 *\n",
      "Epoch: 6, Train_Loss: 1.1628401279449463, Test_Loss: 1.3150255680084229\n",
      "Epoch: 6, Train_Loss: 1.2374564409255981, Test_Loss: 1.4895524978637695\n",
      "Epoch: 6, Train_Loss: 1.234659194946289, Test_Loss: 1.4232734441757202 *\n",
      "Epoch: 6, Train_Loss: 1.216537594795227, Test_Loss: 1.3567272424697876 *\n",
      "Epoch: 6, Train_Loss: 1.2481629848480225, Test_Loss: 1.2791770696640015 *\n",
      "Epoch: 6, Train_Loss: 1.2212960720062256, Test_Loss: 1.171021819114685 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 6\n",
      "Epoch: 6, Train_Loss: 1.1653707027435303, Test_Loss: 7.158212661743164\n",
      "Epoch: 6, Train_Loss: 1.1695122718811035, Test_Loss: 2.0968854427337646 *\n",
      "Epoch: 6, Train_Loss: 1.1581753492355347, Test_Loss: 1.2648084163665771 *\n",
      "Epoch: 6, Train_Loss: 1.1547348499298096, Test_Loss: 1.2525722980499268 *\n",
      "Epoch: 6, Train_Loss: 1.1522084474563599, Test_Loss: 1.2182501554489136 *\n",
      "Epoch: 6, Train_Loss: 1.1492449045181274, Test_Loss: 1.1699354648590088 *\n",
      "Epoch: 6, Train_Loss: 1.1481436491012573, Test_Loss: 1.2505292892456055\n",
      "Epoch: 6, Train_Loss: 1.1470950841903687, Test_Loss: 1.2804090976715088\n",
      "Epoch: 6, Train_Loss: 1.1476877927780151, Test_Loss: 1.1801270246505737 *\n",
      "Epoch: 6, Train_Loss: 1.1457377672195435, Test_Loss: 1.1931077241897583\n",
      "Epoch: 6, Train_Loss: 1.1450871229171753, Test_Loss: 1.2057515382766724\n",
      "Epoch: 6, Train_Loss: 1.1485081911087036, Test_Loss: 1.2366278171539307\n",
      "Epoch: 6, Train_Loss: 1.1647456884384155, Test_Loss: 1.177982211112976 *\n",
      "Epoch: 6, Train_Loss: 1.175537347793579, Test_Loss: 1.1716203689575195 *\n",
      "Epoch: 6, Train_Loss: 1.1706010103225708, Test_Loss: 1.21570885181427\n",
      "Epoch: 6, Train_Loss: 1.16005277633667, Test_Loss: 1.1956236362457275 *\n",
      "Epoch: 6, Train_Loss: 2.5118634700775146, Test_Loss: 1.1538360118865967 *\n",
      "Epoch: 6, Train_Loss: 8.675617218017578, Test_Loss: 1.2069923877716064\n",
      "Epoch: 6, Train_Loss: 1.1607189178466797, Test_Loss: 1.2038395404815674 *\n",
      "Epoch: 6, Train_Loss: 1.1799598932266235, Test_Loss: 1.2188596725463867\n",
      "Epoch: 6, Train_Loss: 1.2315274477005005, Test_Loss: 1.2182132005691528 *\n",
      "Epoch: 6, Train_Loss: 1.184374451637268, Test_Loss: 1.2389713525772095\n",
      "Epoch: 6, Train_Loss: 1.1461862325668335, Test_Loss: 1.3287895917892456\n",
      "Epoch: 6, Train_Loss: 1.211014986038208, Test_Loss: 1.31319260597229 *\n",
      "Epoch: 6, Train_Loss: 1.2906923294067383, Test_Loss: 1.2780975103378296 *\n",
      "Epoch: 6, Train_Loss: 1.399961233139038, Test_Loss: 1.2232294082641602 *\n",
      "Epoch: 6, Train_Loss: 1.299119234085083, Test_Loss: 1.1921387910842896 *\n",
      "Epoch: 6, Train_Loss: 1.2259066104888916, Test_Loss: 1.2140864133834839\n",
      "Epoch: 6, Train_Loss: 1.163724660873413, Test_Loss: 1.207198143005371 *\n",
      "Epoch: 6, Train_Loss: 1.2846412658691406, Test_Loss: 1.1964470148086548 *\n",
      "Epoch: 6, Train_Loss: 1.2506197690963745, Test_Loss: 1.2356265783309937\n",
      "Epoch: 6, Train_Loss: 1.2930383682250977, Test_Loss: 1.6473374366760254\n",
      "Epoch: 6, Train_Loss: 1.2499593496322632, Test_Loss: 6.457563400268555\n",
      "Epoch: 6, Train_Loss: 1.217976689338684, Test_Loss: 1.1519116163253784 *\n",
      "Epoch: 6, Train_Loss: 1.1374205350875854, Test_Loss: 1.1351168155670166 *\n",
      "Epoch: 6, Train_Loss: 1.1865272521972656, Test_Loss: 1.158721685409546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_Loss: 1.1927001476287842, Test_Loss: 1.1501948833465576 *\n",
      "Epoch: 6, Train_Loss: 1.1526058912277222, Test_Loss: 1.168060064315796\n",
      "Epoch: 6, Train_Loss: 1.135626196861267, Test_Loss: 1.1423288583755493 *\n",
      "Epoch: 6, Train_Loss: 1.1441062688827515, Test_Loss: 1.2761645317077637\n",
      "Epoch: 6, Train_Loss: 1.1453434228897095, Test_Loss: 1.1710495948791504 *\n",
      "Epoch: 6, Train_Loss: 2.738419532775879, Test_Loss: 1.131859302520752 *\n",
      "Epoch: 6, Train_Loss: 5.132850170135498, Test_Loss: 1.1814254522323608\n",
      "Epoch: 6, Train_Loss: 1.1345118284225464, Test_Loss: 1.139390468597412 *\n",
      "Epoch: 6, Train_Loss: 1.171708345413208, Test_Loss: 1.1444131135940552\n",
      "Epoch: 6, Train_Loss: 1.1592141389846802, Test_Loss: 1.1678919792175293\n",
      "Epoch: 6, Train_Loss: 1.1429756879806519, Test_Loss: 1.1666479110717773 *\n",
      "Epoch: 6, Train_Loss: 1.134647250175476, Test_Loss: 1.2020922899246216\n",
      "Epoch: 6, Train_Loss: 1.1352403163909912, Test_Loss: 1.2644919157028198\n",
      "Epoch: 6, Train_Loss: 1.1419285535812378, Test_Loss: 1.1716699600219727 *\n",
      "Epoch: 6, Train_Loss: 1.1488096714019775, Test_Loss: 1.152344822883606 *\n",
      "Epoch: 6, Train_Loss: 1.160157561302185, Test_Loss: 1.1292601823806763 *\n",
      "Epoch: 6, Train_Loss: 1.1401448249816895, Test_Loss: 1.1257578134536743 *\n",
      "Epoch: 6, Train_Loss: 1.1295337677001953, Test_Loss: 1.1275784969329834\n",
      "Epoch: 6, Train_Loss: 1.1274538040161133, Test_Loss: 1.128926157951355\n",
      "Epoch: 6, Train_Loss: 1.1438074111938477, Test_Loss: 1.12967050075531\n",
      "Epoch: 6, Train_Loss: 1.1274508237838745, Test_Loss: 1.1277016401290894 *\n",
      "Epoch: 6, Train_Loss: 1.1249741315841675, Test_Loss: 1.1266467571258545 *\n",
      "Epoch: 6, Train_Loss: 1.1663669347763062, Test_Loss: 1.127914309501648\n",
      "Epoch: 6, Train_Loss: 1.1722215414047241, Test_Loss: 1.1252875328063965 *\n",
      "Epoch: 6, Train_Loss: 1.1278749704360962, Test_Loss: 1.1426194906234741\n",
      "Epoch: 6, Train_Loss: 1.1235005855560303, Test_Loss: 1.1252411603927612 *\n",
      "Epoch: 6, Train_Loss: 1.138275384902954, Test_Loss: 1.1450326442718506\n",
      "Epoch: 6, Train_Loss: 1.194022297859192, Test_Loss: 1.1537721157073975\n",
      "Epoch: 6, Train_Loss: 1.1447348594665527, Test_Loss: 1.3915672302246094\n",
      "Epoch: 6, Train_Loss: 1.1665313243865967, Test_Loss: 1.5611629486083984\n",
      "Epoch: 6, Train_Loss: 1.1697622537612915, Test_Loss: 1.2945268154144287 *\n",
      "Epoch: 6, Train_Loss: 1.2196033000946045, Test_Loss: 1.147166132926941 *\n",
      "Epoch: 6, Train_Loss: 1.1671264171600342, Test_Loss: 1.1329257488250732 *\n",
      "Epoch: 6, Train_Loss: 1.1739428043365479, Test_Loss: 1.1341370344161987\n",
      "Epoch: 6, Train_Loss: 1.153747320175171, Test_Loss: 1.2273554801940918\n",
      "Epoch: 6, Train_Loss: 1.2636299133300781, Test_Loss: 2.0581955909729004\n",
      "Epoch: 6, Train_Loss: 1.1378400325775146, Test_Loss: 2.226085901260376\n",
      "Epoch: 6, Train_Loss: 1.1180559396743774, Test_Loss: 1.1867679357528687 *\n",
      "Epoch: 6, Train_Loss: 1.1170295476913452, Test_Loss: 1.1625639200210571 *\n",
      "Epoch: 6, Train_Loss: 1.1160399913787842, Test_Loss: 1.1161426305770874 *\n",
      "Epoch: 6, Train_Loss: 1.1151758432388306, Test_Loss: 1.1259562969207764\n",
      "Epoch: 6, Train_Loss: 1.1176925897598267, Test_Loss: 1.1244096755981445 *\n",
      "Epoch: 6, Train_Loss: 2.934495449066162, Test_Loss: 1.1438379287719727\n",
      "Epoch: 6, Train_Loss: 4.220803260803223, Test_Loss: 1.1641839742660522\n",
      "Epoch: 6, Train_Loss: 1.1130121946334839, Test_Loss: 1.138678789138794 *\n",
      "Epoch: 6, Train_Loss: 1.129624366760254, Test_Loss: 1.119550347328186 *\n",
      "Epoch: 6, Train_Loss: 1.1238740682601929, Test_Loss: 1.2180472612380981\n",
      "Epoch: 6, Train_Loss: 1.1096407175064087, Test_Loss: 1.5175232887268066\n",
      "Epoch: 6, Train_Loss: 1.110211730003357, Test_Loss: 1.2698731422424316 *\n",
      "Epoch: 6, Train_Loss: 1.1095472574234009, Test_Loss: 1.1875770092010498 *\n",
      "Epoch: 6, Train_Loss: 1.1090580224990845, Test_Loss: 1.1176632642745972 *\n",
      "Epoch: 6, Train_Loss: 1.107873558998108, Test_Loss: 1.1172596216201782 *\n",
      "Epoch: 6, Train_Loss: 1.1164180040359497, Test_Loss: 1.1170142889022827 *\n",
      "Epoch: 6, Train_Loss: 1.1958166360855103, Test_Loss: 1.1162680387496948 *\n",
      "Epoch: 6, Train_Loss: 1.1771571636199951, Test_Loss: 1.1961380243301392\n",
      "Epoch: 6, Train_Loss: 1.2069203853607178, Test_Loss: 6.418593406677246\n",
      "Epoch: 6, Train_Loss: 1.144228458404541, Test_Loss: 1.2611404657363892 *\n",
      "Epoch: 6, Train_Loss: 1.1057568788528442, Test_Loss: 1.1136764287948608 *\n",
      "Epoch: 6, Train_Loss: 1.2811708450317383, Test_Loss: 1.1069610118865967 *\n",
      "Epoch: 6, Train_Loss: 1.32809317111969, Test_Loss: 1.1101019382476807\n",
      "Epoch: 6, Train_Loss: 1.3058160543441772, Test_Loss: 1.1113240718841553\n",
      "Epoch: 6, Train_Loss: 1.203189492225647, Test_Loss: 1.1052334308624268 *\n",
      "Epoch: 6, Train_Loss: 1.1032482385635376, Test_Loss: 1.112338662147522\n",
      "Epoch: 6, Train_Loss: 1.1026279926300049, Test_Loss: 1.1062066555023193 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 6\n",
      "Epoch: 6, Train_Loss: 1.1051881313323975, Test_Loss: 1.106464147567749\n",
      "Epoch: 6, Train_Loss: 1.1116927862167358, Test_Loss: 1.1084493398666382\n",
      "Epoch: 6, Train_Loss: 1.1140178442001343, Test_Loss: 1.1135811805725098\n",
      "Epoch: 6, Train_Loss: 1.106594443321228, Test_Loss: 1.1101099252700806 *\n",
      "Epoch: 6, Train_Loss: 1.1008089780807495, Test_Loss: 1.1096001863479614 *\n",
      "Epoch: 6, Train_Loss: 1.0998507738113403, Test_Loss: 1.1129961013793945\n",
      "Epoch: 6, Train_Loss: 1.1076526641845703, Test_Loss: 1.1043537855148315 *\n",
      "Epoch: 6, Train_Loss: 1.1541169881820679, Test_Loss: 1.100218653678894 *\n",
      "Epoch: 6, Train_Loss: 1.2851715087890625, Test_Loss: 1.1065285205841064\n",
      "Epoch: 6, Train_Loss: 1.270254135131836, Test_Loss: 1.1026359796524048 *\n",
      "Epoch: 6, Train_Loss: 1.2313987016677856, Test_Loss: 1.1005229949951172 *\n",
      "Epoch: 6, Train_Loss: 1.1938894987106323, Test_Loss: 1.1062226295471191\n",
      "Epoch: 6, Train_Loss: 1.2469663619995117, Test_Loss: 1.1035922765731812 *\n",
      "Epoch: 6, Train_Loss: 1.1519721746444702, Test_Loss: 1.1125365495681763\n",
      "Epoch: 6, Train_Loss: 1.2425141334533691, Test_Loss: 1.1111091375350952 *\n",
      "Epoch: 6, Train_Loss: 1.2317166328430176, Test_Loss: 1.1016509532928467 *\n",
      "Epoch: 6, Train_Loss: 1.3919826745986938, Test_Loss: 1.0997930765151978 *\n",
      "Epoch: 6, Train_Loss: 1.104278326034546, Test_Loss: 1.1027870178222656\n",
      "Epoch: 6, Train_Loss: 1.153918743133545, Test_Loss: 1.0981314182281494 *\n",
      "Epoch: 6, Train_Loss: 4.116885185241699, Test_Loss: 1.0981204509735107 *\n",
      "Epoch: 6, Train_Loss: 1.2973651885986328, Test_Loss: 1.103663682937622\n",
      "Epoch: 6, Train_Loss: 1.138208031654358, Test_Loss: 1.15384042263031\n",
      "Epoch: 6, Train_Loss: 1.14778470993042, Test_Loss: 2.781370162963867\n",
      "Epoch: 6, Train_Loss: 1.1472339630126953, Test_Loss: 4.928256988525391\n",
      "Epoch: 6, Train_Loss: 1.1007308959960938, Test_Loss: 1.0982921123504639 *\n",
      "Epoch: 6, Train_Loss: 1.09475839138031, Test_Loss: 1.090710163116455 *\n",
      "Epoch: 6, Train_Loss: 1.1739544868469238, Test_Loss: 1.1376179456710815\n",
      "Epoch: 6, Train_Loss: 1.2247543334960938, Test_Loss: 1.136411428451538 *\n",
      "Epoch: 6, Train_Loss: 1.1979821920394897, Test_Loss: 1.144174575805664\n",
      "Epoch: 6, Train_Loss: 1.1700563430786133, Test_Loss: 1.118801474571228 *\n",
      "Epoch: 6, Train_Loss: 1.1705557107925415, Test_Loss: 1.2208203077316284\n",
      "Epoch: 6, Train_Loss: 1.1131051778793335, Test_Loss: 1.098203420639038 *\n",
      "Epoch: 6, Train_Loss: 1.1151551008224487, Test_Loss: 1.0998846292495728\n",
      "Epoch: 6, Train_Loss: 1.0941756963729858, Test_Loss: 1.1155611276626587\n",
      "Epoch: 6, Train_Loss: 1.128096580505371, Test_Loss: 1.103537917137146 *\n",
      "Epoch: 6, Train_Loss: 1.110711693763733, Test_Loss: 1.0947617292404175 *\n",
      "Epoch: 6, Train_Loss: 1.087740421295166, Test_Loss: 1.156056523323059\n",
      "Epoch: 6, Train_Loss: 1.105019211769104, Test_Loss: 1.178871750831604\n",
      "Epoch: 6, Train_Loss: 1.1421449184417725, Test_Loss: 1.155484676361084 *\n",
      "Epoch: 6, Train_Loss: 1.1324570178985596, Test_Loss: 1.1837280988693237\n",
      "Epoch: 6, Train_Loss: 1.0834349393844604, Test_Loss: 1.1063497066497803 *\n",
      "Epoch: 6, Train_Loss: 1.0827536582946777, Test_Loss: 1.1208473443984985\n",
      "Epoch: 6, Train_Loss: 1.081883430480957, Test_Loss: 1.0985902547836304 *\n",
      "Epoch: 6, Train_Loss: 1.08138108253479, Test_Loss: 1.0950791835784912 *\n",
      "Epoch: 6, Train_Loss: 1.081854224205017, Test_Loss: 1.0968908071517944\n",
      "Epoch: 6, Train_Loss: 1.0813820362091064, Test_Loss: 1.0987706184387207\n",
      "Epoch: 6, Train_Loss: 1.0811477899551392, Test_Loss: 1.0969727039337158 *\n",
      "Epoch: 6, Train_Loss: 1.0803951025009155, Test_Loss: 1.0964651107788086 *\n",
      "Epoch: 6, Train_Loss: 1.0797994136810303, Test_Loss: 1.1046351194381714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_Loss: 1.0813865661621094, Test_Loss: 1.0967329740524292 *\n",
      "Epoch: 6, Train_Loss: 1.0858784914016724, Test_Loss: 1.0981700420379639\n",
      "Epoch: 6, Train_Loss: 1.0904288291931152, Test_Loss: 1.0817997455596924 *\n",
      "Epoch: 6, Train_Loss: 1.0945671796798706, Test_Loss: 1.100064992904663\n",
      "Epoch: 6, Train_Loss: 1.0943959951400757, Test_Loss: 1.1443841457366943\n",
      "Epoch: 7, Train_Loss: 1.0860830545425415, Test_Loss: 1.086343765258789 *\n",
      "Epoch: 7, Train_Loss: 1.0875775814056396, Test_Loss: 1.552469253540039\n",
      "Epoch: 7, Train_Loss: 1.0773155689239502, Test_Loss: 1.624284267425537\n",
      "Epoch: 7, Train_Loss: 1.0761011838912964, Test_Loss: 1.2474300861358643 *\n",
      "Epoch: 7, Train_Loss: 1.0928648710250854, Test_Loss: 1.1050852537155151 *\n",
      "Epoch: 7, Train_Loss: 1.0995162725448608, Test_Loss: 1.0996482372283936 *\n",
      "Epoch: 7, Train_Loss: 1.0746952295303345, Test_Loss: 1.0939780473709106 *\n",
      "Epoch: 7, Train_Loss: 1.075211524963379, Test_Loss: 1.2492451667785645\n",
      "Epoch: 7, Train_Loss: 1.0740911960601807, Test_Loss: 2.2399611473083496\n",
      "Epoch: 7, Train_Loss: 1.139243245124817, Test_Loss: 1.9497148990631104 *\n",
      "Epoch: 7, Train_Loss: 1.1086690425872803, Test_Loss: 1.12290358543396 *\n",
      "Epoch: 7, Train_Loss: 1.1192079782485962, Test_Loss: 1.1338225603103638\n",
      "Epoch: 7, Train_Loss: 1.081648349761963, Test_Loss: 1.0740492343902588 *\n",
      "Epoch: 7, Train_Loss: 1.0765749216079712, Test_Loss: 1.0757482051849365\n",
      "Epoch: 7, Train_Loss: 1.1329200267791748, Test_Loss: 1.0788519382476807\n",
      "Epoch: 7, Train_Loss: 1.0717328786849976, Test_Loss: 1.0892378091812134\n",
      "Epoch: 7, Train_Loss: 1.0838841199874878, Test_Loss: 1.135381817817688\n",
      "Epoch: 7, Train_Loss: 1.1042063236236572, Test_Loss: 1.0746045112609863 *\n",
      "Epoch: 7, Train_Loss: 1.1017415523529053, Test_Loss: 1.089585304260254\n",
      "Epoch: 7, Train_Loss: 1.2100534439086914, Test_Loss: 1.1918586492538452\n",
      "Epoch: 7, Train_Loss: 1.1559585332870483, Test_Loss: 1.4591763019561768\n",
      "Epoch: 7, Train_Loss: 1.1079319715499878, Test_Loss: 1.2938826084136963 *\n",
      "Epoch: 7, Train_Loss: 1.0756136178970337, Test_Loss: 1.0808436870574951 *\n",
      "Epoch: 7, Train_Loss: 1.0943642854690552, Test_Loss: 1.0754108428955078 *\n",
      "Epoch: 7, Train_Loss: 1.068455457687378, Test_Loss: 1.0748298168182373 *\n",
      "Epoch: 7, Train_Loss: 1.0723745822906494, Test_Loss: 1.074462652206421 *\n",
      "Epoch: 7, Train_Loss: 1.075472116470337, Test_Loss: 1.074543833732605\n",
      "Epoch: 7, Train_Loss: 1.0789681673049927, Test_Loss: 1.585598349571228\n",
      "Epoch: 7, Train_Loss: 1.1160392761230469, Test_Loss: 6.054369926452637\n",
      "Epoch: 7, Train_Loss: 1.1205406188964844, Test_Loss: 1.1085628271102905 *\n",
      "Epoch: 7, Train_Loss: 1.1110668182373047, Test_Loss: 1.0696816444396973 *\n",
      "Epoch: 7, Train_Loss: 1.1031996011734009, Test_Loss: 1.0655028820037842 *\n",
      "Epoch: 7, Train_Loss: 1.0896625518798828, Test_Loss: 1.0692503452301025\n",
      "Epoch: 7, Train_Loss: 1.072326421737671, Test_Loss: 1.0667319297790527 *\n",
      "Epoch: 7, Train_Loss: 1.2013038396835327, Test_Loss: 1.0642497539520264 *\n",
      "Epoch: 7, Train_Loss: 1.253504753112793, Test_Loss: 1.068250298500061\n",
      "Epoch: 7, Train_Loss: 1.061848521232605, Test_Loss: 1.0640007257461548 *\n",
      "Epoch: 7, Train_Loss: 1.096142053604126, Test_Loss: 1.064937710762024\n",
      "Epoch: 7, Train_Loss: 1.060243844985962, Test_Loss: 1.0633673667907715 *\n",
      "Epoch: 7, Train_Loss: 1.0594803094863892, Test_Loss: 1.0681726932525635\n",
      "Epoch: 7, Train_Loss: 1.0609705448150635, Test_Loss: 1.0740396976470947\n",
      "Epoch: 7, Train_Loss: 1.0619927644729614, Test_Loss: 1.0693906545639038 *\n",
      "Epoch: 7, Train_Loss: 1.0768121480941772, Test_Loss: 1.0711369514465332\n",
      "Epoch: 7, Train_Loss: 1.0816009044647217, Test_Loss: 1.0594416856765747 *\n",
      "Epoch: 7, Train_Loss: 1.0691829919815063, Test_Loss: 1.0585920810699463 *\n",
      "Epoch: 7, Train_Loss: 1.070912480354309, Test_Loss: 1.0595566034317017\n",
      "Epoch: 7, Train_Loss: 1.0859367847442627, Test_Loss: 1.059014916419983 *\n",
      "Epoch: 7, Train_Loss: 1.05704665184021, Test_Loss: 1.057175874710083 *\n",
      "Epoch: 7, Train_Loss: 1.0587973594665527, Test_Loss: 1.061079502105713\n",
      "Epoch: 7, Train_Loss: 1.0553158521652222, Test_Loss: 1.0570876598358154 *\n",
      "Epoch: 7, Train_Loss: 1.0826077461242676, Test_Loss: 1.061666488647461\n",
      "Epoch: 7, Train_Loss: 1.0826959609985352, Test_Loss: 1.059229850769043 *\n",
      "Epoch: 7, Train_Loss: 1.0657495260238647, Test_Loss: 1.0562623739242554 *\n",
      "Epoch: 7, Train_Loss: 1.0727180242538452, Test_Loss: 1.055248498916626 *\n",
      "Epoch: 7, Train_Loss: 1.1251554489135742, Test_Loss: 1.0575785636901855\n",
      "Epoch: 7, Train_Loss: 1.1110258102416992, Test_Loss: 1.053720235824585 *\n",
      "Epoch: 7, Train_Loss: 1.067781925201416, Test_Loss: 1.0543885231018066\n",
      "Epoch: 7, Train_Loss: 1.0684674978256226, Test_Loss: 1.066541314125061\n",
      "Epoch: 7, Train_Loss: 1.0670585632324219, Test_Loss: 1.1086230278015137\n",
      "Epoch: 7, Train_Loss: 1.0637633800506592, Test_Loss: 3.8178462982177734\n",
      "Epoch: 7, Train_Loss: 1.0609326362609863, Test_Loss: 3.8092570304870605 *\n",
      "Epoch: 7, Train_Loss: 1.0759837627410889, Test_Loss: 1.0516066551208496 *\n",
      "Epoch: 7, Train_Loss: 1.1043561697006226, Test_Loss: 1.0491845607757568 *\n",
      "Epoch: 7, Train_Loss: 3.311417818069458, Test_Loss: 1.103247046470642\n",
      "Epoch: 7, Train_Loss: 4.224220275878906, Test_Loss: 1.0975768566131592 *\n",
      "Epoch: 7, Train_Loss: 1.0621424913406372, Test_Loss: 1.0914499759674072 *\n",
      "Epoch: 7, Train_Loss: 1.0529356002807617, Test_Loss: 1.104167103767395\n",
      "Epoch: 7, Train_Loss: 1.1105046272277832, Test_Loss: 1.1639353036880493\n",
      "Epoch: 7, Train_Loss: 1.2322791814804077, Test_Loss: 1.0485812425613403 *\n",
      "Epoch: 7, Train_Loss: 1.0682843923568726, Test_Loss: 1.07241690158844\n",
      "Epoch: 7, Train_Loss: 1.049904704093933, Test_Loss: 1.0663024187088013 *\n",
      "Epoch: 7, Train_Loss: 1.054602026939392, Test_Loss: 1.060860514640808 *\n",
      "Epoch: 7, Train_Loss: 1.1112191677093506, Test_Loss: 1.0519943237304688 *\n",
      "Epoch: 7, Train_Loss: 1.052567481994629, Test_Loss: 1.131087303161621\n",
      "Epoch: 7, Train_Loss: 1.0572866201400757, Test_Loss: 1.1172693967819214 *\n",
      "Epoch: 7, Train_Loss: 1.964723825454712, Test_Loss: 1.1332778930664062\n",
      "Epoch: 7, Train_Loss: 2.4274799823760986, Test_Loss: 1.1462353467941284\n",
      "Epoch: 7, Train_Loss: 1.6113240718841553, Test_Loss: 1.066521406173706 *\n",
      "Epoch: 7, Train_Loss: 1.1656129360198975, Test_Loss: 1.074147343635559\n",
      "Epoch: 7, Train_Loss: 1.9683921337127686, Test_Loss: 1.0591310262680054 *\n",
      "Epoch: 7, Train_Loss: 3.292140007019043, Test_Loss: 1.0563634634017944 *\n",
      "Epoch: 7, Train_Loss: 1.3439505100250244, Test_Loss: 1.056941270828247\n",
      "Epoch: 7, Train_Loss: 1.0960441827774048, Test_Loss: 1.0604795217514038\n",
      "Epoch: 7, Train_Loss: 1.0662561655044556, Test_Loss: 1.0587040185928345 *\n",
      "Epoch: 7, Train_Loss: 2.521556854248047, Test_Loss: 1.0524581670761108 *\n",
      "Epoch: 7, Train_Loss: 2.5611941814422607, Test_Loss: 1.0606164932250977\n",
      "Epoch: 7, Train_Loss: 1.1329972743988037, Test_Loss: 1.053627610206604 *\n",
      "Epoch: 7, Train_Loss: 1.0607507228851318, Test_Loss: 1.0481821298599243 *\n",
      "Epoch: 7, Train_Loss: 1.038920521736145, Test_Loss: 1.0441868305206299 *\n",
      "Epoch: 7, Train_Loss: 1.7397558689117432, Test_Loss: 1.0570887327194214\n",
      "Epoch: 7, Train_Loss: 1.1585967540740967, Test_Loss: 1.0716379880905151\n",
      "Epoch: 7, Train_Loss: 1.0839797258377075, Test_Loss: 1.0698590278625488 *\n",
      "Epoch: 7, Train_Loss: 1.0701440572738647, Test_Loss: 1.573592185974121\n",
      "Epoch: 7, Train_Loss: 1.2014341354370117, Test_Loss: 1.503009557723999 *\n",
      "Epoch: 7, Train_Loss: 1.159275770187378, Test_Loss: 1.177017092704773 *\n",
      "Epoch: 7, Train_Loss: 1.1977462768554688, Test_Loss: 1.0580800771713257 *\n",
      "Epoch: 7, Train_Loss: 1.3802369832992554, Test_Loss: 1.060841679573059\n",
      "Epoch: 7, Train_Loss: 1.0925201177597046, Test_Loss: 1.0550117492675781 *\n",
      "Epoch: 7, Train_Loss: 1.125112771987915, Test_Loss: 1.2715705633163452\n",
      "Epoch: 7, Train_Loss: 1.2680346965789795, Test_Loss: 2.067619800567627\n",
      "Model saved at location save_new\\model.ckpt at epoch 7\n",
      "Epoch: 7, Train_Loss: 1.3809728622436523, Test_Loss: 1.5589812994003296 *\n",
      "Epoch: 7, Train_Loss: 1.3580347299575806, Test_Loss: 1.098832607269287 *\n",
      "Epoch: 7, Train_Loss: 1.0886085033416748, Test_Loss: 1.0590580701828003 *\n",
      "Epoch: 7, Train_Loss: 1.0986518859863281, Test_Loss: 1.0406395196914673 *\n",
      "Epoch: 7, Train_Loss: 1.1639959812164307, Test_Loss: 1.0372861623764038 *\n",
      "Epoch: 7, Train_Loss: 1.0631120204925537, Test_Loss: 1.0553348064422607\n",
      "Epoch: 7, Train_Loss: 1.0419186353683472, Test_Loss: 1.0367282629013062 *\n",
      "Epoch: 7, Train_Loss: 1.0325442552566528, Test_Loss: 1.0817848443984985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 1.0358209609985352, Test_Loss: 1.038872241973877 *\n",
      "Epoch: 7, Train_Loss: 1.0314714908599854, Test_Loss: 1.0875914096832275\n",
      "Epoch: 7, Train_Loss: 1.0398712158203125, Test_Loss: 1.151273250579834\n",
      "Epoch: 7, Train_Loss: 1.0964853763580322, Test_Loss: 1.4235583543777466\n",
      "Epoch: 7, Train_Loss: 1.0529797077178955, Test_Loss: 1.3344281911849976 *\n",
      "Epoch: 7, Train_Loss: 1.1060420274734497, Test_Loss: 1.054918885231018 *\n",
      "Epoch: 7, Train_Loss: 1.1639324426651, Test_Loss: 1.0320781469345093 *\n",
      "Epoch: 7, Train_Loss: 1.42704176902771, Test_Loss: 1.0304625034332275 *\n",
      "Epoch: 7, Train_Loss: 1.0319241285324097, Test_Loss: 1.0291249752044678 *\n",
      "Epoch: 7, Train_Loss: 1.074942708015442, Test_Loss: 1.0584584474563599\n",
      "Epoch: 7, Train_Loss: 1.1620899438858032, Test_Loss: 2.529116153717041\n",
      "Epoch: 7, Train_Loss: 1.5194220542907715, Test_Loss: 5.057735919952393\n",
      "Epoch: 7, Train_Loss: 1.28639817237854, Test_Loss: 1.0456777811050415 *\n",
      "Epoch: 7, Train_Loss: 1.0465492010116577, Test_Loss: 1.0364985466003418 *\n",
      "Epoch: 7, Train_Loss: 1.1158232688903809, Test_Loss: 1.0312087535858154 *\n",
      "Epoch: 7, Train_Loss: 1.5294616222381592, Test_Loss: 1.0305174589157104 *\n",
      "Epoch: 7, Train_Loss: 1.395268440246582, Test_Loss: 1.047299861907959\n",
      "Epoch: 7, Train_Loss: 1.0626486539840698, Test_Loss: 1.0527722835540771\n",
      "Epoch: 7, Train_Loss: 1.0368071794509888, Test_Loss: 1.0629271268844604\n",
      "Epoch: 7, Train_Loss: 1.032828450202942, Test_Loss: 1.0333911180496216 *\n",
      "Epoch: 7, Train_Loss: 1.9211585521697998, Test_Loss: 1.0728951692581177\n",
      "Epoch: 7, Train_Loss: 1.8273597955703735, Test_Loss: 1.0693073272705078 *\n",
      "Epoch: 7, Train_Loss: 1.0572919845581055, Test_Loss: 1.091819167137146\n",
      "Epoch: 7, Train_Loss: 1.0406813621520996, Test_Loss: 1.0294255018234253 *\n",
      "Epoch: 7, Train_Loss: 1.0218846797943115, Test_Loss: 1.038341760635376\n",
      "Epoch: 7, Train_Loss: 1.0616943836212158, Test_Loss: 1.0733423233032227\n",
      "Epoch: 7, Train_Loss: 1.5008494853973389, Test_Loss: 1.0261521339416504 *\n",
      "Epoch: 7, Train_Loss: 1.0224997997283936, Test_Loss: 1.0464410781860352\n",
      "Epoch: 7, Train_Loss: 1.2259998321533203, Test_Loss: 1.041369080543518 *\n",
      "Epoch: 7, Train_Loss: 1.143509864807129, Test_Loss: 1.0636168718338013\n",
      "Epoch: 7, Train_Loss: 1.0431709289550781, Test_Loss: 1.0227174758911133 *\n",
      "Epoch: 7, Train_Loss: 1.0379923582077026, Test_Loss: 1.0442105531692505\n",
      "Epoch: 7, Train_Loss: 1.1918165683746338, Test_Loss: 1.052474856376648\n",
      "Epoch: 7, Train_Loss: 1.1351749897003174, Test_Loss: 1.094934105873108\n",
      "Epoch: 7, Train_Loss: 1.0400679111480713, Test_Loss: 1.0820691585540771 *\n",
      "Epoch: 7, Train_Loss: 1.0428584814071655, Test_Loss: 1.0533705949783325 *\n",
      "Epoch: 7, Train_Loss: 1.0787642002105713, Test_Loss: 1.0268008708953857 *\n",
      "Epoch: 7, Train_Loss: 1.0964337587356567, Test_Loss: 1.0524253845214844\n",
      "Epoch: 7, Train_Loss: 1.0675761699676514, Test_Loss: 1.0505311489105225 *\n",
      "Epoch: 7, Train_Loss: 1.0512994527816772, Test_Loss: 1.0321986675262451 *\n",
      "Epoch: 7, Train_Loss: 1.029390811920166, Test_Loss: 1.1409780979156494\n",
      "Epoch: 7, Train_Loss: 1.0461180210113525, Test_Loss: 1.0841376781463623 *\n",
      "Epoch: 7, Train_Loss: 1.3990439176559448, Test_Loss: 4.812156677246094\n",
      "Epoch: 7, Train_Loss: 1.273057222366333, Test_Loss: 2.564422130584717 *\n",
      "Epoch: 7, Train_Loss: 1.5049973726272583, Test_Loss: 1.0415682792663574 *\n",
      "Epoch: 7, Train_Loss: 1.4261857271194458, Test_Loss: 1.0494133234024048\n",
      "Epoch: 7, Train_Loss: 1.2105599641799927, Test_Loss: 1.0382791757583618 *\n",
      "Epoch: 7, Train_Loss: 1.2518013715744019, Test_Loss: 1.018516182899475 *\n",
      "Epoch: 7, Train_Loss: 1.0790380239486694, Test_Loss: 1.0259549617767334\n",
      "Epoch: 7, Train_Loss: 1.0258557796478271, Test_Loss: 1.0674675703048706\n",
      "Epoch: 7, Train_Loss: 1.019077181816101, Test_Loss: 1.05731201171875 *\n",
      "Epoch: 7, Train_Loss: 1.0671509504318237, Test_Loss: 1.0207754373550415 *\n",
      "Epoch: 7, Train_Loss: 1.298224925994873, Test_Loss: 1.0428022146224976\n",
      "Epoch: 7, Train_Loss: 1.4819393157958984, Test_Loss: 1.1212788820266724\n",
      "Epoch: 7, Train_Loss: 1.7226324081420898, Test_Loss: 1.0973960161209106 *\n",
      "Epoch: 7, Train_Loss: 2.282994270324707, Test_Loss: 1.0613020658493042 *\n",
      "Epoch: 7, Train_Loss: 1.3880157470703125, Test_Loss: 1.0295116901397705 *\n",
      "Epoch: 7, Train_Loss: 1.2893942594528198, Test_Loss: 1.082880973815918\n",
      "Epoch: 7, Train_Loss: 1.0584114789962769, Test_Loss: 1.0320558547973633 *\n",
      "Epoch: 7, Train_Loss: 1.040834903717041, Test_Loss: 1.052817702293396\n",
      "Epoch: 7, Train_Loss: 1.4503315687179565, Test_Loss: 1.2657884359359741\n",
      "Epoch: 7, Train_Loss: 2.2734577655792236, Test_Loss: 1.0330381393432617 *\n",
      "Epoch: 7, Train_Loss: 1.130295753479004, Test_Loss: 1.0527530908584595\n",
      "Epoch: 7, Train_Loss: 1.1289958953857422, Test_Loss: 1.0722826719284058\n",
      "Epoch: 7, Train_Loss: 1.1015743017196655, Test_Loss: 1.072806477546692\n",
      "Epoch: 7, Train_Loss: 1.1300910711288452, Test_Loss: 1.0465251207351685 *\n",
      "Epoch: 7, Train_Loss: 1.2820082902908325, Test_Loss: 1.0660374164581299\n",
      "Epoch: 7, Train_Loss: 1.4325518608093262, Test_Loss: 1.1074872016906738\n",
      "Epoch: 7, Train_Loss: 1.4465869665145874, Test_Loss: 1.0762791633605957 *\n",
      "Epoch: 7, Train_Loss: 1.3833274841308594, Test_Loss: 1.0878890752792358\n",
      "Epoch: 7, Train_Loss: 1.0311858654022217, Test_Loss: 1.098127841949463\n",
      "Epoch: 7, Train_Loss: 1.0143773555755615, Test_Loss: 1.1763999462127686\n",
      "Epoch: 7, Train_Loss: 1.0194753408432007, Test_Loss: 1.0566588640213013 *\n",
      "Epoch: 7, Train_Loss: 1.1250414848327637, Test_Loss: 1.0760242938995361\n",
      "Epoch: 7, Train_Loss: 1.0157151222229004, Test_Loss: 1.1214884519577026\n",
      "Epoch: 7, Train_Loss: 1.0423516035079956, Test_Loss: 1.1349520683288574\n",
      "Epoch: 7, Train_Loss: 16.703935623168945, Test_Loss: 1.0722849369049072 *\n",
      "Epoch: 7, Train_Loss: 1.449575662612915, Test_Loss: 1.0439749956130981 *\n",
      "Epoch: 7, Train_Loss: 2.3488681316375732, Test_Loss: 1.1921623945236206\n",
      "Epoch: 7, Train_Loss: 3.086332082748413, Test_Loss: 1.2029401063919067\n",
      "Epoch: 7, Train_Loss: 1.057334065437317, Test_Loss: 1.1709659099578857 *\n",
      "Epoch: 7, Train_Loss: 1.1449226140975952, Test_Loss: 1.1718742847442627\n",
      "Epoch: 7, Train_Loss: 2.3851513862609863, Test_Loss: 1.32406747341156\n",
      "Epoch: 7, Train_Loss: 8.783829689025879, Test_Loss: 1.5038455724716187\n",
      "Epoch: 7, Train_Loss: 1.3445765972137451, Test_Loss: 1.15489661693573 *\n",
      "Epoch: 7, Train_Loss: 1.093897819519043, Test_Loss: 1.1347073316574097 *\n",
      "Epoch: 7, Train_Loss: 5.355654716491699, Test_Loss: 1.2790782451629639\n",
      "Epoch: 7, Train_Loss: 1.59762704372406, Test_Loss: 1.4878753423690796\n",
      "Epoch: 7, Train_Loss: 1.243533730506897, Test_Loss: 1.7992668151855469\n",
      "Epoch: 7, Train_Loss: 1.0172960758209229, Test_Loss: 1.1903529167175293 *\n",
      "Epoch: 7, Train_Loss: 0.9985938668251038, Test_Loss: 1.8808224201202393\n",
      "Epoch: 7, Train_Loss: 1.0583648681640625, Test_Loss: 1.4550907611846924 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 7\n",
      "Epoch: 7, Train_Loss: 1.0333881378173828, Test_Loss: 1.482978105545044\n",
      "Epoch: 7, Train_Loss: 1.0537176132202148, Test_Loss: 1.9597012996673584\n",
      "Epoch: 7, Train_Loss: 0.9891659021377563, Test_Loss: 1.1922191381454468 *\n",
      "Epoch: 7, Train_Loss: 0.9891303777694702, Test_Loss: 1.1850075721740723 *\n",
      "Epoch: 7, Train_Loss: 1.013818621635437, Test_Loss: 1.7302441596984863\n",
      "Epoch: 7, Train_Loss: 1.088611125946045, Test_Loss: 1.674417495727539 *\n",
      "Epoch: 7, Train_Loss: 1.0294222831726074, Test_Loss: 1.4832451343536377 *\n",
      "Epoch: 7, Train_Loss: 1.1215168237686157, Test_Loss: 1.30353581905365 *\n",
      "Epoch: 7, Train_Loss: 1.0523728132247925, Test_Loss: 1.0188714265823364 *\n",
      "Epoch: 7, Train_Loss: 1.0245864391326904, Test_Loss: 4.294265270233154\n",
      "Epoch: 7, Train_Loss: 1.0046186447143555, Test_Loss: 4.158058166503906 *\n",
      "Epoch: 7, Train_Loss: 0.9918025732040405, Test_Loss: 1.1534788608551025 *\n",
      "Epoch: 7, Train_Loss: 0.9932900667190552, Test_Loss: 1.1435717344284058 *\n",
      "Epoch: 7, Train_Loss: 0.989220380783081, Test_Loss: 1.1296919584274292 *\n",
      "Epoch: 7, Train_Loss: 0.9864751100540161, Test_Loss: 1.0063053369522095 *\n",
      "Epoch: 7, Train_Loss: 0.9851034283638, Test_Loss: 1.140245795249939\n",
      "Epoch: 7, Train_Loss: 0.984100341796875, Test_Loss: 1.156017541885376\n",
      "Epoch: 7, Train_Loss: 0.9843498468399048, Test_Loss: 1.062846064567566 *\n",
      "Epoch: 7, Train_Loss: 0.9842602014541626, Test_Loss: 1.0334242582321167 *\n",
      "Epoch: 7, Train_Loss: 0.9830072522163391, Test_Loss: 1.0610136985778809\n",
      "Epoch: 7, Train_Loss: 0.9838950634002686, Test_Loss: 1.0666040182113647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 1.0000449419021606, Test_Loss: 1.0944855213165283\n",
      "Epoch: 7, Train_Loss: 1.0073330402374268, Test_Loss: 1.0147085189819336 *\n",
      "Epoch: 7, Train_Loss: 1.017907738685608, Test_Loss: 1.0667418241500854\n",
      "Epoch: 7, Train_Loss: 0.9919539093971252, Test_Loss: 1.0615113973617554 *\n",
      "Epoch: 7, Train_Loss: 1.0348058938980103, Test_Loss: 1.0006057024002075 *\n",
      "Epoch: 7, Train_Loss: 9.672167778015137, Test_Loss: 1.0725696086883545\n",
      "Epoch: 7, Train_Loss: 1.0652703046798706, Test_Loss: 1.0759060382843018\n",
      "Epoch: 7, Train_Loss: 1.0119003057479858, Test_Loss: 1.124913215637207\n",
      "Epoch: 7, Train_Loss: 1.080956220626831, Test_Loss: 1.0713003873825073 *\n",
      "Epoch: 7, Train_Loss: 1.088847279548645, Test_Loss: 1.08686101436615\n",
      "Epoch: 7, Train_Loss: 0.9949418306350708, Test_Loss: 1.1681970357894897\n",
      "Epoch: 7, Train_Loss: 1.0277708768844604, Test_Loss: 1.235650658607483\n",
      "Epoch: 7, Train_Loss: 1.1356488466262817, Test_Loss: 1.194953203201294 *\n",
      "Epoch: 7, Train_Loss: 1.2921451330184937, Test_Loss: 1.1001871824264526 *\n",
      "Epoch: 7, Train_Loss: 1.1552205085754395, Test_Loss: 1.0456889867782593 *\n",
      "Epoch: 7, Train_Loss: 1.0948736667633057, Test_Loss: 1.0721009969711304\n",
      "Epoch: 7, Train_Loss: 0.9817137718200684, Test_Loss: 1.0521602630615234 *\n",
      "Epoch: 7, Train_Loss: 1.1195886135101318, Test_Loss: 1.0243381261825562 *\n",
      "Epoch: 7, Train_Loss: 1.0788761377334595, Test_Loss: 1.107834815979004\n",
      "Epoch: 7, Train_Loss: 1.1442054510116577, Test_Loss: 1.0034115314483643 *\n",
      "Epoch: 7, Train_Loss: 1.092110514640808, Test_Loss: 6.219897270202637\n",
      "Epoch: 7, Train_Loss: 1.0732756853103638, Test_Loss: 1.5193829536437988 *\n",
      "Epoch: 7, Train_Loss: 0.9947155117988586, Test_Loss: 0.9757475852966309 *\n",
      "Epoch: 7, Train_Loss: 1.0070576667785645, Test_Loss: 0.9898154139518738\n",
      "Epoch: 7, Train_Loss: 1.0466715097427368, Test_Loss: 0.9977885484695435\n",
      "Epoch: 7, Train_Loss: 0.9984408617019653, Test_Loss: 1.0089001655578613\n",
      "Epoch: 7, Train_Loss: 0.9762881994247437, Test_Loss: 0.9783077239990234 *\n",
      "Epoch: 7, Train_Loss: 0.976930558681488, Test_Loss: 1.0776610374450684\n",
      "Epoch: 7, Train_Loss: 0.9801581501960754, Test_Loss: 1.0555442571640015 *\n",
      "Epoch: 7, Train_Loss: 1.327277660369873, Test_Loss: 0.97174072265625 *\n",
      "Epoch: 7, Train_Loss: 6.262676239013672, Test_Loss: 1.0240199565887451\n",
      "Epoch: 7, Train_Loss: 0.9783238768577576, Test_Loss: 0.9791147708892822 *\n",
      "Epoch: 7, Train_Loss: 0.9897652268409729, Test_Loss: 0.9867658615112305\n",
      "Epoch: 7, Train_Loss: 0.9937707781791687, Test_Loss: 0.9754183888435364 *\n",
      "Epoch: 7, Train_Loss: 0.9835242033004761, Test_Loss: 1.035711646080017\n",
      "Epoch: 7, Train_Loss: 0.9770733714103699, Test_Loss: 1.008364200592041 *\n",
      "Epoch: 7, Train_Loss: 0.972594141960144, Test_Loss: 1.1124061346054077\n",
      "Epoch: 7, Train_Loss: 0.9794222712516785, Test_Loss: 1.0486167669296265 *\n",
      "Epoch: 7, Train_Loss: 0.9990979433059692, Test_Loss: 0.9918302893638611 *\n",
      "Epoch: 7, Train_Loss: 0.9817548394203186, Test_Loss: 0.9701749086380005 *\n",
      "Epoch: 7, Train_Loss: 0.9817238450050354, Test_Loss: 0.9685624241828918 *\n",
      "Epoch: 7, Train_Loss: 0.9716607928276062, Test_Loss: 0.9690510034561157\n",
      "Epoch: 7, Train_Loss: 0.9691981673240662, Test_Loss: 0.9680721759796143 *\n",
      "Epoch: 7, Train_Loss: 0.9865648150444031, Test_Loss: 0.9705911874771118\n",
      "Epoch: 7, Train_Loss: 0.9681800007820129, Test_Loss: 0.9698764085769653 *\n",
      "Epoch: 7, Train_Loss: 0.9665700197219849, Test_Loss: 0.9651681184768677 *\n",
      "Epoch: 7, Train_Loss: 0.9997726082801819, Test_Loss: 0.9727213382720947\n",
      "Epoch: 7, Train_Loss: 1.0161879062652588, Test_Loss: 0.9673520922660828 *\n",
      "Epoch: 7, Train_Loss: 0.9819443225860596, Test_Loss: 0.9718378782272339\n",
      "Epoch: 7, Train_Loss: 0.9655495882034302, Test_Loss: 0.976566731929779\n",
      "Epoch: 7, Train_Loss: 0.969127357006073, Test_Loss: 0.9880624413490295\n",
      "Epoch: 7, Train_Loss: 1.02997624874115, Test_Loss: 0.9812376499176025 *\n",
      "Epoch: 7, Train_Loss: 0.9978855848312378, Test_Loss: 1.1379560232162476\n",
      "Epoch: 7, Train_Loss: 1.0256776809692383, Test_Loss: 1.4080573320388794\n",
      "Epoch: 7, Train_Loss: 0.9782748222351074, Test_Loss: 1.2036429643630981 *\n",
      "Epoch: 7, Train_Loss: 1.0343557596206665, Test_Loss: 1.0358219146728516 *\n",
      "Epoch: 7, Train_Loss: 1.0157946348190308, Test_Loss: 0.9711991548538208 *\n",
      "Epoch: 7, Train_Loss: 1.0308387279510498, Test_Loss: 0.9707710146903992 *\n",
      "Epoch: 7, Train_Loss: 0.977777361869812, Test_Loss: 1.0460666418075562\n",
      "Epoch: 7, Train_Loss: 1.1424401998519897, Test_Loss: 1.6055316925048828\n",
      "Epoch: 7, Train_Loss: 0.9782642722129822, Test_Loss: 2.1638870239257812\n",
      "Epoch: 7, Train_Loss: 0.9702592492103577, Test_Loss: 1.2187654972076416 *\n",
      "Epoch: 7, Train_Loss: 0.9629406332969666, Test_Loss: 1.0162609815597534 *\n",
      "Epoch: 7, Train_Loss: 0.9597786664962769, Test_Loss: 0.9621430039405823 *\n",
      "Epoch: 7, Train_Loss: 0.9592583775520325, Test_Loss: 0.9718669652938843\n",
      "Epoch: 7, Train_Loss: 0.9610816836357117, Test_Loss: 0.965238630771637 *\n",
      "Epoch: 7, Train_Loss: 1.3553026914596558, Test_Loss: 0.9797875285148621\n",
      "Epoch: 7, Train_Loss: 5.386068820953369, Test_Loss: 0.9922641515731812\n",
      "Epoch: 7, Train_Loss: 0.9754564762115479, Test_Loss: 1.0012304782867432\n",
      "Epoch: 7, Train_Loss: 0.9693391919136047, Test_Loss: 0.9633725881576538 *\n",
      "Epoch: 7, Train_Loss: 0.9727513790130615, Test_Loss: 1.056328535079956\n",
      "Epoch: 7, Train_Loss: 0.9544119834899902, Test_Loss: 1.319717288017273\n",
      "Epoch: 7, Train_Loss: 0.9539352059364319, Test_Loss: 1.0498265027999878 *\n",
      "Epoch: 7, Train_Loss: 0.9547737836837769, Test_Loss: 1.1337555646896362\n",
      "Epoch: 7, Train_Loss: 0.9544587731361389, Test_Loss: 0.9613631963729858 *\n",
      "Epoch: 7, Train_Loss: 0.9528065323829651, Test_Loss: 0.9610692262649536 *\n",
      "Epoch: 7, Train_Loss: 0.952624499797821, Test_Loss: 0.9607074856758118 *\n",
      "Epoch: 7, Train_Loss: 1.0267746448516846, Test_Loss: 0.9603137373924255 *\n",
      "Epoch: 7, Train_Loss: 1.010276436805725, Test_Loss: 0.974277913570404\n",
      "Model saved at location save_new\\model.ckpt at epoch 7\n",
      "Epoch: 7, Train_Loss: 1.047644019126892, Test_Loss: 5.1866044998168945\n",
      "Epoch: 7, Train_Loss: 1.0040414333343506, Test_Loss: 2.2188074588775635 *\n",
      "Epoch: 7, Train_Loss: 0.951014518737793, Test_Loss: 0.9588193297386169 *\n",
      "Epoch: 7, Train_Loss: 1.0755932331085205, Test_Loss: 0.9543866515159607 *\n",
      "Epoch: 7, Train_Loss: 1.1456624269485474, Test_Loss: 0.9533588886260986 *\n",
      "Epoch: 7, Train_Loss: 1.1451590061187744, Test_Loss: 0.9573497176170349\n",
      "Epoch: 7, Train_Loss: 1.0841611623764038, Test_Loss: 0.9577539563179016\n",
      "Epoch: 7, Train_Loss: 0.9523062705993652, Test_Loss: 0.9692577719688416\n",
      "Epoch: 7, Train_Loss: 0.948794424533844, Test_Loss: 0.9555401802062988 *\n",
      "Epoch: 7, Train_Loss: 0.9489352107048035, Test_Loss: 0.9628998041152954\n",
      "Epoch: 7, Train_Loss: 0.9555930495262146, Test_Loss: 0.9688539505004883\n",
      "Epoch: 7, Train_Loss: 0.9572461843490601, Test_Loss: 0.9746619462966919\n",
      "Epoch: 7, Train_Loss: 0.9522141218185425, Test_Loss: 0.9616579413414001 *\n",
      "Epoch: 7, Train_Loss: 0.9489490985870361, Test_Loss: 0.9511958956718445 *\n",
      "Epoch: 7, Train_Loss: 0.9472073316574097, Test_Loss: 0.967085599899292\n",
      "Epoch: 7, Train_Loss: 0.9510716795921326, Test_Loss: 0.9770707488059998\n",
      "Epoch: 7, Train_Loss: 0.9689160585403442, Test_Loss: 0.94664067029953 *\n",
      "Epoch: 7, Train_Loss: 1.0871487855911255, Test_Loss: 0.9593592882156372\n",
      "Epoch: 7, Train_Loss: 1.1179436445236206, Test_Loss: 0.9498061537742615 *\n",
      "Epoch: 7, Train_Loss: 1.1155890226364136, Test_Loss: 0.9543331861495972\n",
      "Epoch: 7, Train_Loss: 1.0066444873809814, Test_Loss: 0.9584937691688538\n",
      "Epoch: 7, Train_Loss: 1.0874320268630981, Test_Loss: 0.9670466780662537\n",
      "Epoch: 7, Train_Loss: 1.0543891191482544, Test_Loss: 0.966041088104248 *\n",
      "Epoch: 7, Train_Loss: 1.05879807472229, Test_Loss: 0.9756307601928711\n",
      "Epoch: 7, Train_Loss: 1.1149709224700928, Test_Loss: 0.9585484266281128 *\n",
      "Epoch: 7, Train_Loss: 1.289097547531128, Test_Loss: 0.9548827409744263 *\n",
      "Epoch: 7, Train_Loss: 0.9517418742179871, Test_Loss: 0.95234215259552 *\n",
      "Epoch: 7, Train_Loss: 0.9534253478050232, Test_Loss: 0.9598164558410645\n",
      "Epoch: 7, Train_Loss: 3.624434471130371, Test_Loss: 0.9508136510848999 *\n",
      "Epoch: 7, Train_Loss: 1.5461516380310059, Test_Loss: 0.9514721035957336\n",
      "Epoch: 7, Train_Loss: 0.9722091555595398, Test_Loss: 1.0065339803695679\n",
      "Epoch: 7, Train_Loss: 0.9826436042785645, Test_Loss: 1.0402042865753174\n",
      "Epoch: 7, Train_Loss: 0.9798774123191833, Test_Loss: 6.417963981628418\n",
      "Epoch: 7, Train_Loss: 0.9642634391784668, Test_Loss: 0.9709595441818237 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 0.9432913661003113, Test_Loss: 0.9393927454948425 *\n",
      "Epoch: 7, Train_Loss: 0.9948028922080994, Test_Loss: 0.9676206707954407\n",
      "Epoch: 7, Train_Loss: 1.0925215482711792, Test_Loss: 0.9835348129272461\n",
      "Epoch: 7, Train_Loss: 1.0451515913009644, Test_Loss: 0.9897633194923401\n",
      "Epoch: 7, Train_Loss: 1.0198981761932373, Test_Loss: 0.9416943788528442 *\n",
      "Epoch: 7, Train_Loss: 1.0102808475494385, Test_Loss: 1.048248052597046\n",
      "Epoch: 7, Train_Loss: 0.9679651260375977, Test_Loss: 0.9853847026824951 *\n",
      "Epoch: 7, Train_Loss: 0.9634952545166016, Test_Loss: 0.938036322593689 *\n",
      "Epoch: 7, Train_Loss: 0.946903645992279, Test_Loss: 0.9601413607597351\n",
      "Epoch: 7, Train_Loss: 0.9762436747550964, Test_Loss: 0.9603114128112793\n",
      "Epoch: 7, Train_Loss: 0.9635348320007324, Test_Loss: 0.9392079710960388 *\n",
      "Epoch: 7, Train_Loss: 0.9416109919548035, Test_Loss: 0.9725713133811951\n",
      "Epoch: 7, Train_Loss: 0.9425219297409058, Test_Loss: 1.0293878316879272\n",
      "Epoch: 7, Train_Loss: 0.9860472083091736, Test_Loss: 0.9799808263778687 *\n",
      "Epoch: 7, Train_Loss: 0.9960240721702576, Test_Loss: 0.9921395182609558\n",
      "Epoch: 7, Train_Loss: 0.9369704723358154, Test_Loss: 0.9692670106887817 *\n",
      "Epoch: 7, Train_Loss: 0.9324688911437988, Test_Loss: 0.9744346737861633\n",
      "Epoch: 7, Train_Loss: 0.9324812293052673, Test_Loss: 0.9469612836837769 *\n",
      "Epoch: 7, Train_Loss: 0.9314646124839783, Test_Loss: 0.9426211714744568 *\n",
      "Epoch: 7, Train_Loss: 0.9317252039909363, Test_Loss: 0.9472804665565491\n",
      "Epoch: 7, Train_Loss: 0.9323067665100098, Test_Loss: 0.9505811333656311\n",
      "Epoch: 7, Train_Loss: 0.9310173988342285, Test_Loss: 0.9497685432434082 *\n",
      "Epoch: 7, Train_Loss: 0.9318186044692993, Test_Loss: 0.9482825398445129 *\n",
      "Epoch: 7, Train_Loss: 0.9301489591598511, Test_Loss: 0.9441921710968018 *\n",
      "Epoch: 7, Train_Loss: 0.9304050207138062, Test_Loss: 0.9491080045700073\n",
      "Epoch: 7, Train_Loss: 0.9398046135902405, Test_Loss: 0.9470471143722534 *\n",
      "Epoch: 7, Train_Loss: 0.9466886520385742, Test_Loss: 0.9352712631225586 *\n",
      "Epoch: 7, Train_Loss: 0.9445910453796387, Test_Loss: 0.9375965595245361\n",
      "Epoch: 7, Train_Loss: 0.9391664266586304, Test_Loss: 0.979873776435852\n",
      "Epoch: 7, Train_Loss: 0.9479417204856873, Test_Loss: 0.9468047618865967 *\n",
      "Epoch: 7, Train_Loss: 0.9457566738128662, Test_Loss: 1.2291080951690674\n",
      "Epoch: 7, Train_Loss: 0.9337210655212402, Test_Loss: 1.493168592453003\n",
      "Epoch: 7, Train_Loss: 0.9292992353439331, Test_Loss: 1.1555309295654297 *\n",
      "Epoch: 7, Train_Loss: 0.9425005912780762, Test_Loss: 0.9908514022827148 *\n",
      "Epoch: 7, Train_Loss: 0.9575189352035522, Test_Loss: 0.9559984803199768 *\n",
      "Epoch: 7, Train_Loss: 0.927709698677063, Test_Loss: 0.9328432679176331 *\n",
      "Epoch: 7, Train_Loss: 0.9298709630966187, Test_Loss: 1.0027738809585571\n",
      "Epoch: 7, Train_Loss: 0.9256172180175781, Test_Loss: 1.6687798500061035\n",
      "Epoch: 7, Train_Loss: 0.9603562951087952, Test_Loss: 1.9823744297027588\n",
      "Epoch: 7, Train_Loss: 0.9817975163459778, Test_Loss: 1.0293596982955933 *\n",
      "Epoch: 7, Train_Loss: 0.9692303538322449, Test_Loss: 1.005407452583313 *\n",
      "Epoch: 7, Train_Loss: 0.9394752979278564, Test_Loss: 0.9240785837173462 *\n",
      "Epoch: 7, Train_Loss: 0.9229393601417542, Test_Loss: 0.9308910369873047\n",
      "Epoch: 7, Train_Loss: 0.9833091497421265, Test_Loss: 0.9301231503486633 *\n",
      "Epoch: 7, Train_Loss: 0.9312814474105835, Test_Loss: 0.939637303352356\n",
      "Epoch: 7, Train_Loss: 0.9293157458305359, Test_Loss: 0.958495557308197\n",
      "Epoch: 7, Train_Loss: 0.9486787915229797, Test_Loss: 0.9518346786499023 *\n",
      "Epoch: 7, Train_Loss: 0.9353050589561462, Test_Loss: 0.9276983737945557 *\n",
      "Epoch: 7, Train_Loss: 1.0649746656417847, Test_Loss: 1.031773567199707\n",
      "Epoch: 7, Train_Loss: 1.020162582397461, Test_Loss: 1.3250224590301514\n",
      "Epoch: 7, Train_Loss: 0.9727736711502075, Test_Loss: 1.0211013555526733 *\n",
      "Epoch: 7, Train_Loss: 0.9364782571792603, Test_Loss: 1.0614278316497803\n",
      "Epoch: 7, Train_Loss: 0.9342755675315857, Test_Loss: 0.9273215532302856 *\n",
      "Epoch: 7, Train_Loss: 0.9356856346130371, Test_Loss: 0.9270319938659668 *\n",
      "Epoch: 7, Train_Loss: 0.9223985075950623, Test_Loss: 0.9266701340675354 *\n",
      "Epoch: 7, Train_Loss: 0.9292300343513489, Test_Loss: 0.926094651222229 *\n",
      "Epoch: 7, Train_Loss: 0.9304510951042175, Test_Loss: 0.9477968811988831\n",
      "Epoch: 7, Train_Loss: 0.9394268989562988, Test_Loss: 6.096771717071533\n",
      "Epoch: 7, Train_Loss: 1.0049976110458374, Test_Loss: 1.248253583908081 *\n",
      "Epoch: 7, Train_Loss: 0.9325011372566223, Test_Loss: 0.9253714084625244 *\n",
      "Epoch: 7, Train_Loss: 0.9904552102088928, Test_Loss: 0.9181969165802002 *\n",
      "Epoch: 7, Train_Loss: 0.9350654482841492, Test_Loss: 0.9194737672805786\n",
      "Epoch: 7, Train_Loss: 0.9346991181373596, Test_Loss: 0.9236773252487183\n",
      "Epoch: 7, Train_Loss: 1.0215704441070557, Test_Loss: 0.9170565009117126 *\n",
      "Epoch: 7, Train_Loss: 1.134404182434082, Test_Loss: 0.9258450865745544\n",
      "Epoch: 7, Train_Loss: 0.9229320287704468, Test_Loss: 0.9180410504341125 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 7\n",
      "Epoch: 7, Train_Loss: 0.9514169096946716, Test_Loss: 0.9201054573059082\n",
      "Epoch: 7, Train_Loss: 0.914242148399353, Test_Loss: 0.9217245578765869\n",
      "Epoch: 7, Train_Loss: 0.9142418503761292, Test_Loss: 0.9299346804618835\n",
      "Epoch: 7, Train_Loss: 0.9152489304542542, Test_Loss: 0.9197701215744019 *\n",
      "Epoch: 7, Train_Loss: 0.9137991666793823, Test_Loss: 0.9210497140884399\n",
      "Epoch: 7, Train_Loss: 0.9318878650665283, Test_Loss: 0.925317108631134\n",
      "Epoch: 7, Train_Loss: 0.9312216639518738, Test_Loss: 0.9189862012863159 *\n",
      "Epoch: 7, Train_Loss: 0.9286127090454102, Test_Loss: 0.9122929573059082 *\n",
      "Epoch: 7, Train_Loss: 0.9217255115509033, Test_Loss: 0.9176875948905945\n",
      "Epoch: 7, Train_Loss: 0.9352020621299744, Test_Loss: 0.9141874313354492 *\n",
      "Epoch: 7, Train_Loss: 0.9190733432769775, Test_Loss: 0.9126411080360413 *\n",
      "Epoch: 7, Train_Loss: 0.9141517281532288, Test_Loss: 0.9183583855628967\n",
      "Epoch: 7, Train_Loss: 0.9102145433425903, Test_Loss: 0.9161126613616943 *\n",
      "Epoch: 7, Train_Loss: 0.9319009184837341, Test_Loss: 0.9179142117500305\n",
      "Epoch: 7, Train_Loss: 0.9352292418479919, Test_Loss: 0.9201624989509583\n",
      "Epoch: 7, Train_Loss: 0.930962324142456, Test_Loss: 0.9139221906661987 *\n",
      "Epoch: 7, Train_Loss: 0.9149901866912842, Test_Loss: 0.9121341109275818 *\n",
      "Epoch: 7, Train_Loss: 0.9824502468109131, Test_Loss: 0.9151479601860046\n",
      "Epoch: 7, Train_Loss: 0.9676234722137451, Test_Loss: 0.91159588098526 *\n",
      "Epoch: 7, Train_Loss: 0.9393118619918823, Test_Loss: 0.9120548367500305\n",
      "Epoch: 7, Train_Loss: 0.9149977564811707, Test_Loss: 0.9173045754432678\n",
      "Epoch: 7, Train_Loss: 0.9322922825813293, Test_Loss: 0.9698124527931213\n",
      "Epoch: 7, Train_Loss: 0.9125717282295227, Test_Loss: 1.8493101596832275\n",
      "Epoch: 7, Train_Loss: 0.9263362288475037, Test_Loss: 5.493917942047119\n",
      "Epoch: 7, Train_Loss: 0.9223750829696655, Test_Loss: 0.9130688309669495 *\n",
      "Epoch: 7, Train_Loss: 0.9370017051696777, Test_Loss: 0.90541672706604 *\n",
      "Epoch: 7, Train_Loss: 2.6870696544647217, Test_Loss: 0.9455458521842957\n",
      "Epoch: 7, Train_Loss: 4.575873374938965, Test_Loss: 0.9490146636962891\n",
      "Epoch: 7, Train_Loss: 0.9393690228462219, Test_Loss: 0.9592540860176086\n",
      "Epoch: 7, Train_Loss: 0.9206723570823669, Test_Loss: 0.9185466766357422 *\n",
      "Epoch: 7, Train_Loss: 0.9295517802238464, Test_Loss: 1.0369436740875244\n",
      "Epoch: 7, Train_Loss: 1.1088566780090332, Test_Loss: 0.9283114075660706 *\n",
      "Epoch: 7, Train_Loss: 0.9499680399894714, Test_Loss: 0.908017098903656 *\n",
      "Epoch: 7, Train_Loss: 0.9135808944702148, Test_Loss: 0.9419639110565186\n",
      "Epoch: 7, Train_Loss: 0.9024789333343506, Test_Loss: 0.9155300259590149 *\n",
      "Epoch: 7, Train_Loss: 0.972735583782196, Test_Loss: 0.9122372269630432 *\n",
      "Epoch: 7, Train_Loss: 0.9162642359733582, Test_Loss: 0.9574527144432068\n",
      "Epoch: 7, Train_Loss: 0.9174688458442688, Test_Loss: 0.9885881543159485\n",
      "Epoch: 7, Train_Loss: 1.4505953788757324, Test_Loss: 0.9667294025421143 *\n",
      "Epoch: 7, Train_Loss: 2.3121297359466553, Test_Loss: 1.0020360946655273\n",
      "Epoch: 7, Train_Loss: 1.828507661819458, Test_Loss: 0.9298933744430542 *\n",
      "Epoch: 7, Train_Loss: 1.038300633430481, Test_Loss: 0.9385761022567749\n",
      "Epoch: 7, Train_Loss: 1.2323698997497559, Test_Loss: 0.9150537848472595 *\n",
      "Epoch: 7, Train_Loss: 3.2487239837646484, Test_Loss: 0.9112159013748169 *\n",
      "Epoch: 7, Train_Loss: 1.6294834613800049, Test_Loss: 0.9157958030700684\n",
      "Epoch: 7, Train_Loss: 0.946755051612854, Test_Loss: 0.9175809621810913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 0.9265171885490417, Test_Loss: 0.9165431261062622 *\n",
      "Epoch: 7, Train_Loss: 1.8911879062652588, Test_Loss: 0.9144673943519592 *\n",
      "Epoch: 7, Train_Loss: 2.4358909130096436, Test_Loss: 0.9164095520973206\n",
      "Epoch: 7, Train_Loss: 1.4291272163391113, Test_Loss: 0.9115331172943115 *\n",
      "Epoch: 7, Train_Loss: 0.9145320653915405, Test_Loss: 0.9073021411895752 *\n",
      "Epoch: 7, Train_Loss: 0.9083009362220764, Test_Loss: 0.9058011770248413 *\n",
      "Epoch: 7, Train_Loss: 1.3827928304672241, Test_Loss: 0.902005136013031 *\n",
      "Epoch: 7, Train_Loss: 1.1950823068618774, Test_Loss: 0.9346660375595093\n",
      "Epoch: 8, Train_Loss: 0.9175038933753967, Test_Loss: 0.9121088981628418 *\n",
      "Epoch: 8, Train_Loss: 0.9600393176078796, Test_Loss: 1.2869707345962524\n",
      "Epoch: 8, Train_Loss: 1.0516893863677979, Test_Loss: 1.4252007007598877\n",
      "Epoch: 8, Train_Loss: 0.987979531288147, Test_Loss: 1.0919029712677002 *\n",
      "Epoch: 8, Train_Loss: 0.9777602553367615, Test_Loss: 0.9265279769897461 *\n",
      "Epoch: 8, Train_Loss: 1.2693554162979126, Test_Loss: 0.9249433279037476 *\n",
      "Epoch: 8, Train_Loss: 0.9693728089332581, Test_Loss: 0.8981708288192749 *\n",
      "Epoch: 8, Train_Loss: 0.9748321175575256, Test_Loss: 0.9487090706825256\n",
      "Epoch: 8, Train_Loss: 1.1336055994033813, Test_Loss: 1.6002275943756104\n",
      "Epoch: 8, Train_Loss: 1.1734198331832886, Test_Loss: 1.5853362083435059 *\n",
      "Epoch: 8, Train_Loss: 1.2452837228775024, Test_Loss: 0.9473425149917603 *\n",
      "Epoch: 8, Train_Loss: 1.076472520828247, Test_Loss: 0.9456709623336792 *\n",
      "Epoch: 8, Train_Loss: 0.9337858557701111, Test_Loss: 0.893255889415741 *\n",
      "Epoch: 8, Train_Loss: 0.988655686378479, Test_Loss: 0.8940846920013428\n",
      "Epoch: 8, Train_Loss: 0.9431332349777222, Test_Loss: 0.9010478258132935\n",
      "Epoch: 8, Train_Loss: 0.9028432965278625, Test_Loss: 0.9221178293228149\n",
      "Epoch: 8, Train_Loss: 0.8914232850074768, Test_Loss: 0.925368070602417\n",
      "Epoch: 8, Train_Loss: 0.8984736204147339, Test_Loss: 0.9147123694419861 *\n",
      "Epoch: 8, Train_Loss: 0.8964032530784607, Test_Loss: 0.9008245468139648 *\n",
      "Epoch: 8, Train_Loss: 0.8969858884811401, Test_Loss: 0.9967790842056274\n",
      "Epoch: 8, Train_Loss: 0.9166613817214966, Test_Loss: 1.2929015159606934\n",
      "Epoch: 8, Train_Loss: 0.9098058342933655, Test_Loss: 1.1767266988754272 *\n",
      "Epoch: 8, Train_Loss: 0.9274935722351074, Test_Loss: 0.9402342438697815 *\n",
      "Epoch: 8, Train_Loss: 1.00919508934021, Test_Loss: 0.8867798447608948 *\n",
      "Epoch: 8, Train_Loss: 1.221065640449524, Test_Loss: 0.8860024213790894 *\n",
      "Epoch: 8, Train_Loss: 0.9608471393585205, Test_Loss: 0.8852129578590393 *\n",
      "Epoch: 8, Train_Loss: 0.9388564825057983, Test_Loss: 0.8989687561988831\n",
      "Epoch: 8, Train_Loss: 0.9391415119171143, Test_Loss: 1.1018823385238647\n",
      "Epoch: 8, Train_Loss: 1.339194655418396, Test_Loss: 5.985739231109619\n",
      "Epoch: 8, Train_Loss: 1.1572785377502441, Test_Loss: 1.0055259466171265 *\n",
      "Epoch: 8, Train_Loss: 0.8972026705741882, Test_Loss: 0.9174730777740479 *\n",
      "Epoch: 8, Train_Loss: 0.9112454056739807, Test_Loss: 0.9093673229217529 *\n",
      "Epoch: 8, Train_Loss: 1.2722651958465576, Test_Loss: 0.898546040058136 *\n",
      "Epoch: 8, Train_Loss: 1.2299134731292725, Test_Loss: 0.9177178144454956\n",
      "Epoch: 8, Train_Loss: 0.9831937551498413, Test_Loss: 0.9418601393699646\n",
      "Epoch: 8, Train_Loss: 0.9038078784942627, Test_Loss: 0.9792670011520386\n",
      "Epoch: 8, Train_Loss: 0.9207668900489807, Test_Loss: 0.8909558653831482 *\n",
      "Epoch: 8, Train_Loss: 1.4670593738555908, Test_Loss: 0.9376770257949829\n",
      "Epoch: 8, Train_Loss: 1.7858600616455078, Test_Loss: 0.9503949880599976\n",
      "Epoch: 8, Train_Loss: 0.959763765335083, Test_Loss: 1.0091784000396729\n",
      "Epoch: 8, Train_Loss: 0.9047273993492126, Test_Loss: 0.9049592614173889 *\n",
      "Epoch: 8, Train_Loss: 0.8905186057090759, Test_Loss: 0.9133992791175842\n",
      "Epoch: 8, Train_Loss: 0.9020338654518127, Test_Loss: 0.917210042476654\n",
      "Epoch: 8, Train_Loss: 1.3376492261886597, Test_Loss: 0.9051027894020081 *\n",
      "Epoch: 8, Train_Loss: 0.9073335528373718, Test_Loss: 0.9005698561668396 *\n",
      "Epoch: 8, Train_Loss: 0.9425785541534424, Test_Loss: 0.9275436997413635\n",
      "Epoch: 8, Train_Loss: 1.056851863861084, Test_Loss: 0.9248843193054199 *\n",
      "Epoch: 8, Train_Loss: 0.9400870203971863, Test_Loss: 0.9047644734382629 *\n",
      "Epoch: 8, Train_Loss: 0.8905094265937805, Test_Loss: 0.9059677124023438\n",
      "Epoch: 8, Train_Loss: 1.0091612339019775, Test_Loss: 0.8922988176345825 *\n",
      "Epoch: 8, Train_Loss: 1.0001838207244873, Test_Loss: 0.9596268534660339\n",
      "Epoch: 8, Train_Loss: 0.9009073972702026, Test_Loss: 0.972453236579895\n",
      "Epoch: 8, Train_Loss: 0.9240481853485107, Test_Loss: 0.9231352806091309 *\n",
      "Epoch: 8, Train_Loss: 0.9108264446258545, Test_Loss: 0.8993992209434509 *\n",
      "Epoch: 8, Train_Loss: 1.026236653327942, Test_Loss: 0.9116904735565186\n",
      "Epoch: 8, Train_Loss: 0.9354185461997986, Test_Loss: 0.9026395082473755 *\n",
      "Epoch: 8, Train_Loss: 0.9035862684249878, Test_Loss: 0.9003655910491943 *\n",
      "Epoch: 8, Train_Loss: 0.8963512182235718, Test_Loss: 0.9480584263801575\n",
      "Epoch: 8, Train_Loss: 0.8982649445533752, Test_Loss: 0.9954735636711121\n",
      "Epoch: 8, Train_Loss: 1.2054286003112793, Test_Loss: 3.115800142288208\n",
      "Epoch: 8, Train_Loss: 1.1667665243148804, Test_Loss: 4.156217098236084\n",
      "Epoch: 8, Train_Loss: 1.22652006149292, Test_Loss: 0.8938112258911133 *\n",
      "Epoch: 8, Train_Loss: 1.2741972208023071, Test_Loss: 0.8962521553039551\n",
      "Epoch: 8, Train_Loss: 1.1474933624267578, Test_Loss: 0.912932276725769\n",
      "Epoch: 8, Train_Loss: 1.1283985376358032, Test_Loss: 0.8784595131874084 *\n",
      "Epoch: 8, Train_Loss: 0.9332827925682068, Test_Loss: 0.9012328386306763\n",
      "Epoch: 8, Train_Loss: 0.8873252868652344, Test_Loss: 0.9207969903945923\n",
      "Epoch: 8, Train_Loss: 0.8966655731201172, Test_Loss: 0.9393102526664734\n",
      "Epoch: 8, Train_Loss: 0.8929618000984192, Test_Loss: 0.8843081593513489 *\n",
      "Epoch: 8, Train_Loss: 1.1183398962020874, Test_Loss: 0.9010151624679565\n",
      "Epoch: 8, Train_Loss: 1.3331818580627441, Test_Loss: 0.9120205640792847\n",
      "Epoch: 8, Train_Loss: 1.2736104726791382, Test_Loss: 0.9582585692405701\n",
      "Epoch: 8, Train_Loss: 2.415688991546631, Test_Loss: 0.8976470232009888 *\n",
      "Epoch: 8, Train_Loss: 1.251738429069519, Test_Loss: 0.9328716397285461\n",
      "Epoch: 8, Train_Loss: 1.1261403560638428, Test_Loss: 0.9099019765853882 *\n",
      "Epoch: 8, Train_Loss: 0.9368900060653687, Test_Loss: 0.9257966876029968\n",
      "Epoch: 8, Train_Loss: 0.9118339419364929, Test_Loss: 0.8729060292243958 *\n",
      "Epoch: 8, Train_Loss: 1.20847749710083, Test_Loss: 0.9727487564086914\n",
      "Epoch: 8, Train_Loss: 1.9993962049484253, Test_Loss: 0.914188027381897 *\n",
      "Epoch: 8, Train_Loss: 1.2086261510849, Test_Loss: 0.8822820782661438 *\n",
      "Epoch: 8, Train_Loss: 0.9666414260864258, Test_Loss: 0.9190115928649902\n",
      "Epoch: 8, Train_Loss: 0.982120931148529, Test_Loss: 0.9131388664245605 *\n",
      "Epoch: 8, Train_Loss: 0.9605039358139038, Test_Loss: 0.9073169827461243 *\n",
      "Epoch: 8, Train_Loss: 1.2255016565322876, Test_Loss: 0.9077677130699158\n",
      "Epoch: 8, Train_Loss: 1.091869592666626, Test_Loss: 0.8917577266693115 *\n",
      "Epoch: 8, Train_Loss: 1.2573349475860596, Test_Loss: 0.9217353463172913\n",
      "Epoch: 8, Train_Loss: 1.0721842050552368, Test_Loss: 0.9376236200332642\n",
      "Epoch: 8, Train_Loss: 0.9128469228744507, Test_Loss: 0.9447941780090332\n",
      "Epoch: 8, Train_Loss: 0.898266077041626, Test_Loss: 1.0444282293319702\n",
      "Epoch: 8, Train_Loss: 0.9024080634117126, Test_Loss: 0.951838493347168 *\n",
      "Epoch: 8, Train_Loss: 0.9223833084106445, Test_Loss: 0.8960035443305969 *\n",
      "Epoch: 8, Train_Loss: 0.922802209854126, Test_Loss: 1.0465599298477173\n",
      "Epoch: 8, Train_Loss: 0.9157589077949524, Test_Loss: 0.9686923027038574 *\n",
      "Epoch: 8, Train_Loss: 7.826732635498047, Test_Loss: 0.9349197745323181 *\n",
      "Epoch: 8, Train_Loss: 10.623675346374512, Test_Loss: 0.8927362561225891 *\n",
      "Epoch: 8, Train_Loss: 1.5025593042373657, Test_Loss: 0.9302686452865601\n",
      "Epoch: 8, Train_Loss: 2.5430495738983154, Test_Loss: 0.9337629079818726\n",
      "Epoch: 8, Train_Loss: 1.2073124647140503, Test_Loss: 0.9102731347084045 *\n",
      "Epoch: 8, Train_Loss: 0.9456655383110046, Test_Loss: 0.9344526529312134\n",
      "Epoch: 8, Train_Loss: 1.0891603231430054, Test_Loss: 1.1295108795166016\n",
      "Model saved at location save_new\\model.ckpt at epoch 8\n",
      "Epoch: 8, Train_Loss: 8.787513732910156, Test_Loss: 1.2187249660491943\n",
      "Epoch: 8, Train_Loss: 2.464533805847168, Test_Loss: 1.041944980621338 *\n",
      "Epoch: 8, Train_Loss: 0.9441461563110352, Test_Loss: 1.0571997165679932\n",
      "Epoch: 8, Train_Loss: 3.2846603393554688, Test_Loss: 1.0115611553192139 *\n",
      "Epoch: 8, Train_Loss: 3.4050779342651367, Test_Loss: 1.1266180276870728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_Loss: 1.300106167793274, Test_Loss: 1.4154911041259766\n",
      "Epoch: 8, Train_Loss: 0.9167758822441101, Test_Loss: 0.9581878185272217 *\n",
      "Epoch: 8, Train_Loss: 0.8912466168403625, Test_Loss: 1.5536587238311768\n",
      "Epoch: 8, Train_Loss: 0.8915260434150696, Test_Loss: 1.059281587600708 *\n",
      "Epoch: 8, Train_Loss: 0.8943491578102112, Test_Loss: 1.3174529075622559\n",
      "Epoch: 8, Train_Loss: 0.8753794431686401, Test_Loss: 1.391228199005127\n",
      "Epoch: 8, Train_Loss: 0.8691718578338623, Test_Loss: 1.065543293952942 *\n",
      "Epoch: 8, Train_Loss: 0.8603391647338867, Test_Loss: 0.9579847455024719 *\n",
      "Epoch: 8, Train_Loss: 0.8786363005638123, Test_Loss: 1.3113069534301758\n",
      "Epoch: 8, Train_Loss: 0.9163650870323181, Test_Loss: 1.268235445022583 *\n",
      "Epoch: 8, Train_Loss: 0.8948370814323425, Test_Loss: 1.1385524272918701 *\n",
      "Epoch: 8, Train_Loss: 0.8962464332580566, Test_Loss: 1.0321468114852905 *\n",
      "Epoch: 8, Train_Loss: 0.9290732741355896, Test_Loss: 0.8997129797935486 *\n",
      "Epoch: 8, Train_Loss: 0.9013230800628662, Test_Loss: 1.8577115535736084\n",
      "Epoch: 8, Train_Loss: 0.858708381652832, Test_Loss: 6.320924282073975\n",
      "Epoch: 8, Train_Loss: 0.8640028834342957, Test_Loss: 0.9340040683746338 *\n",
      "Epoch: 8, Train_Loss: 0.8566359877586365, Test_Loss: 0.9433915019035339\n",
      "Epoch: 8, Train_Loss: 0.8581274151802063, Test_Loss: 0.9314382076263428 *\n",
      "Epoch: 8, Train_Loss: 0.8559972643852234, Test_Loss: 0.8766619563102722 *\n",
      "Epoch: 8, Train_Loss: 0.8540395498275757, Test_Loss: 0.9024337530136108\n",
      "Epoch: 8, Train_Loss: 0.8530040979385376, Test_Loss: 0.9211997389793396\n",
      "Epoch: 8, Train_Loss: 0.8529547452926636, Test_Loss: 0.9283697605133057\n",
      "Epoch: 8, Train_Loss: 0.8545740842819214, Test_Loss: 0.865397036075592 *\n",
      "Epoch: 8, Train_Loss: 0.852885901927948, Test_Loss: 0.8868760466575623\n",
      "Epoch: 8, Train_Loss: 0.8519874215126038, Test_Loss: 0.8938223123550415\n",
      "Epoch: 8, Train_Loss: 0.8645449280738831, Test_Loss: 0.9274007081985474\n",
      "Epoch: 8, Train_Loss: 0.8785842061042786, Test_Loss: 0.8658491373062134 *\n",
      "Epoch: 8, Train_Loss: 0.908099889755249, Test_Loss: 0.8881704807281494\n",
      "Epoch: 8, Train_Loss: 0.8584369421005249, Test_Loss: 0.9218782782554626\n",
      "Epoch: 8, Train_Loss: 0.8558089733123779, Test_Loss: 0.8597211241722107 *\n",
      "Epoch: 8, Train_Loss: 9.335911750793457, Test_Loss: 0.8776071667671204\n",
      "Epoch: 8, Train_Loss: 1.6269595623016357, Test_Loss: 0.8865363597869873\n",
      "Epoch: 8, Train_Loss: 0.8601018786430359, Test_Loss: 0.9376046061515808\n",
      "Epoch: 8, Train_Loss: 0.8913933038711548, Test_Loss: 0.8804781436920166 *\n",
      "Epoch: 8, Train_Loss: 0.9193816184997559, Test_Loss: 0.9246912598609924\n",
      "Epoch: 8, Train_Loss: 0.8528422713279724, Test_Loss: 0.9174159169197083 *\n",
      "Epoch: 8, Train_Loss: 0.8576377034187317, Test_Loss: 0.9913357496261597\n",
      "Epoch: 8, Train_Loss: 0.9174816012382507, Test_Loss: 0.9702625274658203 *\n",
      "Epoch: 8, Train_Loss: 1.0190150737762451, Test_Loss: 0.9023323059082031 *\n",
      "Epoch: 8, Train_Loss: 1.0328023433685303, Test_Loss: 0.8757796287536621 *\n",
      "Epoch: 8, Train_Loss: 0.992100715637207, Test_Loss: 0.9036356210708618\n",
      "Epoch: 8, Train_Loss: 0.8603881597518921, Test_Loss: 0.8830042481422424 *\n",
      "Epoch: 8, Train_Loss: 0.9255548119544983, Test_Loss: 0.8734683394432068 *\n",
      "Epoch: 8, Train_Loss: 0.9444373250007629, Test_Loss: 0.9216306209564209\n",
      "Epoch: 8, Train_Loss: 0.9731369614601135, Test_Loss: 0.8975158929824829 *\n",
      "Epoch: 8, Train_Loss: 0.9602378606796265, Test_Loss: 4.138582229614258\n",
      "Epoch: 8, Train_Loss: 0.9400621056556702, Test_Loss: 3.2743687629699707 *\n",
      "Epoch: 8, Train_Loss: 0.8883320689201355, Test_Loss: 0.8491416573524475 *\n",
      "Epoch: 8, Train_Loss: 0.8605107665061951, Test_Loss: 0.8478665947914124 *\n",
      "Epoch: 8, Train_Loss: 0.9289430975914001, Test_Loss: 0.8798999786376953\n",
      "Epoch: 8, Train_Loss: 0.8738467693328857, Test_Loss: 0.8708163499832153 *\n",
      "Epoch: 8, Train_Loss: 0.8511389493942261, Test_Loss: 0.8702275156974792 *\n",
      "Epoch: 8, Train_Loss: 0.8468377590179443, Test_Loss: 0.9186694622039795\n",
      "Epoch: 8, Train_Loss: 0.8516640663146973, Test_Loss: 0.9656098484992981\n",
      "Epoch: 8, Train_Loss: 0.9020906090736389, Test_Loss: 0.8443229794502258 *\n",
      "Epoch: 8, Train_Loss: 6.414886951446533, Test_Loss: 0.8793262839317322\n",
      "Epoch: 8, Train_Loss: 0.8770581483840942, Test_Loss: 0.8624463677406311 *\n",
      "Epoch: 8, Train_Loss: 0.8504320979118347, Test_Loss: 0.8565713763237 *\n",
      "Epoch: 8, Train_Loss: 0.8699905872344971, Test_Loss: 0.850282609462738 *\n",
      "Epoch: 8, Train_Loss: 0.8561000823974609, Test_Loss: 0.8971667289733887\n",
      "Epoch: 8, Train_Loss: 0.8512630462646484, Test_Loss: 0.8846943974494934 *\n",
      "Epoch: 8, Train_Loss: 0.8445525765419006, Test_Loss: 0.9545550346374512\n",
      "Epoch: 8, Train_Loss: 0.8501548767089844, Test_Loss: 0.9583501219749451\n",
      "Epoch: 8, Train_Loss: 0.8724203109741211, Test_Loss: 0.8570462465286255 *\n",
      "Epoch: 8, Train_Loss: 0.8546953201293945, Test_Loss: 0.8471375703811646 *\n",
      "Epoch: 8, Train_Loss: 0.8667796850204468, Test_Loss: 0.8397390842437744 *\n",
      "Epoch: 8, Train_Loss: 0.8438069224357605, Test_Loss: 0.840155303478241\n",
      "Epoch: 8, Train_Loss: 0.8422600030899048, Test_Loss: 0.8398312330245972 *\n",
      "Epoch: 8, Train_Loss: 0.8584058880805969, Test_Loss: 0.8415861129760742\n",
      "Epoch: 8, Train_Loss: 0.8395398259162903, Test_Loss: 0.8415305614471436 *\n",
      "Epoch: 8, Train_Loss: 0.8396705389022827, Test_Loss: 0.8373990058898926 *\n",
      "Epoch: 8, Train_Loss: 0.8592076897621155, Test_Loss: 0.8416817784309387\n",
      "Epoch: 8, Train_Loss: 0.8833320140838623, Test_Loss: 0.8369967937469482 *\n",
      "Epoch: 8, Train_Loss: 0.8630205988883972, Test_Loss: 0.8404808044433594\n",
      "Epoch: 8, Train_Loss: 0.8373966813087463, Test_Loss: 0.8560049533843994\n",
      "Epoch: 8, Train_Loss: 0.8387873768806458, Test_Loss: 0.8424932360649109 *\n",
      "Epoch: 8, Train_Loss: 0.8997501134872437, Test_Loss: 0.8535521030426025\n",
      "Epoch: 8, Train_Loss: 0.8946032524108887, Test_Loss: 0.8944539427757263\n",
      "Epoch: 8, Train_Loss: 0.8786413073539734, Test_Loss: 1.2492954730987549\n",
      "Epoch: 8, Train_Loss: 0.8551821708679199, Test_Loss: 1.1659200191497803 *\n",
      "Epoch: 8, Train_Loss: 0.9203131198883057, Test_Loss: 0.927160918712616 *\n",
      "Epoch: 8, Train_Loss: 0.9075464010238647, Test_Loss: 0.8397332429885864 *\n",
      "Epoch: 8, Train_Loss: 0.8945145606994629, Test_Loss: 0.8547676205635071\n",
      "Epoch: 8, Train_Loss: 0.869211733341217, Test_Loss: 0.891219437122345\n",
      "Epoch: 8, Train_Loss: 1.0074706077575684, Test_Loss: 1.2379329204559326\n",
      "Epoch: 8, Train_Loss: 0.8587547540664673, Test_Loss: 2.0816311836242676\n",
      "Epoch: 8, Train_Loss: 0.8518406748771667, Test_Loss: 1.4078543186187744 *\n",
      "Epoch: 8, Train_Loss: 0.8343448042869568, Test_Loss: 0.8959064483642578 *\n",
      "Epoch: 8, Train_Loss: 0.8326587677001953, Test_Loss: 0.849433958530426 *\n",
      "Epoch: 8, Train_Loss: 0.832207977771759, Test_Loss: 0.8451994061470032 *\n",
      "Epoch: 8, Train_Loss: 0.8323143720626831, Test_Loss: 0.8391392827033997 *\n",
      "Epoch: 8, Train_Loss: 0.8420014381408691, Test_Loss: 0.8455930352210999\n",
      "Epoch: 8, Train_Loss: 5.437371253967285, Test_Loss: 0.8570486903190613\n",
      "Epoch: 8, Train_Loss: 1.1187891960144043, Test_Loss: 0.8926184177398682\n",
      "Epoch: 8, Train_Loss: 0.8340627551078796, Test_Loss: 0.8311545252799988 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 8\n",
      "Epoch: 8, Train_Loss: 0.8470150232315063, Test_Loss: 0.8937527537345886\n",
      "Epoch: 8, Train_Loss: 0.831609845161438, Test_Loss: 0.9698737859725952\n",
      "Epoch: 8, Train_Loss: 0.8279563784599304, Test_Loss: 1.1735153198242188\n",
      "Epoch: 8, Train_Loss: 0.8286724090576172, Test_Loss: 1.0503482818603516 *\n",
      "Epoch: 8, Train_Loss: 0.8284461498260498, Test_Loss: 0.8418965935707092 *\n",
      "Epoch: 8, Train_Loss: 0.827118456363678, Test_Loss: 0.837466835975647 *\n",
      "Epoch: 8, Train_Loss: 0.826928436756134, Test_Loss: 0.8372012376785278 *\n",
      "Epoch: 8, Train_Loss: 0.8752137422561646, Test_Loss: 0.8371568322181702 *\n",
      "Epoch: 8, Train_Loss: 0.9002476930618286, Test_Loss: 0.8502904176712036\n",
      "Epoch: 8, Train_Loss: 0.9110137820243835, Test_Loss: 2.8513689041137695\n",
      "Epoch: 8, Train_Loss: 0.897484540939331, Test_Loss: 4.266733646392822\n",
      "Epoch: 8, Train_Loss: 0.8271118998527527, Test_Loss: 0.8379330039024353 *\n",
      "Epoch: 8, Train_Loss: 0.9036067128181458, Test_Loss: 0.8287853598594666 *\n",
      "Epoch: 8, Train_Loss: 1.0339199304580688, Test_Loss: 0.8281023502349854 *\n",
      "Epoch: 8, Train_Loss: 1.0346119403839111, Test_Loss: 0.8345991373062134\n",
      "Epoch: 8, Train_Loss: 1.0261671543121338, Test_Loss: 0.8276852965354919 *\n",
      "Epoch: 8, Train_Loss: 0.8275263905525208, Test_Loss: 0.8309870958328247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_Loss: 0.8234224915504456, Test_Loss: 0.8282628655433655 *\n",
      "Epoch: 8, Train_Loss: 0.8237513303756714, Test_Loss: 0.8279702067375183 *\n",
      "Epoch: 8, Train_Loss: 0.8352400064468384, Test_Loss: 0.8307574391365051\n",
      "Epoch: 8, Train_Loss: 0.8370606303215027, Test_Loss: 0.8283926248550415 *\n",
      "Epoch: 8, Train_Loss: 0.8323442935943604, Test_Loss: 0.8333331942558289\n",
      "Epoch: 8, Train_Loss: 0.8277224898338318, Test_Loss: 0.839122474193573\n",
      "Epoch: 8, Train_Loss: 0.8221977949142456, Test_Loss: 0.8300239443778992 *\n",
      "Epoch: 8, Train_Loss: 0.8249121308326721, Test_Loss: 0.8346880674362183\n",
      "Epoch: 8, Train_Loss: 0.8364316821098328, Test_Loss: 0.8226043581962585 *\n",
      "Epoch: 8, Train_Loss: 0.9684332013130188, Test_Loss: 0.827623724937439\n",
      "Epoch: 8, Train_Loss: 0.9940810203552246, Test_Loss: 0.8239514827728271 *\n",
      "Epoch: 8, Train_Loss: 1.0156052112579346, Test_Loss: 0.826422929763794\n",
      "Epoch: 8, Train_Loss: 0.8654125928878784, Test_Loss: 0.8262912631034851 *\n",
      "Epoch: 8, Train_Loss: 0.9636566638946533, Test_Loss: 0.8263328671455383\n",
      "Epoch: 8, Train_Loss: 0.9549721479415894, Test_Loss: 0.8262879848480225 *\n",
      "Epoch: 8, Train_Loss: 0.8679667711257935, Test_Loss: 0.8339250683784485\n",
      "Epoch: 8, Train_Loss: 0.9834257364273071, Test_Loss: 0.8258664011955261 *\n",
      "Epoch: 8, Train_Loss: 1.004665493965149, Test_Loss: 0.8249451518058777 *\n",
      "Epoch: 8, Train_Loss: 0.9982502460479736, Test_Loss: 0.8217557668685913 *\n",
      "Epoch: 8, Train_Loss: 0.8321040868759155, Test_Loss: 0.8257340788841248\n",
      "Epoch: 8, Train_Loss: 2.609186887741089, Test_Loss: 0.8229784369468689 *\n",
      "Epoch: 8, Train_Loss: 2.2453765869140625, Test_Loss: 0.8221142292022705 *\n",
      "Epoch: 8, Train_Loss: 0.8507080078125, Test_Loss: 0.868171751499176\n",
      "Epoch: 8, Train_Loss: 0.867973804473877, Test_Loss: 0.8483444452285767 *\n",
      "Epoch: 8, Train_Loss: 0.8708239197731018, Test_Loss: 5.167575836181641\n",
      "Epoch: 8, Train_Loss: 0.85673987865448, Test_Loss: 1.9997605085372925 *\n",
      "Epoch: 8, Train_Loss: 0.8169909715652466, Test_Loss: 0.816750168800354 *\n",
      "Epoch: 8, Train_Loss: 0.8370646238327026, Test_Loss: 0.8296712636947632\n",
      "Epoch: 8, Train_Loss: 0.9598770141601562, Test_Loss: 0.8678426742553711\n",
      "Epoch: 8, Train_Loss: 0.9116464853286743, Test_Loss: 0.8761813640594482\n",
      "Epoch: 8, Train_Loss: 0.9038159847259521, Test_Loss: 0.8287544250488281 *\n",
      "Epoch: 8, Train_Loss: 0.8987055420875549, Test_Loss: 0.8953889012336731\n",
      "Epoch: 8, Train_Loss: 0.8627732992172241, Test_Loss: 0.8894398212432861 *\n",
      "Epoch: 8, Train_Loss: 0.8413675427436829, Test_Loss: 0.8169759511947632 *\n",
      "Epoch: 8, Train_Loss: 0.832196831703186, Test_Loss: 0.8437773585319519\n",
      "Epoch: 8, Train_Loss: 0.8413755893707275, Test_Loss: 0.8357449769973755 *\n",
      "Epoch: 8, Train_Loss: 0.8467432856559753, Test_Loss: 0.8241419196128845 *\n",
      "Epoch: 8, Train_Loss: 0.8234591484069824, Test_Loss: 0.8179135322570801 *\n",
      "Epoch: 8, Train_Loss: 0.8136143684387207, Test_Loss: 0.9288507699966431\n",
      "Epoch: 8, Train_Loss: 0.8571030497550964, Test_Loss: 0.8565221428871155 *\n",
      "Epoch: 8, Train_Loss: 0.8740307092666626, Test_Loss: 0.9028414487838745\n",
      "Epoch: 8, Train_Loss: 0.8291110992431641, Test_Loss: 0.867866575717926 *\n",
      "Epoch: 8, Train_Loss: 0.8103819489479065, Test_Loss: 0.8598924279212952 *\n",
      "Epoch: 8, Train_Loss: 0.810314416885376, Test_Loss: 0.8297450542449951 *\n",
      "Epoch: 8, Train_Loss: 0.809543788433075, Test_Loss: 0.8274953961372375 *\n",
      "Epoch: 8, Train_Loss: 0.8097795248031616, Test_Loss: 0.8290309906005859\n",
      "Epoch: 8, Train_Loss: 0.8095216155052185, Test_Loss: 0.8298838138580322\n",
      "Epoch: 8, Train_Loss: 0.8095647096633911, Test_Loss: 0.8337734341621399\n",
      "Epoch: 8, Train_Loss: 0.8102673292160034, Test_Loss: 0.8311255574226379 *\n",
      "Epoch: 8, Train_Loss: 0.8089504837989807, Test_Loss: 0.8207547664642334 *\n",
      "Epoch: 8, Train_Loss: 0.8081289529800415, Test_Loss: 0.8360515236854553\n",
      "Epoch: 8, Train_Loss: 0.8108035922050476, Test_Loss: 0.828090250492096 *\n",
      "Epoch: 8, Train_Loss: 0.8214895725250244, Test_Loss: 0.8205419182777405 *\n",
      "Epoch: 8, Train_Loss: 0.8222569823265076, Test_Loss: 0.8177027106285095 *\n",
      "Epoch: 8, Train_Loss: 0.8198879957199097, Test_Loss: 0.8599460124969482\n",
      "Epoch: 8, Train_Loss: 0.8280410766601562, Test_Loss: 0.8403489589691162 *\n",
      "Epoch: 8, Train_Loss: 0.8127318620681763, Test_Loss: 0.9901872873306274\n",
      "Epoch: 8, Train_Loss: 0.809211254119873, Test_Loss: 1.383089303970337\n",
      "Epoch: 8, Train_Loss: 0.8064983487129211, Test_Loss: 1.1656832695007324 *\n",
      "Epoch: 8, Train_Loss: 0.8128612041473389, Test_Loss: 0.9199655652046204 *\n",
      "Epoch: 8, Train_Loss: 0.8363602161407471, Test_Loss: 0.8381052613258362 *\n",
      "Epoch: 8, Train_Loss: 0.8155508041381836, Test_Loss: 0.8165716528892517 *\n",
      "Epoch: 8, Train_Loss: 0.809402585029602, Test_Loss: 0.8703036904335022\n",
      "Epoch: 8, Train_Loss: 0.8050820231437683, Test_Loss: 1.361712098121643\n",
      "Epoch: 8, Train_Loss: 0.8167640566825867, Test_Loss: 2.030414342880249\n",
      "Epoch: 8, Train_Loss: 0.8696774840354919, Test_Loss: 1.157637357711792 *\n",
      "Epoch: 8, Train_Loss: 0.8474345803260803, Test_Loss: 0.8988967537879944 *\n",
      "Epoch: 8, Train_Loss: 0.8360750079154968, Test_Loss: 0.8084641098976135 *\n",
      "Epoch: 8, Train_Loss: 0.8031837940216064, Test_Loss: 0.8097540140151978\n",
      "Epoch: 8, Train_Loss: 0.8586216568946838, Test_Loss: 0.8047335743904114 *\n",
      "Epoch: 8, Train_Loss: 0.8237693905830383, Test_Loss: 0.8147604465484619\n",
      "Epoch: 8, Train_Loss: 0.8046644926071167, Test_Loss: 0.827386736869812\n",
      "Epoch: 8, Train_Loss: 0.8171389698982239, Test_Loss: 0.8457965850830078\n",
      "Epoch: 8, Train_Loss: 0.8271533250808716, Test_Loss: 0.8051072955131531 *\n",
      "Epoch: 8, Train_Loss: 0.9170153141021729, Test_Loss: 0.9039194583892822\n",
      "Epoch: 8, Train_Loss: 0.8976183533668518, Test_Loss: 1.1308468580245972\n",
      "Epoch: 8, Train_Loss: 0.8658955097198486, Test_Loss: 0.9501708149909973 *\n",
      "Epoch: 8, Train_Loss: 0.8247752785682678, Test_Loss: 1.0042977333068848\n",
      "Epoch: 8, Train_Loss: 0.8037267327308655, Test_Loss: 0.81073397397995 *\n",
      "Epoch: 8, Train_Loss: 0.8228840231895447, Test_Loss: 0.8097726106643677 *\n",
      "Epoch: 8, Train_Loss: 0.8003965616226196, Test_Loss: 0.809278130531311 *\n",
      "Epoch: 8, Train_Loss: 0.8067112565040588, Test_Loss: 0.8088752031326294 *\n",
      "Epoch: 8, Train_Loss: 0.8131087422370911, Test_Loss: 0.8255096077919006\n",
      "Model saved at location save_new\\model.ckpt at epoch 8\n",
      "Epoch: 8, Train_Loss: 0.8150942921638489, Test_Loss: 4.245876312255859\n",
      "Epoch: 8, Train_Loss: 0.8963446617126465, Test_Loss: 2.8150010108947754 *\n",
      "Epoch: 8, Train_Loss: 0.8001875877380371, Test_Loss: 0.807062029838562 *\n",
      "Epoch: 8, Train_Loss: 0.873385488986969, Test_Loss: 0.8006941080093384 *\n",
      "Epoch: 8, Train_Loss: 0.8078016042709351, Test_Loss: 0.7999286651611328 *\n",
      "Epoch: 8, Train_Loss: 0.8283876180648804, Test_Loss: 0.8084524869918823\n",
      "Epoch: 8, Train_Loss: 0.8497503995895386, Test_Loss: 0.799461841583252 *\n",
      "Epoch: 8, Train_Loss: 1.063934564590454, Test_Loss: 0.8041813373565674\n",
      "Epoch: 8, Train_Loss: 0.8112971186637878, Test_Loss: 0.7975618839263916 *\n",
      "Epoch: 8, Train_Loss: 0.8386646509170532, Test_Loss: 0.7996575236320496\n",
      "Epoch: 8, Train_Loss: 0.7950288653373718, Test_Loss: 0.8017062544822693\n",
      "Epoch: 8, Train_Loss: 0.795072615146637, Test_Loss: 0.8006600737571716 *\n",
      "Epoch: 8, Train_Loss: 0.7964441776275635, Test_Loss: 0.8051103949546814\n",
      "Epoch: 8, Train_Loss: 0.7948431372642517, Test_Loss: 0.8133921027183533\n",
      "Epoch: 8, Train_Loss: 0.8078575134277344, Test_Loss: 0.8027886748313904 *\n",
      "Epoch: 8, Train_Loss: 0.8071537613868713, Test_Loss: 0.8028634190559387\n",
      "Epoch: 8, Train_Loss: 0.812468945980072, Test_Loss: 0.7937226891517639 *\n",
      "Epoch: 8, Train_Loss: 0.8043555021286011, Test_Loss: 0.8001324534416199\n",
      "Epoch: 8, Train_Loss: 0.8101879954338074, Test_Loss: 0.7952470779418945 *\n",
      "Epoch: 8, Train_Loss: 0.8092673420906067, Test_Loss: 0.7958614826202393\n",
      "Epoch: 8, Train_Loss: 0.7958282232284546, Test_Loss: 0.7968668937683105\n",
      "Epoch: 8, Train_Loss: 0.7915650606155396, Test_Loss: 0.7980698943138123\n",
      "Epoch: 8, Train_Loss: 0.8102001547813416, Test_Loss: 0.7972933650016785 *\n",
      "Epoch: 8, Train_Loss: 0.8148486614227295, Test_Loss: 0.8037828207015991\n",
      "Epoch: 8, Train_Loss: 0.8252222537994385, Test_Loss: 0.7958065867424011 *\n",
      "Epoch: 8, Train_Loss: 0.7907549142837524, Test_Loss: 0.7950349450111389 *\n",
      "Epoch: 8, Train_Loss: 0.84687739610672, Test_Loss: 0.7935831546783447 *\n",
      "Epoch: 8, Train_Loss: 0.853145956993103, Test_Loss: 0.7955724000930786\n",
      "Epoch: 8, Train_Loss: 0.8289665579795837, Test_Loss: 0.7943678498268127 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_Loss: 0.7908445000648499, Test_Loss: 0.7943307161331177 *\n",
      "Epoch: 8, Train_Loss: 0.8245629668235779, Test_Loss: 0.8388499021530151\n",
      "Epoch: 8, Train_Loss: 0.7908663749694824, Test_Loss: 0.8206379413604736 *\n",
      "Epoch: 8, Train_Loss: 0.8099004030227661, Test_Loss: 6.145480155944824\n",
      "Epoch: 8, Train_Loss: 0.7921381592750549, Test_Loss: 0.9679168462753296 *\n",
      "Epoch: 8, Train_Loss: 0.8131534457206726, Test_Loss: 0.788663923740387 *\n",
      "Epoch: 8, Train_Loss: 1.1717538833618164, Test_Loss: 0.8136880993843079\n",
      "Epoch: 8, Train_Loss: 4.66336727142334, Test_Loss: 0.8420238494873047\n",
      "Epoch: 8, Train_Loss: 2.017228126525879, Test_Loss: 0.8480320572853088\n",
      "Epoch: 8, Train_Loss: 0.805993914604187, Test_Loss: 0.792542576789856 *\n",
      "Epoch: 8, Train_Loss: 0.7913362979888916, Test_Loss: 0.8907767534255981\n",
      "Epoch: 8, Train_Loss: 0.9579379558563232, Test_Loss: 0.8478506207466125 *\n",
      "Epoch: 8, Train_Loss: 0.8771482706069946, Test_Loss: 0.7870434522628784 *\n",
      "Epoch: 8, Train_Loss: 0.7990965843200684, Test_Loss: 0.8216601014137268\n",
      "Epoch: 8, Train_Loss: 0.7851880192756653, Test_Loss: 0.8044924736022949 *\n",
      "Epoch: 8, Train_Loss: 0.8418394327163696, Test_Loss: 0.7926883101463318 *\n",
      "Epoch: 8, Train_Loss: 0.8050265908241272, Test_Loss: 0.8051245808601379\n",
      "Epoch: 8, Train_Loss: 0.8012741208076477, Test_Loss: 0.9029184579849243\n",
      "Epoch: 8, Train_Loss: 1.0053057670593262, Test_Loss: 0.8203796148300171 *\n",
      "Epoch: 8, Train_Loss: 2.108551025390625, Test_Loss: 0.8850566744804382\n",
      "Epoch: 8, Train_Loss: 2.013167381286621, Test_Loss: 0.8309717774391174 *\n",
      "Epoch: 8, Train_Loss: 0.8935627937316895, Test_Loss: 0.8342376947402954\n",
      "Epoch: 8, Train_Loss: 0.8649464845657349, Test_Loss: 0.8030582070350647 *\n",
      "Epoch: 8, Train_Loss: 2.994713544845581, Test_Loss: 0.8003271818161011 *\n",
      "Epoch: 8, Train_Loss: 2.018005609512329, Test_Loss: 0.8033543825149536\n",
      "Epoch: 8, Train_Loss: 0.842578113079071, Test_Loss: 0.8028132319450378 *\n",
      "Epoch: 8, Train_Loss: 0.824492335319519, Test_Loss: 0.8050047755241394\n",
      "Epoch: 8, Train_Loss: 1.2864978313446045, Test_Loss: 0.803378164768219 *\n",
      "Epoch: 8, Train_Loss: 2.389612913131714, Test_Loss: 0.7907734513282776 *\n",
      "Epoch: 8, Train_Loss: 1.7081725597381592, Test_Loss: 0.8033342957496643\n",
      "Epoch: 8, Train_Loss: 0.7909095287322998, Test_Loss: 0.7953569293022156 *\n",
      "Epoch: 8, Train_Loss: 0.8002110719680786, Test_Loss: 0.7870692610740662 *\n",
      "Epoch: 8, Train_Loss: 1.0446895360946655, Test_Loss: 0.7893356680870056\n",
      "Epoch: 8, Train_Loss: 1.3431546688079834, Test_Loss: 0.8310631513595581\n",
      "Epoch: 8, Train_Loss: 0.8045009970664978, Test_Loss: 0.7976831197738647 *\n",
      "Epoch: 8, Train_Loss: 0.852013111114502, Test_Loss: 1.0268216133117676\n",
      "Epoch: 8, Train_Loss: 0.9078665971755981, Test_Loss: 1.3498667478561401\n",
      "Epoch: 8, Train_Loss: 0.8829752802848816, Test_Loss: 1.0515060424804688 *\n",
      "Epoch: 8, Train_Loss: 0.878709614276886, Test_Loss: 0.8658138513565063 *\n",
      "Epoch: 8, Train_Loss: 1.1058335304260254, Test_Loss: 0.8011587262153625 *\n",
      "Epoch: 8, Train_Loss: 0.9434358477592468, Test_Loss: 0.7823763489723206 *\n",
      "Epoch: 8, Train_Loss: 0.833907961845398, Test_Loss: 0.8427587151527405\n",
      "Epoch: 8, Train_Loss: 1.015490174293518, Test_Loss: 1.3610726594924927\n",
      "Epoch: 8, Train_Loss: 1.0122140645980835, Test_Loss: 1.7586945295333862\n",
      "Epoch: 8, Train_Loss: 1.2006324529647827, Test_Loss: 0.9349650740623474 *\n",
      "Epoch: 8, Train_Loss: 1.0210330486297607, Test_Loss: 0.8361984491348267 *\n",
      "Epoch: 8, Train_Loss: 0.8193526268005371, Test_Loss: 0.7784894108772278 *\n",
      "Epoch: 8, Train_Loss: 0.8926286697387695, Test_Loss: 0.7882676124572754\n",
      "Epoch: 8, Train_Loss: 0.8607223033905029, Test_Loss: 0.7840719223022461 *\n",
      "Epoch: 8, Train_Loss: 0.7895265817642212, Test_Loss: 0.8058834075927734\n",
      "Epoch: 8, Train_Loss: 0.7784923911094666, Test_Loss: 0.7904991507530212 *\n",
      "Epoch: 8, Train_Loss: 0.7776089310646057, Test_Loss: 0.8041173219680786\n",
      "Epoch: 8, Train_Loss: 0.7745955586433411, Test_Loss: 0.7789334654808044 *\n",
      "Epoch: 8, Train_Loss: 0.7823488712310791, Test_Loss: 0.8947218656539917\n",
      "Epoch: 8, Train_Loss: 0.7912934422492981, Test_Loss: 1.1610665321350098\n",
      "Epoch: 8, Train_Loss: 0.827460527420044, Test_Loss: 0.9044777154922485 *\n",
      "Epoch: 8, Train_Loss: 0.8120703101158142, Test_Loss: 0.978516697883606\n",
      "Epoch: 8, Train_Loss: 0.8909449577331543, Test_Loss: 0.782940685749054 *\n",
      "Epoch: 8, Train_Loss: 0.9380291104316711, Test_Loss: 0.7813577651977539 *\n",
      "Epoch: 8, Train_Loss: 1.0822944641113281, Test_Loss: 0.7803822755813599 *\n",
      "Epoch: 8, Train_Loss: 0.8026643395423889, Test_Loss: 0.7811091542243958\n",
      "Epoch: 8, Train_Loss: 0.8098312616348267, Test_Loss: 0.8324264883995056\n",
      "Epoch: 8, Train_Loss: 1.1697766780853271, Test_Loss: 5.7212419509887695\n",
      "Epoch: 8, Train_Loss: 1.2136555910110474, Test_Loss: 1.5276620388031006 *\n",
      "Epoch: 8, Train_Loss: 0.8170142769813538, Test_Loss: 0.7857457995414734 *\n",
      "Epoch: 8, Train_Loss: 0.8106353878974915, Test_Loss: 0.7789100408554077 *\n",
      "Epoch: 8, Train_Loss: 1.1487514972686768, Test_Loss: 0.7793411612510681\n",
      "Epoch: 8, Train_Loss: 1.270432710647583, Test_Loss: 0.7785959839820862 *\n",
      "Epoch: 8, Train_Loss: 0.9145753979682922, Test_Loss: 0.7896386981010437\n",
      "Epoch: 8, Train_Loss: 0.7865320444107056, Test_Loss: 0.8196890354156494\n",
      "Epoch: 8, Train_Loss: 0.8006637096405029, Test_Loss: 0.7860221862792969 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 8\n",
      "Epoch: 8, Train_Loss: 1.068112850189209, Test_Loss: 0.8043249845504761\n",
      "Epoch: 8, Train_Loss: 2.008218765258789, Test_Loss: 0.8129785060882568\n",
      "Epoch: 8, Train_Loss: 1.077668309211731, Test_Loss: 0.8419995307922363\n",
      "Epoch: 8, Train_Loss: 0.8077940940856934, Test_Loss: 0.7952291369438171 *\n",
      "Epoch: 8, Train_Loss: 0.7793261408805847, Test_Loss: 0.7861688733100891 *\n",
      "Epoch: 8, Train_Loss: 0.7795081734657288, Test_Loss: 0.814949095249176\n",
      "Epoch: 8, Train_Loss: 1.1718950271606445, Test_Loss: 0.8120290637016296 *\n",
      "Epoch: 8, Train_Loss: 0.8638913035392761, Test_Loss: 0.7734233736991882 *\n",
      "Epoch: 8, Train_Loss: 0.7830526828765869, Test_Loss: 0.8072535991668701\n",
      "Epoch: 8, Train_Loss: 1.1979117393493652, Test_Loss: 0.7920483946800232 *\n",
      "Epoch: 8, Train_Loss: 0.8001722097396851, Test_Loss: 0.7903227210044861 *\n",
      "Epoch: 8, Train_Loss: 0.7751710414886475, Test_Loss: 0.7897059321403503 *\n",
      "Epoch: 8, Train_Loss: 0.8292692303657532, Test_Loss: 0.8053078651428223\n",
      "Epoch: 8, Train_Loss: 0.9936138391494751, Test_Loss: 0.8143565654754639\n",
      "Epoch: 8, Train_Loss: 0.7979992032051086, Test_Loss: 0.8181161880493164\n",
      "Epoch: 8, Train_Loss: 0.8569951057434082, Test_Loss: 0.8036661148071289 *\n",
      "Epoch: 8, Train_Loss: 0.7740092277526855, Test_Loss: 0.7884060144424438 *\n",
      "Epoch: 8, Train_Loss: 0.9462795853614807, Test_Loss: 0.7887097597122192\n",
      "Epoch: 8, Train_Loss: 0.8213823437690735, Test_Loss: 0.7934085726737976\n",
      "Epoch: 8, Train_Loss: 0.7819510102272034, Test_Loss: 0.7884085178375244 *\n",
      "Epoch: 8, Train_Loss: 0.7711516618728638, Test_Loss: 0.7968100905418396\n",
      "Epoch: 8, Train_Loss: 0.7800350189208984, Test_Loss: 0.8495140075683594\n",
      "Epoch: 8, Train_Loss: 0.9587241411209106, Test_Loss: 1.1334929466247559\n",
      "Epoch: 8, Train_Loss: 1.1060481071472168, Test_Loss: 5.90617561340332\n",
      "Epoch: 8, Train_Loss: 1.0037356615066528, Test_Loss: 0.7713021039962769 *\n",
      "Epoch: 8, Train_Loss: 1.2939893007278442, Test_Loss: 0.8327080607414246\n",
      "Epoch: 8, Train_Loss: 1.0721607208251953, Test_Loss: 0.853825569152832\n",
      "Epoch: 8, Train_Loss: 0.9929853677749634, Test_Loss: 0.773909866809845 *\n",
      "Epoch: 8, Train_Loss: 0.8608969449996948, Test_Loss: 0.8030778765678406\n",
      "Epoch: 8, Train_Loss: 0.7887677550315857, Test_Loss: 0.7749506831169128 *\n",
      "Epoch: 8, Train_Loss: 0.7783294320106506, Test_Loss: 0.823371946811676\n",
      "Epoch: 8, Train_Loss: 0.7725086212158203, Test_Loss: 0.8012814521789551 *\n",
      "Epoch: 8, Train_Loss: 0.9026389718055725, Test_Loss: 0.8331593871116638\n",
      "Epoch: 8, Train_Loss: 1.199159860610962, Test_Loss: 0.8227244019508362 *\n",
      "Epoch: 8, Train_Loss: 1.0251787900924683, Test_Loss: 0.9779226779937744\n",
      "Epoch: 8, Train_Loss: 2.3530538082122803, Test_Loss: 0.8698956370353699 *\n",
      "Epoch: 8, Train_Loss: 1.3827474117279053, Test_Loss: 0.9044606685638428\n",
      "Epoch: 8, Train_Loss: 1.3441389799118042, Test_Loss: 0.7674915790557861 *\n",
      "Epoch: 8, Train_Loss: 0.893743634223938, Test_Loss: 0.8509756326675415\n",
      "Epoch: 8, Train_Loss: 0.7769044041633606, Test_Loss: 0.7860236763954163 *\n",
      "Epoch: 8, Train_Loss: 0.9423214197158813, Test_Loss: 0.9670766592025757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_Loss: 1.8106074333190918, Test_Loss: 1.0101852416992188\n",
      "Epoch: 8, Train_Loss: 1.4732551574707031, Test_Loss: 0.7667179107666016 *\n",
      "Epoch: 8, Train_Loss: 0.8630013465881348, Test_Loss: 0.7921813726425171\n",
      "Epoch: 8, Train_Loss: 0.8695085644721985, Test_Loss: 0.7863484025001526 *\n",
      "Epoch: 8, Train_Loss: 0.831741452217102, Test_Loss: 0.7929197549819946\n",
      "Epoch: 8, Train_Loss: 1.0915957689285278, Test_Loss: 0.783270001411438 *\n",
      "Epoch: 8, Train_Loss: 0.9726617932319641, Test_Loss: 0.7871074080467224\n",
      "Epoch: 8, Train_Loss: 1.2250386476516724, Test_Loss: 0.7906660437583923\n",
      "Epoch: 8, Train_Loss: 0.9748920202255249, Test_Loss: 0.7806801199913025 *\n",
      "Epoch: 8, Train_Loss: 0.8375499248504639, Test_Loss: 0.7947768568992615\n",
      "Epoch: 8, Train_Loss: 0.7903392910957336, Test_Loss: 0.8537905812263489\n",
      "Epoch: 8, Train_Loss: 0.7908175587654114, Test_Loss: 0.8243213891983032 *\n",
      "Epoch: 8, Train_Loss: 0.8046488165855408, Test_Loss: 0.7615203261375427 *\n",
      "Epoch: 9, Train_Loss: 0.8102847933769226, Test_Loss: 0.8615318536758423 *\n",
      "Epoch: 9, Train_Loss: 0.7767369151115417, Test_Loss: 0.9118790030479431\n",
      "Epoch: 9, Train_Loss: 0.7901315093040466, Test_Loss: 0.9731977581977844\n",
      "Epoch: 9, Train_Loss: 17.42290496826172, Test_Loss: 0.8229904174804688 *\n",
      "Epoch: 9, Train_Loss: 0.8489276170730591, Test_Loss: 0.7790704369544983 *\n",
      "Epoch: 9, Train_Loss: 3.147139072418213, Test_Loss: 0.824957013130188\n",
      "Epoch: 9, Train_Loss: 1.777436375617981, Test_Loss: 0.82187420129776 *\n",
      "Epoch: 9, Train_Loss: 0.8220198154449463, Test_Loss: 0.9198339581489563\n",
      "Epoch: 9, Train_Loss: 0.9453839063644409, Test_Loss: 1.1084322929382324\n",
      "Epoch: 9, Train_Loss: 6.341855049133301, Test_Loss: 1.31130051612854\n",
      "Epoch: 9, Train_Loss: 3.9190452098846436, Test_Loss: 1.588163137435913\n",
      "Epoch: 9, Train_Loss: 0.9397253394126892, Test_Loss: 1.225195050239563 *\n",
      "Epoch: 9, Train_Loss: 1.4960994720458984, Test_Loss: 0.9500889182090759 *\n",
      "Epoch: 9, Train_Loss: 4.036188125610352, Test_Loss: 1.6639962196350098\n",
      "Epoch: 9, Train_Loss: 1.9015036821365356, Test_Loss: 1.6381499767303467 *\n",
      "Epoch: 9, Train_Loss: 0.8696993589401245, Test_Loss: 1.4043389558792114 *\n",
      "Epoch: 9, Train_Loss: 0.7684186100959778, Test_Loss: 1.565909504890442\n",
      "Epoch: 9, Train_Loss: 0.7817981243133545, Test_Loss: 1.5153133869171143 *\n",
      "Epoch: 9, Train_Loss: 0.8281980752944946, Test_Loss: 1.4886436462402344 *\n",
      "Epoch: 9, Train_Loss: 0.7552282214164734, Test_Loss: 0.9700557589530945 *\n",
      "Epoch: 9, Train_Loss: 0.7903262376785278, Test_Loss: 1.2312699556350708\n",
      "Epoch: 9, Train_Loss: 0.7472350001335144, Test_Loss: 0.8369792103767395 *\n",
      "Epoch: 9, Train_Loss: 0.7872672080993652, Test_Loss: 1.1452131271362305\n",
      "Epoch: 9, Train_Loss: 0.8456816077232361, Test_Loss: 1.1568326950073242\n",
      "Epoch: 9, Train_Loss: 0.9142577648162842, Test_Loss: 1.064995288848877 *\n",
      "Epoch: 9, Train_Loss: 0.8185136914253235, Test_Loss: 0.9851226806640625 *\n",
      "Epoch: 9, Train_Loss: 0.870736837387085, Test_Loss: 0.8846191763877869 *\n",
      "Epoch: 9, Train_Loss: 0.866989016532898, Test_Loss: 0.8609619736671448 *\n",
      "Epoch: 9, Train_Loss: 0.7633819580078125, Test_Loss: 7.213415145874023\n",
      "Epoch: 9, Train_Loss: 0.7878731489181519, Test_Loss: 1.117100715637207 *\n",
      "Epoch: 9, Train_Loss: 0.7729550004005432, Test_Loss: 0.8684203624725342 *\n",
      "Epoch: 9, Train_Loss: 0.7637309432029724, Test_Loss: 0.8557663559913635 *\n",
      "Epoch: 9, Train_Loss: 0.7601451873779297, Test_Loss: 0.8116710782051086 *\n",
      "Epoch: 9, Train_Loss: 0.7549194693565369, Test_Loss: 0.7962205410003662 *\n",
      "Epoch: 9, Train_Loss: 0.7498489022254944, Test_Loss: 0.8708379864692688\n",
      "Epoch: 9, Train_Loss: 0.7529711723327637, Test_Loss: 0.9090079069137573\n",
      "Epoch: 9, Train_Loss: 0.7493804097175598, Test_Loss: 0.8093163967132568 *\n",
      "Epoch: 9, Train_Loss: 0.7433781027793884, Test_Loss: 0.8295897245407104\n",
      "Epoch: 9, Train_Loss: 0.7432931065559387, Test_Loss: 0.8465322852134705\n",
      "Epoch: 9, Train_Loss: 0.7508836984634399, Test_Loss: 0.892399251461029\n",
      "Epoch: 9, Train_Loss: 0.7462641596794128, Test_Loss: 0.7800009846687317 *\n",
      "Epoch: 9, Train_Loss: 0.8204185366630554, Test_Loss: 0.7714853286743164 *\n",
      "Epoch: 9, Train_Loss: 0.7608396410942078, Test_Loss: 0.8168959617614746\n",
      "Epoch: 9, Train_Loss: 0.7840473651885986, Test_Loss: 0.7801240682601929 *\n",
      "Epoch: 9, Train_Loss: 5.708059310913086, Test_Loss: 0.7684770226478577 *\n",
      "Epoch: 9, Train_Loss: 4.566453456878662, Test_Loss: 0.7886773347854614\n",
      "Epoch: 9, Train_Loss: 0.7564561367034912, Test_Loss: 0.8251001834869385\n",
      "Epoch: 9, Train_Loss: 0.7827240228652954, Test_Loss: 0.7930727005004883 *\n",
      "Epoch: 9, Train_Loss: 0.8312057852745056, Test_Loss: 0.8107897639274597\n",
      "Epoch: 9, Train_Loss: 0.7644481062889099, Test_Loss: 0.803168773651123 *\n",
      "Epoch: 9, Train_Loss: 0.7454488277435303, Test_Loss: 0.8568334579467773\n",
      "Epoch: 9, Train_Loss: 0.7979250550270081, Test_Loss: 0.8527252078056335 *\n",
      "Epoch: 9, Train_Loss: 0.8956766724586487, Test_Loss: 0.8323453664779663 *\n",
      "Epoch: 9, Train_Loss: 1.0677833557128906, Test_Loss: 0.7965566515922546 *\n",
      "Epoch: 9, Train_Loss: 0.9306551814079285, Test_Loss: 0.8091241121292114\n",
      "Epoch: 9, Train_Loss: 0.7995263338088989, Test_Loss: 0.7946724891662598 *\n",
      "Epoch: 9, Train_Loss: 0.8183207511901855, Test_Loss: 0.7900401949882507 *\n",
      "Epoch: 9, Train_Loss: 0.8886908292770386, Test_Loss: 0.80771803855896\n",
      "Epoch: 9, Train_Loss: 0.9015896320343018, Test_Loss: 0.8392634391784668\n",
      "Epoch: 9, Train_Loss: 0.8935785889625549, Test_Loss: 2.5162196159362793\n",
      "Epoch: 9, Train_Loss: 0.8527153730392456, Test_Loss: 5.2022857666015625\n",
      "Epoch: 9, Train_Loss: 0.8170820474624634, Test_Loss: 0.7645259499549866 *\n",
      "Epoch: 9, Train_Loss: 0.7420499324798584, Test_Loss: 0.7430676817893982 *\n",
      "Epoch: 9, Train_Loss: 0.792549729347229, Test_Loss: 0.7543530464172363\n",
      "Epoch: 9, Train_Loss: 0.7640432119369507, Test_Loss: 0.7407049536705017 *\n",
      "Epoch: 9, Train_Loss: 0.7585119009017944, Test_Loss: 0.755664050579071\n",
      "Epoch: 9, Train_Loss: 0.7488709092140198, Test_Loss: 0.7896118760108948\n",
      "Epoch: 9, Train_Loss: 0.7503970861434937, Test_Loss: 0.9349475502967834\n",
      "Epoch: 9, Train_Loss: 0.7585545182228088, Test_Loss: 0.7534618973731995 *\n",
      "Epoch: 9, Train_Loss: 4.741589546203613, Test_Loss: 0.7494027614593506 *\n",
      "Epoch: 9, Train_Loss: 2.1844277381896973, Test_Loss: 0.771024227142334\n",
      "Epoch: 9, Train_Loss: 0.7436492443084717, Test_Loss: 0.7509557008743286 *\n",
      "Epoch: 9, Train_Loss: 0.796206533908844, Test_Loss: 0.7468165159225464 *\n",
      "Epoch: 9, Train_Loss: 0.7682392001152039, Test_Loss: 0.7763181328773499\n",
      "Epoch: 9, Train_Loss: 0.7526205778121948, Test_Loss: 0.759724497795105 *\n",
      "Epoch: 9, Train_Loss: 0.7441226243972778, Test_Loss: 0.8298687934875488\n",
      "Epoch: 9, Train_Loss: 0.7534612417221069, Test_Loss: 0.8259020447731018 *\n",
      "Epoch: 9, Train_Loss: 0.7491275668144226, Test_Loss: 0.7580339312553406 *\n",
      "Epoch: 9, Train_Loss: 0.7423775792121887, Test_Loss: 0.7517399191856384 *\n",
      "Epoch: 9, Train_Loss: 0.788195013999939, Test_Loss: 0.7338907718658447 *\n",
      "Epoch: 9, Train_Loss: 0.7462714314460754, Test_Loss: 0.733440101146698 *\n",
      "Epoch: 9, Train_Loss: 0.7400211691856384, Test_Loss: 0.7343800067901611\n",
      "Epoch: 9, Train_Loss: 0.7549229264259338, Test_Loss: 0.7357977032661438\n",
      "Epoch: 9, Train_Loss: 0.735714852809906, Test_Loss: 0.7344765663146973 *\n",
      "Epoch: 9, Train_Loss: 0.7344164848327637, Test_Loss: 0.7338420748710632 *\n",
      "Epoch: 9, Train_Loss: 0.7406086921691895, Test_Loss: 0.7349613904953003\n",
      "Epoch: 9, Train_Loss: 0.7651412487030029, Test_Loss: 0.7330849766731262 *\n",
      "Epoch: 9, Train_Loss: 0.7575004696846008, Test_Loss: 0.7331150770187378\n",
      "Epoch: 9, Train_Loss: 0.735977828502655, Test_Loss: 0.7585601806640625\n",
      "Epoch: 9, Train_Loss: 0.7365071177482605, Test_Loss: 0.7322043776512146 *\n",
      "Epoch: 9, Train_Loss: 0.7761726379394531, Test_Loss: 0.7487751245498657\n",
      "Epoch: 9, Train_Loss: 0.7764413356781006, Test_Loss: 0.7644334435462952\n",
      "Epoch: 9, Train_Loss: 0.753406286239624, Test_Loss: 1.0380616188049316\n",
      "Epoch: 9, Train_Loss: 0.7534887790679932, Test_Loss: 1.0555272102355957\n",
      "Epoch: 9, Train_Loss: 0.802090048789978, Test_Loss: 0.8166822195053101 *\n",
      "Epoch: 9, Train_Loss: 0.8241653442382812, Test_Loss: 0.7375190258026123 *\n",
      "Epoch: 9, Train_Loss: 0.7794641852378845, Test_Loss: 0.750465989112854\n",
      "Epoch: 9, Train_Loss: 0.7865009307861328, Test_Loss: 0.7728394865989685\n",
      "Epoch: 9, Train_Loss: 0.8942618370056152, Test_Loss: 0.8775833249092102\n",
      "Epoch: 9, Train_Loss: 0.7689071893692017, Test_Loss: 1.5052505731582642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at location save_new\\model.ckpt at epoch 9\n",
      "Epoch: 9, Train_Loss: 0.7571637630462646, Test_Loss: 1.43886399269104 *\n",
      "Epoch: 9, Train_Loss: 0.7327241897583008, Test_Loss: 0.7771880626678467 *\n",
      "Epoch: 9, Train_Loss: 0.7286770343780518, Test_Loss: 0.7740258574485779 *\n",
      "Epoch: 9, Train_Loss: 0.727796733379364, Test_Loss: 0.7350509762763977 *\n",
      "Epoch: 9, Train_Loss: 0.7278344035148621, Test_Loss: 0.7396388053894043\n",
      "Epoch: 9, Train_Loss: 0.7325230836868286, Test_Loss: 0.744276225566864\n",
      "Epoch: 9, Train_Loss: 4.539154529571533, Test_Loss: 0.7451067566871643\n",
      "Epoch: 9, Train_Loss: 1.882433295249939, Test_Loss: 0.8119057416915894\n",
      "Epoch: 9, Train_Loss: 0.7285922765731812, Test_Loss: 0.7331769466400146 *\n",
      "Epoch: 9, Train_Loss: 0.7467318773269653, Test_Loss: 0.7458005547523499\n",
      "Epoch: 9, Train_Loss: 0.7324833869934082, Test_Loss: 0.8259562849998474\n",
      "Epoch: 9, Train_Loss: 0.7247533798217773, Test_Loss: 1.0873116254806519\n",
      "Epoch: 9, Train_Loss: 0.7258540391921997, Test_Loss: 0.9474210739135742 *\n",
      "Epoch: 9, Train_Loss: 0.7248860001564026, Test_Loss: 0.7342316508293152 *\n",
      "Epoch: 9, Train_Loss: 0.7241771221160889, Test_Loss: 0.725321352481842 *\n",
      "Epoch: 9, Train_Loss: 0.7244660258293152, Test_Loss: 0.7251554131507874 *\n",
      "Epoch: 9, Train_Loss: 0.7501217722892761, Test_Loss: 0.7248879075050354 *\n",
      "Epoch: 9, Train_Loss: 0.7873849272727966, Test_Loss: 0.7288332581520081\n",
      "Epoch: 9, Train_Loss: 0.79231196641922, Test_Loss: 1.1641764640808105\n",
      "Epoch: 9, Train_Loss: 0.8094423413276672, Test_Loss: 5.96708345413208\n",
      "Epoch: 9, Train_Loss: 0.7372711300849915, Test_Loss: 0.7857056260108948 *\n",
      "Epoch: 9, Train_Loss: 0.7443578839302063, Test_Loss: 0.7303822040557861 *\n",
      "Epoch: 9, Train_Loss: 0.8876501321792603, Test_Loss: 0.7256861329078674 *\n",
      "Epoch: 9, Train_Loss: 0.8904401063919067, Test_Loss: 0.7239973545074463 *\n",
      "Epoch: 9, Train_Loss: 0.882642388343811, Test_Loss: 0.727894127368927\n",
      "Epoch: 9, Train_Loss: 0.7617952227592468, Test_Loss: 0.7285728454589844\n",
      "Epoch: 9, Train_Loss: 0.7228703498840332, Test_Loss: 0.7490208148956299\n",
      "Epoch: 9, Train_Loss: 0.7245946526527405, Test_Loss: 0.7282401323318481 *\n",
      "Epoch: 9, Train_Loss: 0.7235866785049438, Test_Loss: 0.7363318800926208\n",
      "Epoch: 9, Train_Loss: 0.7278784513473511, Test_Loss: 0.7388955950737\n",
      "Epoch: 9, Train_Loss: 0.7261235117912292, Test_Loss: 0.7521690130233765\n",
      "Epoch: 9, Train_Loss: 0.723179817199707, Test_Loss: 0.7243863940238953 *\n",
      "Epoch: 9, Train_Loss: 0.7230873703956604, Test_Loss: 0.722036600112915 *\n",
      "Epoch: 9, Train_Loss: 0.7220579385757446, Test_Loss: 0.7394906282424927\n",
      "Epoch: 9, Train_Loss: 0.7311637997627258, Test_Loss: 0.7233290076255798 *\n",
      "Epoch: 9, Train_Loss: 0.8015291094779968, Test_Loss: 0.7272112965583801\n",
      "Epoch: 9, Train_Loss: 0.8998618125915527, Test_Loss: 0.7280547618865967\n",
      "Epoch: 9, Train_Loss: 0.8852840662002563, Test_Loss: 0.7433420419692993\n",
      "Epoch: 9, Train_Loss: 0.7901496291160583, Test_Loss: 0.7255354523658752 *\n",
      "Epoch: 9, Train_Loss: 0.8623074293136597, Test_Loss: 0.7365564703941345\n",
      "Epoch: 9, Train_Loss: 0.8941396474838257, Test_Loss: 0.7300735116004944 *\n",
      "Epoch: 9, Train_Loss: 0.7369636297225952, Test_Loss: 0.7430930733680725\n",
      "Epoch: 9, Train_Loss: 0.936979353427887, Test_Loss: 0.7432914972305298\n",
      "Epoch: 9, Train_Loss: 0.8771125078201294, Test_Loss: 0.7369174361228943 *\n",
      "Epoch: 9, Train_Loss: 0.999923586845398, Test_Loss: 0.7270484566688538 *\n",
      "Epoch: 9, Train_Loss: 0.7315953373908997, Test_Loss: 0.736017644405365\n",
      "Epoch: 9, Train_Loss: 1.553443193435669, Test_Loss: 0.7285342216491699 *\n",
      "Epoch: 9, Train_Loss: 3.028782606124878, Test_Loss: 0.7282050251960754 *\n",
      "Epoch: 9, Train_Loss: 0.7572213411331177, Test_Loss: 0.748601496219635\n",
      "Epoch: 9, Train_Loss: 0.7653845548629761, Test_Loss: 0.7798296213150024\n",
      "Epoch: 9, Train_Loss: 0.7433434724807739, Test_Loss: 3.5293116569519043\n",
      "Epoch: 9, Train_Loss: 0.7475311160087585, Test_Loss: 3.7007927894592285\n",
      "Epoch: 9, Train_Loss: 0.7171719074249268, Test_Loss: 0.7198660373687744 *\n",
      "Epoch: 9, Train_Loss: 0.7245086431503296, Test_Loss: 0.7155500054359436 *\n",
      "Epoch: 9, Train_Loss: 0.8417682647705078, Test_Loss: 0.74985671043396\n",
      "Epoch: 9, Train_Loss: 0.8441609144210815, Test_Loss: 0.7308014035224915 *\n",
      "Epoch: 9, Train_Loss: 0.8186345100402832, Test_Loss: 0.7445834279060364\n",
      "Epoch: 9, Train_Loss: 0.7812870740890503, Test_Loss: 0.7803866267204285\n",
      "Epoch: 9, Train_Loss: 0.7678854465484619, Test_Loss: 0.845076322555542\n",
      "Epoch: 9, Train_Loss: 0.7513418793678284, Test_Loss: 0.716093122959137 *\n",
      "Epoch: 9, Train_Loss: 0.7528043389320374, Test_Loss: 0.735259473323822\n",
      "Epoch: 9, Train_Loss: 0.7219138145446777, Test_Loss: 0.7334508895874023 *\n",
      "Epoch: 9, Train_Loss: 0.7292230725288391, Test_Loss: 0.743466854095459\n",
      "Epoch: 9, Train_Loss: 0.7194936871528625, Test_Loss: 0.7197695970535278 *\n",
      "Epoch: 9, Train_Loss: 0.711892306804657, Test_Loss: 0.7723969221115112\n",
      "Epoch: 9, Train_Loss: 0.7557626366615295, Test_Loss: 0.7612364888191223 *\n",
      "Epoch: 9, Train_Loss: 0.8011131882667542, Test_Loss: 0.775076150894165\n",
      "Epoch: 9, Train_Loss: 0.7600817680358887, Test_Loss: 0.7592148780822754 *\n",
      "Epoch: 9, Train_Loss: 0.710954487323761, Test_Loss: 0.7360678911209106 *\n",
      "Epoch: 9, Train_Loss: 0.7116709351539612, Test_Loss: 0.7340789437294006 *\n",
      "Epoch: 9, Train_Loss: 0.7121320366859436, Test_Loss: 0.7167308330535889 *\n",
      "Epoch: 9, Train_Loss: 0.7101881504058838, Test_Loss: 0.7163102030754089 *\n",
      "Epoch: 9, Train_Loss: 0.7115592360496521, Test_Loss: 0.7155948877334595 *\n",
      "Epoch: 9, Train_Loss: 0.7091378569602966, Test_Loss: 0.7212856411933899\n",
      "Epoch: 9, Train_Loss: 0.710128128528595, Test_Loss: 0.7203810214996338 *\n",
      "Epoch: 9, Train_Loss: 0.7107322812080383, Test_Loss: 0.715537965297699 *\n",
      "Epoch: 9, Train_Loss: 0.7105927467346191, Test_Loss: 0.7214019894599915\n",
      "Epoch: 9, Train_Loss: 0.7178570032119751, Test_Loss: 0.7143842577934265 *\n",
      "Epoch: 9, Train_Loss: 0.7254504561424255, Test_Loss: 0.7121778130531311 *\n",
      "Epoch: 9, Train_Loss: 0.7221274971961975, Test_Loss: 0.717932403087616\n",
      "Epoch: 9, Train_Loss: 0.7181636095046997, Test_Loss: 0.7195488214492798\n",
      "Epoch: 9, Train_Loss: 0.7228655815124512, Test_Loss: 0.743359386920929\n",
      "Epoch: 9, Train_Loss: 0.7114744782447815, Test_Loss: 0.7362450361251831 *\n",
      "Epoch: 9, Train_Loss: 0.730736494064331, Test_Loss: 1.1885029077529907\n",
      "Epoch: 9, Train_Loss: 0.7072885632514954, Test_Loss: 1.1296136379241943 *\n",
      "Epoch: 9, Train_Loss: 0.7109047174453735, Test_Loss: 0.8270710110664368 *\n",
      "Epoch: 9, Train_Loss: 0.7247020602226257, Test_Loss: 0.7262251377105713 *\n",
      "Epoch: 9, Train_Loss: 0.7227466106414795, Test_Loss: 0.7251912355422974 *\n",
      "Epoch: 9, Train_Loss: 0.7118373513221741, Test_Loss: 0.7435084581375122\n",
      "Epoch: 9, Train_Loss: 0.7086890339851379, Test_Loss: 0.9441804885864258\n",
      "Epoch: 9, Train_Loss: 0.714138925075531, Test_Loss: 1.5392310619354248\n",
      "Epoch: 9, Train_Loss: 0.752765417098999, Test_Loss: 1.223734974861145 *\n",
      "Epoch: 9, Train_Loss: 0.7286348342895508, Test_Loss: 0.7765330672264099 *\n",
      "Epoch: 9, Train_Loss: 0.7422531247138977, Test_Loss: 0.736758291721344 *\n",
      "Epoch: 9, Train_Loss: 0.7049395442008972, Test_Loss: 0.7111682295799255 *\n",
      "Epoch: 9, Train_Loss: 0.7382063269615173, Test_Loss: 0.7084706425666809 *\n",
      "Epoch: 9, Train_Loss: 0.7453309893608093, Test_Loss: 0.714288592338562\n",
      "Epoch: 9, Train_Loss: 0.7076013684272766, Test_Loss: 0.7181336283683777\n",
      "Epoch: 9, Train_Loss: 0.7129054069519043, Test_Loss: 0.7692509293556213\n",
      "Epoch: 9, Train_Loss: 0.7390499114990234, Test_Loss: 0.7070724964141846 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 9\n",
      "Epoch: 9, Train_Loss: 0.79695063829422, Test_Loss: 0.752934992313385\n",
      "Epoch: 9, Train_Loss: 0.8045390248298645, Test_Loss: 0.8237676024436951\n",
      "Epoch: 9, Train_Loss: 0.7813504934310913, Test_Loss: 1.0630079507827759\n",
      "Epoch: 9, Train_Loss: 0.7423096299171448, Test_Loss: 0.9251663088798523 *\n",
      "Epoch: 9, Train_Loss: 0.705009400844574, Test_Loss: 0.7144017815589905 *\n",
      "Epoch: 9, Train_Loss: 0.7302048802375793, Test_Loss: 0.7065759301185608 *\n",
      "Epoch: 9, Train_Loss: 0.7021563649177551, Test_Loss: 0.7061542272567749 *\n",
      "Epoch: 9, Train_Loss: 0.7107873558998108, Test_Loss: 0.7059481143951416 *\n",
      "Epoch: 9, Train_Loss: 0.7075552940368652, Test_Loss: 0.715930163860321\n",
      "Epoch: 9, Train_Loss: 0.7107278108596802, Test_Loss: 2.0599279403686523\n",
      "Epoch: 9, Train_Loss: 0.7877951860427856, Test_Loss: 4.8813276290893555\n",
      "Epoch: 9, Train_Loss: 0.7075316905975342, Test_Loss: 0.7139697670936584 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_Loss: 0.7771826982498169, Test_Loss: 0.7028898000717163 *\n",
      "Epoch: 9, Train_Loss: 0.7043006420135498, Test_Loss: 0.701483428478241 *\n",
      "Epoch: 9, Train_Loss: 0.7217249870300293, Test_Loss: 0.7050880789756775\n",
      "Epoch: 9, Train_Loss: 0.7088934779167175, Test_Loss: 0.7035574316978455 *\n",
      "Epoch: 9, Train_Loss: 0.9729832410812378, Test_Loss: 0.7051372528076172\n",
      "Epoch: 9, Train_Loss: 0.7536291480064392, Test_Loss: 0.7104540467262268\n",
      "Epoch: 9, Train_Loss: 0.7265390157699585, Test_Loss: 0.7022648453712463 *\n",
      "Epoch: 9, Train_Loss: 0.7075912952423096, Test_Loss: 0.708522617816925\n",
      "Epoch: 9, Train_Loss: 0.6989893913269043, Test_Loss: 0.7067439556121826 *\n",
      "Epoch: 9, Train_Loss: 0.7001798152923584, Test_Loss: 0.7151092290878296\n",
      "Epoch: 9, Train_Loss: 0.6983391046524048, Test_Loss: 0.7078294157981873 *\n",
      "Epoch: 9, Train_Loss: 0.7072303295135498, Test_Loss: 0.7035504579544067 *\n",
      "Epoch: 9, Train_Loss: 0.7067234516143799, Test_Loss: 0.7113938331604004\n",
      "Epoch: 9, Train_Loss: 0.7216004729270935, Test_Loss: 0.6973024606704712 *\n",
      "Epoch: 9, Train_Loss: 0.7087270021438599, Test_Loss: 0.7011297345161438\n",
      "Epoch: 9, Train_Loss: 0.7117742300033569, Test_Loss: 0.6985722184181213 *\n",
      "Epoch: 9, Train_Loss: 0.7170354127883911, Test_Loss: 0.7053879499435425\n",
      "Epoch: 9, Train_Loss: 0.6991322040557861, Test_Loss: 0.6988300085067749 *\n",
      "Epoch: 9, Train_Loss: 0.6952758431434631, Test_Loss: 0.702953577041626\n",
      "Epoch: 9, Train_Loss: 0.7039775252342224, Test_Loss: 0.6994668841362 *\n",
      "Epoch: 9, Train_Loss: 0.7158362865447998, Test_Loss: 0.7070996165275574\n",
      "Epoch: 9, Train_Loss: 0.7283686399459839, Test_Loss: 0.7019348740577698 *\n",
      "Epoch: 9, Train_Loss: 0.6963279247283936, Test_Loss: 0.7003799676895142 *\n",
      "Epoch: 9, Train_Loss: 0.7412421703338623, Test_Loss: 0.6971376538276672 *\n",
      "Epoch: 9, Train_Loss: 0.7760757207870483, Test_Loss: 0.7034512162208557\n",
      "Epoch: 9, Train_Loss: 0.7424125671386719, Test_Loss: 0.6985921859741211 *\n",
      "Epoch: 9, Train_Loss: 0.6934507489204407, Test_Loss: 0.6975366473197937 *\n",
      "Epoch: 9, Train_Loss: 0.7213450074195862, Test_Loss: 0.7418868541717529\n",
      "Epoch: 9, Train_Loss: 0.6984944343566895, Test_Loss: 0.7289526462554932 *\n",
      "Epoch: 9, Train_Loss: 0.7142080664634705, Test_Loss: 4.447229862213135\n",
      "Epoch: 9, Train_Loss: 0.6946558952331543, Test_Loss: 2.574552536010742 *\n",
      "Epoch: 9, Train_Loss: 0.7157156467437744, Test_Loss: 0.6940149068832397 *\n",
      "Epoch: 9, Train_Loss: 0.7452801465988159, Test_Loss: 0.6983063817024231\n",
      "Epoch: 9, Train_Loss: 3.160205364227295, Test_Loss: 0.7395341992378235\n",
      "Epoch: 9, Train_Loss: 3.55380916595459, Test_Loss: 0.729806661605835 *\n",
      "Epoch: 9, Train_Loss: 0.7184240818023682, Test_Loss: 0.7098726034164429 *\n",
      "Epoch: 9, Train_Loss: 0.6942057013511658, Test_Loss: 0.7721871733665466\n",
      "Epoch: 9, Train_Loss: 0.8419274091720581, Test_Loss: 0.7879900932312012\n",
      "Epoch: 9, Train_Loss: 0.8532036542892456, Test_Loss: 0.6928853392601013 *\n",
      "Epoch: 9, Train_Loss: 0.7174608707427979, Test_Loss: 0.719494640827179\n",
      "Epoch: 9, Train_Loss: 0.6919801831245422, Test_Loss: 0.7096201777458191 *\n",
      "Epoch: 9, Train_Loss: 0.7344158887863159, Test_Loss: 0.7050946950912476 *\n",
      "Epoch: 9, Train_Loss: 0.7287322282791138, Test_Loss: 0.6939759850502014 *\n",
      "Epoch: 9, Train_Loss: 0.7001916170120239, Test_Loss: 0.7706257700920105\n",
      "Epoch: 9, Train_Loss: 0.7432919144630432, Test_Loss: 0.7366282939910889 *\n",
      "Epoch: 9, Train_Loss: 1.873706340789795, Test_Loss: 0.7703273892402649\n",
      "Epoch: 9, Train_Loss: 2.1599485874176025, Test_Loss: 0.749276876449585 *\n",
      "Epoch: 9, Train_Loss: 0.8958367705345154, Test_Loss: 0.7263534665107727 *\n",
      "Epoch: 9, Train_Loss: 0.770034670829773, Test_Loss: 0.7066034078598022 *\n",
      "Epoch: 9, Train_Loss: 2.2668814659118652, Test_Loss: 0.6983528137207031 *\n",
      "Epoch: 9, Train_Loss: 2.3838257789611816, Test_Loss: 0.6981938481330872 *\n",
      "Epoch: 9, Train_Loss: 0.7273299694061279, Test_Loss: 0.6996962428092957\n",
      "Epoch: 9, Train_Loss: 0.7119468450546265, Test_Loss: 0.7030171155929565\n",
      "Epoch: 9, Train_Loss: 0.9029439687728882, Test_Loss: 0.7014581561088562 *\n",
      "Epoch: 9, Train_Loss: 2.082151412963867, Test_Loss: 0.6924801468849182 *\n",
      "Epoch: 9, Train_Loss: 1.9354768991470337, Test_Loss: 0.7017965912818909\n",
      "Epoch: 9, Train_Loss: 0.7024027109146118, Test_Loss: 0.6912491917610168 *\n",
      "Epoch: 9, Train_Loss: 0.7114705443382263, Test_Loss: 0.6911882162094116 *\n",
      "Epoch: 9, Train_Loss: 0.7498406171798706, Test_Loss: 0.6988605260848999\n",
      "Epoch: 9, Train_Loss: 1.4893213510513306, Test_Loss: 0.7032076120376587\n",
      "Epoch: 9, Train_Loss: 0.7668145895004272, Test_Loss: 0.712335467338562\n",
      "Epoch: 9, Train_Loss: 0.8090798258781433, Test_Loss: 0.786264181137085\n",
      "Epoch: 9, Train_Loss: 0.7877805233001709, Test_Loss: 1.1531774997711182\n",
      "Epoch: 9, Train_Loss: 0.8225813508033752, Test_Loss: 1.0317643880844116 *\n",
      "Epoch: 9, Train_Loss: 0.8036307096481323, Test_Loss: 0.7881745100021362 *\n",
      "Epoch: 9, Train_Loss: 0.907631516456604, Test_Loss: 0.6984406113624573 *\n",
      "Epoch: 9, Train_Loss: 0.8856408596038818, Test_Loss: 0.7037126421928406\n",
      "Epoch: 9, Train_Loss: 0.7241776585578918, Test_Loss: 0.7238379120826721\n",
      "Epoch: 9, Train_Loss: 0.8444784879684448, Test_Loss: 0.9785484075546265\n",
      "Epoch: 9, Train_Loss: 0.8779778480529785, Test_Loss: 1.3879497051239014\n",
      "Epoch: 9, Train_Loss: 1.0794447660446167, Test_Loss: 1.063383936882019 *\n",
      "Epoch: 9, Train_Loss: 0.9635981321334839, Test_Loss: 0.7632672190666199 *\n",
      "Epoch: 9, Train_Loss: 0.710211992263794, Test_Loss: 0.6972079873085022 *\n",
      "Epoch: 9, Train_Loss: 0.7730300426483154, Test_Loss: 0.6923456788063049 *\n",
      "Epoch: 9, Train_Loss: 0.8181040287017822, Test_Loss: 0.6866742968559265 *\n",
      "Epoch: 9, Train_Loss: 0.6994348168373108, Test_Loss: 0.7079746723175049\n",
      "Epoch: 9, Train_Loss: 0.6903662085533142, Test_Loss: 0.6960214972496033 *\n",
      "Epoch: 9, Train_Loss: 0.6827307343482971, Test_Loss: 0.7215505838394165\n",
      "Epoch: 9, Train_Loss: 0.6830160617828369, Test_Loss: 0.6941850781440735 *\n",
      "Epoch: 9, Train_Loss: 0.6860100030899048, Test_Loss: 0.7732053995132446\n",
      "Epoch: 9, Train_Loss: 0.6882017254829407, Test_Loss: 0.9015774130821228\n",
      "Epoch: 9, Train_Loss: 0.7255059480667114, Test_Loss: 0.965319037437439\n",
      "Epoch: 9, Train_Loss: 0.697838544845581, Test_Loss: 0.9710196852684021\n",
      "Epoch: 9, Train_Loss: 0.7547619938850403, Test_Loss: 0.6994710564613342 *\n",
      "Epoch: 9, Train_Loss: 0.797642707824707, Test_Loss: 0.6880869269371033 *\n",
      "Epoch: 9, Train_Loss: 1.0659939050674438, Test_Loss: 0.6865060329437256 *\n",
      "Epoch: 9, Train_Loss: 0.6967124938964844, Test_Loss: 0.6846696734428406 *\n",
      "Epoch: 9, Train_Loss: 0.7161798477172852, Test_Loss: 0.7456222772598267\n",
      "Model saved at location save_new\\model.ckpt at epoch 9\n",
      "Epoch: 9, Train_Loss: 0.916917085647583, Test_Loss: 3.2686424255371094\n",
      "Epoch: 9, Train_Loss: 1.0310460329055786, Test_Loss: 3.421696662902832\n",
      "Epoch: 9, Train_Loss: 0.8524816632270813, Test_Loss: 0.689070463180542 *\n",
      "Epoch: 9, Train_Loss: 0.6933020353317261, Test_Loss: 0.6851069331169128 *\n",
      "Epoch: 9, Train_Loss: 0.9491647481918335, Test_Loss: 0.6861267685890198\n",
      "Epoch: 9, Train_Loss: 1.161182165145874, Test_Loss: 0.6854085326194763 *\n",
      "Epoch: 9, Train_Loss: 0.967496395111084, Test_Loss: 0.6956430673599243\n",
      "Epoch: 9, Train_Loss: 0.6941936016082764, Test_Loss: 0.708867609500885\n",
      "Epoch: 9, Train_Loss: 0.6942052841186523, Test_Loss: 0.7008898854255676 *\n",
      "Epoch: 9, Train_Loss: 0.7446852326393127, Test_Loss: 0.6951625347137451 *\n",
      "Epoch: 9, Train_Loss: 1.7571368217468262, Test_Loss: 0.7148342132568359\n",
      "Epoch: 9, Train_Loss: 1.1947667598724365, Test_Loss: 0.7129558324813843 *\n",
      "Epoch: 9, Train_Loss: 0.6881048083305359, Test_Loss: 0.7265094518661499\n",
      "Epoch: 9, Train_Loss: 0.6942845582962036, Test_Loss: 0.6780890226364136 *\n",
      "Epoch: 9, Train_Loss: 0.6808035969734192, Test_Loss: 0.6861892342567444\n",
      "Epoch: 9, Train_Loss: 0.8818929195404053, Test_Loss: 0.7131601572036743\n",
      "Epoch: 9, Train_Loss: 0.9505290985107422, Test_Loss: 0.6797356605529785 *\n",
      "Epoch: 9, Train_Loss: 0.6827044486999512, Test_Loss: 0.7012180089950562\n",
      "Epoch: 9, Train_Loss: 1.0165053606033325, Test_Loss: 0.696947455406189 *\n",
      "Epoch: 9, Train_Loss: 0.6945796608924866, Test_Loss: 0.7082521915435791\n",
      "Epoch: 9, Train_Loss: 0.6892938017845154, Test_Loss: 0.6826479434967041 *\n",
      "Epoch: 9, Train_Loss: 0.7212433815002441, Test_Loss: 0.6966977119445801\n",
      "Epoch: 9, Train_Loss: 0.8732848763465881, Test_Loss: 0.7053764462471008\n",
      "Epoch: 9, Train_Loss: 0.7304218411445618, Test_Loss: 0.7306350469589233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_Loss: 0.7574726343154907, Test_Loss: 0.7166926860809326 *\n",
      "Epoch: 9, Train_Loss: 0.6759422421455383, Test_Loss: 0.7019545435905457 *\n",
      "Epoch: 9, Train_Loss: 0.8046664595603943, Test_Loss: 0.6856274008750916 *\n",
      "Epoch: 9, Train_Loss: 0.695874810218811, Test_Loss: 0.7075323462486267\n",
      "Epoch: 9, Train_Loss: 0.7041816711425781, Test_Loss: 0.6962960958480835 *\n",
      "Epoch: 9, Train_Loss: 0.6821162700653076, Test_Loss: 0.6906904578208923 *\n",
      "Epoch: 9, Train_Loss: 0.685158371925354, Test_Loss: 0.7841373682022095\n",
      "Epoch: 9, Train_Loss: 0.7457873225212097, Test_Loss: 0.7251327037811279 *\n",
      "Epoch: 9, Train_Loss: 0.9167385101318359, Test_Loss: 5.8990325927734375\n",
      "Epoch: 9, Train_Loss: 0.8319137692451477, Test_Loss: 1.2072314023971558 *\n",
      "Epoch: 9, Train_Loss: 1.174740195274353, Test_Loss: 0.7501580715179443 *\n",
      "Epoch: 9, Train_Loss: 1.0180875062942505, Test_Loss: 0.7438815236091614 *\n",
      "Epoch: 9, Train_Loss: 0.8527765274047852, Test_Loss: 0.6935475468635559 *\n",
      "Epoch: 9, Train_Loss: 0.7772597074508667, Test_Loss: 0.6924525499343872 *\n",
      "Epoch: 9, Train_Loss: 0.7167438864707947, Test_Loss: 0.6800880432128906 *\n",
      "Epoch: 9, Train_Loss: 0.6907587051391602, Test_Loss: 0.7279691100120544\n",
      "Epoch: 9, Train_Loss: 0.6762392520904541, Test_Loss: 0.7049928903579712 *\n",
      "Epoch: 9, Train_Loss: 0.7744187116622925, Test_Loss: 0.7164000272750854\n",
      "Epoch: 9, Train_Loss: 1.0438990592956543, Test_Loss: 0.7315152287483215\n",
      "Epoch: 9, Train_Loss: 1.0132867097854614, Test_Loss: 0.8547117710113525\n",
      "Epoch: 9, Train_Loss: 1.9362049102783203, Test_Loss: 0.8123123645782471 *\n",
      "Epoch: 9, Train_Loss: 1.540978193283081, Test_Loss: 0.7618445754051208 *\n",
      "Epoch: 9, Train_Loss: 1.1056034564971924, Test_Loss: 0.6763084530830383 *\n",
      "Epoch: 9, Train_Loss: 0.8937591314315796, Test_Loss: 0.7399000525474548\n",
      "Epoch: 9, Train_Loss: 0.6841821074485779, Test_Loss: 0.6843476891517639 *\n",
      "Epoch: 9, Train_Loss: 0.7526994943618774, Test_Loss: 0.7646745443344116\n",
      "Epoch: 9, Train_Loss: 1.3494640588760376, Test_Loss: 0.9133099913597107\n",
      "Epoch: 9, Train_Loss: 1.5828123092651367, Test_Loss: 0.6879972219467163 *\n",
      "Epoch: 9, Train_Loss: 0.7104093432426453, Test_Loss: 0.7039573192596436\n",
      "Epoch: 9, Train_Loss: 0.7515243887901306, Test_Loss: 0.7062309384346008\n",
      "Epoch: 9, Train_Loss: 0.7846965789794922, Test_Loss: 0.7255169749259949\n",
      "Epoch: 9, Train_Loss: 0.9268766045570374, Test_Loss: 0.7030452489852905 *\n",
      "Epoch: 9, Train_Loss: 0.8288638591766357, Test_Loss: 0.7083567380905151\n",
      "Epoch: 9, Train_Loss: 1.0242074728012085, Test_Loss: 0.7350340485572815\n",
      "Epoch: 9, Train_Loss: 0.8346453309059143, Test_Loss: 0.6985948085784912 *\n",
      "Epoch: 9, Train_Loss: 0.9423651695251465, Test_Loss: 0.70870041847229\n",
      "Epoch: 9, Train_Loss: 0.6950969696044922, Test_Loss: 0.7235891222953796\n",
      "Epoch: 9, Train_Loss: 0.6935252547264099, Test_Loss: 0.7619000673294067\n",
      "Epoch: 9, Train_Loss: 0.6897308826446533, Test_Loss: 0.6803300976753235 *\n",
      "Epoch: 9, Train_Loss: 0.7598658204078674, Test_Loss: 0.716209888458252\n",
      "Epoch: 9, Train_Loss: 0.6896919012069702, Test_Loss: 0.7858952879905701\n",
      "Epoch: 9, Train_Loss: 0.7039216756820679, Test_Loss: 0.8561906218528748\n",
      "Epoch: 9, Train_Loss: 16.963031768798828, Test_Loss: 0.7773458361625671 *\n",
      "Epoch: 9, Train_Loss: 0.7250765562057495, Test_Loss: 0.7049465775489807 *\n",
      "Epoch: 9, Train_Loss: 2.518488645553589, Test_Loss: 0.6736331582069397 *\n",
      "Epoch: 9, Train_Loss: 2.141667604446411, Test_Loss: 0.6910907626152039\n",
      "Epoch: 9, Train_Loss: 0.6987164616584778, Test_Loss: 0.7528192400932312\n",
      "Epoch: 9, Train_Loss: 0.8072274923324585, Test_Loss: 0.9076697826385498\n",
      "Epoch: 9, Train_Loss: 4.33956241607666, Test_Loss: 0.9626730680465698\n",
      "Epoch: 9, Train_Loss: 6.96641731262207, Test_Loss: 0.9842296838760376\n",
      "Epoch: 9, Train_Loss: 0.78545743227005, Test_Loss: 0.6998106837272644 *\n",
      "Epoch: 9, Train_Loss: 0.7497708201408386, Test_Loss: 0.7049247622489929\n",
      "Epoch: 9, Train_Loss: 6.4059224128723145, Test_Loss: 0.8317649364471436\n",
      "Epoch: 9, Train_Loss: 0.7728310823440552, Test_Loss: 0.8445485234260559\n",
      "Epoch: 9, Train_Loss: 0.7315554618835449, Test_Loss: 1.0058529376983643\n",
      "Epoch: 9, Train_Loss: 0.6960269212722778, Test_Loss: 0.7497052550315857 *\n",
      "Epoch: 9, Train_Loss: 0.6912513375282288, Test_Loss: 0.8558587431907654\n",
      "Epoch: 9, Train_Loss: 0.7378518581390381, Test_Loss: 0.7857402563095093 *\n",
      "Epoch: 9, Train_Loss: 0.7040600776672363, Test_Loss: 0.7852908372879028 *\n",
      "Epoch: 9, Train_Loss: 0.6760774850845337, Test_Loss: 1.1376354694366455\n",
      "Epoch: 9, Train_Loss: 0.663493812084198, Test_Loss: 0.7830499410629272 *\n",
      "Epoch: 9, Train_Loss: 0.6623810529708862, Test_Loss: 0.8317476511001587\n",
      "Epoch: 9, Train_Loss: 0.6806348562240601, Test_Loss: 0.9537990093231201\n",
      "Epoch: 9, Train_Loss: 0.710392415523529, Test_Loss: 1.009430170059204\n",
      "Epoch: 9, Train_Loss: 0.7645570635795593, Test_Loss: 0.999933123588562 *\n",
      "Epoch: 9, Train_Loss: 0.8005032539367676, Test_Loss: 0.9592516422271729 *\n",
      "Epoch: 9, Train_Loss: 0.7833166122436523, Test_Loss: 0.690148115158081 *\n",
      "Epoch: 9, Train_Loss: 0.6701579093933105, Test_Loss: 6.168166160583496\n",
      "Epoch: 9, Train_Loss: 0.703953742980957, Test_Loss: 2.640815496444702 *\n",
      "Epoch: 9, Train_Loss: 0.6682453155517578, Test_Loss: 0.9642372131347656 *\n",
      "Epoch: 9, Train_Loss: 0.6650406122207642, Test_Loss: 0.9610508680343628 *\n",
      "Epoch: 9, Train_Loss: 0.6605489253997803, Test_Loss: 0.9566468000411987 *\n",
      "Epoch: 9, Train_Loss: 0.658261775970459, Test_Loss: 0.7321657538414001 *\n",
      "Epoch: 9, Train_Loss: 0.6576945185661316, Test_Loss: 1.1122738122940063\n",
      "Epoch: 9, Train_Loss: 0.6592772603034973, Test_Loss: 1.1528092622756958\n",
      "Epoch: 9, Train_Loss: 0.6585597991943359, Test_Loss: 0.9602538347244263 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 9\n",
      "Epoch: 9, Train_Loss: 0.6564345359802246, Test_Loss: 0.9673371315002441\n",
      "Epoch: 9, Train_Loss: 0.6560245752334595, Test_Loss: 1.010787844657898\n",
      "Epoch: 9, Train_Loss: 0.6648867130279541, Test_Loss: 1.0072057247161865 *\n",
      "Epoch: 9, Train_Loss: 0.6761910915374756, Test_Loss: 1.0576341152191162\n",
      "Epoch: 9, Train_Loss: 0.7011269330978394, Test_Loss: 0.8702511191368103 *\n",
      "Epoch: 9, Train_Loss: 0.7057225704193115, Test_Loss: 0.9933905601501465\n",
      "Epoch: 9, Train_Loss: 0.7094275951385498, Test_Loss: 0.9107813835144043 *\n",
      "Epoch: 9, Train_Loss: 2.3620495796203613, Test_Loss: 0.7038838267326355 *\n",
      "Epoch: 9, Train_Loss: 6.795126438140869, Test_Loss: 0.7969516515731812\n",
      "Epoch: 9, Train_Loss: 0.6870757937431335, Test_Loss: 0.7815592288970947 *\n",
      "Epoch: 9, Train_Loss: 0.8035979270935059, Test_Loss: 0.8737274408340454\n",
      "Epoch: 9, Train_Loss: 0.8637952208518982, Test_Loss: 0.808753490447998 *\n",
      "Epoch: 9, Train_Loss: 0.7892698049545288, Test_Loss: 0.8560401797294617\n",
      "Epoch: 9, Train_Loss: 0.6994847655296326, Test_Loss: 0.9316455125808716\n",
      "Epoch: 9, Train_Loss: 0.8240930438041687, Test_Loss: 0.9227930307388306 *\n",
      "Epoch: 9, Train_Loss: 0.8967847228050232, Test_Loss: 0.8675990104675293 *\n",
      "Epoch: 9, Train_Loss: 0.8942317962646484, Test_Loss: 0.7526978254318237 *\n",
      "Epoch: 9, Train_Loss: 0.7430412769317627, Test_Loss: 0.7236557602882385 *\n",
      "Epoch: 9, Train_Loss: 0.6982905864715576, Test_Loss: 0.7806600332260132\n",
      "Epoch: 9, Train_Loss: 0.6773176789283752, Test_Loss: 0.740337610244751 *\n",
      "Epoch: 9, Train_Loss: 0.8284035921096802, Test_Loss: 0.7092706561088562 *\n",
      "Epoch: 9, Train_Loss: 0.7763873338699341, Test_Loss: 0.818651020526886\n",
      "Epoch: 9, Train_Loss: 0.8124343752861023, Test_Loss: 0.7521330118179321 *\n",
      "Epoch: 9, Train_Loss: 0.7392489910125732, Test_Loss: 6.485305309295654\n",
      "Epoch: 9, Train_Loss: 0.6955254077911377, Test_Loss: 0.7253671884536743 *\n",
      "Epoch: 9, Train_Loss: 0.6550384759902954, Test_Loss: 0.6554744839668274 *\n",
      "Epoch: 9, Train_Loss: 0.6884918212890625, Test_Loss: 0.667081892490387\n",
      "Epoch: 9, Train_Loss: 0.6924143433570862, Test_Loss: 0.6524977087974548 *\n",
      "Epoch: 9, Train_Loss: 0.6723679900169373, Test_Loss: 0.6657488346099854\n",
      "Epoch: 9, Train_Loss: 0.6535994410514832, Test_Loss: 0.6625721454620361 *\n",
      "Epoch: 9, Train_Loss: 0.6586156487464905, Test_Loss: 0.8091511130332947\n",
      "Epoch: 9, Train_Loss: 0.6674617528915405, Test_Loss: 0.726370632648468 *\n",
      "Epoch: 9, Train_Loss: 2.432363986968994, Test_Loss: 0.6512024402618408 *\n",
      "Epoch: 9, Train_Loss: 4.524238109588623, Test_Loss: 0.7043680548667908\n",
      "Epoch: 9, Train_Loss: 0.6504687070846558, Test_Loss: 0.6559109091758728 *\n",
      "Epoch: 9, Train_Loss: 0.6687142252922058, Test_Loss: 0.6644409894943237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_Loss: 0.6717960834503174, Test_Loss: 0.673846960067749\n",
      "Epoch: 9, Train_Loss: 0.6573594808578491, Test_Loss: 0.6628898978233337 *\n",
      "Epoch: 9, Train_Loss: 0.6576386094093323, Test_Loss: 0.7154296636581421\n",
      "Epoch: 9, Train_Loss: 0.6568412780761719, Test_Loss: 0.796291172504425\n",
      "Epoch: 9, Train_Loss: 0.6614943146705627, Test_Loss: 0.7105674147605896 *\n",
      "Epoch: 9, Train_Loss: 0.661001443862915, Test_Loss: 0.6617733240127563 *\n",
      "Epoch: 9, Train_Loss: 0.6808260679244995, Test_Loss: 0.648751437664032 *\n",
      "Epoch: 9, Train_Loss: 0.6580588221549988, Test_Loss: 0.6524215340614319\n",
      "Epoch: 9, Train_Loss: 0.6530464887619019, Test_Loss: 0.6515806317329407 *\n",
      "Epoch: 9, Train_Loss: 0.6547791957855225, Test_Loss: 0.651221513748169 *\n",
      "Epoch: 9, Train_Loss: 0.6667630076408386, Test_Loss: 0.6485277414321899 *\n",
      "Epoch: 9, Train_Loss: 0.6523330807685852, Test_Loss: 0.6493669152259827\n",
      "Epoch: 9, Train_Loss: 0.6507603526115417, Test_Loss: 0.6538883447647095\n",
      "Epoch: 9, Train_Loss: 0.6829119324684143, Test_Loss: 0.6475399732589722 *\n",
      "Epoch: 9, Train_Loss: 0.6855512857437134, Test_Loss: 0.6466227173805237 *\n",
      "Epoch: 9, Train_Loss: 0.6521324515342712, Test_Loss: 0.6583055853843689\n",
      "Epoch: 9, Train_Loss: 0.648841142654419, Test_Loss: 0.6540610194206238 *\n",
      "Epoch: 9, Train_Loss: 0.6674427390098572, Test_Loss: 0.6685290336608887\n",
      "Epoch: 10, Train_Loss: 0.7173364162445068, Test_Loss: 0.6717447638511658 *\n",
      "Epoch: 10, Train_Loss: 0.6751965880393982, Test_Loss: 0.8160102367401123\n",
      "Epoch: 10, Train_Loss: 0.7063342928886414, Test_Loss: 0.9760814309120178\n",
      "Epoch: 10, Train_Loss: 0.6844018697738647, Test_Loss: 0.7803812623023987 *\n",
      "Epoch: 10, Train_Loss: 0.7540650367736816, Test_Loss: 0.6831859946250916 *\n",
      "Epoch: 10, Train_Loss: 0.6852555274963379, Test_Loss: 0.6501134634017944 *\n",
      "Epoch: 10, Train_Loss: 0.7123114466667175, Test_Loss: 0.6576398611068726\n",
      "Epoch: 10, Train_Loss: 0.7151017189025879, Test_Loss: 0.7386283874511719\n",
      "Epoch: 10, Train_Loss: 0.7806928753852844, Test_Loss: 1.270766019821167\n",
      "Epoch: 10, Train_Loss: 0.6880393624305725, Test_Loss: 1.4453141689300537\n",
      "Epoch: 10, Train_Loss: 0.6566182971000671, Test_Loss: 0.75696861743927 *\n",
      "Epoch: 10, Train_Loss: 0.6486847996711731, Test_Loss: 0.6944351196289062 *\n",
      "Epoch: 10, Train_Loss: 0.6436734795570374, Test_Loss: 0.6483263969421387 *\n",
      "Epoch: 10, Train_Loss: 0.6408524513244629, Test_Loss: 0.6655900478363037\n",
      "Epoch: 10, Train_Loss: 0.6508815288543701, Test_Loss: 0.6585693359375 *\n",
      "Epoch: 10, Train_Loss: 2.824791193008423, Test_Loss: 0.6719886660575867\n",
      "Epoch: 10, Train_Loss: 3.4361212253570557, Test_Loss: 0.6760085225105286\n",
      "Epoch: 10, Train_Loss: 0.6416136026382446, Test_Loss: 0.6758068203926086 *\n",
      "Epoch: 10, Train_Loss: 0.6504770517349243, Test_Loss: 0.6498140096664429 *\n",
      "Epoch: 10, Train_Loss: 0.6462719440460205, Test_Loss: 0.7378203272819519\n",
      "Epoch: 10, Train_Loss: 0.6404508948326111, Test_Loss: 1.0186938047409058\n",
      "Epoch: 10, Train_Loss: 0.6397630572319031, Test_Loss: 0.7452078461647034 *\n",
      "Epoch: 10, Train_Loss: 0.6403498649597168, Test_Loss: 0.8017237186431885\n",
      "Epoch: 10, Train_Loss: 0.6385295987129211, Test_Loss: 0.646095871925354 *\n",
      "Epoch: 10, Train_Loss: 0.6393258571624756, Test_Loss: 0.6456305980682373 *\n",
      "Epoch: 10, Train_Loss: 0.6491650938987732, Test_Loss: 0.6455801129341125 *\n",
      "Epoch: 10, Train_Loss: 0.6841054558753967, Test_Loss: 0.6481302976608276\n",
      "Epoch: 10, Train_Loss: 0.6737406253814697, Test_Loss: 0.6753711700439453\n",
      "Epoch: 10, Train_Loss: 0.7113768458366394, Test_Loss: 5.7867655754089355\n",
      "Epoch: 10, Train_Loss: 0.6661335825920105, Test_Loss: 1.0214617252349854 *\n",
      "Epoch: 10, Train_Loss: 0.6399738192558289, Test_Loss: 0.6474156379699707 *\n",
      "Epoch: 10, Train_Loss: 0.8137916326522827, Test_Loss: 0.6397426128387451 *\n",
      "Epoch: 10, Train_Loss: 0.8302699327468872, Test_Loss: 0.6393201351165771 *\n",
      "Epoch: 10, Train_Loss: 0.8122699856758118, Test_Loss: 0.6447449326515198\n",
      "Epoch: 10, Train_Loss: 0.7139644622802734, Test_Loss: 0.6454477310180664\n",
      "Epoch: 10, Train_Loss: 0.6378712058067322, Test_Loss: 0.6683283448219299\n",
      "Epoch: 10, Train_Loss: 0.6376008987426758, Test_Loss: 0.6439971923828125 *\n",
      "Epoch: 10, Train_Loss: 0.6406453251838684, Test_Loss: 0.6483484506607056\n",
      "Epoch: 10, Train_Loss: 0.6488035917282104, Test_Loss: 0.656493604183197\n",
      "Epoch: 10, Train_Loss: 0.6572616100311279, Test_Loss: 0.6775502562522888\n",
      "Epoch: 10, Train_Loss: 0.6453791856765747, Test_Loss: 0.6420426368713379 *\n",
      "Epoch: 10, Train_Loss: 0.6352543830871582, Test_Loss: 0.6383737921714783 *\n",
      "Epoch: 10, Train_Loss: 0.6347998976707458, Test_Loss: 0.648560106754303\n",
      "Epoch: 10, Train_Loss: 0.652446985244751, Test_Loss: 0.6430832743644714 *\n",
      "Epoch: 10, Train_Loss: 0.7132166028022766, Test_Loss: 0.6362862586975098 *\n",
      "Epoch: 10, Train_Loss: 0.8384529948234558, Test_Loss: 0.6455795764923096\n",
      "Epoch: 10, Train_Loss: 0.8054057955741882, Test_Loss: 0.6499370336532593\n",
      "Epoch: 10, Train_Loss: 0.7427142262458801, Test_Loss: 0.6433924436569214 *\n",
      "Epoch: 10, Train_Loss: 0.7355248332023621, Test_Loss: 0.6396245360374451 *\n",
      "Epoch: 10, Train_Loss: 0.7804709672927856, Test_Loss: 0.6439005136489868\n",
      "Epoch: 10, Train_Loss: 0.7029547691345215, Test_Loss: 0.6572160720825195\n",
      "Epoch: 10, Train_Loss: 0.8048626780509949, Test_Loss: 0.6575782895088196\n",
      "Epoch: 10, Train_Loss: 0.7763684391975403, Test_Loss: 0.6502644419670105 *\n",
      "Epoch: 10, Train_Loss: 0.9266442060470581, Test_Loss: 0.6403234601020813 *\n",
      "Epoch: 10, Train_Loss: 0.6421278119087219, Test_Loss: 0.6430563926696777\n",
      "Epoch: 10, Train_Loss: 0.7271900773048401, Test_Loss: 0.6436047554016113\n",
      "Epoch: 10, Train_Loss: 3.5945143699645996, Test_Loss: 0.6405918002128601 *\n",
      "Epoch: 10, Train_Loss: 0.7851415276527405, Test_Loss: 0.6466193199157715\n",
      "Epoch: 10, Train_Loss: 0.6809225678443909, Test_Loss: 0.7009427547454834\n",
      "Epoch: 10, Train_Loss: 0.6703171730041504, Test_Loss: 1.4614415168762207\n",
      "Epoch: 10, Train_Loss: 0.6595169901847839, Test_Loss: 5.359698295593262\n",
      "Epoch: 10, Train_Loss: 0.6377912759780884, Test_Loss: 0.638290524482727 *\n",
      "Epoch: 10, Train_Loss: 0.6372226476669312, Test_Loss: 0.6315522789955139 *\n",
      "Epoch: 10, Train_Loss: 0.7140705585479736, Test_Loss: 0.6695814728736877\n",
      "Epoch: 10, Train_Loss: 0.7664826512336731, Test_Loss: 0.6548162698745728 *\n",
      "Epoch: 10, Train_Loss: 0.7304129600524902, Test_Loss: 0.6740644574165344\n",
      "Epoch: 10, Train_Loss: 0.701009213924408, Test_Loss: 0.6444588899612427 *\n",
      "Epoch: 10, Train_Loss: 0.6846728324890137, Test_Loss: 0.7651636600494385\n",
      "Epoch: 10, Train_Loss: 0.66358482837677, Test_Loss: 0.6547392010688782 *\n",
      "Epoch: 10, Train_Loss: 0.6601523160934448, Test_Loss: 0.6339936256408691 *\n",
      "Epoch: 10, Train_Loss: 0.6356627345085144, Test_Loss: 0.6613402366638184\n",
      "Epoch: 10, Train_Loss: 0.664456844329834, Test_Loss: 0.6514012813568115 *\n",
      "Epoch: 10, Train_Loss: 0.645723283290863, Test_Loss: 0.634188175201416 *\n",
      "Epoch: 10, Train_Loss: 0.6294935941696167, Test_Loss: 0.6828057765960693\n",
      "Epoch: 10, Train_Loss: 0.6465944647789001, Test_Loss: 0.6926234364509583\n",
      "Epoch: 10, Train_Loss: 0.6782134771347046, Test_Loss: 0.6869338154792786 *\n",
      "Epoch: 10, Train_Loss: 0.6665141582489014, Test_Loss: 0.7112434506416321\n",
      "Epoch: 10, Train_Loss: 0.6291220188140869, Test_Loss: 0.6566116809844971 *\n",
      "Epoch: 10, Train_Loss: 0.6273382306098938, Test_Loss: 0.6722220778465271\n",
      "Epoch: 10, Train_Loss: 0.6266905665397644, Test_Loss: 0.6352471113204956 *\n",
      "Epoch: 10, Train_Loss: 0.6267644762992859, Test_Loss: 0.6318186521530151 *\n",
      "Epoch: 10, Train_Loss: 0.6291481256484985, Test_Loss: 0.636054515838623\n",
      "Epoch: 10, Train_Loss: 0.6277364492416382, Test_Loss: 0.6382284760475159\n",
      "Epoch: 10, Train_Loss: 0.6300622820854187, Test_Loss: 0.6389389038085938\n",
      "Epoch: 10, Train_Loss: 0.6266597509384155, Test_Loss: 0.6357401609420776 *\n",
      "Epoch: 10, Train_Loss: 0.6259783506393433, Test_Loss: 0.634925901889801 *\n",
      "Epoch: 10, Train_Loss: 0.6294151544570923, Test_Loss: 0.6330643892288208 *\n",
      "Epoch: 10, Train_Loss: 0.6345018744468689, Test_Loss: 0.6381614804267883\n",
      "Epoch: 10, Train_Loss: 0.6379234194755554, Test_Loss: 0.6308422684669495 *\n",
      "Epoch: 10, Train_Loss: 0.6438860893249512, Test_Loss: 0.6358317136764526\n",
      "Epoch: 10, Train_Loss: 0.6429702043533325, Test_Loss: 0.6922051906585693\n",
      "Epoch: 10, Train_Loss: 0.6331003904342651, Test_Loss: 0.638238251209259 *\n",
      "Epoch: 10, Train_Loss: 0.6404404044151306, Test_Loss: 0.997062087059021\n",
      "Epoch: 10, Train_Loss: 0.6254528164863586, Test_Loss: 1.0957342386245728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_Loss: 0.6248800754547119, Test_Loss: 0.7884435653686523 *\n",
      "Epoch: 10, Train_Loss: 0.6443206667900085, Test_Loss: 0.653296947479248 *\n",
      "Epoch: 10, Train_Loss: 0.6478879451751709, Test_Loss: 0.6466406583786011 *\n",
      "Epoch: 10, Train_Loss: 0.6251354813575745, Test_Loss: 0.6374451518058777 *\n",
      "Epoch: 10, Train_Loss: 0.6269963383674622, Test_Loss: 0.720871090888977\n",
      "Epoch: 10, Train_Loss: 0.6259182095527649, Test_Loss: 1.35555100440979\n",
      "Model saved at location save_new\\model.ckpt at epoch 10\n",
      "Epoch: 10, Train_Loss: 0.6666560769081116, Test_Loss: 1.3327670097351074 *\n",
      "Epoch: 10, Train_Loss: 0.6589740514755249, Test_Loss: 0.6761853694915771 *\n",
      "Epoch: 10, Train_Loss: 0.6732456684112549, Test_Loss: 0.6961508393287659\n",
      "Epoch: 10, Train_Loss: 0.6311387419700623, Test_Loss: 0.6243019700050354 *\n",
      "Epoch: 10, Train_Loss: 0.6326018571853638, Test_Loss: 0.6285573840141296\n",
      "Epoch: 10, Train_Loss: 0.7004024982452393, Test_Loss: 0.6304993629455566\n",
      "Epoch: 10, Train_Loss: 0.6247618794441223, Test_Loss: 0.6367425322532654\n",
      "Epoch: 10, Train_Loss: 0.632559061050415, Test_Loss: 0.6684032678604126\n",
      "Epoch: 10, Train_Loss: 0.6487846970558167, Test_Loss: 0.6364027857780457 *\n",
      "Epoch: 10, Train_Loss: 0.6628919243812561, Test_Loss: 0.6307659149169922 *\n",
      "Epoch: 10, Train_Loss: 0.7393653392791748, Test_Loss: 0.7330448031425476\n",
      "Epoch: 10, Train_Loss: 0.6856781244277954, Test_Loss: 1.0201388597488403\n",
      "Epoch: 10, Train_Loss: 0.6497407555580139, Test_Loss: 0.8436781764030457 *\n",
      "Epoch: 10, Train_Loss: 0.6266888380050659, Test_Loss: 0.6633344888687134 *\n",
      "Epoch: 10, Train_Loss: 0.645655632019043, Test_Loss: 0.6293610334396362 *\n",
      "Epoch: 10, Train_Loss: 0.6212384700775146, Test_Loss: 0.6285682320594788 *\n",
      "Epoch: 10, Train_Loss: 0.6251627206802368, Test_Loss: 0.6285429000854492 *\n",
      "Epoch: 10, Train_Loss: 0.6273025274276733, Test_Loss: 0.6307982206344604\n",
      "Epoch: 10, Train_Loss: 0.6310188174247742, Test_Loss: 0.7982114553451538\n",
      "Epoch: 10, Train_Loss: 0.676207959651947, Test_Loss: 5.882771968841553\n",
      "Epoch: 10, Train_Loss: 0.6788985133171082, Test_Loss: 0.7174827456474304 *\n",
      "Epoch: 10, Train_Loss: 0.6604471206665039, Test_Loss: 0.6269073486328125 *\n",
      "Epoch: 10, Train_Loss: 0.6462056636810303, Test_Loss: 0.6203619837760925 *\n",
      "Epoch: 10, Train_Loss: 0.6415534615516663, Test_Loss: 0.6230010986328125\n",
      "Epoch: 10, Train_Loss: 0.6314737796783447, Test_Loss: 0.6237465143203735\n",
      "Epoch: 10, Train_Loss: 0.7767267823219299, Test_Loss: 0.6218425631523132 *\n",
      "Epoch: 10, Train_Loss: 0.7954482436180115, Test_Loss: 0.6388830542564392\n",
      "Epoch: 10, Train_Loss: 0.6209990382194519, Test_Loss: 0.6205191612243652 *\n",
      "Epoch: 10, Train_Loss: 0.6575106978416443, Test_Loss: 0.6227596402168274\n",
      "Epoch: 10, Train_Loss: 0.6170737743377686, Test_Loss: 0.629325807094574\n",
      "Epoch: 10, Train_Loss: 0.6166113018989563, Test_Loss: 0.6455618739128113\n",
      "Epoch: 10, Train_Loss: 0.616804838180542, Test_Loss: 0.6224741339683533 *\n",
      "Epoch: 10, Train_Loss: 0.6195529103279114, Test_Loss: 0.6209524273872375 *\n",
      "Epoch: 10, Train_Loss: 0.6248230338096619, Test_Loss: 0.6314087510108948\n",
      "Epoch: 10, Train_Loss: 0.633711576461792, Test_Loss: 0.6168897747993469 *\n",
      "Epoch: 10, Train_Loss: 0.6249069571495056, Test_Loss: 0.6173082590103149\n",
      "Epoch: 10, Train_Loss: 0.6272711157798767, Test_Loss: 0.6175574660301208\n",
      "Epoch: 10, Train_Loss: 0.6309590339660645, Test_Loss: 0.6263777017593384\n",
      "Epoch: 10, Train_Loss: 0.6148573756217957, Test_Loss: 0.6169077754020691 *\n",
      "Epoch: 10, Train_Loss: 0.6158728003501892, Test_Loss: 0.6193221807479858\n",
      "Epoch: 10, Train_Loss: 0.6146678924560547, Test_Loss: 0.6169750690460205 *\n",
      "Epoch: 10, Train_Loss: 0.6396600008010864, Test_Loss: 0.6259593963623047\n",
      "Epoch: 10, Train_Loss: 0.6424885988235474, Test_Loss: 0.6262592077255249\n",
      "Epoch: 10, Train_Loss: 0.6295611262321472, Test_Loss: 0.6204391121864319 *\n",
      "Epoch: 10, Train_Loss: 0.6322216987609863, Test_Loss: 0.6161227226257324 *\n",
      "Epoch: 10, Train_Loss: 0.6941104531288147, Test_Loss: 0.6197294592857361\n",
      "Epoch: 10, Train_Loss: 0.6587624549865723, Test_Loss: 0.6170482635498047 *\n",
      "Epoch: 10, Train_Loss: 0.624008059501648, Test_Loss: 0.61705082654953\n",
      "Epoch: 10, Train_Loss: 0.633979082107544, Test_Loss: 0.6270437240600586\n",
      "Epoch: 10, Train_Loss: 0.6285024881362915, Test_Loss: 0.676064133644104\n",
      "Epoch: 10, Train_Loss: 0.6240082383155823, Test_Loss: 2.6733596324920654\n",
      "Epoch: 10, Train_Loss: 0.620179295539856, Test_Loss: 4.090269088745117\n",
      "Epoch: 10, Train_Loss: 0.6389280557632446, Test_Loss: 0.6169922351837158 *\n",
      "Epoch: 10, Train_Loss: 0.6615061163902283, Test_Loss: 0.6113041043281555 *\n",
      "Epoch: 10, Train_Loss: 2.8648955821990967, Test_Loss: 0.6582338213920593\n",
      "Epoch: 10, Train_Loss: 3.7060933113098145, Test_Loss: 0.6341486573219299 *\n",
      "Epoch: 10, Train_Loss: 0.6346623301506042, Test_Loss: 0.6533492207527161\n",
      "Epoch: 10, Train_Loss: 0.6158547401428223, Test_Loss: 0.6530544757843018 *\n",
      "Epoch: 10, Train_Loss: 0.6822385787963867, Test_Loss: 0.7454404830932617\n",
      "Epoch: 10, Train_Loss: 0.7907955050468445, Test_Loss: 0.6162508726119995 *\n",
      "Epoch: 10, Train_Loss: 0.6345998048782349, Test_Loss: 0.6264969110488892\n",
      "Epoch: 10, Train_Loss: 0.6133266687393188, Test_Loss: 0.6349999308586121\n",
      "Epoch: 10, Train_Loss: 0.6220299601554871, Test_Loss: 0.6293211579322815 *\n",
      "Epoch: 10, Train_Loss: 0.6670536994934082, Test_Loss: 0.6175872087478638 *\n",
      "Epoch: 10, Train_Loss: 0.6188690662384033, Test_Loss: 0.6706477403640747\n",
      "Epoch: 10, Train_Loss: 0.6199182868003845, Test_Loss: 0.6659262776374817 *\n",
      "Epoch: 10, Train_Loss: 1.4874863624572754, Test_Loss: 0.6784217953681946\n",
      "Epoch: 10, Train_Loss: 1.8993589878082275, Test_Loss: 0.6842144131660461\n",
      "Epoch: 10, Train_Loss: 1.1234872341156006, Test_Loss: 0.6304417848587036 *\n",
      "Epoch: 10, Train_Loss: 0.6969638466835022, Test_Loss: 0.6442595720291138\n",
      "Epoch: 10, Train_Loss: 1.6145999431610107, Test_Loss: 0.6160792708396912 *\n",
      "Epoch: 10, Train_Loss: 2.712179660797119, Test_Loss: 0.6139793992042542 *\n",
      "Epoch: 10, Train_Loss: 0.8210868835449219, Test_Loss: 0.6146745085716248\n",
      "Epoch: 10, Train_Loss: 0.6322938203811646, Test_Loss: 0.6237789988517761\n",
      "Epoch: 10, Train_Loss: 0.6456795334815979, Test_Loss: 0.6192138195037842 *\n",
      "Epoch: 10, Train_Loss: 1.7966265678405762, Test_Loss: 0.6151910424232483 *\n",
      "Epoch: 10, Train_Loss: 1.8752448558807373, Test_Loss: 0.6148228645324707 *\n",
      "Epoch: 10, Train_Loss: 0.6557368636131287, Test_Loss: 0.6094330549240112 *\n",
      "Epoch: 10, Train_Loss: 0.6146143674850464, Test_Loss: 0.6091904640197754 *\n",
      "Epoch: 10, Train_Loss: 0.608873188495636, Test_Loss: 0.6218664646148682\n",
      "Epoch: 10, Train_Loss: 1.3728816509246826, Test_Loss: 0.6097487807273865 *\n",
      "Epoch: 10, Train_Loss: 0.8054558038711548, Test_Loss: 0.6521194577217102\n",
      "Epoch: 10, Train_Loss: 0.720615565776825, Test_Loss: 0.6276269555091858 *\n",
      "Epoch: 10, Train_Loss: 0.6803597807884216, Test_Loss: 0.969544529914856\n",
      "Epoch: 10, Train_Loss: 0.780909538269043, Test_Loss: 0.9637172222137451 *\n",
      "Epoch: 10, Train_Loss: 0.7100772857666016, Test_Loss: 0.7104442715644836 *\n",
      "Epoch: 10, Train_Loss: 0.758209764957428, Test_Loss: 0.616827130317688 *\n",
      "Epoch: 10, Train_Loss: 0.8577204942703247, Test_Loss: 0.62716144323349\n",
      "Epoch: 10, Train_Loss: 0.6404941082000732, Test_Loss: 0.6328285932540894\n",
      "Epoch: 10, Train_Loss: 0.6960991621017456, Test_Loss: 0.7531670331954956\n",
      "Epoch: 10, Train_Loss: 0.7771117687225342, Test_Loss: 1.2905809879302979\n",
      "Epoch: 10, Train_Loss: 0.8893381357192993, Test_Loss: 1.08927321434021 *\n",
      "Epoch: 10, Train_Loss: 0.870724081993103, Test_Loss: 0.6574428081512451 *\n",
      "Epoch: 10, Train_Loss: 0.6513398289680481, Test_Loss: 0.6448982954025269 *\n",
      "Epoch: 10, Train_Loss: 0.6804280281066895, Test_Loss: 0.6154099702835083 *\n",
      "Epoch: 10, Train_Loss: 0.7520171403884888, Test_Loss: 0.6122428178787231 *\n",
      "Epoch: 10, Train_Loss: 0.6318104267120361, Test_Loss: 0.6278058290481567\n",
      "Epoch: 10, Train_Loss: 0.6116148829460144, Test_Loss: 0.6212528347969055 *\n",
      "Epoch: 10, Train_Loss: 0.6052226424217224, Test_Loss: 0.6520605683326721\n",
      "Epoch: 10, Train_Loss: 0.6126428246498108, Test_Loss: 0.6330857276916504 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 10\n",
      "Epoch: 10, Train_Loss: 0.6095395684242249, Test_Loss: 0.6409148573875427\n",
      "Epoch: 10, Train_Loss: 0.6110323667526245, Test_Loss: 0.732842743396759\n",
      "Epoch: 10, Train_Loss: 0.6465787887573242, Test_Loss: 0.9435478448867798\n",
      "Epoch: 10, Train_Loss: 0.6212274432182312, Test_Loss: 0.970584511756897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_Loss: 0.6502918004989624, Test_Loss: 0.6461933851242065 *\n",
      "Epoch: 10, Train_Loss: 0.7190588116645813, Test_Loss: 0.6101608872413635 *\n",
      "Epoch: 10, Train_Loss: 0.9576089382171631, Test_Loss: 0.6085068583488464 *\n",
      "Epoch: 10, Train_Loss: 0.605473518371582, Test_Loss: 0.6086090803146362\n",
      "Epoch: 10, Train_Loss: 0.6479450464248657, Test_Loss: 0.6541155576705933\n",
      "Epoch: 10, Train_Loss: 0.7226837873458862, Test_Loss: 1.310308575630188\n",
      "Epoch: 10, Train_Loss: 0.9594250321388245, Test_Loss: 5.223883628845215\n",
      "Epoch: 10, Train_Loss: 0.8253601789474487, Test_Loss: 0.6357870101928711 *\n",
      "Epoch: 10, Train_Loss: 0.6167531609535217, Test_Loss: 0.6180812120437622 *\n",
      "Epoch: 10, Train_Loss: 0.7256669998168945, Test_Loss: 0.611382782459259 *\n",
      "Epoch: 10, Train_Loss: 1.0352603197097778, Test_Loss: 0.6063366532325745 *\n",
      "Epoch: 10, Train_Loss: 0.9408209323883057, Test_Loss: 0.6161126494407654\n",
      "Epoch: 10, Train_Loss: 0.6224022507667542, Test_Loss: 0.6329126954078674\n",
      "Epoch: 10, Train_Loss: 0.6086604595184326, Test_Loss: 0.6553371548652649\n",
      "Epoch: 10, Train_Loss: 0.6164381504058838, Test_Loss: 0.6057409644126892 *\n",
      "Epoch: 10, Train_Loss: 1.3599116802215576, Test_Loss: 0.6402556300163269\n",
      "Epoch: 10, Train_Loss: 1.2065179347991943, Test_Loss: 0.639704704284668 *\n",
      "Epoch: 10, Train_Loss: 0.6189578771591187, Test_Loss: 0.6816022992134094\n",
      "Epoch: 10, Train_Loss: 0.623047411441803, Test_Loss: 0.6057313680648804 *\n",
      "Epoch: 10, Train_Loss: 0.6021853685379028, Test_Loss: 0.6106124520301819\n",
      "Epoch: 10, Train_Loss: 0.6577029824256897, Test_Loss: 0.6475505828857422\n",
      "Epoch: 10, Train_Loss: 0.9908767342567444, Test_Loss: 0.6054647564888 *\n",
      "Epoch: 10, Train_Loss: 0.6023881435394287, Test_Loss: 0.6134243011474609\n",
      "Epoch: 10, Train_Loss: 0.822192907333374, Test_Loss: 0.6301652789115906\n",
      "Epoch: 10, Train_Loss: 0.6546539068222046, Test_Loss: 0.6673178672790527\n",
      "Epoch: 10, Train_Loss: 0.622576117515564, Test_Loss: 0.6082432866096497 *\n",
      "Epoch: 10, Train_Loss: 0.623608410358429, Test_Loss: 0.6238899827003479\n",
      "Epoch: 10, Train_Loss: 0.7254528999328613, Test_Loss: 0.6244637370109558\n",
      "Epoch: 10, Train_Loss: 0.7018903493881226, Test_Loss: 0.6723310351371765\n",
      "Epoch: 10, Train_Loss: 0.646395742893219, Test_Loss: 0.6802105903625488\n",
      "Epoch: 10, Train_Loss: 0.6119307279586792, Test_Loss: 0.6394252181053162 *\n",
      "Epoch: 10, Train_Loss: 0.6337435841560364, Test_Loss: 0.6056738495826721 *\n",
      "Epoch: 10, Train_Loss: 0.6508911848068237, Test_Loss: 0.635772705078125\n",
      "Epoch: 10, Train_Loss: 0.6414542198181152, Test_Loss: 0.6318004727363586 *\n",
      "Epoch: 10, Train_Loss: 0.6243842840194702, Test_Loss: 0.6158325672149658 *\n",
      "Epoch: 10, Train_Loss: 0.6052483320236206, Test_Loss: 0.6899478435516357\n",
      "Epoch: 10, Train_Loss: 0.6574238538742065, Test_Loss: 0.6902182698249817\n",
      "Epoch: 10, Train_Loss: 0.9024990200996399, Test_Loss: 3.7877042293548584\n",
      "Epoch: 10, Train_Loss: 0.8564327955245972, Test_Loss: 2.757699966430664 *\n",
      "Epoch: 10, Train_Loss: 1.0524460077285767, Test_Loss: 0.6394860744476318 *\n",
      "Epoch: 10, Train_Loss: 0.9252505302429199, Test_Loss: 0.6265082955360413 *\n",
      "Epoch: 10, Train_Loss: 0.8061213493347168, Test_Loss: 0.6223416328430176 *\n",
      "Epoch: 10, Train_Loss: 0.7645422220230103, Test_Loss: 0.5981148481369019 *\n",
      "Epoch: 10, Train_Loss: 0.654236912727356, Test_Loss: 0.6188520789146423\n",
      "Epoch: 10, Train_Loss: 0.6130596399307251, Test_Loss: 0.649417519569397\n",
      "Epoch: 10, Train_Loss: 0.6032379269599915, Test_Loss: 0.6403979063034058 *\n",
      "Epoch: 10, Train_Loss: 0.6777561902999878, Test_Loss: 0.6124153137207031 *\n",
      "Epoch: 10, Train_Loss: 0.8729665875434875, Test_Loss: 0.6377466917037964\n",
      "Epoch: 10, Train_Loss: 1.00562584400177, Test_Loss: 0.6704238653182983\n",
      "Epoch: 10, Train_Loss: 1.3748747110366821, Test_Loss: 0.7115689516067505\n",
      "Epoch: 10, Train_Loss: 1.7376277446746826, Test_Loss: 0.6380821466445923 *\n",
      "Epoch: 10, Train_Loss: 0.9174209237098694, Test_Loss: 0.6309112906455994 *\n",
      "Epoch: 10, Train_Loss: 0.904322624206543, Test_Loss: 0.6470518708229065\n",
      "Epoch: 10, Train_Loss: 0.6128067374229431, Test_Loss: 0.630935788154602 *\n",
      "Epoch: 10, Train_Loss: 0.6223557591438293, Test_Loss: 0.6063324809074402 *\n",
      "Epoch: 10, Train_Loss: 1.0120254755020142, Test_Loss: 0.7349608540534973\n",
      "Epoch: 10, Train_Loss: 1.7420382499694824, Test_Loss: 0.6268180012702942 *\n",
      "Epoch: 10, Train_Loss: 0.6780877113342285, Test_Loss: 0.6170973777770996 *\n",
      "Epoch: 10, Train_Loss: 0.6375604867935181, Test_Loss: 0.6401547193527222\n",
      "Epoch: 10, Train_Loss: 0.6874202489852905, Test_Loss: 0.652114987373352\n",
      "Epoch: 10, Train_Loss: 0.730352520942688, Test_Loss: 0.628739595413208 *\n",
      "Epoch: 10, Train_Loss: 0.8737951517105103, Test_Loss: 0.6343737840652466\n",
      "Epoch: 10, Train_Loss: 0.8686438798904419, Test_Loss: 0.6618766784667969\n",
      "Epoch: 10, Train_Loss: 0.8486282825469971, Test_Loss: 0.6555401682853699 *\n",
      "Epoch: 10, Train_Loss: 0.9364874362945557, Test_Loss: 0.6494633555412292 *\n",
      "Epoch: 10, Train_Loss: 0.6199434399604797, Test_Loss: 0.6386697292327881 *\n",
      "Epoch: 10, Train_Loss: 0.627477765083313, Test_Loss: 0.6870822906494141\n",
      "Epoch: 10, Train_Loss: 0.6312552690505981, Test_Loss: 0.6250464916229248 *\n",
      "Epoch: 10, Train_Loss: 0.6959144473075867, Test_Loss: 0.6125119924545288 *\n",
      "Epoch: 10, Train_Loss: 0.5995409488677979, Test_Loss: 0.6772019863128662\n",
      "Epoch: 10, Train_Loss: 0.637977659702301, Test_Loss: 0.7417378425598145\n",
      "Epoch: 10, Train_Loss: 16.491662979125977, Test_Loss: 0.7296158075332642 *\n",
      "Epoch: 10, Train_Loss: 0.7719650268554688, Test_Loss: 0.6221933364868164 *\n",
      "Epoch: 10, Train_Loss: 1.9827520847320557, Test_Loss: 0.6059741973876953 *\n",
      "Epoch: 10, Train_Loss: 2.3374810218811035, Test_Loss: 0.632348895072937\n",
      "Epoch: 10, Train_Loss: 0.6309236288070679, Test_Loss: 0.643856406211853\n",
      "Epoch: 10, Train_Loss: 0.6431799530982971, Test_Loss: 0.7154694199562073\n",
      "Epoch: 10, Train_Loss: 2.3153326511383057, Test_Loss: 0.8577018976211548\n",
      "Epoch: 10, Train_Loss: 8.471774101257324, Test_Loss: 0.9432696104049683\n",
      "Epoch: 10, Train_Loss: 0.8129460215568542, Test_Loss: 0.6512340903282166 *\n",
      "Epoch: 10, Train_Loss: 0.6557397842407227, Test_Loss: 0.6879124045372009\n",
      "Epoch: 10, Train_Loss: 5.637585639953613, Test_Loss: 0.7860317230224609\n",
      "Epoch: 10, Train_Loss: 0.9648053646087646, Test_Loss: 0.8415383696556091\n",
      "Epoch: 10, Train_Loss: 0.6898112297058105, Test_Loss: 1.0434640645980835\n",
      "Epoch: 10, Train_Loss: 0.6259211301803589, Test_Loss: 0.6973681449890137 *\n",
      "Epoch: 10, Train_Loss: 0.6315595507621765, Test_Loss: 0.9008091688156128\n",
      "Epoch: 10, Train_Loss: 0.6731995344161987, Test_Loss: 0.6902059316635132 *\n",
      "Epoch: 10, Train_Loss: 0.6084850430488586, Test_Loss: 0.8758844137191772\n",
      "Epoch: 10, Train_Loss: 0.608535647392273, Test_Loss: 1.4094023704528809\n",
      "Epoch: 10, Train_Loss: 0.5834424495697021, Test_Loss: 0.8358591794967651 *\n",
      "Epoch: 10, Train_Loss: 0.5832113027572632, Test_Loss: 0.7523661851882935 *\n",
      "Epoch: 10, Train_Loss: 0.5940936207771301, Test_Loss: 0.9125697612762451\n",
      "Epoch: 10, Train_Loss: 0.6637948155403137, Test_Loss: 1.1275556087493896\n",
      "Epoch: 10, Train_Loss: 0.683000922203064, Test_Loss: 1.1190533638000488 *\n",
      "Epoch: 10, Train_Loss: 0.7963953018188477, Test_Loss: 1.0806670188903809 *\n",
      "Epoch: 10, Train_Loss: 0.7096145153045654, Test_Loss: 0.7088563442230225 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 10\n",
      "Epoch: 10, Train_Loss: 0.631855845451355, Test_Loss: 3.1244919300079346\n",
      "Epoch: 10, Train_Loss: 0.6325384378433228, Test_Loss: 5.132270336151123\n",
      "Epoch: 10, Train_Loss: 0.5927609205245972, Test_Loss: 0.938321590423584 *\n",
      "Epoch: 10, Train_Loss: 0.5911554098129272, Test_Loss: 0.9412814974784851\n",
      "Epoch: 10, Train_Loss: 0.5878004431724548, Test_Loss: 0.9571455717086792\n",
      "Epoch: 10, Train_Loss: 0.5860901474952698, Test_Loss: 0.6678183078765869 *\n",
      "Epoch: 10, Train_Loss: 0.5844243168830872, Test_Loss: 0.9214766621589661\n",
      "Epoch: 10, Train_Loss: 0.5864838361740112, Test_Loss: 1.0518214702606201\n",
      "Epoch: 10, Train_Loss: 0.5847691297531128, Test_Loss: 0.9436178207397461 *\n",
      "Epoch: 10, Train_Loss: 0.5842874646186829, Test_Loss: 0.7680047750473022 *\n",
      "Epoch: 10, Train_Loss: 0.5832245945930481, Test_Loss: 0.9274225831031799\n",
      "Epoch: 10, Train_Loss: 0.5869861245155334, Test_Loss: 0.8307070732116699 *\n",
      "Epoch: 10, Train_Loss: 0.6053258776664734, Test_Loss: 1.0361696481704712\n",
      "Epoch: 10, Train_Loss: 0.6009019017219543, Test_Loss: 0.7969120144844055 *\n",
      "Epoch: 10, Train_Loss: 0.6763636469841003, Test_Loss: 0.9518396854400635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_Loss: 0.5975415706634521, Test_Loss: 0.923495888710022 *\n",
      "Epoch: 10, Train_Loss: 0.7270722389221191, Test_Loss: 0.6182814240455627 *\n",
      "Epoch: 10, Train_Loss: 8.111637115478516, Test_Loss: 0.7040042877197266\n",
      "Epoch: 10, Train_Loss: 0.6224163174629211, Test_Loss: 0.6653004288673401 *\n",
      "Epoch: 10, Train_Loss: 0.6770727634429932, Test_Loss: 0.9214993715286255\n",
      "Epoch: 10, Train_Loss: 0.7685421109199524, Test_Loss: 0.7358918786048889 *\n",
      "Epoch: 10, Train_Loss: 0.7832237482070923, Test_Loss: 0.8390333652496338\n",
      "Epoch: 10, Train_Loss: 0.6708627343177795, Test_Loss: 0.836854100227356 *\n",
      "Epoch: 10, Train_Loss: 0.739936113357544, Test_Loss: 0.9856338500976562\n",
      "Epoch: 10, Train_Loss: 0.866276741027832, Test_Loss: 0.9268864393234253 *\n",
      "Epoch: 10, Train_Loss: 0.8172102570533752, Test_Loss: 0.7192151546478271 *\n",
      "Epoch: 10, Train_Loss: 0.6910927891731262, Test_Loss: 0.6322797536849976 *\n",
      "Epoch: 10, Train_Loss: 0.6364226937294006, Test_Loss: 0.7276294827461243\n",
      "Epoch: 10, Train_Loss: 0.5807225108146667, Test_Loss: 0.6672106385231018 *\n",
      "Epoch: 10, Train_Loss: 0.7359744906425476, Test_Loss: 0.6279664635658264 *\n",
      "Epoch: 10, Train_Loss: 0.6529513597488403, Test_Loss: 0.773512065410614\n",
      "Epoch: 10, Train_Loss: 0.7731092572212219, Test_Loss: 0.6153450012207031 *\n",
      "Epoch: 10, Train_Loss: 0.6621115207672119, Test_Loss: 4.918342590332031\n",
      "Epoch: 10, Train_Loss: 0.60875403881073, Test_Loss: 1.9335962533950806 *\n",
      "Epoch: 10, Train_Loss: 0.5904526710510254, Test_Loss: 0.5792185068130493 *\n",
      "Epoch: 10, Train_Loss: 0.6137595772743225, Test_Loss: 0.5877540111541748\n",
      "Epoch: 10, Train_Loss: 0.6531867980957031, Test_Loss: 0.588811457157135\n",
      "Epoch: 10, Train_Loss: 0.6024413704872131, Test_Loss: 0.5958467721939087\n",
      "Epoch: 10, Train_Loss: 0.5781660079956055, Test_Loss: 0.5868677496910095 *\n",
      "Epoch: 10, Train_Loss: 0.5787466764450073, Test_Loss: 0.6627721786499023\n",
      "Epoch: 10, Train_Loss: 0.5890129804611206, Test_Loss: 0.6716744303703308\n",
      "Epoch: 10, Train_Loss: 1.1126540899276733, Test_Loss: 0.577017605304718 *\n",
      "Epoch: 10, Train_Loss: 5.712913513183594, Test_Loss: 0.6207125186920166\n",
      "Epoch: 10, Train_Loss: 0.5781137347221375, Test_Loss: 0.589695394039154 *\n",
      "Epoch: 10, Train_Loss: 0.5831407904624939, Test_Loss: 0.5897865891456604\n",
      "Epoch: 10, Train_Loss: 0.5883636474609375, Test_Loss: 0.5801870822906494 *\n",
      "Epoch: 10, Train_Loss: 0.5835554003715515, Test_Loss: 0.60776686668396\n",
      "Epoch: 10, Train_Loss: 0.5785161852836609, Test_Loss: 0.6145505905151367\n",
      "Epoch: 10, Train_Loss: 0.5771641135215759, Test_Loss: 0.7117626667022705\n",
      "Epoch: 10, Train_Loss: 0.583706259727478, Test_Loss: 0.6700860261917114 *\n",
      "Epoch: 10, Train_Loss: 0.5964832901954651, Test_Loss: 0.5966161489486694 *\n",
      "Epoch: 10, Train_Loss: 0.591485857963562, Test_Loss: 0.5755279064178467 *\n",
      "Epoch: 10, Train_Loss: 0.5890641808509827, Test_Loss: 0.5751695036888123 *\n",
      "Epoch: 10, Train_Loss: 0.5764943957328796, Test_Loss: 0.5744504928588867 *\n",
      "Epoch: 10, Train_Loss: 0.5751336216926575, Test_Loss: 0.5745632648468018\n",
      "Epoch: 10, Train_Loss: 0.5938910841941833, Test_Loss: 0.5751433372497559\n",
      "Epoch: 10, Train_Loss: 0.5774809718132019, Test_Loss: 0.5752330422401428\n",
      "Epoch: 10, Train_Loss: 0.5771329402923584, Test_Loss: 0.5736880302429199 *\n",
      "Epoch: 10, Train_Loss: 0.6053937077522278, Test_Loss: 0.5769630670547485\n",
      "Epoch: 10, Train_Loss: 0.6149089932441711, Test_Loss: 0.5747880935668945 *\n",
      "Epoch: 10, Train_Loss: 0.5879236459732056, Test_Loss: 0.5780733823776245\n",
      "Epoch: 10, Train_Loss: 0.5729048252105713, Test_Loss: 0.5883201360702515\n",
      "Epoch: 10, Train_Loss: 0.5775630474090576, Test_Loss: 0.5872331261634827 *\n",
      "Epoch: 10, Train_Loss: 0.6257811784744263, Test_Loss: 0.59173184633255\n",
      "Epoch: 10, Train_Loss: 0.5936688184738159, Test_Loss: 0.6890124678611755\n",
      "Epoch: 10, Train_Loss: 0.6372041702270508, Test_Loss: 0.9117326140403748\n",
      "Epoch: 10, Train_Loss: 0.5872159600257874, Test_Loss: 0.8109884262084961 *\n",
      "Epoch: 10, Train_Loss: 0.6813055276870728, Test_Loss: 0.6552813053131104 *\n",
      "Epoch: 10, Train_Loss: 0.6081712245941162, Test_Loss: 0.5778729319572449 *\n",
      "Epoch: 10, Train_Loss: 0.6424383521080017, Test_Loss: 0.5839410424232483\n",
      "Epoch: 10, Train_Loss: 0.6011956930160522, Test_Loss: 0.628566563129425\n",
      "Epoch: 10, Train_Loss: 0.7778143882751465, Test_Loss: 0.9495283961296082\n",
      "Epoch: 10, Train_Loss: 0.6253713369369507, Test_Loss: 1.3313217163085938\n",
      "Epoch: 10, Train_Loss: 0.6020271182060242, Test_Loss: 0.8916250467300415 *\n",
      "Epoch: 10, Train_Loss: 0.586764395236969, Test_Loss: 0.6374046206474304 *\n",
      "Epoch: 10, Train_Loss: 0.5722635984420776, Test_Loss: 0.5836618542671204 *\n",
      "Epoch: 10, Train_Loss: 0.5723803043365479, Test_Loss: 0.5856761932373047\n",
      "Epoch: 10, Train_Loss: 0.577505886554718, Test_Loss: 0.5784152746200562 *\n",
      "Epoch: 10, Train_Loss: 1.1536047458648682, Test_Loss: 0.5899026989936829\n",
      "Epoch: 10, Train_Loss: 4.76345157623291, Test_Loss: 0.5943026542663574\n",
      "Epoch: 10, Train_Loss: 0.5769945383071899, Test_Loss: 0.6138220429420471\n",
      "Epoch: 10, Train_Loss: 0.5745314359664917, Test_Loss: 0.5730128884315491 *\n",
      "Epoch: 10, Train_Loss: 0.5740721225738525, Test_Loss: 0.6614091396331787\n",
      "Epoch: 10, Train_Loss: 0.571569561958313, Test_Loss: 0.8676455020904541\n",
      "Epoch: 10, Train_Loss: 0.5714351534843445, Test_Loss: 0.7321799993515015 *\n",
      "Epoch: 10, Train_Loss: 0.568950891494751, Test_Loss: 0.8022624850273132\n",
      "Epoch: 10, Train_Loss: 0.5675655007362366, Test_Loss: 0.5855244994163513 *\n",
      "Epoch: 10, Train_Loss: 0.5713103413581848, Test_Loss: 0.5828618407249451 *\n",
      "Epoch: 10, Train_Loss: 0.5679285526275635, Test_Loss: 0.5830739736557007\n",
      "Epoch: 10, Train_Loss: 0.5961082577705383, Test_Loss: 0.5835528373718262\n",
      "Epoch: 10, Train_Loss: 0.5872635841369629, Test_Loss: 0.6110506653785706\n",
      "Epoch: 10, Train_Loss: 0.6246525645256042, Test_Loss: 3.7231082916259766\n",
      "Epoch: 10, Train_Loss: 0.5905353426933289, Test_Loss: 2.605227470397949 *\n",
      "Epoch: 10, Train_Loss: 0.5701907873153687, Test_Loss: 0.576355516910553 *\n",
      "Epoch: 10, Train_Loss: 0.7181188464164734, Test_Loss: 0.5702961683273315 *\n",
      "Epoch: 10, Train_Loss: 0.7744246125221252, Test_Loss: 0.5676624774932861 *\n",
      "Epoch: 10, Train_Loss: 0.7741114497184753, Test_Loss: 0.579186201095581\n",
      "Epoch: 10, Train_Loss: 0.6903208494186401, Test_Loss: 0.5734246373176575 *\n",
      "Epoch: 10, Train_Loss: 0.5664643049240112, Test_Loss: 0.5784488320350647\n",
      "Epoch: 10, Train_Loss: 0.5641962289810181, Test_Loss: 0.568742573261261 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 10\n",
      "Epoch: 10, Train_Loss: 0.5661358833312988, Test_Loss: 0.5679834485054016 *\n",
      "Epoch: 10, Train_Loss: 0.5762456655502319, Test_Loss: 0.5707951784133911\n",
      "Epoch: 10, Train_Loss: 0.5760552287101746, Test_Loss: 0.5770642161369324\n",
      "Epoch: 10, Train_Loss: 0.5743234157562256, Test_Loss: 0.584159791469574\n",
      "Epoch: 10, Train_Loss: 0.5671151876449585, Test_Loss: 0.5710210204124451 *\n",
      "Epoch: 10, Train_Loss: 0.5630986094474792, Test_Loss: 0.567945122718811 *\n",
      "Epoch: 10, Train_Loss: 0.5724664330482483, Test_Loss: 0.5729999542236328\n",
      "Epoch: 10, Train_Loss: 0.5992358922958374, Test_Loss: 0.563419759273529 *\n",
      "Epoch: 10, Train_Loss: 0.7363054752349854, Test_Loss: 0.5697697401046753\n",
      "Epoch: 10, Train_Loss: 0.7267407178878784, Test_Loss: 0.5687384605407715 *\n",
      "Epoch: 10, Train_Loss: 0.710828423500061, Test_Loss: 0.5763593316078186\n",
      "Epoch: 10, Train_Loss: 0.6291306614875793, Test_Loss: 0.5668946504592896 *\n",
      "Epoch: 10, Train_Loss: 0.6998947858810425, Test_Loss: 0.5708234310150146\n",
      "Epoch: 10, Train_Loss: 0.6817445158958435, Test_Loss: 0.5786705613136292\n",
      "Epoch: 10, Train_Loss: 0.6921387314796448, Test_Loss: 0.5930178165435791\n",
      "Epoch: 10, Train_Loss: 0.7324872016906738, Test_Loss: 0.5810597538948059 *\n",
      "Epoch: 10, Train_Loss: 0.8997253179550171, Test_Loss: 0.57280033826828 *\n",
      "Epoch: 10, Train_Loss: 0.5712749361991882, Test_Loss: 0.5685024857521057 *\n",
      "Epoch: 10, Train_Loss: 0.5700382590293884, Test_Loss: 0.5770914554595947\n",
      "Epoch: 10, Train_Loss: 3.2837960720062256, Test_Loss: 0.5692718029022217 *\n",
      "Epoch: 10, Train_Loss: 1.0048654079437256, Test_Loss: 0.568244457244873 *\n",
      "Epoch: 10, Train_Loss: 0.6010330319404602, Test_Loss: 0.625480055809021\n",
      "Epoch: 10, Train_Loss: 0.6062790751457214, Test_Loss: 0.5909358859062195 *\n",
      "Epoch: 10, Train_Loss: 0.583141028881073, Test_Loss: 5.927971839904785\n",
      "Epoch: 10, Train_Loss: 0.5831230282783508, Test_Loss: 0.7994402647018433 *\n",
      "Epoch: 10, Train_Loss: 0.5662009716033936, Test_Loss: 0.5605336427688599 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_Loss: 0.6156536936759949, Test_Loss: 0.5841385126113892\n",
      "Epoch: 10, Train_Loss: 0.7014861106872559, Test_Loss: 0.595012903213501\n",
      "Epoch: 10, Train_Loss: 0.659824788570404, Test_Loss: 0.6021618843078613\n",
      "Epoch: 10, Train_Loss: 0.6352329254150391, Test_Loss: 0.5647903084754944 *\n",
      "Epoch: 10, Train_Loss: 0.6024643182754517, Test_Loss: 0.6628899574279785\n",
      "Epoch: 10, Train_Loss: 0.5895825028419495, Test_Loss: 0.6216955184936523 *\n",
      "Epoch: 10, Train_Loss: 0.5874937176704407, Test_Loss: 0.559913158416748 *\n",
      "Epoch: 10, Train_Loss: 0.568574845790863, Test_Loss: 0.5850479006767273\n",
      "Epoch: 10, Train_Loss: 0.5958813428878784, Test_Loss: 0.5788295865058899 *\n",
      "Epoch: 10, Train_Loss: 0.5789907574653625, Test_Loss: 0.5642014145851135 *\n",
      "Epoch: 10, Train_Loss: 0.5599149465560913, Test_Loss: 0.5749651789665222\n",
      "Epoch: 10, Train_Loss: 0.5667932629585266, Test_Loss: 0.641979455947876\n",
      "Epoch: 10, Train_Loss: 0.5912653207778931, Test_Loss: 0.594871997833252 *\n",
      "Epoch: 10, Train_Loss: 0.595798671245575, Test_Loss: 0.624661386013031\n",
      "Epoch: 10, Train_Loss: 0.5621617436408997, Test_Loss: 0.595617413520813 *\n",
      "Epoch: 10, Train_Loss: 0.558104932308197, Test_Loss: 0.6004164814949036\n",
      "Epoch: 10, Train_Loss: 0.5584326982498169, Test_Loss: 0.5694335103034973 *\n",
      "Epoch: 10, Train_Loss: 0.5567412376403809, Test_Loss: 0.5675947070121765 *\n",
      "Epoch: 10, Train_Loss: 0.5635002851486206, Test_Loss: 0.5678621530532837\n",
      "Epoch: 10, Train_Loss: 0.5603452920913696, Test_Loss: 0.5679765939712524\n",
      "Epoch: 10, Train_Loss: 0.562701404094696, Test_Loss: 0.5698798894882202\n",
      "Epoch: 10, Train_Loss: 0.5591110587120056, Test_Loss: 0.5701500177383423\n",
      "Epoch: 10, Train_Loss: 0.555538535118103, Test_Loss: 0.5631141066551208 *\n",
      "Epoch: 10, Train_Loss: 0.5575554370880127, Test_Loss: 0.5744633078575134\n",
      "Epoch: 10, Train_Loss: 0.5648652911186218, Test_Loss: 0.5661038160324097 *\n",
      "Epoch: 10, Train_Loss: 0.5735646486282349, Test_Loss: 0.5621700286865234 *\n",
      "Epoch: 10, Train_Loss: 0.5764323472976685, Test_Loss: 0.5643982887268066\n",
      "Epoch: 10, Train_Loss: 0.5655510425567627, Test_Loss: 0.6073699593544006\n",
      "Epoch: 11, Train_Loss: 0.5780754089355469, Test_Loss: 0.5780760049819946 *\n",
      "Epoch: 11, Train_Loss: 0.5768351554870605, Test_Loss: 0.809985339641571\n",
      "Epoch: 11, Train_Loss: 0.5586633086204529, Test_Loss: 1.0820379257202148\n",
      "Epoch: 11, Train_Loss: 0.5548399686813354, Test_Loss: 0.8008866310119629 *\n",
      "Epoch: 11, Train_Loss: 0.5672453045845032, Test_Loss: 0.6347861886024475 *\n",
      "Epoch: 11, Train_Loss: 0.5789589881896973, Test_Loss: 0.5773013830184937 *\n",
      "Epoch: 11, Train_Loss: 0.5539664030075073, Test_Loss: 0.5609071254730225 *\n",
      "Epoch: 11, Train_Loss: 0.5565221905708313, Test_Loss: 0.623239278793335\n",
      "Epoch: 11, Train_Loss: 0.5540030598640442, Test_Loss: 1.044013261795044\n",
      "Epoch: 11, Train_Loss: 0.585031270980835, Test_Loss: 1.267349362373352\n",
      "Epoch: 11, Train_Loss: 0.6036746501922607, Test_Loss: 0.7276917695999146 *\n",
      "Epoch: 11, Train_Loss: 0.6029707789421082, Test_Loss: 0.6465046405792236 *\n",
      "Epoch: 11, Train_Loss: 0.5694612264633179, Test_Loss: 0.5533360242843628 *\n",
      "Epoch: 11, Train_Loss: 0.5524239540100098, Test_Loss: 0.5588071942329407\n",
      "Epoch: 11, Train_Loss: 0.6218284964561462, Test_Loss: 0.5555530190467834 *\n",
      "Epoch: 11, Train_Loss: 0.559258759021759, Test_Loss: 0.5683323740959167\n",
      "Epoch: 11, Train_Loss: 0.5598852038383484, Test_Loss: 0.5800486207008362\n",
      "Epoch: 11, Train_Loss: 0.5808148980140686, Test_Loss: 0.588921844959259\n",
      "Epoch: 11, Train_Loss: 0.5635836720466614, Test_Loss: 0.5568475127220154 *\n",
      "Epoch: 11, Train_Loss: 0.6846243739128113, Test_Loss: 0.6675704121589661\n",
      "Epoch: 11, Train_Loss: 0.623510479927063, Test_Loss: 0.9395778179168701\n",
      "Epoch: 11, Train_Loss: 0.5895844101905823, Test_Loss: 0.6502418518066406 *\n",
      "Epoch: 11, Train_Loss: 0.5626146793365479, Test_Loss: 0.7245194911956787\n",
      "Epoch: 11, Train_Loss: 0.5675106048583984, Test_Loss: 0.5598719716072083 *\n",
      "Epoch: 11, Train_Loss: 0.561753511428833, Test_Loss: 0.5597623586654663 *\n",
      "Epoch: 11, Train_Loss: 0.5533987283706665, Test_Loss: 0.5594673156738281 *\n",
      "Epoch: 11, Train_Loss: 0.5613002777099609, Test_Loss: 0.5593493580818176 *\n",
      "Epoch: 11, Train_Loss: 0.563310980796814, Test_Loss: 0.5738334655761719\n",
      "Epoch: 11, Train_Loss: 0.579939603805542, Test_Loss: 5.20799446105957\n",
      "Epoch: 11, Train_Loss: 0.6413599252700806, Test_Loss: 1.3976490497589111 *\n",
      "Epoch: 11, Train_Loss: 0.5638812780380249, Test_Loss: 0.558887779712677 *\n",
      "Epoch: 11, Train_Loss: 0.6089938879013062, Test_Loss: 0.551681637763977 *\n",
      "Epoch: 11, Train_Loss: 0.570319652557373, Test_Loss: 0.5520955324172974\n",
      "Epoch: 11, Train_Loss: 0.5696946382522583, Test_Loss: 0.5592211484909058\n",
      "Epoch: 11, Train_Loss: 0.6483782529830933, Test_Loss: 0.5518021583557129 *\n",
      "Epoch: 11, Train_Loss: 0.7776959538459778, Test_Loss: 0.5570602416992188\n",
      "Epoch: 11, Train_Loss: 0.5571457147598267, Test_Loss: 0.5504339933395386 *\n",
      "Epoch: 11, Train_Loss: 0.586426317691803, Test_Loss: 0.5512775778770447\n",
      "Epoch: 11, Train_Loss: 0.5476968288421631, Test_Loss: 0.5535792708396912\n",
      "Epoch: 11, Train_Loss: 0.547331690788269, Test_Loss: 0.5618771314620972\n",
      "Epoch: 11, Train_Loss: 0.5482911467552185, Test_Loss: 0.553790271282196 *\n",
      "Epoch: 11, Train_Loss: 0.5473467111587524, Test_Loss: 0.5612611770629883\n",
      "Epoch: 11, Train_Loss: 0.5617414116859436, Test_Loss: 0.5567663311958313 *\n",
      "Epoch: 11, Train_Loss: 0.5625805854797363, Test_Loss: 0.552207887172699 *\n",
      "Epoch: 11, Train_Loss: 0.5593298077583313, Test_Loss: 0.5467149615287781 *\n",
      "Epoch: 11, Train_Loss: 0.5568082928657532, Test_Loss: 0.5496838092803955\n",
      "Epoch: 11, Train_Loss: 0.5631200671195984, Test_Loss: 0.5494088530540466 *\n",
      "Epoch: 11, Train_Loss: 0.5507946014404297, Test_Loss: 0.5496984124183655\n",
      "Epoch: 11, Train_Loss: 0.5476728677749634, Test_Loss: 0.5496276021003723 *\n",
      "Epoch: 11, Train_Loss: 0.5451799631118774, Test_Loss: 0.5484989285469055 *\n",
      "Epoch: 11, Train_Loss: 0.5696359276771545, Test_Loss: 0.5519499778747559\n",
      "Epoch: 11, Train_Loss: 0.5741063356399536, Test_Loss: 0.5554341673851013\n",
      "Epoch: 11, Train_Loss: 0.5688794255256653, Test_Loss: 0.5528262853622437 *\n",
      "Epoch: 11, Train_Loss: 0.5513984560966492, Test_Loss: 0.5479337573051453 *\n",
      "Epoch: 11, Train_Loss: 0.6156623363494873, Test_Loss: 0.5478251576423645 *\n",
      "Epoch: 11, Train_Loss: 0.594994068145752, Test_Loss: 0.5511155724525452\n",
      "Epoch: 11, Train_Loss: 0.5709297060966492, Test_Loss: 0.5485899448394775 *\n",
      "Epoch: 11, Train_Loss: 0.5540155172348022, Test_Loss: 0.5507381558418274\n",
      "Epoch: 11, Train_Loss: 0.5719771981239319, Test_Loss: 0.6074314117431641\n",
      "Epoch: 11, Train_Loss: 0.5490168333053589, Test_Loss: 0.8225299715995789\n",
      "Epoch: 11, Train_Loss: 0.5610233545303345, Test_Loss: 5.782919883728027\n",
      "Epoch: 11, Train_Loss: 0.5633094906806946, Test_Loss: 0.5553069710731506 *\n",
      "Epoch: 11, Train_Loss: 0.5785078406333923, Test_Loss: 0.5437170267105103 *\n",
      "Epoch: 11, Train_Loss: 2.4465529918670654, Test_Loss: 0.5768762230873108\n",
      "Epoch: 11, Train_Loss: 4.098213195800781, Test_Loss: 0.5828779935836792\n",
      "Epoch: 11, Train_Loss: 0.5565313100814819, Test_Loss: 0.593850314617157\n",
      "Epoch: 11, Train_Loss: 0.5573340654373169, Test_Loss: 0.5477383732795715 *\n",
      "Epoch: 11, Train_Loss: 0.5709588527679443, Test_Loss: 0.6705777049064636\n",
      "Epoch: 11, Train_Loss: 0.7198082208633423, Test_Loss: 0.583260715007782 *\n",
      "Epoch: 11, Train_Loss: 0.5787926316261292, Test_Loss: 0.5433432459831238 *\n",
      "Epoch: 11, Train_Loss: 0.5506064891815186, Test_Loss: 0.5828459858894348\n",
      "Epoch: 11, Train_Loss: 0.5418188571929932, Test_Loss: 0.5578182935714722 *\n",
      "Epoch: 11, Train_Loss: 0.60575270652771, Test_Loss: 0.5505275130271912 *\n",
      "Epoch: 11, Train_Loss: 0.5544195175170898, Test_Loss: 0.5851216316223145\n",
      "Epoch: 11, Train_Loss: 0.5564078688621521, Test_Loss: 0.6135814785957336\n",
      "Epoch: 11, Train_Loss: 1.0999447107315063, Test_Loss: 0.5960296392440796 *\n",
      "Epoch: 11, Train_Loss: 1.7997996807098389, Test_Loss: 0.6221963167190552\n",
      "Epoch: 11, Train_Loss: 1.3833812475204468, Test_Loss: 0.5777731537818909 *\n",
      "Epoch: 11, Train_Loss: 0.632790207862854, Test_Loss: 0.582380473613739\n",
      "Epoch: 11, Train_Loss: 0.9296110272407532, Test_Loss: 0.5514013767242432 *\n",
      "Epoch: 11, Train_Loss: 2.7704262733459473, Test_Loss: 0.5468553304672241 *\n",
      "Epoch: 11, Train_Loss: 1.1471748352050781, Test_Loss: 0.5496587157249451\n",
      "Epoch: 11, Train_Loss: 0.5780001878738403, Test_Loss: 0.5518242120742798\n",
      "Epoch: 11, Train_Loss: 0.5588362812995911, Test_Loss: 0.5522336363792419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train_Loss: 1.3083577156066895, Test_Loss: 0.5499199628829956 *\n",
      "Epoch: 11, Train_Loss: 1.6477525234222412, Test_Loss: 0.5446292161941528 *\n",
      "Epoch: 11, Train_Loss: 0.9428768157958984, Test_Loss: 0.5449531078338623\n",
      "Epoch: 11, Train_Loss: 0.5411322116851807, Test_Loss: 0.5404061079025269 *\n",
      "Epoch: 11, Train_Loss: 0.5474002957344055, Test_Loss: 0.5610576868057251\n",
      "Epoch: 11, Train_Loss: 1.07778799533844, Test_Loss: 0.5472426414489746 *\n",
      "Epoch: 11, Train_Loss: 0.9089866876602173, Test_Loss: 0.5533859133720398\n",
      "Epoch: 11, Train_Loss: 0.6185440421104431, Test_Loss: 0.5681130886077881\n",
      "Epoch: 11, Train_Loss: 0.6468051671981812, Test_Loss: 0.8081663250923157\n",
      "Epoch: 11, Train_Loss: 0.7004169821739197, Test_Loss: 0.8828138709068298\n",
      "Epoch: 11, Train_Loss: 0.6049695014953613, Test_Loss: 0.657906174659729 *\n",
      "Epoch: 11, Train_Loss: 0.6321704983711243, Test_Loss: 0.5639793872833252 *\n",
      "Epoch: 11, Train_Loss: 0.7888525724411011, Test_Loss: 0.5475690364837646 *\n",
      "Epoch: 11, Train_Loss: 0.5770105719566345, Test_Loss: 0.550649106502533\n",
      "Epoch: 11, Train_Loss: 0.6308508515357971, Test_Loss: 0.6266679763793945\n",
      "Epoch: 11, Train_Loss: 0.6750376224517822, Test_Loss: 0.9679820537567139\n",
      "Model saved at location save_new\\model.ckpt at epoch 11\n",
      "Epoch: 11, Train_Loss: 0.7361502051353455, Test_Loss: 1.0370864868164062\n",
      "Epoch: 11, Train_Loss: 0.7734154462814331, Test_Loss: 0.6378195881843567 *\n",
      "Epoch: 11, Train_Loss: 0.7060324549674988, Test_Loss: 0.574819803237915 *\n",
      "Epoch: 11, Train_Loss: 0.5973930954933167, Test_Loss: 0.5462428331375122 *\n",
      "Epoch: 11, Train_Loss: 0.6846733689308167, Test_Loss: 0.5483156442642212\n",
      "Epoch: 11, Train_Loss: 0.5932357311248779, Test_Loss: 0.5528976917266846\n",
      "Epoch: 11, Train_Loss: 0.5506311058998108, Test_Loss: 0.5929556488990784\n",
      "Epoch: 11, Train_Loss: 0.5370562076568604, Test_Loss: 0.5549055933952332 *\n",
      "Epoch: 11, Train_Loss: 0.5462526679039001, Test_Loss: 0.5824766159057617\n",
      "Epoch: 11, Train_Loss: 0.5380071997642517, Test_Loss: 0.5538425445556641 *\n",
      "Epoch: 11, Train_Loss: 0.5458243489265442, Test_Loss: 0.6493463516235352\n",
      "Epoch: 11, Train_Loss: 0.5604439377784729, Test_Loss: 0.9058481454849243\n",
      "Epoch: 11, Train_Loss: 0.549228847026825, Test_Loss: 0.7811957597732544 *\n",
      "Epoch: 11, Train_Loss: 0.5504096746444702, Test_Loss: 0.7391682267189026 *\n",
      "Epoch: 11, Train_Loss: 0.6257820725440979, Test_Loss: 0.5378721356391907 *\n",
      "Epoch: 11, Train_Loss: 0.8519260883331299, Test_Loss: 0.536128580570221 *\n",
      "Epoch: 11, Train_Loss: 0.5746139287948608, Test_Loss: 0.5359119772911072 *\n",
      "Epoch: 11, Train_Loss: 0.5962395668029785, Test_Loss: 0.561331570148468\n",
      "Epoch: 11, Train_Loss: 0.596114456653595, Test_Loss: 0.6452098488807678\n",
      "Epoch: 11, Train_Loss: 0.8150428533554077, Test_Loss: 5.7952470779418945\n",
      "Epoch: 11, Train_Loss: 0.7454100847244263, Test_Loss: 0.7559488415718079 *\n",
      "Epoch: 11, Train_Loss: 0.5457125902175903, Test_Loss: 0.5581133961677551 *\n",
      "Epoch: 11, Train_Loss: 0.5614297389984131, Test_Loss: 0.5463162660598755 *\n",
      "Epoch: 11, Train_Loss: 0.9285129308700562, Test_Loss: 0.5433219075202942 *\n",
      "Epoch: 11, Train_Loss: 0.8839519023895264, Test_Loss: 0.5449068546295166\n",
      "Epoch: 11, Train_Loss: 0.6059396266937256, Test_Loss: 0.5690399408340454\n",
      "Epoch: 11, Train_Loss: 0.5483385920524597, Test_Loss: 0.598314106464386\n",
      "Epoch: 11, Train_Loss: 0.5771199464797974, Test_Loss: 0.544844388961792 *\n",
      "Epoch: 11, Train_Loss: 1.0327128171920776, Test_Loss: 0.5616826415061951\n",
      "Epoch: 11, Train_Loss: 1.1376888751983643, Test_Loss: 0.5753002166748047\n",
      "Epoch: 11, Train_Loss: 0.6207156181335449, Test_Loss: 0.635141909122467\n",
      "Epoch: 11, Train_Loss: 0.5611101388931274, Test_Loss: 0.563713788986206 *\n",
      "Epoch: 11, Train_Loss: 0.5501901507377625, Test_Loss: 0.5526548027992249 *\n",
      "Epoch: 11, Train_Loss: 0.5579462647438049, Test_Loss: 0.5829469561576843\n",
      "Epoch: 11, Train_Loss: 0.9867904782295227, Test_Loss: 0.5533859133720398 *\n",
      "Epoch: 11, Train_Loss: 0.5718185901641846, Test_Loss: 0.5463743805885315 *\n",
      "Epoch: 11, Train_Loss: 0.6218653321266174, Test_Loss: 0.5820555686950684\n",
      "Epoch: 11, Train_Loss: 0.6794372200965881, Test_Loss: 0.6060546636581421\n",
      "Epoch: 11, Train_Loss: 0.5872290730476379, Test_Loss: 0.5649236440658569 *\n",
      "Epoch: 11, Train_Loss: 0.5410870313644409, Test_Loss: 0.5491335988044739 *\n",
      "Epoch: 11, Train_Loss: 0.5984445214271545, Test_Loss: 0.5696220397949219\n",
      "Epoch: 11, Train_Loss: 0.6737087368965149, Test_Loss: 0.6112760901451111\n",
      "Epoch: 11, Train_Loss: 0.5597249865531921, Test_Loss: 0.6117480993270874\n",
      "Epoch: 11, Train_Loss: 0.5774176120758057, Test_Loss: 0.5901432037353516 *\n",
      "Epoch: 11, Train_Loss: 0.5477142333984375, Test_Loss: 0.5529148578643799 *\n",
      "Epoch: 11, Train_Loss: 0.6473739743232727, Test_Loss: 0.569446861743927\n",
      "Epoch: 11, Train_Loss: 0.5956753492355347, Test_Loss: 0.5730027556419373\n",
      "Epoch: 11, Train_Loss: 0.5547264218330383, Test_Loss: 0.5582339763641357 *\n",
      "Epoch: 11, Train_Loss: 0.5386824607849121, Test_Loss: 0.5707693099975586\n",
      "Epoch: 11, Train_Loss: 0.5694320797920227, Test_Loss: 0.6445946097373962\n",
      "Epoch: 11, Train_Loss: 0.858044445514679, Test_Loss: 1.9590249061584473\n",
      "Epoch: 11, Train_Loss: 0.8044154047966003, Test_Loss: 4.550024509429932\n",
      "Epoch: 11, Train_Loss: 0.9375631809234619, Test_Loss: 0.547620952129364 *\n",
      "Epoch: 11, Train_Loss: 0.921526312828064, Test_Loss: 0.5473718643188477 *\n",
      "Epoch: 11, Train_Loss: 0.7518220543861389, Test_Loss: 0.5635136961936951\n",
      "Epoch: 11, Train_Loss: 0.7733668088912964, Test_Loss: 0.5324118137359619 *\n",
      "Epoch: 11, Train_Loss: 0.5824212431907654, Test_Loss: 0.5527254939079285\n",
      "Epoch: 11, Train_Loss: 0.5372996926307678, Test_Loss: 0.5673007369041443\n",
      "Epoch: 11, Train_Loss: 0.5415986180305481, Test_Loss: 0.6101316213607788\n",
      "Epoch: 11, Train_Loss: 0.5655161142349243, Test_Loss: 0.5428798198699951 *\n",
      "Epoch: 11, Train_Loss: 0.7763413190841675, Test_Loss: 0.5506851077079773\n",
      "Epoch: 11, Train_Loss: 0.9080735445022583, Test_Loss: 0.5706453323364258\n",
      "Epoch: 11, Train_Loss: 0.9735801815986633, Test_Loss: 0.6314998865127563\n",
      "Epoch: 11, Train_Loss: 1.9156599044799805, Test_Loss: 0.5612250566482544 *\n",
      "Epoch: 11, Train_Loss: 0.8308225870132446, Test_Loss: 0.5943959951400757\n",
      "Epoch: 11, Train_Loss: 0.8428415060043335, Test_Loss: 0.5456387400627136 *\n",
      "Epoch: 11, Train_Loss: 0.5602655410766602, Test_Loss: 0.598624587059021\n",
      "Epoch: 11, Train_Loss: 0.541886568069458, Test_Loss: 0.5341835021972656 *\n",
      "Epoch: 11, Train_Loss: 0.858095645904541, Test_Loss: 0.6260320544242859\n",
      "Epoch: 11, Train_Loss: 1.523240089416504, Test_Loss: 0.5766077637672424 *\n",
      "Epoch: 11, Train_Loss: 0.7855342626571655, Test_Loss: 0.5466453433036804 *\n",
      "Epoch: 11, Train_Loss: 0.5924200415611267, Test_Loss: 0.5865827798843384\n",
      "Epoch: 11, Train_Loss: 0.645568311214447, Test_Loss: 0.5774823427200317 *\n",
      "Epoch: 11, Train_Loss: 0.5934865474700928, Test_Loss: 0.5677801966667175 *\n",
      "Epoch: 11, Train_Loss: 0.818301260471344, Test_Loss: 0.5741219520568848\n",
      "Epoch: 11, Train_Loss: 0.7672181129455566, Test_Loss: 0.5823377370834351\n",
      "Epoch: 11, Train_Loss: 0.7481852173805237, Test_Loss: 0.6126924157142639\n",
      "Epoch: 11, Train_Loss: 0.7858025431632996, Test_Loss: 0.597880482673645 *\n",
      "Epoch: 11, Train_Loss: 0.5739117860794067, Test_Loss: 0.5783050060272217 *\n",
      "Epoch: 11, Train_Loss: 0.5656862258911133, Test_Loss: 0.6579318642616272\n",
      "Epoch: 11, Train_Loss: 0.5852781534194946, Test_Loss: 0.5898410677909851 *\n",
      "Epoch: 11, Train_Loss: 0.5919167995452881, Test_Loss: 0.5354591012001038 *\n",
      "Epoch: 11, Train_Loss: 0.5681506991386414, Test_Loss: 0.632848858833313\n",
      "Epoch: 11, Train_Loss: 0.568450927734375, Test_Loss: 0.6357030272483826\n",
      "Epoch: 11, Train_Loss: 10.151206016540527, Test_Loss: 0.6522148847579956\n",
      "Epoch: 11, Train_Loss: 7.085670471191406, Test_Loss: 0.5423889756202698 *\n",
      "Epoch: 11, Train_Loss: 1.4649876356124878, Test_Loss: 0.546992838382721\n",
      "Epoch: 11, Train_Loss: 2.6331982612609863, Test_Loss: 0.5652587413787842\n",
      "Epoch: 11, Train_Loss: 0.7834939956665039, Test_Loss: 0.5705646276473999\n",
      "Epoch: 11, Train_Loss: 0.5870745182037354, Test_Loss: 0.5830929279327393\n",
      "Epoch: 11, Train_Loss: 0.9213303327560425, Test_Loss: 0.7662609815597534\n",
      "Epoch: 11, Train_Loss: 9.319770812988281, Test_Loss: 0.8283138275146484\n",
      "Epoch: 11, Train_Loss: 1.8727664947509766, Test_Loss: 0.592471718788147 *\n",
      "Epoch: 11, Train_Loss: 0.6081058382987976, Test_Loss: 0.5714216232299805 *\n",
      "Epoch: 11, Train_Loss: 3.704943895339966, Test_Loss: 0.5942246913909912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train_Loss: 3.0559170246124268, Test_Loss: 0.5970050096511841\n",
      "Epoch: 11, Train_Loss: 0.649025559425354, Test_Loss: 0.6967018246650696\n",
      "Epoch: 11, Train_Loss: 0.5322032570838928, Test_Loss: 0.6200534105300903 *\n",
      "Epoch: 11, Train_Loss: 0.5636036396026611, Test_Loss: 0.7127429246902466\n",
      "Epoch: 11, Train_Loss: 0.5654228329658508, Test_Loss: 0.5670861601829529 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 11\n",
      "Epoch: 11, Train_Loss: 0.5443374514579773, Test_Loss: 0.6892791986465454\n",
      "Epoch: 11, Train_Loss: 0.5310729742050171, Test_Loss: 0.6744608283042908 *\n",
      "Epoch: 11, Train_Loss: 0.5220217108726501, Test_Loss: 0.8089655637741089\n",
      "Epoch: 11, Train_Loss: 0.5198736190795898, Test_Loss: 0.6226394772529602 *\n",
      "Epoch: 11, Train_Loss: 0.5421152710914612, Test_Loss: 0.707062304019928\n",
      "Epoch: 11, Train_Loss: 0.5923826694488525, Test_Loss: 0.7785788774490356\n",
      "Epoch: 11, Train_Loss: 0.5873585939407349, Test_Loss: 0.7717413902282715 *\n",
      "Epoch: 11, Train_Loss: 0.6301723122596741, Test_Loss: 0.7546748518943787 *\n",
      "Epoch: 11, Train_Loss: 0.6826654076576233, Test_Loss: 0.6584987640380859 *\n",
      "Epoch: 11, Train_Loss: 0.6581981182098389, Test_Loss: 1.1312702894210815\n",
      "Epoch: 11, Train_Loss: 0.5452402234077454, Test_Loss: 6.688089370727539\n",
      "Epoch: 11, Train_Loss: 0.5339728593826294, Test_Loss: 0.7493071556091309 *\n",
      "Epoch: 11, Train_Loss: 0.5362539291381836, Test_Loss: 0.6416473388671875 *\n",
      "Epoch: 11, Train_Loss: 0.5195410251617432, Test_Loss: 0.6378383040428162 *\n",
      "Epoch: 11, Train_Loss: 0.5201402306556702, Test_Loss: 0.5713770985603333 *\n",
      "Epoch: 11, Train_Loss: 0.5182515382766724, Test_Loss: 0.5927762389183044\n",
      "Epoch: 11, Train_Loss: 0.5182737112045288, Test_Loss: 0.6634207963943481\n",
      "Epoch: 11, Train_Loss: 0.5184150338172913, Test_Loss: 0.6990180611610413\n",
      "Epoch: 11, Train_Loss: 0.5185788869857788, Test_Loss: 0.5967758893966675 *\n",
      "Epoch: 11, Train_Loss: 0.5177629590034485, Test_Loss: 0.6472083926200867\n",
      "Epoch: 11, Train_Loss: 0.5182793140411377, Test_Loss: 0.6239404678344727 *\n",
      "Epoch: 11, Train_Loss: 0.5277993679046631, Test_Loss: 0.7020131349563599\n",
      "Epoch: 11, Train_Loss: 0.5276874303817749, Test_Loss: 0.5582106113433838 *\n",
      "Epoch: 11, Train_Loss: 0.5585160851478577, Test_Loss: 0.5702750086784363\n",
      "Epoch: 11, Train_Loss: 0.5344946384429932, Test_Loss: 0.615820050239563\n",
      "Epoch: 11, Train_Loss: 0.5513384342193604, Test_Loss: 0.5440224409103394 *\n",
      "Epoch: 11, Train_Loss: 8.707554817199707, Test_Loss: 0.557724118232727\n",
      "Epoch: 11, Train_Loss: 0.9040054082870483, Test_Loss: 0.5475130081176758 *\n",
      "Epoch: 11, Train_Loss: 0.5399903655052185, Test_Loss: 0.666779637336731\n",
      "Epoch: 11, Train_Loss: 0.5918716788291931, Test_Loss: 0.5847971439361572 *\n",
      "Epoch: 11, Train_Loss: 0.6413202285766602, Test_Loss: 0.6428194642066956\n",
      "Epoch: 11, Train_Loss: 0.5434255599975586, Test_Loss: 0.6209938526153564 *\n",
      "Epoch: 11, Train_Loss: 0.5571330785751343, Test_Loss: 0.722412109375\n",
      "Epoch: 11, Train_Loss: 0.663831353187561, Test_Loss: 0.7130846381187439 *\n",
      "Epoch: 11, Train_Loss: 0.7704815864562988, Test_Loss: 0.6421920657157898 *\n",
      "Epoch: 11, Train_Loss: 0.7135125994682312, Test_Loss: 0.5752959251403809 *\n",
      "Epoch: 11, Train_Loss: 0.6451032161712646, Test_Loss: 0.6015081405639648\n",
      "Epoch: 11, Train_Loss: 0.5248287320137024, Test_Loss: 0.5907155871391296 *\n",
      "Epoch: 11, Train_Loss: 0.6546807885169983, Test_Loss: 0.5665209889411926 *\n",
      "Epoch: 11, Train_Loss: 0.6573059558868408, Test_Loss: 0.6039794683456421\n",
      "Epoch: 11, Train_Loss: 0.7383304238319397, Test_Loss: 0.6446455717086792\n",
      "Epoch: 11, Train_Loss: 0.659678041934967, Test_Loss: 3.3304824829101562\n",
      "Epoch: 11, Train_Loss: 0.6233469843864441, Test_Loss: 3.696267604827881\n",
      "Epoch: 11, Train_Loss: 0.5569340586662292, Test_Loss: 0.5236867666244507 *\n",
      "Epoch: 11, Train_Loss: 0.5328147411346436, Test_Loss: 0.5159783959388733 *\n",
      "Epoch: 11, Train_Loss: 0.5991923213005066, Test_Loss: 0.5273846983909607\n",
      "Epoch: 11, Train_Loss: 0.5437750816345215, Test_Loss: 0.5174615979194641 *\n",
      "Epoch: 11, Train_Loss: 0.5188300013542175, Test_Loss: 0.5386561751365662\n",
      "Epoch: 11, Train_Loss: 0.5156890153884888, Test_Loss: 0.5757940411567688\n",
      "Epoch: 11, Train_Loss: 0.516769528388977, Test_Loss: 0.646056056022644\n",
      "Epoch: 11, Train_Loss: 0.5897118449211121, Test_Loss: 0.5167863965034485 *\n",
      "Epoch: 11, Train_Loss: 6.045694351196289, Test_Loss: 0.5394458174705505\n",
      "Epoch: 11, Train_Loss: 0.535474419593811, Test_Loss: 0.5355700254440308 *\n",
      "Epoch: 11, Train_Loss: 0.51849365234375, Test_Loss: 0.5287606716156006 *\n",
      "Epoch: 11, Train_Loss: 0.5297028422355652, Test_Loss: 0.5221330523490906 *\n",
      "Epoch: 11, Train_Loss: 0.5243163704872131, Test_Loss: 0.5547688007354736\n",
      "Epoch: 11, Train_Loss: 0.5196812152862549, Test_Loss: 0.549574613571167 *\n",
      "Epoch: 11, Train_Loss: 0.5158488750457764, Test_Loss: 0.6264989376068115\n",
      "Epoch: 11, Train_Loss: 0.5221326351165771, Test_Loss: 0.631483793258667\n",
      "Epoch: 11, Train_Loss: 0.5310660004615784, Test_Loss: 0.5276520848274231 *\n",
      "Epoch: 11, Train_Loss: 0.5205205678939819, Test_Loss: 0.5222992897033691 *\n",
      "Epoch: 11, Train_Loss: 0.5434084534645081, Test_Loss: 0.5214422345161438 *\n",
      "Epoch: 11, Train_Loss: 0.515378475189209, Test_Loss: 0.526081919670105\n",
      "Epoch: 11, Train_Loss: 0.5135601758956909, Test_Loss: 0.5287028551101685\n",
      "Epoch: 11, Train_Loss: 0.5327717065811157, Test_Loss: 0.5181695222854614 *\n",
      "Epoch: 11, Train_Loss: 0.5136452913284302, Test_Loss: 0.520587146282196\n",
      "Epoch: 11, Train_Loss: 0.5139703750610352, Test_Loss: 0.526892900466919\n",
      "Epoch: 11, Train_Loss: 0.5340699553489685, Test_Loss: 0.5227641463279724 *\n",
      "Epoch: 11, Train_Loss: 0.5489334464073181, Test_Loss: 0.5146872997283936 *\n",
      "Epoch: 11, Train_Loss: 0.5327419638633728, Test_Loss: 0.5175164341926575\n",
      "Epoch: 11, Train_Loss: 0.5135308504104614, Test_Loss: 0.5456557273864746\n",
      "Epoch: 11, Train_Loss: 0.5144043564796448, Test_Loss: 0.5120360255241394 *\n",
      "Epoch: 11, Train_Loss: 0.5534974932670593, Test_Loss: 0.5211637616157532\n",
      "Epoch: 11, Train_Loss: 0.5321632027626038, Test_Loss: 0.572152316570282\n",
      "Epoch: 11, Train_Loss: 0.5419566631317139, Test_Loss: 0.7390538454055786\n",
      "Epoch: 11, Train_Loss: 0.5209701061248779, Test_Loss: 0.7707407474517822\n",
      "Epoch: 11, Train_Loss: 0.6079747080802917, Test_Loss: 0.6119394302368164 *\n",
      "Epoch: 11, Train_Loss: 0.5735062956809998, Test_Loss: 0.5230509042739868 *\n",
      "Epoch: 11, Train_Loss: 0.5728065967559814, Test_Loss: 0.5291958451271057\n",
      "Epoch: 11, Train_Loss: 0.5795471668243408, Test_Loss: 0.5457897186279297\n",
      "Epoch: 11, Train_Loss: 0.7606905698776245, Test_Loss: 0.6758382320404053\n",
      "Epoch: 11, Train_Loss: 0.5409948229789734, Test_Loss: 1.1337318420410156\n",
      "Epoch: 11, Train_Loss: 0.5567235350608826, Test_Loss: 1.0009069442749023 *\n",
      "Epoch: 11, Train_Loss: 0.5183603763580322, Test_Loss: 0.5708800554275513 *\n",
      "Epoch: 11, Train_Loss: 0.5097113847732544, Test_Loss: 0.5385315418243408 *\n",
      "Epoch: 11, Train_Loss: 0.5099889636039734, Test_Loss: 0.5193265080451965 *\n",
      "Epoch: 11, Train_Loss: 0.5101580619812012, Test_Loss: 0.5167422890663147 *\n",
      "Epoch: 11, Train_Loss: 0.5344298481941223, Test_Loss: 0.5249457955360413\n",
      "Epoch: 11, Train_Loss: 5.060896873474121, Test_Loss: 0.52118319272995 *\n",
      "Epoch: 11, Train_Loss: 0.708649218082428, Test_Loss: 0.5706092715263367\n",
      "Epoch: 11, Train_Loss: 0.5134633183479309, Test_Loss: 0.5086269378662109 *\n",
      "Epoch: 11, Train_Loss: 0.5234744548797607, Test_Loss: 0.5535926818847656\n",
      "Epoch: 11, Train_Loss: 0.5088118314743042, Test_Loss: 0.6321455836296082\n",
      "Epoch: 11, Train_Loss: 0.5085511207580566, Test_Loss: 0.8772072792053223\n",
      "Epoch: 11, Train_Loss: 0.506911039352417, Test_Loss: 0.7499301433563232 *\n",
      "Epoch: 11, Train_Loss: 0.5062584280967712, Test_Loss: 0.5241934061050415 *\n",
      "Epoch: 11, Train_Loss: 0.5084984302520752, Test_Loss: 0.5143340229988098 *\n",
      "Epoch: 11, Train_Loss: 0.506199836730957, Test_Loss: 0.5144890546798706\n",
      "Epoch: 11, Train_Loss: 0.5448758602142334, Test_Loss: 0.5147544741630554\n",
      "Epoch: 11, Train_Loss: 0.5291903614997864, Test_Loss: 0.5230357050895691\n",
      "Model saved at location save_new\\model.ckpt at epoch 11\n",
      "Epoch: 11, Train_Loss: 0.56462162733078, Test_Loss: 1.7336232662200928\n",
      "Epoch: 11, Train_Loss: 0.5369695425033569, Test_Loss: 4.563501358032227\n",
      "Epoch: 11, Train_Loss: 0.5066074132919312, Test_Loss: 0.5204066038131714 *\n",
      "Epoch: 11, Train_Loss: 0.5884214043617249, Test_Loss: 0.5089012980461121 *\n",
      "Epoch: 11, Train_Loss: 0.7024624347686768, Test_Loss: 0.5078010559082031 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train_Loss: 0.704819917678833, Test_Loss: 0.5124006271362305\n",
      "Epoch: 11, Train_Loss: 0.678875207901001, Test_Loss: 0.5092608332633972 *\n",
      "Epoch: 11, Train_Loss: 0.5086619853973389, Test_Loss: 0.5094531774520874\n",
      "Epoch: 11, Train_Loss: 0.5041667222976685, Test_Loss: 0.5114344358444214\n",
      "Epoch: 11, Train_Loss: 0.5048086047172546, Test_Loss: 0.5066505670547485 *\n",
      "Epoch: 11, Train_Loss: 0.5122317671775818, Test_Loss: 0.5104306936264038\n",
      "Epoch: 11, Train_Loss: 0.5135506987571716, Test_Loss: 0.5078968405723572 *\n",
      "Epoch: 11, Train_Loss: 0.5114666819572449, Test_Loss: 0.5177189111709595\n",
      "Epoch: 11, Train_Loss: 0.5095465779304504, Test_Loss: 0.519518256187439\n",
      "Epoch: 11, Train_Loss: 0.5038133859634399, Test_Loss: 0.5137811899185181 *\n",
      "Epoch: 11, Train_Loss: 0.5080465078353882, Test_Loss: 0.5148834586143494\n",
      "Epoch: 11, Train_Loss: 0.5390929579734802, Test_Loss: 0.5037251710891724 *\n",
      "Epoch: 11, Train_Loss: 0.655450701713562, Test_Loss: 0.5057375431060791\n",
      "Epoch: 11, Train_Loss: 0.7083531618118286, Test_Loss: 0.5079829096794128\n",
      "Epoch: 11, Train_Loss: 0.7008136510848999, Test_Loss: 0.5126306414604187\n",
      "Epoch: 11, Train_Loss: 0.5462948679924011, Test_Loss: 0.5047251582145691 *\n",
      "Epoch: 11, Train_Loss: 0.6393533945083618, Test_Loss: 0.5062623620033264\n",
      "Epoch: 11, Train_Loss: 0.6360020637512207, Test_Loss: 0.5068114995956421\n",
      "Epoch: 11, Train_Loss: 0.5679953098297119, Test_Loss: 0.5172286629676819\n",
      "Epoch: 11, Train_Loss: 0.6750616431236267, Test_Loss: 0.5138252377510071 *\n",
      "Epoch: 11, Train_Loss: 0.7143028974533081, Test_Loss: 0.5088102221488953 *\n",
      "Epoch: 11, Train_Loss: 0.6488226056098938, Test_Loss: 0.50505530834198 *\n",
      "Epoch: 11, Train_Loss: 0.5134561061859131, Test_Loss: 0.5076003074645996\n",
      "Epoch: 11, Train_Loss: 2.452392816543579, Test_Loss: 0.5057589411735535 *\n",
      "Epoch: 11, Train_Loss: 1.6755568981170654, Test_Loss: 0.5041862726211548 *\n",
      "Epoch: 11, Train_Loss: 0.5392900109291077, Test_Loss: 0.5439239144325256\n",
      "Epoch: 11, Train_Loss: 0.5542532205581665, Test_Loss: 0.5413305163383484 *\n",
      "Epoch: 11, Train_Loss: 0.5409497618675232, Test_Loss: 4.045902252197266\n",
      "Epoch: 11, Train_Loss: 0.5333909392356873, Test_Loss: 2.483149766921997 *\n",
      "Epoch: 11, Train_Loss: 0.5033477544784546, Test_Loss: 0.5027570128440857 *\n",
      "Epoch: 11, Train_Loss: 0.5292145609855652, Test_Loss: 0.5059471726417542\n",
      "Epoch: 11, Train_Loss: 0.6453641057014465, Test_Loss: 0.552626371383667\n",
      "Epoch: 11, Train_Loss: 0.5976650714874268, Test_Loss: 0.5467604398727417 *\n",
      "Epoch: 11, Train_Loss: 0.5796293020248413, Test_Loss: 0.5257507562637329 *\n",
      "Epoch: 11, Train_Loss: 0.5386148691177368, Test_Loss: 0.567115843296051\n",
      "Epoch: 11, Train_Loss: 0.53212571144104, Test_Loss: 0.5901455283164978\n",
      "Epoch: 11, Train_Loss: 0.5280646681785583, Test_Loss: 0.5033324360847473 *\n",
      "Epoch: 11, Train_Loss: 0.5152458548545837, Test_Loss: 0.5224189758300781\n",
      "Epoch: 11, Train_Loss: 0.5316140651702881, Test_Loss: 0.5263721942901611\n",
      "Epoch: 11, Train_Loss: 0.5233935117721558, Test_Loss: 0.5252264738082886 *\n",
      "Epoch: 11, Train_Loss: 0.5097735524177551, Test_Loss: 0.5061329007148743 *\n",
      "Epoch: 11, Train_Loss: 0.5022827982902527, Test_Loss: 0.5885536074638367\n",
      "Epoch: 11, Train_Loss: 0.5265960097312927, Test_Loss: 0.5528891086578369 *\n",
      "Epoch: 11, Train_Loss: 0.5233619809150696, Test_Loss: 0.5632073283195496\n",
      "Epoch: 11, Train_Loss: 0.5105123519897461, Test_Loss: 0.5338238477706909 *\n",
      "Epoch: 11, Train_Loss: 0.4993857145309448, Test_Loss: 0.5520353317260742\n",
      "Epoch: 11, Train_Loss: 0.4982314705848694, Test_Loss: 0.5209153890609741 *\n",
      "Epoch: 11, Train_Loss: 0.4977814555168152, Test_Loss: 0.5129215717315674 *\n",
      "Epoch: 11, Train_Loss: 0.5016388297080994, Test_Loss: 0.5109418630599976 *\n",
      "Epoch: 11, Train_Loss: 0.5015200972557068, Test_Loss: 0.5131251811981201\n",
      "Epoch: 11, Train_Loss: 0.503302812576294, Test_Loss: 0.5152450799942017\n",
      "Epoch: 11, Train_Loss: 0.5008818507194519, Test_Loss: 0.5141783952713013 *\n",
      "Epoch: 11, Train_Loss: 0.4976460635662079, Test_Loss: 0.5090888738632202 *\n",
      "Epoch: 11, Train_Loss: 0.49652940034866333, Test_Loss: 0.5204765200614929\n",
      "Epoch: 11, Train_Loss: 0.5094020366668701, Test_Loss: 0.5105597376823425 *\n",
      "Epoch: 11, Train_Loss: 0.5107542872428894, Test_Loss: 0.5065220594406128 *\n",
      "Epoch: 11, Train_Loss: 0.5118551254272461, Test_Loss: 0.5044881105422974 *\n",
      "Epoch: 11, Train_Loss: 0.5072399377822876, Test_Loss: 0.5297497510910034\n",
      "Epoch: 11, Train_Loss: 0.5195534825325012, Test_Loss: 0.5424251556396484\n",
      "Epoch: 11, Train_Loss: 0.5088040232658386, Test_Loss: 0.5979539752006531\n",
      "Epoch: 11, Train_Loss: 0.5199334621429443, Test_Loss: 1.0262999534606934\n",
      "Epoch: 11, Train_Loss: 0.5017799139022827, Test_Loss: 0.8783033490180969 *\n",
      "Epoch: 11, Train_Loss: 0.5066486597061157, Test_Loss: 0.6152423620223999 *\n",
      "Epoch: 11, Train_Loss: 0.5242513418197632, Test_Loss: 0.5239123106002808 *\n",
      "Epoch: 11, Train_Loss: 0.5027069449424744, Test_Loss: 0.5146452188491821 *\n",
      "Epoch: 11, Train_Loss: 0.499968945980072, Test_Loss: 0.5345202088356018\n",
      "Epoch: 11, Train_Loss: 0.496117502450943, Test_Loss: 0.7662160396575928\n",
      "Epoch: 11, Train_Loss: 0.5066381692886353, Test_Loss: 1.1335954666137695\n",
      "Epoch: 11, Train_Loss: 0.5512949824333191, Test_Loss: 0.8806519508361816 *\n",
      "Epoch: 11, Train_Loss: 0.5409948229789734, Test_Loss: 0.5898523330688477 *\n",
      "Epoch: 11, Train_Loss: 0.522478461265564, Test_Loss: 0.5079652070999146 *\n",
      "Epoch: 11, Train_Loss: 0.4941239058971405, Test_Loss: 0.5008142590522766 *\n",
      "Epoch: 11, Train_Loss: 0.5508439540863037, Test_Loss: 0.4957570731639862 *\n",
      "Epoch: 11, Train_Loss: 0.5123717784881592, Test_Loss: 0.5072444081306458\n",
      "Epoch: 11, Train_Loss: 0.4977251887321472, Test_Loss: 0.516564130783081\n",
      "Epoch: 11, Train_Loss: 0.5139161944389343, Test_Loss: 0.543216347694397\n",
      "Epoch: 11, Train_Loss: 0.5177170038223267, Test_Loss: 0.49572527408599854 *\n",
      "Epoch: 11, Train_Loss: 0.6082842350006104, Test_Loss: 0.5721374750137329\n",
      "Epoch: 11, Train_Loss: 0.5712592601776123, Test_Loss: 0.6960222125053406\n",
      "Epoch: 11, Train_Loss: 0.5463728308677673, Test_Loss: 0.7773399353027344\n",
      "Epoch: 11, Train_Loss: 0.5137845873832703, Test_Loss: 0.705774188041687 *\n",
      "Epoch: 11, Train_Loss: 0.4974116086959839, Test_Loss: 0.5034711956977844 *\n",
      "Epoch: 11, Train_Loss: 0.5162631869316101, Test_Loss: 0.5014785528182983 *\n",
      "Epoch: 11, Train_Loss: 0.4940136969089508, Test_Loss: 0.5012548565864563 *\n",
      "Epoch: 11, Train_Loss: 0.5005816221237183, Test_Loss: 0.5010551810264587 *\n",
      "Epoch: 11, Train_Loss: 0.5063270330429077, Test_Loss: 0.5125647783279419\n",
      "Epoch: 11, Train_Loss: 0.5104681253433228, Test_Loss: 2.997532367706299\n",
      "Epoch: 11, Train_Loss: 0.59291672706604, Test_Loss: 3.41960072517395\n",
      "Epoch: 11, Train_Loss: 0.49448537826538086, Test_Loss: 0.5026217103004456 *\n",
      "Epoch: 11, Train_Loss: 0.5628877878189087, Test_Loss: 0.4946945607662201 *\n",
      "Epoch: 11, Train_Loss: 0.5035905241966248, Test_Loss: 0.493323415517807 *\n",
      "Epoch: 11, Train_Loss: 0.5239814519882202, Test_Loss: 0.5007615089416504\n",
      "Epoch: 11, Train_Loss: 0.5567261576652527, Test_Loss: 0.49459514021873474 *\n",
      "Epoch: 11, Train_Loss: 0.7463114261627197, Test_Loss: 0.49812671542167664\n",
      "Epoch: 11, Train_Loss: 0.5048611164093018, Test_Loss: 0.4946976900100708 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 11\n",
      "Epoch: 11, Train_Loss: 0.5278312563896179, Test_Loss: 0.49453845620155334 *\n",
      "Epoch: 11, Train_Loss: 0.49043190479278564, Test_Loss: 0.49687618017196655\n",
      "Epoch: 11, Train_Loss: 0.4906691014766693, Test_Loss: 0.49485644698143005 *\n",
      "Epoch: 11, Train_Loss: 0.49165594577789307, Test_Loss: 0.5028899312019348\n",
      "Epoch: 11, Train_Loss: 0.4899550676345825, Test_Loss: 0.5098499059677124\n",
      "Epoch: 11, Train_Loss: 0.5037522315979004, Test_Loss: 0.49990981817245483 *\n",
      "Epoch: 11, Train_Loss: 0.5018776655197144, Test_Loss: 0.49508002400398254 *\n",
      "Epoch: 11, Train_Loss: 0.5095292329788208, Test_Loss: 0.489618182182312 *\n",
      "Epoch: 11, Train_Loss: 0.4975578486919403, Test_Loss: 0.49258437752723694\n",
      "Epoch: 11, Train_Loss: 0.5021160244941711, Test_Loss: 0.4914856255054474 *\n",
      "Epoch: 11, Train_Loss: 0.5033645629882812, Test_Loss: 0.4946288466453552\n",
      "Epoch: 11, Train_Loss: 0.49175286293029785, Test_Loss: 0.49246230721473694 *\n",
      "Epoch: 11, Train_Loss: 0.48844045400619507, Test_Loss: 0.48960772156715393 *\n",
      "Epoch: 11, Train_Loss: 0.5078877806663513, Test_Loss: 0.49211907386779785\n",
      "Epoch: 11, Train_Loss: 0.5147202014923096, Test_Loss: 0.49930402636528015\n",
      "Epoch: 11, Train_Loss: 0.5221134424209595, Test_Loss: 0.4944203197956085 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train_Loss: 0.4883076250553131, Test_Loss: 0.4922375977039337 *\n",
      "Epoch: 11, Train_Loss: 0.5464025139808655, Test_Loss: 0.48944464325904846 *\n",
      "Epoch: 11, Train_Loss: 0.5469713807106018, Test_Loss: 0.4915943443775177\n",
      "Epoch: 11, Train_Loss: 0.5249214172363281, Test_Loss: 0.49107620120048523 *\n",
      "Epoch: 11, Train_Loss: 0.49034595489501953, Test_Loss: 0.4901059865951538 *\n",
      "Epoch: 11, Train_Loss: 0.5212025046348572, Test_Loss: 0.5371584296226501\n",
      "Epoch: 11, Train_Loss: 0.48860853910446167, Test_Loss: 0.5187450051307678 *\n",
      "Epoch: 11, Train_Loss: 0.508921205997467, Test_Loss: 5.259766578674316\n",
      "Epoch: 11, Train_Loss: 0.4926179349422455, Test_Loss: 1.255048155784607 *\n",
      "Epoch: 11, Train_Loss: 0.5105053186416626, Test_Loss: 0.4876372814178467 *\n",
      "Epoch: 11, Train_Loss: 1.0942721366882324, Test_Loss: 0.5045076012611389\n",
      "Epoch: 11, Train_Loss: 4.41502571105957, Test_Loss: 0.5356259346008301\n",
      "Epoch: 11, Train_Loss: 1.4429627656936646, Test_Loss: 0.5416821837425232\n",
      "Epoch: 11, Train_Loss: 0.5056204795837402, Test_Loss: 0.495943546295166 *\n",
      "Epoch: 11, Train_Loss: 0.49274781346321106, Test_Loss: 0.5769243836402893\n",
      "Epoch: 11, Train_Loss: 0.6369234323501587, Test_Loss: 0.5647610425949097 *\n",
      "Epoch: 11, Train_Loss: 0.5678479671478271, Test_Loss: 0.48738086223602295 *\n",
      "Epoch: 11, Train_Loss: 0.5003883838653564, Test_Loss: 0.5246397256851196\n",
      "Epoch: 11, Train_Loss: 0.48545020818710327, Test_Loss: 0.5003697872161865 *\n",
      "Epoch: 11, Train_Loss: 0.5407417416572571, Test_Loss: 0.4965181052684784 *\n",
      "Epoch: 11, Train_Loss: 0.5071755647659302, Test_Loss: 0.4903607964515686 *\n",
      "Epoch: 11, Train_Loss: 0.49748459458351135, Test_Loss: 0.5797003507614136\n",
      "Epoch: 11, Train_Loss: 0.7384121417999268, Test_Loss: 0.5235231518745422 *\n",
      "Epoch: 11, Train_Loss: 1.6049885749816895, Test_Loss: 0.5835386514663696\n",
      "Epoch: 11, Train_Loss: 1.672868013381958, Test_Loss: 0.5281761884689331 *\n",
      "Epoch: 11, Train_Loss: 0.577556312084198, Test_Loss: 0.5309780240058899\n",
      "Epoch: 11, Train_Loss: 0.5894072651863098, Test_Loss: 0.499973326921463 *\n",
      "Epoch: 11, Train_Loss: 2.5639705657958984, Test_Loss: 0.49597859382629395 *\n",
      "Epoch: 11, Train_Loss: 1.5251953601837158, Test_Loss: 0.4969773590564728\n",
      "Epoch: 11, Train_Loss: 0.5173105597496033, Test_Loss: 0.4959903955459595 *\n",
      "Epoch: 11, Train_Loss: 0.4954947531223297, Test_Loss: 0.5025450587272644\n",
      "Epoch: 11, Train_Loss: 0.9267982244491577, Test_Loss: 0.4997941851615906 *\n",
      "Epoch: 11, Train_Loss: 1.4334405660629272, Test_Loss: 0.4862462878227234 *\n",
      "Epoch: 11, Train_Loss: 1.23310387134552, Test_Loss: 0.49553558230400085\n",
      "Epoch: 11, Train_Loss: 0.48867589235305786, Test_Loss: 0.4844145178794861 *\n",
      "Epoch: 11, Train_Loss: 0.5097836256027222, Test_Loss: 0.49230870604515076\n",
      "Epoch: 11, Train_Loss: 0.699948251247406, Test_Loss: 0.5030592679977417\n",
      "Epoch: 11, Train_Loss: 0.9902328252792358, Test_Loss: 0.49654296040534973 *\n",
      "Epoch: 12, Train_Loss: 0.5566679239273071, Test_Loss: 0.5076929926872253 *\n",
      "Epoch: 12, Train_Loss: 0.5848184823989868, Test_Loss: 0.668105959892273\n",
      "Epoch: 12, Train_Loss: 0.6110818982124329, Test_Loss: 0.8375177383422852\n",
      "Epoch: 12, Train_Loss: 0.5710269212722778, Test_Loss: 0.6931511163711548 *\n",
      "Epoch: 12, Train_Loss: 0.592021107673645, Test_Loss: 0.5746442079544067 *\n",
      "Epoch: 12, Train_Loss: 0.6439485549926758, Test_Loss: 0.496013879776001 *\n",
      "Epoch: 12, Train_Loss: 0.5512505769729614, Test_Loss: 0.5082889199256897\n",
      "Epoch: 12, Train_Loss: 0.5577396154403687, Test_Loss: 0.5085777044296265\n",
      "Epoch: 12, Train_Loss: 0.6042945981025696, Test_Loss: 0.6946998238563538\n",
      "Epoch: 12, Train_Loss: 0.6294931173324585, Test_Loss: 0.7336628437042236\n",
      "Epoch: 12, Train_Loss: 0.7315992116928101, Test_Loss: 0.7286379337310791 *\n",
      "Epoch: 12, Train_Loss: 0.7502254247665405, Test_Loss: 0.5340989828109741 *\n",
      "Epoch: 12, Train_Loss: 0.5083533525466919, Test_Loss: 0.5060665011405945 *\n",
      "Epoch: 12, Train_Loss: 0.5753383040428162, Test_Loss: 0.5149877667427063\n",
      "Epoch: 12, Train_Loss: 0.5586007833480835, Test_Loss: 0.5110379457473755 *\n",
      "Epoch: 12, Train_Loss: 0.5206999778747559, Test_Loss: 0.5786690711975098\n",
      "Epoch: 12, Train_Loss: 0.48768946528434753, Test_Loss: 0.5064436793327332 *\n",
      "Epoch: 12, Train_Loss: 0.4883372187614441, Test_Loss: 0.5314477682113647\n",
      "Epoch: 12, Train_Loss: 0.48689165711402893, Test_Loss: 0.5157926678657532 *\n",
      "Epoch: 12, Train_Loss: 0.4902585744857788, Test_Loss: 0.6077659130096436\n",
      "Epoch: 12, Train_Loss: 0.505449116230011, Test_Loss: 0.8063809871673584\n",
      "Epoch: 12, Train_Loss: 0.4948294162750244, Test_Loss: 0.6353358626365662 *\n",
      "Epoch: 12, Train_Loss: 0.4909786283969879, Test_Loss: 0.8553323745727539\n",
      "Epoch: 12, Train_Loss: 0.5354942083358765, Test_Loss: 0.4889397621154785 *\n",
      "Epoch: 12, Train_Loss: 0.6116327047348022, Test_Loss: 0.4872722029685974 *\n",
      "Epoch: 12, Train_Loss: 0.7051585912704468, Test_Loss: 0.48756057024002075\n",
      "Epoch: 12, Train_Loss: 0.5157477259635925, Test_Loss: 0.49292445182800293\n",
      "Epoch: 12, Train_Loss: 0.539932131767273, Test_Loss: 0.5918075442314148\n",
      "Epoch: 12, Train_Loss: 0.6831049919128418, Test_Loss: 4.412233352661133\n",
      "Epoch: 12, Train_Loss: 0.6581201553344727, Test_Loss: 2.0248758792877197 *\n",
      "Epoch: 12, Train_Loss: 0.5168004035949707, Test_Loss: 0.5074703097343445 *\n",
      "Epoch: 12, Train_Loss: 0.5158056020736694, Test_Loss: 0.5073536038398743 *\n",
      "Epoch: 12, Train_Loss: 0.759796142578125, Test_Loss: 0.5058692693710327 *\n",
      "Epoch: 12, Train_Loss: 0.7982473373413086, Test_Loss: 0.482410192489624 *\n",
      "Epoch: 12, Train_Loss: 0.5680993795394897, Test_Loss: 0.5407676696777344\n",
      "Epoch: 12, Train_Loss: 0.5049586892127991, Test_Loss: 0.5646172165870667\n",
      "Epoch: 12, Train_Loss: 0.5301105976104736, Test_Loss: 0.5109230875968933 *\n",
      "Epoch: 12, Train_Loss: 0.7317990064620972, Test_Loss: 0.5152108669281006\n",
      "Epoch: 12, Train_Loss: 1.240229845046997, Test_Loss: 0.5321797728538513\n",
      "Epoch: 12, Train_Loss: 0.7096171379089355, Test_Loss: 0.5553566217422485\n",
      "Epoch: 12, Train_Loss: 0.5299364328384399, Test_Loss: 0.5626636743545532\n",
      "Epoch: 12, Train_Loss: 0.5080655217170715, Test_Loss: 0.4890522062778473 *\n",
      "Epoch: 12, Train_Loss: 0.4882080852985382, Test_Loss: 0.5088068842887878\n",
      "Epoch: 12, Train_Loss: 0.8832689523696899, Test_Loss: 0.5162081718444824\n",
      "Epoch: 12, Train_Loss: 0.591487467288971, Test_Loss: 0.4931469261646271 *\n",
      "Epoch: 12, Train_Loss: 0.4948171377182007, Test_Loss: 0.5202764868736267\n",
      "Epoch: 12, Train_Loss: 0.738474428653717, Test_Loss: 0.5693725347518921\n",
      "Epoch: 12, Train_Loss: 0.5324385762214661, Test_Loss: 0.5380632281303406 *\n",
      "Epoch: 12, Train_Loss: 0.4846135973930359, Test_Loss: 0.4912610650062561 *\n",
      "Epoch: 12, Train_Loss: 0.5093650221824646, Test_Loss: 0.5065969824790955\n",
      "Epoch: 12, Train_Loss: 0.6424200534820557, Test_Loss: 0.5310771465301514\n",
      "Epoch: 12, Train_Loss: 0.5077175498008728, Test_Loss: 0.5501511096954346\n",
      "Epoch: 12, Train_Loss: 0.6147816777229309, Test_Loss: 0.5553487539291382\n",
      "Epoch: 12, Train_Loss: 0.4872681200504303, Test_Loss: 0.5087116360664368 *\n",
      "Epoch: 12, Train_Loss: 0.6353036761283875, Test_Loss: 0.4944912791252136 *\n",
      "Epoch: 12, Train_Loss: 0.5352157950401306, Test_Loss: 0.5161994695663452\n",
      "Epoch: 12, Train_Loss: 0.49829351902008057, Test_Loss: 0.5073133707046509 *\n",
      "Epoch: 12, Train_Loss: 0.4862784445285797, Test_Loss: 0.4927203059196472 *\n",
      "Epoch: 12, Train_Loss: 0.521301805973053, Test_Loss: 0.6005474328994751\n",
      "Epoch: 12, Train_Loss: 0.700046181678772, Test_Loss: 0.5475976467132568 *\n",
      "Epoch: 12, Train_Loss: 0.7357152104377747, Test_Loss: 5.945420265197754\n",
      "Epoch: 12, Train_Loss: 0.7719695568084717, Test_Loss: 0.5309117436408997 *\n",
      "Epoch: 12, Train_Loss: 0.968998908996582, Test_Loss: 0.5164292454719543 *\n",
      "Epoch: 12, Train_Loss: 0.7165166139602661, Test_Loss: 0.5224433541297913\n",
      "Epoch: 12, Train_Loss: 0.6993274092674255, Test_Loss: 0.47676321864128113 *\n",
      "Epoch: 12, Train_Loss: 0.5572338104248047, Test_Loss: 0.49622929096221924\n",
      "Epoch: 12, Train_Loss: 0.4892809987068176, Test_Loss: 0.4880029559135437 *\n",
      "Epoch: 12, Train_Loss: 0.48856574296951294, Test_Loss: 0.5401579141616821\n",
      "Epoch: 12, Train_Loss: 0.48487916588783264, Test_Loss: 0.5140150189399719 *\n",
      "Epoch: 12, Train_Loss: 0.6424845457077026, Test_Loss: 0.48323938250541687 *\n",
      "Epoch: 12, Train_Loss: 0.7949904799461365, Test_Loss: 0.495161235332489\n",
      "Epoch: 12, Train_Loss: 0.73433518409729, Test_Loss: 0.5679484605789185\n",
      "Epoch: 12, Train_Loss: 1.8694782257080078, Test_Loss: 0.5219642519950867 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train_Loss: 1.1216152906417847, Test_Loss: 0.5322738885879517\n",
      "Epoch: 12, Train_Loss: 0.8207945823669434, Test_Loss: 0.47596505284309387 *\n",
      "Epoch: 12, Train_Loss: 0.5609306693077087, Test_Loss: 0.5488957762718201\n",
      "Epoch: 12, Train_Loss: 0.4876837730407715, Test_Loss: 0.48372548818588257 *\n",
      "Epoch: 12, Train_Loss: 0.6469735503196716, Test_Loss: 0.5306809544563293\n",
      "Epoch: 12, Train_Loss: 1.2637608051300049, Test_Loss: 0.5557955503463745\n",
      "Epoch: 12, Train_Loss: 1.1419368982315063, Test_Loss: 0.48420146107673645 *\n",
      "Epoch: 12, Train_Loss: 0.5051401257514954, Test_Loss: 0.5201017260551453\n",
      "Epoch: 12, Train_Loss: 0.5603989362716675, Test_Loss: 0.5200772881507874 *\n",
      "Epoch: 12, Train_Loss: 0.5568021535873413, Test_Loss: 0.52921462059021\n",
      "Epoch: 12, Train_Loss: 0.789351761341095, Test_Loss: 0.5087543725967407 *\n",
      "Epoch: 12, Train_Loss: 0.6446702480316162, Test_Loss: 0.5205820798873901\n",
      "Epoch: 12, Train_Loss: 0.60699462890625, Test_Loss: 0.5625516176223755\n",
      "Epoch: 12, Train_Loss: 0.5936325788497925, Test_Loss: 0.5147398114204407 *\n",
      "Epoch: 12, Train_Loss: 0.5642214417457581, Test_Loss: 0.5141595005989075 *\n",
      "Epoch: 12, Train_Loss: 0.4953455328941345, Test_Loss: 0.5592735409736633\n",
      "Epoch: 12, Train_Loss: 0.5269533395767212, Test_Loss: 0.5515280365943909 *\n",
      "Epoch: 12, Train_Loss: 0.5146170258522034, Test_Loss: 0.4815789461135864 *\n",
      "Epoch: 12, Train_Loss: 0.5360170602798462, Test_Loss: 0.54791259765625\n",
      "Epoch: 12, Train_Loss: 0.5124651193618774, Test_Loss: 0.5761290788650513\n",
      "Epoch: 12, Train_Loss: 0.6467204093933105, Test_Loss: 0.611596941947937\n",
      "Epoch: 12, Train_Loss: 16.0662784576416, Test_Loss: 0.5224303603172302 *\n",
      "Epoch: 12, Train_Loss: 0.6917501091957092, Test_Loss: 0.5013574361801147 *\n",
      "Epoch: 12, Train_Loss: 2.234269857406616, Test_Loss: 0.4801821708679199 *\n",
      "Epoch: 12, Train_Loss: 1.2717586755752563, Test_Loss: 0.5027612447738647\n",
      "Epoch: 12, Train_Loss: 0.5132680535316467, Test_Loss: 0.5293658375740051\n",
      "Epoch: 12, Train_Loss: 0.6078447103500366, Test_Loss: 0.6549230217933655\n",
      "Model saved at location save_new\\model.ckpt at epoch 12\n",
      "Epoch: 12, Train_Loss: 6.806757926940918, Test_Loss: 0.604454755783081 *\n",
      "Epoch: 12, Train_Loss: 3.5389158725738525, Test_Loss: 0.6550332903862\n",
      "Epoch: 12, Train_Loss: 0.5402631759643555, Test_Loss: 0.5114694833755493 *\n",
      "Epoch: 12, Train_Loss: 1.104524850845337, Test_Loss: 0.6360172033309937\n",
      "Epoch: 12, Train_Loss: 5.024389743804932, Test_Loss: 0.9524230360984802\n",
      "Epoch: 12, Train_Loss: 0.8070188164710999, Test_Loss: 1.1105573177337646\n",
      "Epoch: 12, Train_Loss: 0.6088331937789917, Test_Loss: 1.0694948434829712 *\n",
      "Epoch: 12, Train_Loss: 0.6719740629196167, Test_Loss: 0.8428714275360107 *\n",
      "Epoch: 12, Train_Loss: 0.6227180361747742, Test_Loss: 0.8404282927513123 *\n",
      "Epoch: 12, Train_Loss: 0.5933904051780701, Test_Loss: 0.8146907091140747 *\n",
      "Epoch: 12, Train_Loss: 0.4732833206653595, Test_Loss: 0.6705260276794434 *\n",
      "Epoch: 12, Train_Loss: 0.48016801476478577, Test_Loss: 1.0241233110427856\n",
      "Epoch: 12, Train_Loss: 0.46963727474212646, Test_Loss: 0.5398549437522888 *\n",
      "Epoch: 12, Train_Loss: 0.48122334480285645, Test_Loss: 0.6089211106300354\n",
      "Epoch: 12, Train_Loss: 0.5014872550964355, Test_Loss: 0.8106023073196411\n",
      "Epoch: 12, Train_Loss: 0.4887832999229431, Test_Loss: 0.7824109792709351 *\n",
      "Epoch: 12, Train_Loss: 0.5516101121902466, Test_Loss: 0.7485644817352295 *\n",
      "Epoch: 12, Train_Loss: 0.5457700490951538, Test_Loss: 0.648633599281311 *\n",
      "Epoch: 12, Train_Loss: 0.5019111633300781, Test_Loss: 0.5412744283676147 *\n",
      "Epoch: 12, Train_Loss: 0.48918798565864563, Test_Loss: 6.378329277038574\n",
      "Epoch: 12, Train_Loss: 0.49403658509254456, Test_Loss: 1.0470210313796997 *\n",
      "Epoch: 12, Train_Loss: 0.48366451263427734, Test_Loss: 0.5609062314033508 *\n",
      "Epoch: 12, Train_Loss: 0.49120068550109863, Test_Loss: 0.5364967584609985 *\n",
      "Epoch: 12, Train_Loss: 0.5042367577552795, Test_Loss: 0.5285549163818359 *\n",
      "Epoch: 12, Train_Loss: 0.4861457049846649, Test_Loss: 0.484853059053421 *\n",
      "Epoch: 12, Train_Loss: 0.48053935170173645, Test_Loss: 0.6014403104782104\n",
      "Epoch: 12, Train_Loss: 0.4908834397792816, Test_Loss: 0.6422748565673828\n",
      "Epoch: 12, Train_Loss: 0.48155903816223145, Test_Loss: 0.5365244150161743 *\n",
      "Epoch: 12, Train_Loss: 0.47388628125190735, Test_Loss: 0.5710862278938293\n",
      "Epoch: 12, Train_Loss: 0.4729345440864563, Test_Loss: 0.58016437292099\n",
      "Epoch: 12, Train_Loss: 0.49358999729156494, Test_Loss: 0.6579753756523132\n",
      "Epoch: 12, Train_Loss: 0.4859750270843506, Test_Loss: 0.5675867199897766 *\n",
      "Epoch: 12, Train_Loss: 0.5774537324905396, Test_Loss: 0.5150215029716492 *\n",
      "Epoch: 12, Train_Loss: 0.49161142110824585, Test_Loss: 0.5497675538063049\n",
      "Epoch: 12, Train_Loss: 0.4900265038013458, Test_Loss: 0.5114152431488037 *\n",
      "Epoch: 12, Train_Loss: 6.176331043243408, Test_Loss: 0.48663094639778137 *\n",
      "Epoch: 12, Train_Loss: 3.612755298614502, Test_Loss: 0.5002952218055725\n",
      "Epoch: 12, Train_Loss: 0.48076537251472473, Test_Loss: 0.5637549161911011\n",
      "Epoch: 12, Train_Loss: 0.5150370597839355, Test_Loss: 0.5606987476348877 *\n",
      "Epoch: 12, Train_Loss: 0.5721619129180908, Test_Loss: 0.5673581957817078\n",
      "Epoch: 12, Train_Loss: 0.49654844403266907, Test_Loss: 0.5646075010299683 *\n",
      "Epoch: 12, Train_Loss: 0.49118417501449585, Test_Loss: 0.6190145015716553\n",
      "Epoch: 12, Train_Loss: 0.5449312925338745, Test_Loss: 0.591787576675415 *\n",
      "Epoch: 12, Train_Loss: 0.6209802627563477, Test_Loss: 0.6047075986862183\n",
      "Epoch: 12, Train_Loss: 0.6590430736541748, Test_Loss: 0.5175519585609436 *\n",
      "Epoch: 12, Train_Loss: 0.5806943774223328, Test_Loss: 0.5249069929122925\n",
      "Epoch: 12, Train_Loss: 0.49050289392471313, Test_Loss: 0.5482599139213562\n",
      "Epoch: 12, Train_Loss: 0.5219538807868958, Test_Loss: 0.5182967185974121 *\n",
      "Epoch: 12, Train_Loss: 0.5622665286064148, Test_Loss: 0.5089333057403564 *\n",
      "Epoch: 12, Train_Loss: 0.6033101081848145, Test_Loss: 0.6078025698661804\n",
      "Epoch: 12, Train_Loss: 0.5419459939002991, Test_Loss: 1.2315694093704224\n",
      "Epoch: 12, Train_Loss: 0.519201934337616, Test_Loss: 5.479516506195068\n",
      "Epoch: 12, Train_Loss: 0.5074485540390015, Test_Loss: 0.47172895073890686 *\n",
      "Epoch: 12, Train_Loss: 0.47122398018836975, Test_Loss: 0.46394646167755127 *\n",
      "Epoch: 12, Train_Loss: 0.5411015748977661, Test_Loss: 0.4937037229537964\n",
      "Epoch: 12, Train_Loss: 0.500396192073822, Test_Loss: 0.4680393636226654 *\n",
      "Epoch: 12, Train_Loss: 0.47209876775741577, Test_Loss: 0.48759424686431885\n",
      "Epoch: 12, Train_Loss: 0.4641912281513214, Test_Loss: 0.48001399636268616 *\n",
      "Epoch: 12, Train_Loss: 0.4656873643398285, Test_Loss: 0.5889818668365479\n",
      "Epoch: 12, Train_Loss: 0.48705482482910156, Test_Loss: 0.4916241466999054 *\n",
      "Epoch: 12, Train_Loss: 4.979572772979736, Test_Loss: 0.4660842716693878 *\n",
      "Epoch: 12, Train_Loss: 1.519351840019226, Test_Loss: 0.49657827615737915\n",
      "Epoch: 12, Train_Loss: 0.46447089314460754, Test_Loss: 0.48172062635421753 *\n",
      "Epoch: 12, Train_Loss: 0.48135218024253845, Test_Loss: 0.4685242176055908 *\n",
      "Epoch: 12, Train_Loss: 0.4722292423248291, Test_Loss: 0.5117397308349609\n",
      "Epoch: 12, Train_Loss: 0.4666404724121094, Test_Loss: 0.4747709333896637 *\n",
      "Epoch: 12, Train_Loss: 0.4662722945213318, Test_Loss: 0.5404802560806274\n",
      "Epoch: 12, Train_Loss: 0.4685560166835785, Test_Loss: 0.5572713613510132\n",
      "Epoch: 12, Train_Loss: 0.48399919271469116, Test_Loss: 0.4932524561882019 *\n",
      "Epoch: 12, Train_Loss: 0.46841105818748474, Test_Loss: 0.4916316568851471 *\n",
      "Epoch: 12, Train_Loss: 0.5055610537528992, Test_Loss: 0.4690777361392975 *\n",
      "Epoch: 12, Train_Loss: 0.46348321437835693, Test_Loss: 0.4841887652873993\n",
      "Epoch: 12, Train_Loss: 0.46268007159233093, Test_Loss: 0.4751308858394623 *\n",
      "Epoch: 12, Train_Loss: 0.4784761369228363, Test_Loss: 0.4738982319831848 *\n",
      "Epoch: 12, Train_Loss: 0.46379849314689636, Test_Loss: 0.4697556793689728 *\n",
      "Epoch: 12, Train_Loss: 0.4612171947956085, Test_Loss: 0.4705837070941925\n",
      "Epoch: 12, Train_Loss: 0.4703853130340576, Test_Loss: 0.4814591109752655\n",
      "Epoch: 12, Train_Loss: 0.4930126368999481, Test_Loss: 0.46490299701690674 *\n",
      "Epoch: 12, Train_Loss: 0.4838949143886566, Test_Loss: 0.46301722526550293 *\n",
      "Epoch: 12, Train_Loss: 0.46184471249580383, Test_Loss: 0.49227336049079895\n",
      "Epoch: 12, Train_Loss: 0.4613112509250641, Test_Loss: 0.4660215973854065 *\n",
      "Epoch: 12, Train_Loss: 0.5099278688430786, Test_Loss: 0.474780797958374\n",
      "Epoch: 12, Train_Loss: 0.4882654547691345, Test_Loss: 0.5131340026855469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train_Loss: 0.47799813747406006, Test_Loss: 0.6357879638671875\n",
      "Epoch: 12, Train_Loss: 0.4828976094722748, Test_Loss: 0.6965504288673401\n",
      "Epoch: 12, Train_Loss: 0.5659160017967224, Test_Loss: 0.5717847943305969 *\n",
      "Epoch: 12, Train_Loss: 0.5547562837600708, Test_Loss: 0.48300108313560486 *\n",
      "Epoch: 12, Train_Loss: 0.4985538125038147, Test_Loss: 0.4765416085720062 *\n",
      "Epoch: 12, Train_Loss: 0.5211119651794434, Test_Loss: 0.47388753294944763 *\n",
      "Epoch: 12, Train_Loss: 0.7411282062530518, Test_Loss: 0.5459907054901123\n",
      "Epoch: 12, Train_Loss: 0.5008600950241089, Test_Loss: 0.9554794430732727\n",
      "Epoch: 12, Train_Loss: 0.48633331060409546, Test_Loss: 0.9955621957778931\n",
      "Epoch: 12, Train_Loss: 0.45951128005981445, Test_Loss: 0.5176444053649902 *\n",
      "Epoch: 12, Train_Loss: 0.4582426846027374, Test_Loss: 0.5008925795555115 *\n",
      "Epoch: 12, Train_Loss: 0.4577968120574951, Test_Loss: 0.4645627439022064 *\n",
      "Epoch: 12, Train_Loss: 0.4575519263744354, Test_Loss: 0.4717223644256592\n",
      "Epoch: 12, Train_Loss: 0.45888176560401917, Test_Loss: 0.4730643928050995\n",
      "Epoch: 12, Train_Loss: 4.546182632446289, Test_Loss: 0.4812769293785095\n",
      "Epoch: 12, Train_Loss: 1.3537914752960205, Test_Loss: 0.49871736764907837\n",
      "Epoch: 12, Train_Loss: 0.4581374526023865, Test_Loss: 0.4751775562763214 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 12\n",
      "Epoch: 12, Train_Loss: 0.4706028401851654, Test_Loss: 0.4662441909313202 *\n",
      "Epoch: 12, Train_Loss: 0.46129918098449707, Test_Loss: 0.5700515508651733\n",
      "Epoch: 12, Train_Loss: 0.4572196900844574, Test_Loss: 0.8542701005935669\n",
      "Epoch: 12, Train_Loss: 0.4570407569408417, Test_Loss: 0.6724882125854492 *\n",
      "Epoch: 12, Train_Loss: 0.4566064476966858, Test_Loss: 0.5176072120666504 *\n",
      "Epoch: 12, Train_Loss: 0.45626300573349, Test_Loss: 0.4696137011051178 *\n",
      "Epoch: 12, Train_Loss: 0.4571867883205414, Test_Loss: 0.46976029872894287\n",
      "Epoch: 12, Train_Loss: 0.48443272709846497, Test_Loss: 0.47032734751701355\n",
      "Epoch: 12, Train_Loss: 0.5143080949783325, Test_Loss: 0.4711727797985077\n",
      "Epoch: 12, Train_Loss: 0.5301603674888611, Test_Loss: 0.6006472110748291\n",
      "Epoch: 12, Train_Loss: 0.5246697068214417, Test_Loss: 5.642520427703857\n",
      "Epoch: 12, Train_Loss: 0.4647224545478821, Test_Loss: 0.5639216899871826 *\n",
      "Epoch: 12, Train_Loss: 0.4960087537765503, Test_Loss: 0.46591275930404663 *\n",
      "Epoch: 12, Train_Loss: 0.6765362620353699, Test_Loss: 0.45960819721221924 *\n",
      "Epoch: 12, Train_Loss: 0.6777712106704712, Test_Loss: 0.4641360640525818\n",
      "Epoch: 12, Train_Loss: 0.6661630868911743, Test_Loss: 0.46314913034439087 *\n",
      "Epoch: 12, Train_Loss: 0.48780083656311035, Test_Loss: 0.4575515687465668 *\n",
      "Epoch: 12, Train_Loss: 0.45523297786712646, Test_Loss: 0.46337300539016724\n",
      "Epoch: 12, Train_Loss: 0.4548379182815552, Test_Loss: 0.45680713653564453 *\n",
      "Epoch: 12, Train_Loss: 0.4625180661678314, Test_Loss: 0.45727503299713135\n",
      "Epoch: 12, Train_Loss: 0.4681156873703003, Test_Loss: 0.4608727693557739\n",
      "Epoch: 12, Train_Loss: 0.4659048318862915, Test_Loss: 0.4714663028717041\n",
      "Epoch: 12, Train_Loss: 0.46366652846336365, Test_Loss: 0.4648706018924713 *\n",
      "Epoch: 12, Train_Loss: 0.45405569672584534, Test_Loss: 0.46655285358428955\n",
      "Epoch: 12, Train_Loss: 0.45545345544815063, Test_Loss: 0.4682820737361908\n",
      "Epoch: 12, Train_Loss: 0.47066035866737366, Test_Loss: 0.4551181197166443 *\n",
      "Epoch: 12, Train_Loss: 0.5777088403701782, Test_Loss: 0.45501264929771423 *\n",
      "Epoch: 12, Train_Loss: 0.6587727665901184, Test_Loss: 0.45475494861602783 *\n",
      "Epoch: 12, Train_Loss: 0.6414746642112732, Test_Loss: 0.46005380153656006\n",
      "Epoch: 12, Train_Loss: 0.5299718379974365, Test_Loss: 0.4549468457698822 *\n",
      "Epoch: 12, Train_Loss: 0.577630341053009, Test_Loss: 0.4570442736148834\n",
      "Epoch: 12, Train_Loss: 0.596498966217041, Test_Loss: 0.45508041977882385 *\n",
      "Epoch: 12, Train_Loss: 0.46251726150512695, Test_Loss: 0.46205705404281616\n",
      "Epoch: 12, Train_Loss: 0.6232303380966187, Test_Loss: 0.4641417860984802\n",
      "Epoch: 12, Train_Loss: 0.5634297132492065, Test_Loss: 0.45878374576568604 *\n",
      "Epoch: 12, Train_Loss: 0.7020326852798462, Test_Loss: 0.4553253948688507 *\n",
      "Epoch: 12, Train_Loss: 0.46162593364715576, Test_Loss: 0.4564824104309082\n",
      "Epoch: 12, Train_Loss: 1.3817157745361328, Test_Loss: 0.4548308849334717 *\n",
      "Epoch: 12, Train_Loss: 2.553501605987549, Test_Loss: 0.45565786957740784\n",
      "Epoch: 12, Train_Loss: 0.49309858679771423, Test_Loss: 0.4622392952442169\n",
      "Epoch: 12, Train_Loss: 0.5184548497200012, Test_Loss: 0.516208291053772\n",
      "Epoch: 12, Train_Loss: 0.4951683580875397, Test_Loss: 2.3600826263427734\n",
      "Epoch: 12, Train_Loss: 0.49176180362701416, Test_Loss: 4.002782821655273\n",
      "Epoch: 12, Train_Loss: 0.4528599679470062, Test_Loss: 0.45573845505714417 *\n",
      "Epoch: 12, Train_Loss: 0.4587695300579071, Test_Loss: 0.45216241478919983 *\n",
      "Epoch: 12, Train_Loss: 0.5634347200393677, Test_Loss: 0.5050461292266846\n",
      "Epoch: 12, Train_Loss: 0.5468012094497681, Test_Loss: 0.47988075017929077 *\n",
      "Epoch: 12, Train_Loss: 0.5269728302955627, Test_Loss: 0.5010136365890503\n",
      "Epoch: 12, Train_Loss: 0.49155816435813904, Test_Loss: 0.48503169417381287 *\n",
      "Epoch: 12, Train_Loss: 0.4806487262248993, Test_Loss: 0.5558386445045471\n",
      "Epoch: 12, Train_Loss: 0.4732885956764221, Test_Loss: 0.4590127766132355 *\n",
      "Epoch: 12, Train_Loss: 0.47385311126708984, Test_Loss: 0.46568015217781067\n",
      "Epoch: 12, Train_Loss: 0.47840753197669983, Test_Loss: 0.47354036569595337\n",
      "Epoch: 12, Train_Loss: 0.4870370328426361, Test_Loss: 0.4823872745037079\n",
      "Epoch: 12, Train_Loss: 0.4647688567638397, Test_Loss: 0.4580334722995758 *\n",
      "Epoch: 12, Train_Loss: 0.4525068998336792, Test_Loss: 0.5201709866523743\n",
      "Epoch: 12, Train_Loss: 0.47202610969543457, Test_Loss: 0.514033317565918 *\n",
      "Epoch: 12, Train_Loss: 0.47393599152565, Test_Loss: 0.5053660273551941 *\n",
      "Epoch: 12, Train_Loss: 0.46456456184387207, Test_Loss: 0.5040313005447388 *\n",
      "Epoch: 12, Train_Loss: 0.45553573966026306, Test_Loss: 0.4757711589336395 *\n",
      "Epoch: 12, Train_Loss: 0.45085346698760986, Test_Loss: 0.4958530068397522\n",
      "Epoch: 12, Train_Loss: 0.449851393699646, Test_Loss: 0.4614337682723999 *\n",
      "Epoch: 12, Train_Loss: 0.451472669839859, Test_Loss: 0.4602455794811249 *\n",
      "Epoch: 12, Train_Loss: 0.45239925384521484, Test_Loss: 0.4626788794994354\n",
      "Epoch: 12, Train_Loss: 0.45688116550445557, Test_Loss: 0.4695199728012085\n",
      "Epoch: 12, Train_Loss: 0.45810604095458984, Test_Loss: 0.4650346040725708 *\n",
      "Epoch: 12, Train_Loss: 0.4502510726451874, Test_Loss: 0.46228569746017456 *\n",
      "Epoch: 12, Train_Loss: 0.44889241456985474, Test_Loss: 0.46565917134284973\n",
      "Epoch: 12, Train_Loss: 0.45170408487319946, Test_Loss: 0.46029776334762573 *\n",
      "Epoch: 12, Train_Loss: 0.4619002640247345, Test_Loss: 0.4546872079372406 *\n",
      "Epoch: 12, Train_Loss: 0.4723306894302368, Test_Loss: 0.4557763934135437\n",
      "Epoch: 12, Train_Loss: 0.4649430215358734, Test_Loss: 0.46178629994392395\n",
      "Epoch: 12, Train_Loss: 0.4732285439968109, Test_Loss: 0.5077396035194397\n",
      "Epoch: 12, Train_Loss: 0.4559303820133209, Test_Loss: 0.4609086215496063 *\n",
      "Epoch: 12, Train_Loss: 0.4636021554470062, Test_Loss: 0.895348310470581\n",
      "Epoch: 12, Train_Loss: 0.44872918725013733, Test_Loss: 0.8930531740188599 *\n",
      "Epoch: 12, Train_Loss: 0.45003682374954224, Test_Loss: 0.596454918384552 *\n",
      "Epoch: 12, Train_Loss: 0.4730831980705261, Test_Loss: 0.47239449620246887 *\n",
      "Epoch: 12, Train_Loss: 0.4600001275539398, Test_Loss: 0.47317665815353394\n",
      "Epoch: 12, Train_Loss: 0.4514503479003906, Test_Loss: 0.4662407636642456 *\n",
      "Epoch: 12, Train_Loss: 0.4482910931110382, Test_Loss: 0.5789239406585693\n",
      "Epoch: 12, Train_Loss: 0.45509135723114014, Test_Loss: 0.998041033744812\n",
      "Epoch: 12, Train_Loss: 0.49411675333976746, Test_Loss: 0.893875002861023 *\n",
      "Epoch: 12, Train_Loss: 0.4866431653499603, Test_Loss: 0.505385160446167 *\n",
      "Epoch: 12, Train_Loss: 0.4968358874320984, Test_Loss: 0.5005960464477539 *\n",
      "Epoch: 12, Train_Loss: 0.44704771041870117, Test_Loss: 0.4505091905593872 *\n",
      "Epoch: 12, Train_Loss: 0.4911676049232483, Test_Loss: 0.4501784145832062 *\n",
      "Epoch: 12, Train_Loss: 0.4905138611793518, Test_Loss: 0.4554767906665802\n",
      "Epoch: 12, Train_Loss: 0.44889411330223083, Test_Loss: 0.4571469724178314\n",
      "Epoch: 12, Train_Loss: 0.4611701965332031, Test_Loss: 0.5009539127349854\n",
      "Epoch: 12, Train_Loss: 0.4723091125488281, Test_Loss: 0.4488160014152527 *\n",
      "Epoch: 12, Train_Loss: 0.5322132110595703, Test_Loss: 0.4741073548793793\n",
      "Epoch: 12, Train_Loss: 0.5217296481132507, Test_Loss: 0.5736361742019653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train_Loss: 0.49664902687072754, Test_Loss: 0.8287569284439087\n",
      "Epoch: 12, Train_Loss: 0.4710310399532318, Test_Loss: 0.6985408067703247 *\n",
      "Epoch: 12, Train_Loss: 0.45225751399993896, Test_Loss: 0.4674699902534485 *\n",
      "Epoch: 12, Train_Loss: 0.4664340615272522, Test_Loss: 0.4583425521850586 *\n",
      "Epoch: 12, Train_Loss: 0.4457862973213196, Test_Loss: 0.4578763246536255 *\n",
      "Epoch: 12, Train_Loss: 0.4515787363052368, Test_Loss: 0.4580375850200653\n",
      "Epoch: 12, Train_Loss: 0.45477062463760376, Test_Loss: 0.4627038538455963\n",
      "Model saved at location save_new\\model.ckpt at epoch 12\n",
      "Epoch: 12, Train_Loss: 0.4621608257293701, Test_Loss: 1.1181226968765259\n",
      "Epoch: 12, Train_Loss: 0.550081729888916, Test_Loss: 5.191469192504883\n",
      "Epoch: 12, Train_Loss: 0.44669458270072937, Test_Loss: 0.4757153391838074 *\n",
      "Epoch: 12, Train_Loss: 0.5102790594100952, Test_Loss: 0.45099377632141113 *\n",
      "Epoch: 12, Train_Loss: 0.44830188155174255, Test_Loss: 0.4485118091106415 *\n",
      "Epoch: 12, Train_Loss: 0.4795912802219391, Test_Loss: 0.45232686400413513\n",
      "Epoch: 12, Train_Loss: 0.45974236726760864, Test_Loss: 0.4492877125740051 *\n",
      "Epoch: 12, Train_Loss: 0.7342962622642517, Test_Loss: 0.44910043478012085 *\n",
      "Epoch: 12, Train_Loss: 0.4910164475440979, Test_Loss: 0.4564327299594879\n",
      "Epoch: 12, Train_Loss: 0.47979745268821716, Test_Loss: 0.44585296511650085 *\n",
      "Epoch: 12, Train_Loss: 0.4477800130844116, Test_Loss: 0.4494492709636688\n",
      "Epoch: 12, Train_Loss: 0.44400250911712646, Test_Loss: 0.45333316922187805\n",
      "Epoch: 12, Train_Loss: 0.4440343677997589, Test_Loss: 0.4678478240966797\n",
      "Epoch: 12, Train_Loss: 0.4454794228076935, Test_Loss: 0.4528832733631134 *\n",
      "Epoch: 12, Train_Loss: 0.4482966661453247, Test_Loss: 0.4481668174266815 *\n",
      "Epoch: 12, Train_Loss: 0.44871893525123596, Test_Loss: 0.4585123360157013\n",
      "Epoch: 12, Train_Loss: 0.4637226462364197, Test_Loss: 0.4438963234424591 *\n",
      "Epoch: 12, Train_Loss: 0.45394301414489746, Test_Loss: 0.44514721632003784\n",
      "Epoch: 12, Train_Loss: 0.4540044367313385, Test_Loss: 0.44390830397605896 *\n",
      "Epoch: 12, Train_Loss: 0.4558893144130707, Test_Loss: 0.4545479714870453\n",
      "Epoch: 12, Train_Loss: 0.44462698698043823, Test_Loss: 0.44354528188705444 *\n",
      "Epoch: 12, Train_Loss: 0.4415675699710846, Test_Loss: 0.4480702579021454\n",
      "Epoch: 12, Train_Loss: 0.4543328881263733, Test_Loss: 0.4437735080718994 *\n",
      "Epoch: 12, Train_Loss: 0.4669620990753174, Test_Loss: 0.45173248648643494\n",
      "Epoch: 12, Train_Loss: 0.4850001633167267, Test_Loss: 0.45314526557922363\n",
      "Epoch: 12, Train_Loss: 0.44383007287979126, Test_Loss: 0.44653627276420593 *\n",
      "Epoch: 12, Train_Loss: 0.4706590175628662, Test_Loss: 0.4430926442146301 *\n",
      "Epoch: 12, Train_Loss: 0.49304822087287903, Test_Loss: 0.4462087154388428\n",
      "Epoch: 12, Train_Loss: 0.47693270444869995, Test_Loss: 0.4449041187763214 *\n",
      "Epoch: 12, Train_Loss: 0.4414130747318268, Test_Loss: 0.44348442554473877 *\n",
      "Epoch: 12, Train_Loss: 0.4785887598991394, Test_Loss: 0.4666747450828552\n",
      "Epoch: 12, Train_Loss: 0.44564583897590637, Test_Loss: 0.4975184202194214\n",
      "Epoch: 12, Train_Loss: 0.45786499977111816, Test_Loss: 3.4689242839813232\n",
      "Epoch: 12, Train_Loss: 0.4418967664241791, Test_Loss: 2.943068742752075 *\n",
      "Epoch: 12, Train_Loss: 0.46691083908081055, Test_Loss: 0.4420413374900818 *\n",
      "Epoch: 12, Train_Loss: 0.5112894773483276, Test_Loss: 0.44167912006378174 *\n",
      "Epoch: 12, Train_Loss: 3.0843935012817383, Test_Loss: 0.49102601408958435\n",
      "Epoch: 12, Train_Loss: 3.1537368297576904, Test_Loss: 0.46750879287719727 *\n",
      "Epoch: 12, Train_Loss: 0.46244704723358154, Test_Loss: 0.4746342599391937\n",
      "Epoch: 12, Train_Loss: 0.44088637828826904, Test_Loss: 0.5019351243972778\n",
      "Epoch: 12, Train_Loss: 0.5474164485931396, Test_Loss: 0.5390370488166809\n",
      "Epoch: 12, Train_Loss: 0.5647191405296326, Test_Loss: 0.443215936422348 *\n",
      "Epoch: 12, Train_Loss: 0.4618890881538391, Test_Loss: 0.4631265699863434\n",
      "Epoch: 12, Train_Loss: 0.43978351354599, Test_Loss: 0.4597458243370056 *\n",
      "Epoch: 12, Train_Loss: 0.48109564185142517, Test_Loss: 0.45961397886276245 *\n",
      "Epoch: 12, Train_Loss: 0.4745643436908722, Test_Loss: 0.4453657269477844 *\n",
      "Epoch: 12, Train_Loss: 0.44954341650009155, Test_Loss: 0.5064646601676941\n",
      "Epoch: 12, Train_Loss: 0.5226691961288452, Test_Loss: 0.49122676253318787 *\n",
      "Epoch: 12, Train_Loss: 1.405349612236023, Test_Loss: 0.512562096118927\n",
      "Epoch: 12, Train_Loss: 1.7460718154907227, Test_Loss: 0.4944213926792145 *\n",
      "Epoch: 12, Train_Loss: 0.5428348779678345, Test_Loss: 0.4716097116470337 *\n",
      "Epoch: 12, Train_Loss: 0.5104029774665833, Test_Loss: 0.46497640013694763 *\n",
      "Epoch: 12, Train_Loss: 2.0271291732788086, Test_Loss: 0.44450071454048157 *\n",
      "Epoch: 12, Train_Loss: 1.8161485195159912, Test_Loss: 0.44465145468711853\n",
      "Epoch: 12, Train_Loss: 0.4690374433994293, Test_Loss: 0.44476065039634705\n",
      "Epoch: 12, Train_Loss: 0.4483260214328766, Test_Loss: 0.45166340470314026\n",
      "Epoch: 12, Train_Loss: 0.7014398574829102, Test_Loss: 0.4505417048931122 *\n",
      "Epoch: 12, Train_Loss: 1.4199352264404297, Test_Loss: 0.4398159980773926 *\n",
      "Epoch: 12, Train_Loss: 1.3574731349945068, Test_Loss: 0.44555434584617615\n",
      "Epoch: 12, Train_Loss: 0.44069281220436096, Test_Loss: 0.4390799403190613 *\n",
      "Epoch: 12, Train_Loss: 0.4565680921077728, Test_Loss: 0.443851500749588\n",
      "Epoch: 12, Train_Loss: 0.510094404220581, Test_Loss: 0.4640929102897644\n",
      "Epoch: 12, Train_Loss: 0.9499597549438477, Test_Loss: 0.4396083354949951 *\n",
      "Epoch: 12, Train_Loss: 0.4740448594093323, Test_Loss: 0.46071189641952515\n",
      "Epoch: 12, Train_Loss: 0.5085629224777222, Test_Loss: 0.4882369637489319\n",
      "Epoch: 12, Train_Loss: 0.48090630769729614, Test_Loss: 0.7611549496650696\n",
      "Epoch: 12, Train_Loss: 0.5165095329284668, Test_Loss: 0.7430588006973267 *\n",
      "Epoch: 12, Train_Loss: 0.5566616058349609, Test_Loss: 0.5259941220283508 *\n",
      "Epoch: 12, Train_Loss: 0.6163265705108643, Test_Loss: 0.4462357759475708 *\n",
      "Epoch: 12, Train_Loss: 0.5444429516792297, Test_Loss: 0.4711453318595886\n",
      "Epoch: 12, Train_Loss: 0.4836146831512451, Test_Loss: 0.46436116099357605 *\n",
      "Epoch: 12, Train_Loss: 0.5288683772087097, Test_Loss: 0.5563184022903442\n",
      "Epoch: 12, Train_Loss: 0.5465843677520752, Test_Loss: 0.6636009216308594\n",
      "Epoch: 12, Train_Loss: 0.695260226726532, Test_Loss: 0.765578031539917\n",
      "Epoch: 12, Train_Loss: 0.6732993721961975, Test_Loss: 0.4886970520019531 *\n",
      "Epoch: 12, Train_Loss: 0.4740617275238037, Test_Loss: 0.479492723941803 *\n",
      "Epoch: 12, Train_Loss: 0.537837028503418, Test_Loss: 0.4558143615722656 *\n",
      "Epoch: 12, Train_Loss: 0.5298819541931152, Test_Loss: 0.44868406653404236 *\n",
      "Epoch: 12, Train_Loss: 0.4812469482421875, Test_Loss: 0.48110851645469666\n",
      "Epoch: 12, Train_Loss: 0.4474504590034485, Test_Loss: 0.4684963822364807 *\n",
      "Epoch: 12, Train_Loss: 0.4412674903869629, Test_Loss: 0.46373581886291504 *\n",
      "Epoch: 12, Train_Loss: 0.4405476152896881, Test_Loss: 0.4708985388278961\n",
      "Epoch: 12, Train_Loss: 0.4396800696849823, Test_Loss: 0.5289740562438965\n",
      "Epoch: 12, Train_Loss: 0.44566649198532104, Test_Loss: 0.5850763320922852\n",
      "Epoch: 12, Train_Loss: 0.4590094983577728, Test_Loss: 0.7776902914047241\n",
      "Epoch: 12, Train_Loss: 0.44574639201164246, Test_Loss: 0.8908236026763916\n",
      "Epoch: 12, Train_Loss: 0.464358389377594, Test_Loss: 0.4707760214805603 *\n",
      "Epoch: 12, Train_Loss: 0.5559874773025513, Test_Loss: 0.442222535610199 *\n",
      "Epoch: 12, Train_Loss: 0.7232661843299866, Test_Loss: 0.44412896037101746\n",
      "Epoch: 12, Train_Loss: 0.45463886857032776, Test_Loss: 0.4454997777938843\n",
      "Epoch: 12, Train_Loss: 0.4798169732093811, Test_Loss: 0.5424902439117432\n",
      "Epoch: 12, Train_Loss: 0.5501076579093933, Test_Loss: 2.2129995822906494\n",
      "Epoch: 12, Train_Loss: 0.546268105506897, Test_Loss: 3.8806369304656982\n",
      "Epoch: 12, Train_Loss: 0.6278829574584961, Test_Loss: 0.46553683280944824 *\n",
      "Epoch: 12, Train_Loss: 0.47756895422935486, Test_Loss: 0.4640887975692749 *\n",
      "Epoch: 12, Train_Loss: 0.6619881987571716, Test_Loss: 0.46821436285972595\n",
      "Epoch: 12, Train_Loss: 0.745050847530365, Test_Loss: 0.4401911497116089 *\n",
      "Epoch: 12, Train_Loss: 0.5821426510810852, Test_Loss: 0.4835891127586365\n",
      "Epoch: 12, Train_Loss: 0.4549235701560974, Test_Loss: 0.5072468519210815\n",
      "Epoch: 12, Train_Loss: 0.4648110568523407, Test_Loss: 0.49779075384140015 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 12\n",
      "Epoch: 12, Train_Loss: 0.5511011481285095, Test_Loss: 0.4584733247756958 *\n",
      "Epoch: 12, Train_Loss: 1.171915054321289, Test_Loss: 0.5135599374771118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train_Loss: 0.8674915432929993, Test_Loss: 0.5099164843559265 *\n",
      "Epoch: 12, Train_Loss: 0.46771129965782166, Test_Loss: 0.5663496255874634\n",
      "Epoch: 12, Train_Loss: 0.4655371308326721, Test_Loss: 0.45581528544425964 *\n",
      "Epoch: 12, Train_Loss: 0.4380777180194855, Test_Loss: 0.4692525565624237\n",
      "Epoch: 12, Train_Loss: 0.6956830024719238, Test_Loss: 0.5079528093338013\n",
      "Epoch: 12, Train_Loss: 0.7035056352615356, Test_Loss: 0.45069992542266846 *\n",
      "Epoch: 12, Train_Loss: 0.4380994737148285, Test_Loss: 0.46414613723754883\n",
      "Epoch: 12, Train_Loss: 0.6248557567596436, Test_Loss: 0.5133708715438843\n",
      "Epoch: 12, Train_Loss: 0.475248783826828, Test_Loss: 0.5362350940704346\n",
      "Epoch: 12, Train_Loss: 0.4499431550502777, Test_Loss: 0.4447113573551178 *\n",
      "Epoch: 12, Train_Loss: 0.46157994866371155, Test_Loss: 0.4661930799484253\n",
      "Epoch: 12, Train_Loss: 0.5868425369262695, Test_Loss: 0.459309458732605 *\n",
      "Epoch: 12, Train_Loss: 0.4692373275756836, Test_Loss: 0.5144559741020203\n",
      "Epoch: 12, Train_Loss: 0.5528483390808105, Test_Loss: 0.5266844034194946\n",
      "Epoch: 12, Train_Loss: 0.45278698205947876, Test_Loss: 0.4691062569618225 *\n",
      "Epoch: 12, Train_Loss: 0.5688962936401367, Test_Loss: 0.4466681480407715 *\n",
      "Epoch: 12, Train_Loss: 0.4632500410079956, Test_Loss: 0.48195022344589233\n",
      "Epoch: 12, Train_Loss: 0.45244771242141724, Test_Loss: 0.47700536251068115 *\n",
      "Epoch: 12, Train_Loss: 0.4443236291408539, Test_Loss: 0.4506607949733734 *\n",
      "Epoch: 12, Train_Loss: 0.4503977298736572, Test_Loss: 0.5756714940071106\n",
      "Epoch: 12, Train_Loss: 0.5236663222312927, Test_Loss: 0.5150237083435059 *\n",
      "Epoch: 12, Train_Loss: 0.6325560212135315, Test_Loss: 4.498537063598633\n",
      "Epoch: 12, Train_Loss: 0.5959519743919373, Test_Loss: 1.7425339221954346 *\n",
      "Epoch: 12, Train_Loss: 0.8605701923370361, Test_Loss: 0.4577125012874603 *\n",
      "Epoch: 12, Train_Loss: 0.7172681093215942, Test_Loss: 0.45411449670791626 *\n",
      "Epoch: 12, Train_Loss: 0.6484220623970032, Test_Loss: 0.44600439071655273 *\n",
      "Epoch: 12, Train_Loss: 0.5079755187034607, Test_Loss: 0.44203248620033264 *\n",
      "Epoch: 12, Train_Loss: 0.4588930010795593, Test_Loss: 0.4435117244720459\n",
      "Epoch: 12, Train_Loss: 0.4463132917881012, Test_Loss: 0.4947698712348938\n",
      "Epoch: 12, Train_Loss: 0.43799087405204773, Test_Loss: 0.47892120480537415 *\n",
      "Epoch: 12, Train_Loss: 0.5294738411903381, Test_Loss: 0.4379878044128418 *\n",
      "Epoch: 12, Train_Loss: 0.7539033889770508, Test_Loss: 0.4664878845214844\n",
      "Epoch: 12, Train_Loss: 0.7811246514320374, Test_Loss: 0.4993445873260498\n",
      "Epoch: 12, Train_Loss: 1.6058433055877686, Test_Loss: 0.5193144679069519\n",
      "Epoch: 12, Train_Loss: 1.3436450958251953, Test_Loss: 0.4539390504360199 *\n",
      "Epoch: 12, Train_Loss: 0.6473087072372437, Test_Loss: 0.4462603032588959 *\n",
      "Epoch: 12, Train_Loss: 0.619064450263977, Test_Loss: 0.49415767192840576\n",
      "Epoch: 12, Train_Loss: 0.4460277557373047, Test_Loss: 0.46354326605796814 *\n",
      "Epoch: 12, Train_Loss: 0.516819953918457, Test_Loss: 0.45272666215896606 *\n",
      "Epoch: 12, Train_Loss: 1.0059248208999634, Test_Loss: 0.5326247811317444\n",
      "Epoch: 12, Train_Loss: 1.2505141496658325, Test_Loss: 0.4610269069671631 *\n",
      "Epoch: 12, Train_Loss: 0.45583227276802063, Test_Loss: 0.4587145447731018 *\n",
      "Epoch: 12, Train_Loss: 0.4844118356704712, Test_Loss: 0.46644327044487\n",
      "Epoch: 12, Train_Loss: 0.5171826481819153, Test_Loss: 0.4686799645423889\n",
      "Epoch: 12, Train_Loss: 0.6856705546379089, Test_Loss: 0.45162802934646606 *\n",
      "Epoch: 12, Train_Loss: 0.5654454827308655, Test_Loss: 0.46049433946609497\n",
      "Epoch: 12, Train_Loss: 0.6644330620765686, Test_Loss: 0.48555877804756165\n",
      "Epoch: 12, Train_Loss: 0.5834970474243164, Test_Loss: 0.4638369679450989 *\n",
      "Epoch: 12, Train_Loss: 0.6672713756561279, Test_Loss: 0.48452889919281006\n",
      "Epoch: 12, Train_Loss: 0.444563090801239, Test_Loss: 0.4898361563682556\n",
      "Epoch: 12, Train_Loss: 0.4547184407711029, Test_Loss: 0.524612307548523\n",
      "Epoch: 12, Train_Loss: 0.44293496012687683, Test_Loss: 0.4478614330291748 *\n",
      "Epoch: 13, Train_Loss: 0.5006619095802307, Test_Loss: 0.4698299765586853 *\n",
      "Epoch: 13, Train_Loss: 0.4457891583442688, Test_Loss: 0.5408336520195007\n",
      "Epoch: 13, Train_Loss: 0.4687327742576599, Test_Loss: 0.5539759993553162\n",
      "Epoch: 13, Train_Loss: 16.274614334106445, Test_Loss: 0.5343742966651917 *\n",
      "Epoch: 13, Train_Loss: 0.49371007084846497, Test_Loss: 0.4616732597351074 *\n",
      "Epoch: 13, Train_Loss: 2.1067280769348145, Test_Loss: 0.4483723044395447 *\n",
      "Epoch: 13, Train_Loss: 1.5888056755065918, Test_Loss: 0.4715431034564972\n",
      "Epoch: 13, Train_Loss: 0.46047067642211914, Test_Loss: 0.5410218834877014\n",
      "Epoch: 13, Train_Loss: 0.6470232605934143, Test_Loss: 0.6112209558486938\n",
      "Epoch: 13, Train_Loss: 3.867840051651001, Test_Loss: 0.6161073446273804\n",
      "Epoch: 13, Train_Loss: 5.751730442047119, Test_Loss: 0.7993614673614502\n",
      "Epoch: 13, Train_Loss: 0.5103232860565186, Test_Loss: 0.4881775677204132 *\n",
      "Epoch: 13, Train_Loss: 0.5438088774681091, Test_Loss: 0.5312177538871765\n",
      "Epoch: 13, Train_Loss: 5.445759296417236, Test_Loss: 0.7823735475540161\n",
      "Epoch: 13, Train_Loss: 0.6215909123420715, Test_Loss: 0.8662817478179932\n",
      "Epoch: 13, Train_Loss: 0.48526865243911743, Test_Loss: 1.0266896486282349\n",
      "Epoch: 13, Train_Loss: 0.4376288652420044, Test_Loss: 0.510152280330658 *\n",
      "Epoch: 13, Train_Loss: 0.4593725800514221, Test_Loss: 0.7357197999954224\n",
      "Epoch: 13, Train_Loss: 0.5201129913330078, Test_Loss: 0.5912876129150391 *\n",
      "Epoch: 13, Train_Loss: 0.42555010318756104, Test_Loss: 0.7910668253898621\n",
      "Epoch: 13, Train_Loss: 0.4667656123638153, Test_Loss: 1.7159818410873413\n",
      "Epoch: 13, Train_Loss: 0.4246808588504791, Test_Loss: 0.8152490854263306 *\n",
      "Epoch: 13, Train_Loss: 0.42530596256256104, Test_Loss: 0.5206878781318665 *\n",
      "Epoch: 13, Train_Loss: 0.5073584914207458, Test_Loss: 0.8252382278442383\n",
      "Epoch: 13, Train_Loss: 0.46890079975128174, Test_Loss: 0.8404955863952637\n",
      "Epoch: 13, Train_Loss: 0.5104851722717285, Test_Loss: 0.7357306480407715 *\n",
      "Epoch: 13, Train_Loss: 0.4769224524497986, Test_Loss: 0.6226379871368408 *\n",
      "Epoch: 13, Train_Loss: 0.4413037896156311, Test_Loss: 0.5016542673110962 *\n",
      "Epoch: 13, Train_Loss: 0.4299118220806122, Test_Loss: 4.5523457527160645\n",
      "Epoch: 13, Train_Loss: 0.46461179852485657, Test_Loss: 3.1181838512420654 *\n",
      "Epoch: 13, Train_Loss: 0.4325590133666992, Test_Loss: 0.7434576153755188 *\n",
      "Epoch: 13, Train_Loss: 0.4327555000782013, Test_Loss: 0.6000762581825256 *\n",
      "Epoch: 13, Train_Loss: 0.4347079396247864, Test_Loss: 0.6988638043403625\n",
      "Epoch: 13, Train_Loss: 0.4300682544708252, Test_Loss: 0.44344669580459595 *\n",
      "Epoch: 13, Train_Loss: 0.4290977418422699, Test_Loss: 0.6950911283493042\n",
      "Epoch: 13, Train_Loss: 0.42826834321022034, Test_Loss: 0.850233793258667\n",
      "Epoch: 13, Train_Loss: 0.4307425916194916, Test_Loss: 0.5868353843688965 *\n",
      "Epoch: 13, Train_Loss: 0.4276028871536255, Test_Loss: 0.5232425332069397 *\n",
      "Epoch: 13, Train_Loss: 0.4269130825996399, Test_Loss: 0.5930865406990051\n",
      "Epoch: 13, Train_Loss: 0.4397493600845337, Test_Loss: 0.5478379130363464 *\n",
      "Epoch: 13, Train_Loss: 0.4604628086090088, Test_Loss: 0.7923413515090942\n",
      "Epoch: 13, Train_Loss: 0.4834347367286682, Test_Loss: 0.6075255870819092 *\n",
      "Epoch: 13, Train_Loss: 0.49352842569351196, Test_Loss: 0.9646790623664856\n",
      "Epoch: 13, Train_Loss: 0.47449690103530884, Test_Loss: 0.6838333606719971 *\n",
      "Epoch: 13, Train_Loss: 2.6439971923828125, Test_Loss: 0.43879520893096924 *\n",
      "Epoch: 13, Train_Loss: 6.619861125946045, Test_Loss: 0.5261962413787842\n",
      "Epoch: 13, Train_Loss: 0.4339183568954468, Test_Loss: 0.565423846244812\n",
      "Epoch: 13, Train_Loss: 0.5296972393989563, Test_Loss: 0.746554434299469\n",
      "Epoch: 13, Train_Loss: 0.5599586963653564, Test_Loss: 0.5451746582984924 *\n",
      "Epoch: 13, Train_Loss: 0.5308206677436829, Test_Loss: 0.6485339999198914\n",
      "Epoch: 13, Train_Loss: 0.47454833984375, Test_Loss: 0.724156379699707\n",
      "Epoch: 13, Train_Loss: 0.6338348388671875, Test_Loss: 0.7905945777893066\n",
      "Epoch: 13, Train_Loss: 0.5891482830047607, Test_Loss: 0.76347416639328 *\n",
      "Epoch: 13, Train_Loss: 0.6053880453109741, Test_Loss: 0.5263999700546265 *\n",
      "Epoch: 13, Train_Loss: 0.5587373971939087, Test_Loss: 0.49382197856903076 *\n",
      "Epoch: 13, Train_Loss: 0.4841283857822418, Test_Loss: 0.5571425557136536\n",
      "Epoch: 13, Train_Loss: 0.4506484866142273, Test_Loss: 0.510135293006897 *\n",
      "Epoch: 13, Train_Loss: 0.5456024408340454, Test_Loss: 0.46693500876426697 *\n",
      "Epoch: 13, Train_Loss: 0.5339747667312622, Test_Loss: 0.5981936454772949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train_Loss: 0.5503867864608765, Test_Loss: 0.4554213583469391 *\n",
      "Epoch: 13, Train_Loss: 0.4821654260158539, Test_Loss: 5.859229564666748\n",
      "Epoch: 13, Train_Loss: 0.4518823027610779, Test_Loss: 0.768462598323822 *\n",
      "Epoch: 13, Train_Loss: 0.42290085554122925, Test_Loss: 0.4225451946258545 *\n",
      "Epoch: 13, Train_Loss: 0.45991888642311096, Test_Loss: 0.43814748525619507\n",
      "Epoch: 13, Train_Loss: 0.4521067142486572, Test_Loss: 0.427823007106781 *\n",
      "Epoch: 13, Train_Loss: 0.44131892919540405, Test_Loss: 0.4410100281238556\n",
      "Epoch: 13, Train_Loss: 0.42273804545402527, Test_Loss: 0.42793044447898865 *\n",
      "Epoch: 13, Train_Loss: 0.42309850454330444, Test_Loss: 0.5347976088523865\n",
      "Epoch: 13, Train_Loss: 0.43251749873161316, Test_Loss: 0.49278122186660767 *\n",
      "Epoch: 13, Train_Loss: 2.6377735137939453, Test_Loss: 0.42100170254707336 *\n",
      "Epoch: 13, Train_Loss: 3.9652316570281982, Test_Loss: 0.46726277470588684\n",
      "Epoch: 13, Train_Loss: 0.42101189494132996, Test_Loss: 0.4302952289581299 *\n",
      "Epoch: 13, Train_Loss: 0.4253373146057129, Test_Loss: 0.4343602657318115\n",
      "Epoch: 13, Train_Loss: 0.4311697483062744, Test_Loss: 0.4314529597759247 *\n",
      "Epoch: 13, Train_Loss: 0.42615821957588196, Test_Loss: 0.45188719034194946\n",
      "Epoch: 13, Train_Loss: 0.4257383942604065, Test_Loss: 0.46681472659111023\n",
      "Epoch: 13, Train_Loss: 0.42345675826072693, Test_Loss: 0.5622541904449463\n",
      "Epoch: 13, Train_Loss: 0.4354071319103241, Test_Loss: 0.49494022130966187 *\n",
      "Epoch: 13, Train_Loss: 0.4410018026828766, Test_Loss: 0.44042742252349854 *\n",
      "Epoch: 13, Train_Loss: 0.4463101625442505, Test_Loss: 0.4204801321029663 *\n",
      "Epoch: 13, Train_Loss: 0.42206552624702454, Test_Loss: 0.42148497700691223\n",
      "Epoch: 13, Train_Loss: 0.42072948813438416, Test_Loss: 0.42080000042915344 *\n",
      "Epoch: 13, Train_Loss: 0.4232701361179352, Test_Loss: 0.42130109667778015\n",
      "Epoch: 13, Train_Loss: 0.43462398648262024, Test_Loss: 0.4206123948097229 *\n",
      "Epoch: 13, Train_Loss: 0.4213354289531708, Test_Loss: 0.42153412103652954\n",
      "Epoch: 13, Train_Loss: 0.42226502299308777, Test_Loss: 0.425368070602417\n",
      "Epoch: 13, Train_Loss: 0.4630926549434662, Test_Loss: 0.42128750681877136 *\n",
      "Epoch: 13, Train_Loss: 0.4607676565647125, Test_Loss: 0.4198363125324249 *\n",
      "Epoch: 13, Train_Loss: 0.4221281111240387, Test_Loss: 0.42893674969673157\n",
      "Epoch: 13, Train_Loss: 0.42070654034614563, Test_Loss: 0.42966771125793457\n",
      "Epoch: 13, Train_Loss: 0.44235852360725403, Test_Loss: 0.4451158344745636\n",
      "Epoch: 13, Train_Loss: 0.47569894790649414, Test_Loss: 0.43633967638015747 *\n",
      "Epoch: 13, Train_Loss: 0.4345649480819702, Test_Loss: 0.5768280029296875\n",
      "Epoch: 13, Train_Loss: 0.4738962948322296, Test_Loss: 0.7473305463790894\n",
      "Epoch: 13, Train_Loss: 0.4705880284309387, Test_Loss: 0.5679229497909546 *\n",
      "Epoch: 13, Train_Loss: 0.504586935043335, Test_Loss: 0.4640745222568512 *\n",
      "Epoch: 13, Train_Loss: 0.4598618149757385, Test_Loss: 0.420134574174881 *\n",
      "Epoch: 13, Train_Loss: 0.47850310802459717, Test_Loss: 0.4369378983974457\n",
      "Epoch: 13, Train_Loss: 0.508188784122467, Test_Loss: 0.5063983201980591\n",
      "Epoch: 13, Train_Loss: 0.5542892217636108, Test_Loss: 0.8542416095733643\n",
      "Model saved at location save_new\\model.ckpt at epoch 13\n",
      "Epoch: 13, Train_Loss: 0.4632541537284851, Test_Loss: 0.8573763370513916\n",
      "Epoch: 13, Train_Loss: 0.4290194511413574, Test_Loss: 0.6189044713973999 *\n",
      "Epoch: 13, Train_Loss: 0.4211748242378235, Test_Loss: 0.473179429769516 *\n",
      "Epoch: 13, Train_Loss: 0.4170987010002136, Test_Loss: 0.42795082926750183 *\n",
      "Epoch: 13, Train_Loss: 0.41597533226013184, Test_Loss: 0.4419475197792053\n",
      "Epoch: 13, Train_Loss: 0.41987916827201843, Test_Loss: 0.4310709834098816 *\n",
      "Epoch: 13, Train_Loss: 2.879554510116577, Test_Loss: 0.4405810236930847\n",
      "Epoch: 13, Train_Loss: 2.903414011001587, Test_Loss: 0.4492169916629791\n",
      "Epoch: 13, Train_Loss: 0.4163418114185333, Test_Loss: 0.45690101385116577\n",
      "Epoch: 13, Train_Loss: 0.43283185362815857, Test_Loss: 0.42443370819091797 *\n",
      "Epoch: 13, Train_Loss: 0.42488670349121094, Test_Loss: 0.5212409496307373\n",
      "Epoch: 13, Train_Loss: 0.41535046696662903, Test_Loss: 0.7802528142929077\n",
      "Epoch: 13, Train_Loss: 0.41556069254875183, Test_Loss: 0.5104414224624634 *\n",
      "Epoch: 13, Train_Loss: 0.4152482748031616, Test_Loss: 0.5934796333312988\n",
      "Epoch: 13, Train_Loss: 0.4146878123283386, Test_Loss: 0.42427003383636475 *\n",
      "Epoch: 13, Train_Loss: 0.41481077671051025, Test_Loss: 0.42442601919174194\n",
      "Epoch: 13, Train_Loss: 0.42760738730430603, Test_Loss: 0.42457008361816406\n",
      "Epoch: 13, Train_Loss: 0.48981916904449463, Test_Loss: 0.4247573912143707\n",
      "Epoch: 13, Train_Loss: 0.48112472891807556, Test_Loss: 0.4424314498901367\n",
      "Epoch: 13, Train_Loss: 0.48432180285453796, Test_Loss: 4.984843730926514\n",
      "Epoch: 13, Train_Loss: 0.44448065757751465, Test_Loss: 1.3672266006469727 *\n",
      "Epoch: 13, Train_Loss: 0.4190731346607208, Test_Loss: 0.42569369077682495 *\n",
      "Epoch: 13, Train_Loss: 0.612692654132843, Test_Loss: 0.41981470584869385 *\n",
      "Epoch: 13, Train_Loss: 0.6261128187179565, Test_Loss: 0.4190514385700226 *\n",
      "Epoch: 13, Train_Loss: 0.6083531379699707, Test_Loss: 0.4250345230102539\n",
      "Epoch: 13, Train_Loss: 0.4967305064201355, Test_Loss: 0.419241726398468 *\n",
      "Epoch: 13, Train_Loss: 0.413848340511322, Test_Loss: 0.4260304272174835\n",
      "Epoch: 13, Train_Loss: 0.413777619600296, Test_Loss: 0.4164600372314453 *\n",
      "Epoch: 13, Train_Loss: 0.41721445322036743, Test_Loss: 0.4169614017009735\n",
      "Epoch: 13, Train_Loss: 0.4237884283065796, Test_Loss: 0.42247655987739563\n",
      "Epoch: 13, Train_Loss: 0.42912471294403076, Test_Loss: 0.43451470136642456\n",
      "Epoch: 13, Train_Loss: 0.42268097400665283, Test_Loss: 0.4221293330192566 *\n",
      "Epoch: 13, Train_Loss: 0.41286787390708923, Test_Loss: 0.42502179741859436\n",
      "Epoch: 13, Train_Loss: 0.4126819670200348, Test_Loss: 0.4256970286369324\n",
      "Epoch: 13, Train_Loss: 0.4253431260585785, Test_Loss: 0.4262681305408478\n",
      "Epoch: 13, Train_Loss: 0.4809185266494751, Test_Loss: 0.41380587220191956 *\n",
      "Epoch: 13, Train_Loss: 0.6058854460716248, Test_Loss: 0.41612130403518677\n",
      "Epoch: 13, Train_Loss: 0.5509792566299438, Test_Loss: 0.42177093029022217\n",
      "Epoch: 13, Train_Loss: 0.5196298956871033, Test_Loss: 0.4214712381362915 *\n",
      "Epoch: 13, Train_Loss: 0.5060818195343018, Test_Loss: 0.41697320342063904 *\n",
      "Epoch: 13, Train_Loss: 0.5445557832717896, Test_Loss: 0.41922929883003235\n",
      "Epoch: 13, Train_Loss: 0.4513690769672394, Test_Loss: 0.4255751371383667\n",
      "Epoch: 13, Train_Loss: 0.5596847534179688, Test_Loss: 0.4358823895454407\n",
      "Epoch: 13, Train_Loss: 0.52628493309021, Test_Loss: 0.4313635230064392 *\n",
      "Epoch: 13, Train_Loss: 0.6881409287452698, Test_Loss: 0.41788092255592346 *\n",
      "Epoch: 13, Train_Loss: 0.422229140996933, Test_Loss: 0.4197295308113098\n",
      "Epoch: 13, Train_Loss: 0.5650743842124939, Test_Loss: 0.42605236172676086\n",
      "Epoch: 13, Train_Loss: 3.024449586868286, Test_Loss: 0.41974687576293945 *\n",
      "Epoch: 13, Train_Loss: 0.5060933828353882, Test_Loss: 0.4169589579105377 *\n",
      "Epoch: 13, Train_Loss: 0.48875510692596436, Test_Loss: 0.4901078939437866\n",
      "Epoch: 13, Train_Loss: 0.44022494554519653, Test_Loss: 0.6371471881866455\n",
      "Epoch: 13, Train_Loss: 0.4295278489589691, Test_Loss: 5.802811622619629\n",
      "Epoch: 13, Train_Loss: 0.4174638092517853, Test_Loss: 0.4196572005748749 *\n",
      "Epoch: 13, Train_Loss: 0.4255545139312744, Test_Loss: 0.41548293828964233 *\n",
      "Epoch: 13, Train_Loss: 0.4905053377151489, Test_Loss: 0.44604456424713135\n",
      "Epoch: 13, Train_Loss: 0.5338900685310364, Test_Loss: 0.43343934416770935 *\n",
      "Epoch: 13, Train_Loss: 0.4597093164920807, Test_Loss: 0.4491328299045563\n",
      "Epoch: 13, Train_Loss: 0.4356318414211273, Test_Loss: 0.4166850447654724 *\n",
      "Epoch: 13, Train_Loss: 0.41649484634399414, Test_Loss: 0.49199557304382324\n",
      "Epoch: 13, Train_Loss: 0.4229530096054077, Test_Loss: 0.4421522915363312 *\n",
      "Epoch: 13, Train_Loss: 0.4245903193950653, Test_Loss: 0.4235376715660095 *\n",
      "Epoch: 13, Train_Loss: 0.438171923160553, Test_Loss: 0.4275376498699188\n",
      "Epoch: 13, Train_Loss: 0.47647300362586975, Test_Loss: 0.47799980640411377\n",
      "Epoch: 13, Train_Loss: 0.43229910731315613, Test_Loss: 0.4239124655723572 *\n",
      "Epoch: 13, Train_Loss: 0.4126933515071869, Test_Loss: 0.46476855874061584\n",
      "Epoch: 13, Train_Loss: 0.4206067621707916, Test_Loss: 0.4618414342403412 *\n",
      "Epoch: 13, Train_Loss: 0.41369035840034485, Test_Loss: 0.45441707968711853 *\n",
      "Epoch: 13, Train_Loss: 0.4159211218357086, Test_Loss: 0.4240412712097168 *\n",
      "Epoch: 13, Train_Loss: 0.43074750900268555, Test_Loss: 0.4455171227455139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train_Loss: 0.41660842299461365, Test_Loss: 0.4850729703903198\n",
      "Epoch: 13, Train_Loss: 0.41158023476600647, Test_Loss: 0.41988667845726013 *\n",
      "Epoch: 13, Train_Loss: 0.41191333532333374, Test_Loss: 0.41690847277641296 *\n",
      "Epoch: 13, Train_Loss: 0.4204477369785309, Test_Loss: 0.42278921604156494\n",
      "Epoch: 13, Train_Loss: 0.4126474857330322, Test_Loss: 0.4242418110370636\n",
      "Epoch: 13, Train_Loss: 0.4148324728012085, Test_Loss: 0.42308342456817627 *\n",
      "Epoch: 13, Train_Loss: 0.410295307636261, Test_Loss: 0.4216301441192627 *\n",
      "Epoch: 13, Train_Loss: 0.41005972027778625, Test_Loss: 0.4177202880382538 *\n",
      "Epoch: 13, Train_Loss: 0.409364253282547, Test_Loss: 0.4198305010795593\n",
      "Epoch: 13, Train_Loss: 0.4126889705657959, Test_Loss: 0.4180358648300171 *\n",
      "Epoch: 13, Train_Loss: 0.41873660683631897, Test_Loss: 0.4146188199520111 *\n",
      "Epoch: 13, Train_Loss: 0.42416924238204956, Test_Loss: 0.41967594623565674\n",
      "Epoch: 13, Train_Loss: 0.4259948134422302, Test_Loss: 0.472790002822876\n",
      "Epoch: 13, Train_Loss: 0.4153696596622467, Test_Loss: 0.4304580092430115 *\n",
      "Epoch: 13, Train_Loss: 0.4190191328525543, Test_Loss: 0.7563110589981079\n",
      "Epoch: 13, Train_Loss: 0.40843865275382996, Test_Loss: 0.91330885887146\n",
      "Epoch: 13, Train_Loss: 0.40774089097976685, Test_Loss: 0.6033394932746887 *\n",
      "Epoch: 13, Train_Loss: 0.42231759428977966, Test_Loss: 0.4519941806793213 *\n",
      "Epoch: 13, Train_Loss: 0.42501044273376465, Test_Loss: 0.42686527967453003 *\n",
      "Epoch: 13, Train_Loss: 0.4082338809967041, Test_Loss: 0.42084720730781555 *\n",
      "Epoch: 13, Train_Loss: 0.4081818163394928, Test_Loss: 0.4868151843547821\n",
      "Epoch: 13, Train_Loss: 0.4100847542285919, Test_Loss: 0.8699469566345215\n",
      "Epoch: 13, Train_Loss: 0.45366477966308594, Test_Loss: 0.8082069158554077 *\n",
      "Epoch: 13, Train_Loss: 0.4404430091381073, Test_Loss: 0.4888661503791809 *\n",
      "Epoch: 13, Train_Loss: 0.4552983045578003, Test_Loss: 0.5084476470947266\n",
      "Epoch: 13, Train_Loss: 0.4134395122528076, Test_Loss: 0.4091501832008362 *\n",
      "Epoch: 13, Train_Loss: 0.4176954925060272, Test_Loss: 0.41349828243255615\n",
      "Epoch: 13, Train_Loss: 0.4686218798160553, Test_Loss: 0.4142475128173828\n",
      "Epoch: 13, Train_Loss: 0.408960223197937, Test_Loss: 0.4191480875015259\n",
      "Epoch: 13, Train_Loss: 0.4190087914466858, Test_Loss: 0.44035929441452026\n",
      "Epoch: 13, Train_Loss: 0.4292767643928528, Test_Loss: 0.43177083134651184 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 13\n",
      "Epoch: 13, Train_Loss: 0.4594561457633972, Test_Loss: 0.4117160737514496 *\n",
      "Epoch: 13, Train_Loss: 0.5074263215065002, Test_Loss: 0.522520899772644\n",
      "Epoch: 13, Train_Loss: 0.4545954465866089, Test_Loss: 0.8132753968238831\n",
      "Epoch: 13, Train_Loss: 0.436005562543869, Test_Loss: 0.5220131278038025 *\n",
      "Epoch: 13, Train_Loss: 0.41084593534469604, Test_Loss: 0.5317323207855225\n",
      "Epoch: 13, Train_Loss: 0.4307994246482849, Test_Loss: 0.4129565954208374 *\n",
      "Epoch: 13, Train_Loss: 0.40692251920700073, Test_Loss: 0.412830650806427 *\n",
      "Epoch: 13, Train_Loss: 0.4115786850452423, Test_Loss: 0.41269052028656006 *\n",
      "Epoch: 13, Train_Loss: 0.4145074486732483, Test_Loss: 0.4134121239185333\n",
      "Epoch: 13, Train_Loss: 0.4247760772705078, Test_Loss: 0.45131799578666687\n",
      "Epoch: 13, Train_Loss: 0.4797649681568146, Test_Loss: 5.5649800300598145\n",
      "Epoch: 13, Train_Loss: 0.4542715847492218, Test_Loss: 0.6320719122886658 *\n",
      "Epoch: 13, Train_Loss: 0.4542287290096283, Test_Loss: 0.4153217673301697 *\n",
      "Epoch: 13, Train_Loss: 0.429231196641922, Test_Loss: 0.4099225699901581 *\n",
      "Epoch: 13, Train_Loss: 0.43387535214424133, Test_Loss: 0.4103476107120514\n",
      "Epoch: 13, Train_Loss: 0.41978326439857483, Test_Loss: 0.41252774000167847\n",
      "Epoch: 13, Train_Loss: 0.5846927165985107, Test_Loss: 0.4071618914604187 *\n",
      "Epoch: 13, Train_Loss: 0.563978374004364, Test_Loss: 0.4132987856864929\n",
      "Epoch: 13, Train_Loss: 0.40968284010887146, Test_Loss: 0.4064696133136749 *\n",
      "Epoch: 13, Train_Loss: 0.4396694004535675, Test_Loss: 0.407073050737381\n",
      "Epoch: 13, Train_Loss: 0.40415772795677185, Test_Loss: 0.41178908944129944\n",
      "Epoch: 13, Train_Loss: 0.4048769474029541, Test_Loss: 0.42722949385643005\n",
      "Epoch: 13, Train_Loss: 0.4047328233718872, Test_Loss: 0.4115661680698395 *\n",
      "Epoch: 13, Train_Loss: 0.4060036540031433, Test_Loss: 0.4216642677783966\n",
      "Epoch: 13, Train_Loss: 0.41100940108299255, Test_Loss: 0.42319732904434204\n",
      "Epoch: 13, Train_Loss: 0.4270358085632324, Test_Loss: 0.40721505880355835 *\n",
      "Epoch: 13, Train_Loss: 0.41651979088783264, Test_Loss: 0.4039127230644226 *\n",
      "Epoch: 13, Train_Loss: 0.4216110408306122, Test_Loss: 0.40412914752960205\n",
      "Epoch: 13, Train_Loss: 0.4191800653934479, Test_Loss: 0.40955355763435364\n",
      "Epoch: 13, Train_Loss: 0.40340328216552734, Test_Loss: 0.4041103720664978 *\n",
      "Epoch: 13, Train_Loss: 0.4043625295162201, Test_Loss: 0.4071769118309021\n",
      "Epoch: 13, Train_Loss: 0.4037805497646332, Test_Loss: 0.40342557430267334 *\n",
      "Epoch: 13, Train_Loss: 0.43268781900405884, Test_Loss: 0.4066162407398224\n",
      "Epoch: 13, Train_Loss: 0.4340082108974457, Test_Loss: 0.4102735221385956\n",
      "Epoch: 13, Train_Loss: 0.413724422454834, Test_Loss: 0.40631768107414246 *\n",
      "Epoch: 13, Train_Loss: 0.4233783781528473, Test_Loss: 0.40357160568237305 *\n",
      "Epoch: 13, Train_Loss: 0.47086817026138306, Test_Loss: 0.40501511096954346\n",
      "Epoch: 13, Train_Loss: 0.443011075258255, Test_Loss: 0.4034059941768646 *\n",
      "Epoch: 13, Train_Loss: 0.41134485602378845, Test_Loss: 0.4040182828903198\n",
      "Epoch: 13, Train_Loss: 0.425176203250885, Test_Loss: 0.40838274359703064\n",
      "Epoch: 13, Train_Loss: 0.41737306118011475, Test_Loss: 0.4660719633102417\n",
      "Epoch: 13, Train_Loss: 0.4135848879814148, Test_Loss: 1.6566133499145508\n",
      "Epoch: 13, Train_Loss: 0.4093303084373474, Test_Loss: 4.736238956451416\n",
      "Epoch: 13, Train_Loss: 0.42831939458847046, Test_Loss: 0.4080325663089752 *\n",
      "Epoch: 13, Train_Loss: 0.4583672881126404, Test_Loss: 0.40156272053718567 *\n",
      "Epoch: 13, Train_Loss: 2.6765594482421875, Test_Loss: 0.44424131512641907\n",
      "Epoch: 13, Train_Loss: 3.485109806060791, Test_Loss: 0.4327189028263092 *\n",
      "Epoch: 13, Train_Loss: 0.42136821150779724, Test_Loss: 0.44959160685539246\n",
      "Epoch: 13, Train_Loss: 0.4047364890575409, Test_Loss: 0.4228869378566742 *\n",
      "Epoch: 13, Train_Loss: 0.46636831760406494, Test_Loss: 0.5316258668899536\n",
      "Epoch: 13, Train_Loss: 0.5545562505722046, Test_Loss: 0.4179142415523529 *\n",
      "Epoch: 13, Train_Loss: 0.42422065138816833, Test_Loss: 0.40951046347618103 *\n",
      "Epoch: 13, Train_Loss: 0.4035739600658417, Test_Loss: 0.430143266916275\n",
      "Epoch: 13, Train_Loss: 0.4129388630390167, Test_Loss: 0.42440834641456604 *\n",
      "Epoch: 13, Train_Loss: 0.4556678533554077, Test_Loss: 0.4070885479450226 *\n",
      "Epoch: 13, Train_Loss: 0.4105856418609619, Test_Loss: 0.45740097761154175\n",
      "Epoch: 13, Train_Loss: 0.4111584424972534, Test_Loss: 0.45846134424209595\n",
      "Epoch: 13, Train_Loss: 1.0805556774139404, Test_Loss: 0.46136021614074707\n",
      "Epoch: 13, Train_Loss: 1.4135639667510986, Test_Loss: 0.4316343367099762 *\n",
      "Epoch: 13, Train_Loss: 0.7898754477500916, Test_Loss: 0.4341850280761719\n",
      "Epoch: 13, Train_Loss: 0.47019729018211365, Test_Loss: 0.46713027358055115\n",
      "Epoch: 13, Train_Loss: 1.350059986114502, Test_Loss: 0.4030482769012451 *\n",
      "Epoch: 13, Train_Loss: 2.04970383644104, Test_Loss: 0.40157490968704224 *\n",
      "Epoch: 13, Train_Loss: 0.5366501212120056, Test_Loss: 0.41077297925949097\n",
      "Epoch: 13, Train_Loss: 0.4041593372821808, Test_Loss: 0.4225519895553589\n",
      "Epoch: 13, Train_Loss: 0.49590104818344116, Test_Loss: 0.4150260090827942 *\n",
      "Epoch: 13, Train_Loss: 1.011308193206787, Test_Loss: 0.40969109535217285 *\n",
      "Epoch: 13, Train_Loss: 1.1477314233779907, Test_Loss: 0.405155748128891 *\n",
      "Epoch: 13, Train_Loss: 0.42758169770240784, Test_Loss: 0.4196152091026306\n",
      "Epoch: 13, Train_Loss: 0.4050410985946655, Test_Loss: 0.422035813331604\n",
      "Epoch: 13, Train_Loss: 0.4160284399986267, Test_Loss: 0.4766826629638672\n",
      "Epoch: 13, Train_Loss: 0.6841549873352051, Test_Loss: 0.43584713339805603 *\n",
      "Epoch: 13, Train_Loss: 0.5679502487182617, Test_Loss: 0.4147927761077881 *\n",
      "Epoch: 13, Train_Loss: 0.523382842540741, Test_Loss: 0.4789750576019287\n",
      "Epoch: 13, Train_Loss: 0.42348116636276245, Test_Loss: 0.6935997009277344\n",
      "Epoch: 13, Train_Loss: 0.40837129950523376, Test_Loss: 0.5542450547218323 *\n",
      "Epoch: 13, Train_Loss: 0.46890899538993835, Test_Loss: 0.420841246843338 *\n",
      "Epoch: 13, Train_Loss: 0.5515627861022949, Test_Loss: 0.43399035930633545\n",
      "Epoch: 13, Train_Loss: 0.5134231448173523, Test_Loss: 0.4693625271320343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train_Loss: 0.4262545108795166, Test_Loss: 0.4220929443836212 *\n",
      "Epoch: 13, Train_Loss: 0.45948168635368347, Test_Loss: 0.4987136721611023\n",
      "Epoch: 13, Train_Loss: 0.4488959312438965, Test_Loss: 0.5195740461349487\n",
      "Epoch: 13, Train_Loss: 0.5281683802604675, Test_Loss: 0.5543205142021179\n",
      "Epoch: 13, Train_Loss: 0.5261082649230957, Test_Loss: 0.501845121383667 *\n",
      "Epoch: 13, Train_Loss: 0.47172313928604126, Test_Loss: 0.47293001413345337 *\n",
      "Epoch: 13, Train_Loss: 0.47572484612464905, Test_Loss: 0.43398210406303406 *\n",
      "Epoch: 13, Train_Loss: 0.5365830659866333, Test_Loss: 0.40572720766067505 *\n",
      "Epoch: 13, Train_Loss: 0.4381067156791687, Test_Loss: 0.42406558990478516\n",
      "Epoch: 13, Train_Loss: 0.4237616956233978, Test_Loss: 0.47467830777168274\n",
      "Epoch: 13, Train_Loss: 0.4005122780799866, Test_Loss: 0.41711315512657166 *\n",
      "Epoch: 13, Train_Loss: 0.40531885623931885, Test_Loss: 0.45329058170318604\n",
      "Epoch: 13, Train_Loss: 0.39861395955085754, Test_Loss: 0.4446733295917511 *\n",
      "Epoch: 13, Train_Loss: 0.4089975357055664, Test_Loss: 0.5451681613922119\n",
      "Epoch: 13, Train_Loss: 0.4314804971218109, Test_Loss: 0.7142882943153381\n",
      "Epoch: 13, Train_Loss: 0.41187259554862976, Test_Loss: 0.8468226194381714\n",
      "Epoch: 13, Train_Loss: 0.4439435601234436, Test_Loss: 0.5035942792892456 *\n",
      "Epoch: 13, Train_Loss: 0.5591638088226318, Test_Loss: 0.3971549868583679 *\n",
      "Epoch: 13, Train_Loss: 0.6364566683769226, Test_Loss: 0.39721450209617615\n",
      "Epoch: 13, Train_Loss: 0.40182530879974365, Test_Loss: 0.39670330286026 *\n",
      "Epoch: 13, Train_Loss: 0.44039908051490784, Test_Loss: 0.4576760530471802\n",
      "Model saved at location save_new\\model.ckpt at epoch 13\n",
      "Epoch: 13, Train_Loss: 0.5105552673339844, Test_Loss: 0.7278010845184326\n",
      "Epoch: 13, Train_Loss: 0.579984188079834, Test_Loss: 5.599990367889404\n",
      "Epoch: 13, Train_Loss: 0.6238875389099121, Test_Loss: 0.4889531135559082 *\n",
      "Epoch: 13, Train_Loss: 0.47776803374290466, Test_Loss: 0.44689512252807617 *\n",
      "Epoch: 13, Train_Loss: 0.5497295260429382, Test_Loss: 0.4391198754310608 *\n",
      "Epoch: 13, Train_Loss: 0.6430398225784302, Test_Loss: 0.41017183661460876 *\n",
      "Epoch: 13, Train_Loss: 0.5573379397392273, Test_Loss: 0.4152316451072693\n",
      "Epoch: 13, Train_Loss: 0.41867774724960327, Test_Loss: 0.45777714252471924\n",
      "Epoch: 13, Train_Loss: 0.40675458312034607, Test_Loss: 0.47031980752944946\n",
      "Epoch: 13, Train_Loss: 0.45006048679351807, Test_Loss: 0.40398678183555603 *\n",
      "Epoch: 13, Train_Loss: 1.0417280197143555, Test_Loss: 0.45084404945373535\n",
      "Epoch: 13, Train_Loss: 0.8099488019943237, Test_Loss: 0.45797526836395264\n",
      "Epoch: 13, Train_Loss: 0.43524235486984253, Test_Loss: 0.5515437126159668\n",
      "Epoch: 13, Train_Loss: 0.4355199933052063, Test_Loss: 0.43973013758659363 *\n",
      "Epoch: 13, Train_Loss: 0.4042181074619293, Test_Loss: 0.4543236196041107\n",
      "Epoch: 13, Train_Loss: 0.4941539466381073, Test_Loss: 0.469274640083313\n",
      "Epoch: 13, Train_Loss: 0.77790367603302, Test_Loss: 0.42751213908195496 *\n",
      "Epoch: 13, Train_Loss: 0.41444283723831177, Test_Loss: 0.4198814928531647 *\n",
      "Epoch: 13, Train_Loss: 0.5208747982978821, Test_Loss: 0.5071558952331543\n",
      "Epoch: 13, Train_Loss: 0.45264115929603577, Test_Loss: 0.4950530529022217 *\n",
      "Epoch: 13, Train_Loss: 0.4368271231651306, Test_Loss: 0.42988893389701843 *\n",
      "Epoch: 13, Train_Loss: 0.43035051226615906, Test_Loss: 0.43943139910697937\n",
      "Epoch: 13, Train_Loss: 0.507086992263794, Test_Loss: 0.4162537455558777 *\n",
      "Epoch: 13, Train_Loss: 0.46871599555015564, Test_Loss: 0.46242231130599976\n",
      "Epoch: 13, Train_Loss: 0.472125381231308, Test_Loss: 0.4907177686691284\n",
      "Epoch: 13, Train_Loss: 0.4329080283641815, Test_Loss: 0.4377981126308441 *\n",
      "Epoch: 13, Train_Loss: 0.4482688307762146, Test_Loss: 0.4051569700241089 *\n",
      "Epoch: 13, Train_Loss: 0.45752036571502686, Test_Loss: 0.4442829489707947\n",
      "Epoch: 13, Train_Loss: 0.4243655800819397, Test_Loss: 0.4575195014476776\n",
      "Epoch: 13, Train_Loss: 0.408966600894928, Test_Loss: 0.4177015721797943 *\n",
      "Epoch: 13, Train_Loss: 0.40252378582954407, Test_Loss: 0.43334099650382996\n",
      "Epoch: 13, Train_Loss: 0.4788492023944855, Test_Loss: 0.5237219929695129\n",
      "Epoch: 13, Train_Loss: 0.5965613722801208, Test_Loss: 2.921495199203491\n",
      "Epoch: 13, Train_Loss: 0.6371904015541077, Test_Loss: 3.284195899963379\n",
      "Epoch: 13, Train_Loss: 0.8315752744674683, Test_Loss: 0.4203324615955353 *\n",
      "Epoch: 13, Train_Loss: 0.646527886390686, Test_Loss: 0.40968766808509827 *\n",
      "Epoch: 13, Train_Loss: 0.6031783223152161, Test_Loss: 0.4317547678947449\n",
      "Epoch: 13, Train_Loss: 0.5002830028533936, Test_Loss: 0.40034985542297363 *\n",
      "Epoch: 13, Train_Loss: 0.4531857967376709, Test_Loss: 0.4186917841434479\n",
      "Epoch: 13, Train_Loss: 0.4124394953250885, Test_Loss: 0.4466640055179596\n",
      "Epoch: 13, Train_Loss: 0.404328852891922, Test_Loss: 0.4638116955757141\n",
      "Epoch: 13, Train_Loss: 0.47149303555488586, Test_Loss: 0.4031091332435608 *\n",
      "Epoch: 13, Train_Loss: 0.6823632717132568, Test_Loss: 0.42843976616859436\n",
      "Epoch: 13, Train_Loss: 0.7971291542053223, Test_Loss: 0.4332137703895569\n",
      "Epoch: 13, Train_Loss: 1.1935780048370361, Test_Loss: 0.5069692134857178\n",
      "Epoch: 13, Train_Loss: 1.5317482948303223, Test_Loss: 0.418326735496521 *\n",
      "Epoch: 13, Train_Loss: 0.555945634841919, Test_Loss: 0.45904964208602905\n",
      "Epoch: 13, Train_Loss: 0.6526644229888916, Test_Loss: 0.4485083222389221 *\n",
      "Epoch: 13, Train_Loss: 0.4001135230064392, Test_Loss: 0.46903419494628906\n",
      "Epoch: 13, Train_Loss: 0.42401179671287537, Test_Loss: 0.3965332508087158 *\n",
      "Epoch: 13, Train_Loss: 0.7583552598953247, Test_Loss: 0.45812103152275085\n",
      "Epoch: 13, Train_Loss: 1.3043389320373535, Test_Loss: 0.4475560784339905 *\n",
      "Epoch: 13, Train_Loss: 0.45742267370224, Test_Loss: 0.4221627712249756 *\n",
      "Epoch: 13, Train_Loss: 0.44708943367004395, Test_Loss: 0.454041063785553\n",
      "Epoch: 13, Train_Loss: 0.4359382092952728, Test_Loss: 0.4688546657562256\n",
      "Epoch: 13, Train_Loss: 0.5412017107009888, Test_Loss: 0.4357101023197174 *\n",
      "Epoch: 13, Train_Loss: 0.6524251699447632, Test_Loss: 0.448475182056427\n",
      "Epoch: 13, Train_Loss: 0.6908602714538574, Test_Loss: 0.45215874910354614\n",
      "Epoch: 13, Train_Loss: 0.5765939950942993, Test_Loss: 0.4726264774799347\n",
      "Epoch: 13, Train_Loss: 0.7253847718238831, Test_Loss: 0.5048161149024963\n",
      "Epoch: 13, Train_Loss: 0.40637296438217163, Test_Loss: 0.47041162848472595 *\n",
      "Epoch: 13, Train_Loss: 0.4219115972518921, Test_Loss: 0.5076659917831421\n",
      "Epoch: 13, Train_Loss: 0.42255598306655884, Test_Loss: 0.4391801655292511 *\n",
      "Epoch: 13, Train_Loss: 0.47345268726348877, Test_Loss: 0.4172990918159485 *\n",
      "Epoch: 13, Train_Loss: 0.40053629875183105, Test_Loss: 0.4722031056880951\n",
      "Epoch: 13, Train_Loss: 0.44170576333999634, Test_Loss: 0.5018213391304016\n",
      "Epoch: 13, Train_Loss: 16.20792579650879, Test_Loss: 0.4976482689380646 *\n",
      "Epoch: 13, Train_Loss: 0.4649447500705719, Test_Loss: 0.4219376742839813 *\n",
      "Epoch: 13, Train_Loss: 1.7848092317581177, Test_Loss: 0.4323914051055908\n",
      "Epoch: 13, Train_Loss: 1.75434148311615, Test_Loss: 0.4644564986228943\n",
      "Epoch: 13, Train_Loss: 0.4287950396537781, Test_Loss: 0.4369771480560303 *\n",
      "Epoch: 13, Train_Loss: 0.48552900552749634, Test_Loss: 0.5180630683898926\n",
      "Epoch: 13, Train_Loss: 2.0596861839294434, Test_Loss: 0.606190025806427\n",
      "Epoch: 13, Train_Loss: 6.54544734954834, Test_Loss: 0.705101490020752\n",
      "Epoch: 13, Train_Loss: 0.5414354801177979, Test_Loss: 0.5845353603363037 *\n",
      "Epoch: 13, Train_Loss: 0.45138096809387207, Test_Loss: 0.6198309659957886\n",
      "Epoch: 13, Train_Loss: 4.422608852386475, Test_Loss: 0.7035462856292725\n",
      "Epoch: 13, Train_Loss: 0.8883365392684937, Test_Loss: 0.8532800078392029\n",
      "Epoch: 13, Train_Loss: 0.6771731376647949, Test_Loss: 1.3901935815811157\n",
      "Epoch: 13, Train_Loss: 0.3995825946331024, Test_Loss: 0.466854453086853 *\n",
      "Epoch: 13, Train_Loss: 0.4154708981513977, Test_Loss: 1.0222303867340088\n",
      "Epoch: 13, Train_Loss: 0.5065513253211975, Test_Loss: 0.46789640188217163 *\n",
      "Epoch: 13, Train_Loss: 0.3961876332759857, Test_Loss: 1.421587347984314\n",
      "Epoch: 13, Train_Loss: 0.4543496370315552, Test_Loss: 2.2959418296813965\n",
      "Epoch: 13, Train_Loss: 0.3880314528942108, Test_Loss: 1.1718158721923828 *\n",
      "Epoch: 13, Train_Loss: 0.38798823952674866, Test_Loss: 0.4874986410140991 *\n",
      "Epoch: 13, Train_Loss: 0.4084062874317169, Test_Loss: 1.0178325176239014\n",
      "Epoch: 13, Train_Loss: 0.4650382101535797, Test_Loss: 1.548836350440979\n",
      "Epoch: 13, Train_Loss: 0.5382815599441528, Test_Loss: 1.4371744394302368 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train_Loss: 0.49710625410079956, Test_Loss: 1.285565972328186 *\n",
      "Epoch: 13, Train_Loss: 0.4353443682193756, Test_Loss: 0.6384704113006592 *\n",
      "Epoch: 13, Train_Loss: 0.39548519253730774, Test_Loss: 1.754607081413269\n",
      "Epoch: 13, Train_Loss: 0.41155725717544556, Test_Loss: 5.327752113342285\n",
      "Epoch: 13, Train_Loss: 0.3931836485862732, Test_Loss: 0.7056010961532593 *\n",
      "Epoch: 13, Train_Loss: 0.39181625843048096, Test_Loss: 0.935924768447876\n",
      "Epoch: 13, Train_Loss: 0.3944813907146454, Test_Loss: 0.8646383881568909 *\n",
      "Epoch: 13, Train_Loss: 0.3913176655769348, Test_Loss: 0.49392956495285034 *\n",
      "Epoch: 13, Train_Loss: 0.38997599482536316, Test_Loss: 0.5875647068023682\n",
      "Epoch: 13, Train_Loss: 0.3905980885028839, Test_Loss: 1.0287708044052124\n",
      "Epoch: 13, Train_Loss: 0.3914293944835663, Test_Loss: 0.7552827596664429 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 13\n",
      "Epoch: 13, Train_Loss: 0.3905647099018097, Test_Loss: 0.4197815954685211 *\n",
      "Epoch: 13, Train_Loss: 0.38988369703292847, Test_Loss: 0.5985962152481079\n",
      "Epoch: 13, Train_Loss: 0.3962850868701935, Test_Loss: 0.4896690845489502 *\n",
      "Epoch: 13, Train_Loss: 0.4224163293838501, Test_Loss: 0.8763629198074341\n",
      "Epoch: 13, Train_Loss: 0.4169681966304779, Test_Loss: 0.5828171968460083 *\n",
      "Epoch: 13, Train_Loss: 0.5254281163215637, Test_Loss: 0.87534499168396\n",
      "Epoch: 13, Train_Loss: 0.40773704648017883, Test_Loss: 0.79457688331604 *\n",
      "Epoch: 13, Train_Loss: 0.6824424266815186, Test_Loss: 0.41145092248916626 *\n",
      "Epoch: 13, Train_Loss: 8.212605476379395, Test_Loss: 0.4617897570133209\n",
      "Epoch: 13, Train_Loss: 0.43900200724601746, Test_Loss: 0.4709802269935608\n",
      "Epoch: 13, Train_Loss: 0.418369859457016, Test_Loss: 0.9503862857818604\n",
      "Epoch: 13, Train_Loss: 0.47048377990722656, Test_Loss: 0.490911602973938 *\n",
      "Epoch: 13, Train_Loss: 0.5324769616127014, Test_Loss: 0.6126540899276733\n",
      "Epoch: 13, Train_Loss: 0.4481806457042694, Test_Loss: 0.6304543018341064\n",
      "Epoch: 13, Train_Loss: 0.615638792514801, Test_Loss: 1.0958913564682007\n",
      "Epoch: 13, Train_Loss: 0.5330321788787842, Test_Loss: 1.0243175029754639 *\n",
      "Epoch: 13, Train_Loss: 0.5474714040756226, Test_Loss: 0.5719907283782959 *\n",
      "Epoch: 13, Train_Loss: 0.5002034902572632, Test_Loss: 0.45739075541496277 *\n",
      "Epoch: 13, Train_Loss: 0.45718806982040405, Test_Loss: 0.5708445906639099\n",
      "Epoch: 13, Train_Loss: 0.3910283148288727, Test_Loss: 0.49806588888168335 *\n",
      "Epoch: 13, Train_Loss: 0.4969416856765747, Test_Loss: 0.44605016708374023 *\n",
      "Epoch: 13, Train_Loss: 0.46246814727783203, Test_Loss: 0.5446376800537109\n",
      "Epoch: 13, Train_Loss: 0.5483902096748352, Test_Loss: 0.45446157455444336 *\n",
      "Epoch: 13, Train_Loss: 0.4429369568824768, Test_Loss: 3.871429681777954\n",
      "Epoch: 13, Train_Loss: 0.4171539545059204, Test_Loss: 2.486621856689453 *\n",
      "Epoch: 13, Train_Loss: 0.39697927236557007, Test_Loss: 0.3892104923725128 *\n",
      "Epoch: 13, Train_Loss: 0.4252624809741974, Test_Loss: 0.3907109797000885\n",
      "Epoch: 13, Train_Loss: 0.47469064593315125, Test_Loss: 0.39698025584220886\n",
      "Epoch: 13, Train_Loss: 0.41495925188064575, Test_Loss: 0.39853814244270325\n",
      "Epoch: 13, Train_Loss: 0.38598114252090454, Test_Loss: 0.41057461500167847\n",
      "Epoch: 13, Train_Loss: 0.3899829685688019, Test_Loss: 0.45190614461898804\n",
      "Epoch: 13, Train_Loss: 0.3995766341686249, Test_Loss: 0.4847458600997925\n",
      "Epoch: 13, Train_Loss: 1.170157551765442, Test_Loss: 0.3870241940021515 *\n",
      "Epoch: 13, Train_Loss: 5.464692115783691, Test_Loss: 0.42226749658584595\n",
      "Epoch: 13, Train_Loss: 0.38822296261787415, Test_Loss: 0.4039374887943268 *\n",
      "Epoch: 13, Train_Loss: 0.38880684971809387, Test_Loss: 0.40008053183555603 *\n",
      "Epoch: 13, Train_Loss: 0.3909975588321686, Test_Loss: 0.39303332567214966 *\n",
      "Epoch: 13, Train_Loss: 0.3902040421962738, Test_Loss: 0.4116573929786682\n",
      "Epoch: 13, Train_Loss: 0.38676801323890686, Test_Loss: 0.4287568926811218\n",
      "Epoch: 13, Train_Loss: 0.38517096638679504, Test_Loss: 0.5022260546684265\n",
      "Epoch: 13, Train_Loss: 0.39320510625839233, Test_Loss: 0.5011205673217773 *\n",
      "Epoch: 13, Train_Loss: 0.4236536920070648, Test_Loss: 0.4048839807510376 *\n",
      "Epoch: 13, Train_Loss: 0.4007156193256378, Test_Loss: 0.3886937201023102 *\n",
      "Epoch: 13, Train_Loss: 0.3935922384262085, Test_Loss: 0.3868994116783142 *\n",
      "Epoch: 13, Train_Loss: 0.3872263729572296, Test_Loss: 0.387613981962204\n",
      "Epoch: 13, Train_Loss: 0.38495808839797974, Test_Loss: 0.38754960894584656 *\n",
      "Epoch: 13, Train_Loss: 0.4032471776008606, Test_Loss: 0.3869048058986664 *\n",
      "Epoch: 13, Train_Loss: 0.3879867494106293, Test_Loss: 0.3876752257347107\n",
      "Epoch: 13, Train_Loss: 0.3884440064430237, Test_Loss: 0.3899499177932739\n",
      "Epoch: 13, Train_Loss: 0.41713210940361023, Test_Loss: 0.38871148228645325 *\n",
      "Epoch: 13, Train_Loss: 0.4223884046077728, Test_Loss: 0.3860504627227783 *\n",
      "Epoch: 13, Train_Loss: 0.3974878787994385, Test_Loss: 0.3884902000427246\n",
      "Epoch: 13, Train_Loss: 0.38427308201789856, Test_Loss: 0.39525681734085083\n",
      "Epoch: 13, Train_Loss: 0.3920222222805023, Test_Loss: 0.40149426460266113\n",
      "Epoch: 14, Train_Loss: 0.4392387866973877, Test_Loss: 0.41118115186691284 *\n",
      "Epoch: 14, Train_Loss: 0.4083666205406189, Test_Loss: 0.46021294593811035\n",
      "Epoch: 14, Train_Loss: 0.46130284667015076, Test_Loss: 0.7028952240943909\n",
      "Epoch: 14, Train_Loss: 0.402621865272522, Test_Loss: 0.6442041397094727 *\n",
      "Epoch: 14, Train_Loss: 0.484859824180603, Test_Loss: 0.45544978976249695 *\n",
      "Epoch: 14, Train_Loss: 0.4288841187953949, Test_Loss: 0.3882654011249542 *\n",
      "Epoch: 14, Train_Loss: 0.44444501399993896, Test_Loss: 0.40385469794273376\n",
      "Epoch: 14, Train_Loss: 0.4033116102218628, Test_Loss: 0.4556291699409485\n",
      "Epoch: 14, Train_Loss: 0.573822021484375, Test_Loss: 0.777338445186615\n",
      "Epoch: 14, Train_Loss: 0.43111297488212585, Test_Loss: 1.282116413116455\n",
      "Epoch: 14, Train_Loss: 0.3981340825557709, Test_Loss: 0.8450332880020142 *\n",
      "Epoch: 14, Train_Loss: 0.3882768750190735, Test_Loss: 0.44226330518722534 *\n",
      "Epoch: 14, Train_Loss: 0.38216444849967957, Test_Loss: 0.4027487635612488 *\n",
      "Epoch: 14, Train_Loss: 0.38202574849128723, Test_Loss: 0.4049336612224579\n",
      "Epoch: 14, Train_Loss: 0.3843159079551697, Test_Loss: 0.3935263454914093 *\n",
      "Epoch: 14, Train_Loss: 1.1515268087387085, Test_Loss: 0.39715680480003357\n",
      "Epoch: 14, Train_Loss: 4.458382606506348, Test_Loss: 0.4060910940170288\n",
      "Epoch: 14, Train_Loss: 0.3863513171672821, Test_Loss: 0.43119677901268005\n",
      "Epoch: 14, Train_Loss: 0.3933335542678833, Test_Loss: 0.38338878750801086 *\n",
      "Epoch: 14, Train_Loss: 0.3945564031600952, Test_Loss: 0.45399224758148193\n",
      "Epoch: 14, Train_Loss: 0.3810523450374603, Test_Loss: 0.555915355682373\n",
      "Epoch: 14, Train_Loss: 0.3810679614543915, Test_Loss: 0.6810681223869324\n",
      "Epoch: 14, Train_Loss: 0.38154804706573486, Test_Loss: 0.6161020994186401 *\n",
      "Epoch: 14, Train_Loss: 0.3807220458984375, Test_Loss: 0.39824140071868896 *\n",
      "Epoch: 14, Train_Loss: 0.38047486543655396, Test_Loss: 0.39503559470176697 *\n",
      "Epoch: 14, Train_Loss: 0.3816582262516022, Test_Loss: 0.39502230286598206 *\n",
      "Epoch: 14, Train_Loss: 0.45696601271629333, Test_Loss: 0.3951273262500763\n",
      "Epoch: 14, Train_Loss: 0.4430933892726898, Test_Loss: 0.40879395604133606\n",
      "Epoch: 14, Train_Loss: 0.4725528359413147, Test_Loss: 2.7424440383911133\n",
      "Epoch: 14, Train_Loss: 0.42163604497909546, Test_Loss: 3.446397304534912\n",
      "Epoch: 14, Train_Loss: 0.3818340599536896, Test_Loss: 0.3946869373321533 *\n",
      "Epoch: 14, Train_Loss: 0.5501339435577393, Test_Loss: 0.3850438892841339 *\n",
      "Epoch: 14, Train_Loss: 0.5998685359954834, Test_Loss: 0.38435378670692444 *\n",
      "Epoch: 14, Train_Loss: 0.5870597958564758, Test_Loss: 0.39475542306900024\n",
      "Epoch: 14, Train_Loss: 0.5035305619239807, Test_Loss: 0.383697509765625 *\n",
      "Epoch: 14, Train_Loss: 0.3815786838531494, Test_Loss: 0.3890051543712616\n",
      "Epoch: 14, Train_Loss: 0.37952494621276855, Test_Loss: 0.38588353991508484 *\n",
      "Epoch: 14, Train_Loss: 0.3816456198692322, Test_Loss: 0.38184496760368347 *\n",
      "Epoch: 14, Train_Loss: 0.39273160696029663, Test_Loss: 0.3852212131023407\n",
      "Epoch: 14, Train_Loss: 0.3978549540042877, Test_Loss: 0.3817281723022461 *\n",
      "Epoch: 14, Train_Loss: 0.3943909704685211, Test_Loss: 0.3962632417678833\n",
      "Epoch: 14, Train_Loss: 0.38194358348846436, Test_Loss: 0.3979794383049011\n",
      "Epoch: 14, Train_Loss: 0.3788755238056183, Test_Loss: 0.3855052590370178 *\n",
      "Epoch: 14, Train_Loss: 0.38977834582328796, Test_Loss: 0.38814011216163635\n",
      "Epoch: 14, Train_Loss: 0.41806042194366455, Test_Loss: 0.37960371375083923 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train_Loss: 0.5530555248260498, Test_Loss: 0.38309285044670105\n",
      "Epoch: 14, Train_Loss: 0.5374562740325928, Test_Loss: 0.3855433464050293\n",
      "Epoch: 14, Train_Loss: 0.5341507792472839, Test_Loss: 0.3934931755065918\n",
      "Epoch: 14, Train_Loss: 0.43787550926208496, Test_Loss: 0.38508373498916626 *\n",
      "Epoch: 14, Train_Loss: 0.5037844181060791, Test_Loss: 0.38092347979545593 *\n",
      "Epoch: 14, Train_Loss: 0.4621008038520813, Test_Loss: 0.392555832862854\n",
      "Epoch: 14, Train_Loss: 0.48177170753479004, Test_Loss: 0.40975385904312134\n",
      "Epoch: 14, Train_Loss: 0.5013576149940491, Test_Loss: 0.41292473673820496\n",
      "Epoch: 14, Train_Loss: 0.67206871509552, Test_Loss: 0.3900132179260254 *\n",
      "Epoch: 14, Train_Loss: 0.39098384976387024, Test_Loss: 0.3825165033340454 *\n",
      "Epoch: 14, Train_Loss: 0.38354817032814026, Test_Loss: 0.39716073870658875\n",
      "Epoch: 14, Train_Loss: 3.226545572280884, Test_Loss: 0.3873249590396881 *\n",
      "Epoch: 14, Train_Loss: 0.7781323194503784, Test_Loss: 0.38398149609565735 *\n",
      "Epoch: 14, Train_Loss: 0.4138408303260803, Test_Loss: 0.4483160078525543\n",
      "Epoch: 14, Train_Loss: 0.41648927330970764, Test_Loss: 0.4111156463623047 *\n",
      "Epoch: 14, Train_Loss: 0.4030029773712158, Test_Loss: 4.902381420135498\n",
      "Epoch: 14, Train_Loss: 0.397032231092453, Test_Loss: 1.2610160112380981 *\n",
      "Epoch: 14, Train_Loss: 0.3864884674549103, Test_Loss: 0.37836122512817383 *\n",
      "Epoch: 14, Train_Loss: 0.4413861632347107, Test_Loss: 0.39500266313552856\n",
      "Epoch: 14, Train_Loss: 0.5305012464523315, Test_Loss: 0.4146846830844879\n",
      "Epoch: 14, Train_Loss: 0.47600284218788147, Test_Loss: 0.433730810880661\n",
      "Epoch: 14, Train_Loss: 0.4484300911426544, Test_Loss: 0.39136895537376404 *\n",
      "Epoch: 14, Train_Loss: 0.4387466311454773, Test_Loss: 0.4506392478942871\n",
      "Epoch: 14, Train_Loss: 0.4037104547023773, Test_Loss: 0.44742292165756226 *\n",
      "Epoch: 14, Train_Loss: 0.4077395498752594, Test_Loss: 0.3796842098236084 *\n",
      "Epoch: 14, Train_Loss: 0.38412779569625854, Test_Loss: 0.41307881474494934\n",
      "Epoch: 14, Train_Loss: 0.4280392825603485, Test_Loss: 0.39519715309143066 *\n",
      "Epoch: 14, Train_Loss: 0.406182199716568, Test_Loss: 0.3879131078720093 *\n",
      "Epoch: 14, Train_Loss: 0.3777381181716919, Test_Loss: 0.38211944699287415 *\n",
      "Epoch: 14, Train_Loss: 0.38726335763931274, Test_Loss: 0.4561561644077301\n",
      "Epoch: 14, Train_Loss: 0.419031023979187, Test_Loss: 0.4191166162490845 *\n",
      "Epoch: 14, Train_Loss: 0.4181811213493347, Test_Loss: 0.4820622205734253\n",
      "Epoch: 14, Train_Loss: 0.37898287177085876, Test_Loss: 0.43888601660728455 *\n",
      "Epoch: 14, Train_Loss: 0.37601006031036377, Test_Loss: 0.4197528660297394 *\n",
      "Epoch: 14, Train_Loss: 0.37614455819129944, Test_Loss: 0.39091944694519043 *\n",
      "Epoch: 14, Train_Loss: 0.3754730224609375, Test_Loss: 0.39342841506004333\n",
      "Epoch: 14, Train_Loss: 0.3760775327682495, Test_Loss: 0.39383429288864136\n",
      "Epoch: 14, Train_Loss: 0.3765266239643097, Test_Loss: 0.39376792311668396 *\n",
      "Epoch: 14, Train_Loss: 0.3773469924926758, Test_Loss: 0.39624956250190735\n",
      "Epoch: 14, Train_Loss: 0.3772665560245514, Test_Loss: 0.3959462642669678 *\n",
      "Epoch: 14, Train_Loss: 0.3756290376186371, Test_Loss: 0.38705211877822876 *\n",
      "Epoch: 14, Train_Loss: 0.37574896216392517, Test_Loss: 0.4032837748527527\n",
      "Epoch: 14, Train_Loss: 0.3769470155239105, Test_Loss: 0.40180420875549316 *\n",
      "Epoch: 14, Train_Loss: 0.3890622854232788, Test_Loss: 0.38866785168647766 *\n",
      "Epoch: 14, Train_Loss: 0.3901630640029907, Test_Loss: 0.3883143663406372 *\n",
      "Epoch: 14, Train_Loss: 0.3897135257720947, Test_Loss: 0.44450557231903076\n",
      "Epoch: 14, Train_Loss: 0.3935180902481079, Test_Loss: 0.4152241349220276 *\n",
      "Epoch: 14, Train_Loss: 0.3846580982208252, Test_Loss: 0.5704009532928467\n",
      "Epoch: 14, Train_Loss: 0.3749285936355591, Test_Loss: 0.883095383644104\n",
      "Epoch: 14, Train_Loss: 0.37600138783454895, Test_Loss: 0.6684842705726624 *\n",
      "Epoch: 14, Train_Loss: 0.38959503173828125, Test_Loss: 0.47623562812805176 *\n",
      "Epoch: 14, Train_Loss: 0.4008784890174866, Test_Loss: 0.4011094570159912 *\n",
      "Epoch: 14, Train_Loss: 0.3754223883152008, Test_Loss: 0.3841351568698883 *\n",
      "Epoch: 14, Train_Loss: 0.3785304129123688, Test_Loss: 0.44270244240760803\n",
      "Epoch: 14, Train_Loss: 0.37670841813087463, Test_Loss: 0.8404483795166016\n",
      "Model saved at location save_new\\model.ckpt at epoch 14\n",
      "Epoch: 14, Train_Loss: 0.40782785415649414, Test_Loss: 1.1227728128433228\n",
      "Epoch: 14, Train_Loss: 0.42150306701660156, Test_Loss: 0.6473114490509033 *\n",
      "Epoch: 14, Train_Loss: 0.43713098764419556, Test_Loss: 0.46886759996414185 *\n",
      "Epoch: 14, Train_Loss: 0.3940353989601135, Test_Loss: 0.37958085536956787 *\n",
      "Epoch: 14, Train_Loss: 0.37623128294944763, Test_Loss: 0.37989240884780884\n",
      "Epoch: 14, Train_Loss: 0.45455673336982727, Test_Loss: 0.37492236495018005 *\n",
      "Epoch: 14, Train_Loss: 0.37976571917533875, Test_Loss: 0.3866262435913086\n",
      "Epoch: 14, Train_Loss: 0.38173025846481323, Test_Loss: 0.39598768949508667\n",
      "Epoch: 14, Train_Loss: 0.3990255892276764, Test_Loss: 0.410747766494751\n",
      "Epoch: 14, Train_Loss: 0.3856218159198761, Test_Loss: 0.37710288166999817 *\n",
      "Epoch: 14, Train_Loss: 0.5042632222175598, Test_Loss: 0.4879075884819031\n",
      "Epoch: 14, Train_Loss: 0.44041574001312256, Test_Loss: 0.7232236862182617\n",
      "Epoch: 14, Train_Loss: 0.4055323600769043, Test_Loss: 0.49919605255126953 *\n",
      "Epoch: 14, Train_Loss: 0.3821915090084076, Test_Loss: 0.5865384936332703\n",
      "Epoch: 14, Train_Loss: 0.3925681710243225, Test_Loss: 0.3885324001312256 *\n",
      "Epoch: 14, Train_Loss: 0.3803289234638214, Test_Loss: 0.38815537095069885 *\n",
      "Epoch: 14, Train_Loss: 0.3764129877090454, Test_Loss: 0.388169527053833\n",
      "Epoch: 14, Train_Loss: 0.38451912999153137, Test_Loss: 0.3878512978553772 *\n",
      "Epoch: 14, Train_Loss: 0.38809505105018616, Test_Loss: 0.40333282947540283\n",
      "Epoch: 14, Train_Loss: 0.4064306616783142, Test_Loss: 4.154418468475342\n",
      "Epoch: 14, Train_Loss: 0.46440884470939636, Test_Loss: 2.0234375 *\n",
      "Epoch: 14, Train_Loss: 0.39022529125213623, Test_Loss: 0.3837563097476959 *\n",
      "Epoch: 14, Train_Loss: 0.4250454902648926, Test_Loss: 0.37712812423706055 *\n",
      "Epoch: 14, Train_Loss: 0.3951764702796936, Test_Loss: 0.3767661154270172 *\n",
      "Epoch: 14, Train_Loss: 0.3920171856880188, Test_Loss: 0.3889947533607483\n",
      "Epoch: 14, Train_Loss: 0.4709671139717102, Test_Loss: 0.3757125437259674 *\n",
      "Epoch: 14, Train_Loss: 0.6108033061027527, Test_Loss: 0.3804839551448822\n",
      "Epoch: 14, Train_Loss: 0.38045865297317505, Test_Loss: 0.3732994794845581 *\n",
      "Epoch: 14, Train_Loss: 0.4121331572532654, Test_Loss: 0.3730354309082031 *\n",
      "Epoch: 14, Train_Loss: 0.3712826669216156, Test_Loss: 0.3754366636276245\n",
      "Epoch: 14, Train_Loss: 0.3710639476776123, Test_Loss: 0.3752745985984802 *\n",
      "Epoch: 14, Train_Loss: 0.37193623185157776, Test_Loss: 0.3836190402507782\n",
      "Epoch: 14, Train_Loss: 0.3709869384765625, Test_Loss: 0.39657658338546753\n",
      "Epoch: 14, Train_Loss: 0.3810557425022125, Test_Loss: 0.3800104558467865 *\n",
      "Epoch: 14, Train_Loss: 0.3866410553455353, Test_Loss: 0.3737260103225708 *\n",
      "Epoch: 14, Train_Loss: 0.3815608620643616, Test_Loss: 0.3710789382457733 *\n",
      "Epoch: 14, Train_Loss: 0.3835369050502777, Test_Loss: 0.37352022528648376\n",
      "Epoch: 14, Train_Loss: 0.3821152448654175, Test_Loss: 0.3794848322868347\n",
      "Epoch: 14, Train_Loss: 0.3732435405254364, Test_Loss: 0.3761875331401825 *\n",
      "Epoch: 14, Train_Loss: 0.37157928943634033, Test_Loss: 0.3737166225910187 *\n",
      "Epoch: 14, Train_Loss: 0.3698115348815918, Test_Loss: 0.37056756019592285 *\n",
      "Epoch: 14, Train_Loss: 0.4008468985557556, Test_Loss: 0.38010916113853455\n",
      "Epoch: 14, Train_Loss: 0.40516769886016846, Test_Loss: 0.39042651653289795\n",
      "Epoch: 14, Train_Loss: 0.3976658284664154, Test_Loss: 0.3906298577785492\n",
      "Epoch: 14, Train_Loss: 0.37673765420913696, Test_Loss: 0.37396034598350525 *\n",
      "Epoch: 14, Train_Loss: 0.4315182566642761, Test_Loss: 0.3723924160003662 *\n",
      "Epoch: 14, Train_Loss: 0.40808767080307007, Test_Loss: 0.37883704900741577\n",
      "Epoch: 14, Train_Loss: 0.39176198840141296, Test_Loss: 0.3747197389602661 *\n",
      "Epoch: 14, Train_Loss: 0.38314634561538696, Test_Loss: 0.37287136912345886 *\n",
      "Epoch: 14, Train_Loss: 0.40290892124176025, Test_Loss: 0.43413013219833374\n",
      "Epoch: 14, Train_Loss: 0.3731919825077057, Test_Loss: 0.41455087065696716 *\n",
      "Epoch: 14, Train_Loss: 0.38344860076904297, Test_Loss: 5.684647560119629\n",
      "Epoch: 14, Train_Loss: 0.39498579502105713, Test_Loss: 0.46037527918815613 *\n",
      "Epoch: 14, Train_Loss: 0.41168972849845886, Test_Loss: 0.3699187934398651 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train_Loss: 2.3364651203155518, Test_Loss: 0.3967426121234894\n",
      "Epoch: 14, Train_Loss: 3.82177996635437, Test_Loss: 0.4058193564414978\n",
      "Epoch: 14, Train_Loss: 0.3782966136932373, Test_Loss: 0.42711901664733887\n",
      "Epoch: 14, Train_Loss: 0.38175931572914124, Test_Loss: 0.3742106556892395 *\n",
      "Epoch: 14, Train_Loss: 0.39940619468688965, Test_Loss: 0.46660518646240234\n",
      "Epoch: 14, Train_Loss: 0.5332258939743042, Test_Loss: 0.42549073696136475 *\n",
      "Epoch: 14, Train_Loss: 0.40049973130226135, Test_Loss: 0.36978721618652344 *\n",
      "Epoch: 14, Train_Loss: 0.3767186403274536, Test_Loss: 0.40745624899864197\n",
      "Epoch: 14, Train_Loss: 0.3689500689506531, Test_Loss: 0.3852238357067108 *\n",
      "Epoch: 14, Train_Loss: 0.4337001144886017, Test_Loss: 0.376982718706131 *\n",
      "Epoch: 14, Train_Loss: 0.38040146231651306, Test_Loss: 0.3930734395980835\n",
      "Epoch: 14, Train_Loss: 0.3843459486961365, Test_Loss: 0.4305691719055176\n",
      "Epoch: 14, Train_Loss: 0.9544350504875183, Test_Loss: 0.4075833559036255 *\n",
      "Epoch: 14, Train_Loss: 1.56981360912323, Test_Loss: 0.4628664553165436\n",
      "Epoch: 14, Train_Loss: 1.094726800918579, Test_Loss: 0.4127083718776703 *\n",
      "Epoch: 14, Train_Loss: 0.4448851943016052, Test_Loss: 0.41467559337615967\n",
      "Epoch: 14, Train_Loss: 0.8355617523193359, Test_Loss: 0.3788564205169678 *\n",
      "Epoch: 14, Train_Loss: 2.3782801628112793, Test_Loss: 0.3742775022983551 *\n",
      "Epoch: 14, Train_Loss: 0.8829306364059448, Test_Loss: 0.37969622015953064\n",
      "Epoch: 14, Train_Loss: 0.39646056294441223, Test_Loss: 0.3807991147041321\n",
      "Epoch: 14, Train_Loss: 0.38640138506889343, Test_Loss: 0.3832685053348541\n",
      "Epoch: 14, Train_Loss: 1.0804044008255005, Test_Loss: 0.37999385595321655 *\n",
      "Epoch: 14, Train_Loss: 1.255788803100586, Test_Loss: 0.3688405454158783 *\n",
      "Epoch: 14, Train_Loss: 0.6334185600280762, Test_Loss: 0.3721630275249481\n",
      "Epoch: 14, Train_Loss: 0.3748593330383301, Test_Loss: 0.3786439001560211\n",
      "Epoch: 14, Train_Loss: 0.38056695461273193, Test_Loss: 0.4218955338001251\n",
      "Epoch: 14, Train_Loss: 0.6772425770759583, Test_Loss: 0.43426910042762756\n",
      "Epoch: 14, Train_Loss: 0.4775134027004242, Test_Loss: 0.3777744174003601 *\n",
      "Epoch: 14, Train_Loss: 0.4379733204841614, Test_Loss: 0.4561430811882019\n",
      "Epoch: 14, Train_Loss: 0.42593491077423096, Test_Loss: 0.5347422957420349\n",
      "Epoch: 14, Train_Loss: 0.38935714960098267, Test_Loss: 0.49919766187667847 *\n",
      "Epoch: 14, Train_Loss: 0.38125768303871155, Test_Loss: 0.38877832889556885 *\n",
      "Epoch: 14, Train_Loss: 0.4983198642730713, Test_Loss: 0.4202684462070465\n",
      "Epoch: 14, Train_Loss: 0.4995511472225189, Test_Loss: 0.4099317491054535 *\n",
      "Epoch: 14, Train_Loss: 0.4142588973045349, Test_Loss: 0.42230987548828125\n",
      "Epoch: 14, Train_Loss: 0.46533581614494324, Test_Loss: 0.4703899621963501\n",
      "Epoch: 14, Train_Loss: 0.42916369438171387, Test_Loss: 0.5522822141647339\n",
      "Epoch: 14, Train_Loss: 0.4806775748729706, Test_Loss: 0.4274369180202484 *\n",
      "Epoch: 14, Train_Loss: 0.4815729856491089, Test_Loss: 0.5689948797225952\n",
      "Epoch: 14, Train_Loss: 0.550593376159668, Test_Loss: 0.44991543889045715 *\n",
      "Epoch: 14, Train_Loss: 0.3933563232421875, Test_Loss: 0.39588844776153564 *\n",
      "Epoch: 14, Train_Loss: 0.4631122350692749, Test_Loss: 0.3864448368549347 *\n",
      "Epoch: 14, Train_Loss: 0.41333600878715515, Test_Loss: 0.3895933926105499\n",
      "Epoch: 14, Train_Loss: 0.4137800931930542, Test_Loss: 0.45151934027671814\n",
      "Epoch: 14, Train_Loss: 0.37928739190101624, Test_Loss: 0.38752537965774536 *\n",
      "Epoch: 14, Train_Loss: 0.3763502538204193, Test_Loss: 0.4483793377876282\n",
      "Model saved at location save_new\\model.ckpt at epoch 14\n",
      "Epoch: 14, Train_Loss: 0.37312188744544983, Test_Loss: 0.4053993821144104 *\n",
      "Epoch: 14, Train_Loss: 0.3826751708984375, Test_Loss: 0.546714723110199\n",
      "Epoch: 14, Train_Loss: 0.38886207342147827, Test_Loss: 0.7169517278671265\n",
      "Epoch: 14, Train_Loss: 0.37449437379837036, Test_Loss: 0.536409854888916 *\n",
      "Epoch: 14, Train_Loss: 0.3744067847728729, Test_Loss: 0.9794487953186035\n",
      "Epoch: 14, Train_Loss: 0.4510706961154938, Test_Loss: 0.4084542989730835 *\n",
      "Epoch: 14, Train_Loss: 0.6439207196235657, Test_Loss: 0.3981648087501526 *\n",
      "Epoch: 14, Train_Loss: 0.3911430239677429, Test_Loss: 0.3973931670188904 *\n",
      "Epoch: 14, Train_Loss: 0.4024239182472229, Test_Loss: 0.4265870451927185\n",
      "Epoch: 14, Train_Loss: 0.4564417004585266, Test_Loss: 0.4609638750553131\n",
      "Epoch: 14, Train_Loss: 0.49904048442840576, Test_Loss: 5.012175559997559\n",
      "Epoch: 14, Train_Loss: 0.537757396697998, Test_Loss: 0.8662760257720947 *\n",
      "Epoch: 14, Train_Loss: 0.3955664336681366, Test_Loss: 0.40303242206573486 *\n",
      "Epoch: 14, Train_Loss: 0.4286629557609558, Test_Loss: 0.39074063301086426 *\n",
      "Epoch: 14, Train_Loss: 0.6290801763534546, Test_Loss: 0.38055676221847534 *\n",
      "Epoch: 14, Train_Loss: 0.5413253903388977, Test_Loss: 0.3772658705711365 *\n",
      "Epoch: 14, Train_Loss: 0.42893850803375244, Test_Loss: 0.42614513635635376\n",
      "Epoch: 14, Train_Loss: 0.38120850920677185, Test_Loss: 0.4264020621776581\n",
      "Epoch: 14, Train_Loss: 0.3877331614494324, Test_Loss: 0.38364166021347046 *\n",
      "Epoch: 14, Train_Loss: 0.9395965337753296, Test_Loss: 0.37072569131851196 *\n",
      "Epoch: 14, Train_Loss: 0.9540222883224487, Test_Loss: 0.39113640785217285\n",
      "Epoch: 14, Train_Loss: 0.4594579339027405, Test_Loss: 0.48432767391204834\n",
      "Epoch: 14, Train_Loss: 0.41309478878974915, Test_Loss: 0.42513710260391235 *\n",
      "Epoch: 14, Train_Loss: 0.3675582706928253, Test_Loss: 0.3979211151599884 *\n",
      "Epoch: 14, Train_Loss: 0.37512367963790894, Test_Loss: 0.432905912399292\n",
      "Epoch: 14, Train_Loss: 0.7455199956893921, Test_Loss: 0.3849795162677765 *\n",
      "Epoch: 14, Train_Loss: 0.38345101475715637, Test_Loss: 0.37247082591056824 *\n",
      "Epoch: 14, Train_Loss: 0.463314950466156, Test_Loss: 0.39429545402526855\n",
      "Epoch: 14, Train_Loss: 0.46500855684280396, Test_Loss: 0.46708381175994873\n",
      "Epoch: 14, Train_Loss: 0.442923903465271, Test_Loss: 0.4096543490886688 *\n",
      "Epoch: 14, Train_Loss: 0.38644173741340637, Test_Loss: 0.40113380551338196 *\n",
      "Epoch: 14, Train_Loss: 0.4154740273952484, Test_Loss: 0.41644948720932007\n",
      "Epoch: 14, Train_Loss: 0.47517937421798706, Test_Loss: 0.4725378453731537\n",
      "Epoch: 14, Train_Loss: 0.3915552794933319, Test_Loss: 0.4824696481227875\n",
      "Epoch: 14, Train_Loss: 0.43179041147232056, Test_Loss: 0.49608850479125977\n",
      "Epoch: 14, Train_Loss: 0.3986150622367859, Test_Loss: 0.39452236890792847 *\n",
      "Epoch: 14, Train_Loss: 0.4394304156303406, Test_Loss: 0.4010836184024811\n",
      "Epoch: 14, Train_Loss: 0.419232577085495, Test_Loss: 0.4508272707462311\n",
      "Epoch: 14, Train_Loss: 0.3904286324977875, Test_Loss: 0.40700453519821167 *\n",
      "Epoch: 14, Train_Loss: 0.37395697832107544, Test_Loss: 0.3871321678161621 *\n",
      "Epoch: 14, Train_Loss: 0.45564964413642883, Test_Loss: 0.523052990436554\n",
      "Epoch: 14, Train_Loss: 0.620580792427063, Test_Loss: 0.9637432098388672\n",
      "Epoch: 14, Train_Loss: 0.6409270167350769, Test_Loss: 5.167922496795654\n",
      "Epoch: 14, Train_Loss: 0.7883661389350891, Test_Loss: 0.3837239146232605 *\n",
      "Epoch: 14, Train_Loss: 0.6500312089920044, Test_Loss: 0.3997995853424072\n",
      "Epoch: 14, Train_Loss: 0.5699020028114319, Test_Loss: 0.41255104541778564\n",
      "Epoch: 14, Train_Loss: 0.513427197933197, Test_Loss: 0.37233850359916687 *\n",
      "Epoch: 14, Train_Loss: 0.41744422912597656, Test_Loss: 0.3875773549079895\n",
      "Epoch: 14, Train_Loss: 0.374525785446167, Test_Loss: 0.38957637548446655\n",
      "Epoch: 14, Train_Loss: 0.3768671154975891, Test_Loss: 0.4248412549495697\n",
      "Epoch: 14, Train_Loss: 0.4126705527305603, Test_Loss: 0.3876340985298157 *\n",
      "Epoch: 14, Train_Loss: 0.6179176568984985, Test_Loss: 0.36941397190093994 *\n",
      "Epoch: 14, Train_Loss: 0.7044272422790527, Test_Loss: 0.38378971815109253\n",
      "Epoch: 14, Train_Loss: 0.8437521457672119, Test_Loss: 0.426648885011673\n",
      "Epoch: 14, Train_Loss: 1.594732403755188, Test_Loss: 0.37401366233825684 *\n",
      "Epoch: 14, Train_Loss: 0.6225860118865967, Test_Loss: 0.4369273781776428\n",
      "Epoch: 14, Train_Loss: 0.654674768447876, Test_Loss: 0.4006587266921997 *\n",
      "Epoch: 14, Train_Loss: 0.3808610439300537, Test_Loss: 0.44600754976272583\n",
      "Epoch: 14, Train_Loss: 0.3719634711742401, Test_Loss: 0.3836086094379425 *\n",
      "Epoch: 14, Train_Loss: 0.655234694480896, Test_Loss: 0.4021582305431366\n",
      "Epoch: 14, Train_Loss: 1.2812646627426147, Test_Loss: 0.40878239274024963\n",
      "Epoch: 14, Train_Loss: 0.5454766154289246, Test_Loss: 0.38190528750419617 *\n",
      "Epoch: 14, Train_Loss: 0.4046013653278351, Test_Loss: 0.4237265884876251\n",
      "Epoch: 14, Train_Loss: 0.41390103101730347, Test_Loss: 0.41174352169036865 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train_Loss: 0.4696284234523773, Test_Loss: 0.42494824528694153\n",
      "Epoch: 14, Train_Loss: 0.6598747968673706, Test_Loss: 0.40565618872642517 *\n",
      "Epoch: 14, Train_Loss: 0.5668996572494507, Test_Loss: 0.4204185903072357\n",
      "Epoch: 14, Train_Loss: 0.5079395771026611, Test_Loss: 0.4734361171722412\n",
      "Epoch: 14, Train_Loss: 0.6510330438613892, Test_Loss: 0.404652863740921 *\n",
      "Epoch: 14, Train_Loss: 0.39966604113578796, Test_Loss: 0.4288984537124634\n",
      "Epoch: 14, Train_Loss: 0.3735817074775696, Test_Loss: 0.49922651052474976\n",
      "Epoch: 14, Train_Loss: 0.41300633549690247, Test_Loss: 0.45976942777633667 *\n",
      "Epoch: 14, Train_Loss: 0.4294818639755249, Test_Loss: 0.3783620297908783 *\n",
      "Epoch: 14, Train_Loss: 0.42363595962524414, Test_Loss: 0.5159252882003784\n",
      "Epoch: 14, Train_Loss: 0.39694660902023315, Test_Loss: 0.4552650451660156 *\n",
      "Epoch: 14, Train_Loss: 11.793498992919922, Test_Loss: 0.4261744022369385 *\n",
      "Epoch: 14, Train_Loss: 5.085305690765381, Test_Loss: 0.38080042600631714 *\n",
      "Epoch: 14, Train_Loss: 1.1793802976608276, Test_Loss: 0.409098356962204\n",
      "Epoch: 14, Train_Loss: 1.709333062171936, Test_Loss: 0.40354254841804504 *\n",
      "Epoch: 14, Train_Loss: 0.5357834100723267, Test_Loss: 0.3736145496368408 *\n",
      "Epoch: 14, Train_Loss: 0.41177430748939514, Test_Loss: 0.4151421785354614\n",
      "Epoch: 14, Train_Loss: 0.9627199172973633, Test_Loss: 0.49535709619522095\n",
      "Epoch: 14, Train_Loss: 7.442129611968994, Test_Loss: 0.5136512517929077\n",
      "Epoch: 14, Train_Loss: 1.3575987815856934, Test_Loss: 0.4783669710159302 *\n",
      "Epoch: 14, Train_Loss: 0.39697688817977905, Test_Loss: 0.5913761854171753\n",
      "Epoch: 14, Train_Loss: 3.487272024154663, Test_Loss: 0.5515493154525757 *\n",
      "Epoch: 14, Train_Loss: 2.2885372638702393, Test_Loss: 0.6634409427642822\n",
      "Epoch: 14, Train_Loss: 0.808272659778595, Test_Loss: 1.0002583265304565\n",
      "Epoch: 14, Train_Loss: 0.3856671452522278, Test_Loss: 0.8016831874847412 *\n",
      "Epoch: 14, Train_Loss: 0.46575427055358887, Test_Loss: 0.5888146162033081 *\n",
      "Epoch: 14, Train_Loss: 0.4339595437049866, Test_Loss: 0.5196341872215271 *\n",
      "Epoch: 14, Train_Loss: 0.43899089097976685, Test_Loss: 0.9067490696907043\n",
      "Epoch: 14, Train_Loss: 0.3834204077720642, Test_Loss: 1.0982542037963867\n",
      "Epoch: 14, Train_Loss: 0.35976412892341614, Test_Loss: 1.3122367858886719\n",
      "Epoch: 14, Train_Loss: 0.3587576448917389, Test_Loss: 0.4858197569847107 *\n",
      "Epoch: 14, Train_Loss: 0.3746170997619629, Test_Loss: 1.0073212385177612\n",
      "Epoch: 14, Train_Loss: 0.40683263540267944, Test_Loss: 1.6036944389343262\n",
      "Epoch: 14, Train_Loss: 0.46506795287132263, Test_Loss: 1.5379599332809448 *\n",
      "Epoch: 14, Train_Loss: 0.49905723333358765, Test_Loss: 1.4263486862182617 *\n",
      "Epoch: 14, Train_Loss: 0.5207974910736084, Test_Loss: 0.9990498423576355 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 14\n",
      "Epoch: 14, Train_Loss: 0.4141342043876648, Test_Loss: 0.5453022122383118 *\n",
      "Epoch: 14, Train_Loss: 0.38806697726249695, Test_Loss: 8.621014595031738\n",
      "Epoch: 14, Train_Loss: 0.3676043450832367, Test_Loss: 0.6993284225463867 *\n",
      "Epoch: 14, Train_Loss: 0.3720342516899109, Test_Loss: 0.8048495650291443\n",
      "Epoch: 14, Train_Loss: 0.3596768081188202, Test_Loss: 0.7596349716186523 *\n",
      "Epoch: 14, Train_Loss: 0.3596300184726715, Test_Loss: 0.5829904079437256 *\n",
      "Epoch: 14, Train_Loss: 0.35785412788391113, Test_Loss: 0.4530523121356964 *\n",
      "Epoch: 14, Train_Loss: 0.3575949966907501, Test_Loss: 0.9457849860191345\n",
      "Epoch: 14, Train_Loss: 0.3580395579338074, Test_Loss: 0.932701587677002 *\n",
      "Epoch: 14, Train_Loss: 0.3592502474784851, Test_Loss: 0.4858945608139038 *\n",
      "Epoch: 14, Train_Loss: 0.3583097755908966, Test_Loss: 0.6810818910598755\n",
      "Epoch: 14, Train_Loss: 0.3598388135433197, Test_Loss: 0.6742600202560425 *\n",
      "Epoch: 14, Train_Loss: 0.38101476430892944, Test_Loss: 1.0828938484191895\n",
      "Epoch: 14, Train_Loss: 0.3776616156101227, Test_Loss: 0.8336215019226074 *\n",
      "Epoch: 14, Train_Loss: 0.44734808802604675, Test_Loss: 1.1356399059295654\n",
      "Epoch: 14, Train_Loss: 0.37394359707832336, Test_Loss: 1.2575284242630005\n",
      "Epoch: 14, Train_Loss: 0.4179682433605194, Test_Loss: 0.5025822520256042 *\n",
      "Epoch: 14, Train_Loss: 7.794612884521484, Test_Loss: 0.5167582631111145\n",
      "Epoch: 14, Train_Loss: 0.5941710472106934, Test_Loss: 0.4996776878833771 *\n",
      "Epoch: 14, Train_Loss: 0.40287697315216064, Test_Loss: 0.8201782703399658\n",
      "Epoch: 14, Train_Loss: 0.45297473669052124, Test_Loss: 0.6055653095245361 *\n",
      "Epoch: 14, Train_Loss: 0.5321062803268433, Test_Loss: 0.6674561500549316\n",
      "Epoch: 14, Train_Loss: 0.4251108765602112, Test_Loss: 0.5794143676757812 *\n",
      "Epoch: 14, Train_Loss: 0.49302732944488525, Test_Loss: 1.0164631605148315\n",
      "Epoch: 14, Train_Loss: 0.5036340355873108, Test_Loss: 0.8352429866790771 *\n",
      "Epoch: 14, Train_Loss: 0.5238638520240784, Test_Loss: 0.6616783738136292 *\n",
      "Epoch: 14, Train_Loss: 0.48533523082733154, Test_Loss: 0.4817120432853699 *\n",
      "Epoch: 14, Train_Loss: 0.4542393088340759, Test_Loss: 0.4950774013996124\n",
      "Epoch: 14, Train_Loss: 0.35853829979896545, Test_Loss: 0.5231922268867493\n",
      "Epoch: 14, Train_Loss: 0.45500272512435913, Test_Loss: 0.4626657962799072 *\n",
      "Epoch: 14, Train_Loss: 0.46039658784866333, Test_Loss: 0.4202597737312317 *\n",
      "Epoch: 14, Train_Loss: 0.5901544094085693, Test_Loss: 0.4894596338272095\n",
      "Epoch: 14, Train_Loss: 0.4816818833351135, Test_Loss: 2.181800127029419\n",
      "Epoch: 14, Train_Loss: 0.42785120010375977, Test_Loss: 4.078732967376709\n",
      "Epoch: 14, Train_Loss: 0.3832419514656067, Test_Loss: 0.3633340001106262 *\n",
      "Epoch: 14, Train_Loss: 0.3809141516685486, Test_Loss: 0.3560636341571808 *\n",
      "Epoch: 14, Train_Loss: 0.4262167811393738, Test_Loss: 0.38746052980422974\n",
      "Epoch: 14, Train_Loss: 0.39208465814590454, Test_Loss: 0.36599424481391907 *\n",
      "Epoch: 14, Train_Loss: 0.3586685359477997, Test_Loss: 0.4064626693725586\n",
      "Epoch: 14, Train_Loss: 0.35508862137794495, Test_Loss: 0.3860836923122406 *\n",
      "Epoch: 14, Train_Loss: 0.3578321635723114, Test_Loss: 0.46682968735694885\n",
      "Epoch: 14, Train_Loss: 0.45803430676460266, Test_Loss: 0.3638343811035156 *\n",
      "Epoch: 14, Train_Loss: 6.136485576629639, Test_Loss: 0.36947688460350037\n",
      "Epoch: 14, Train_Loss: 0.36582860350608826, Test_Loss: 0.38580647110939026\n",
      "Epoch: 14, Train_Loss: 0.35723546147346497, Test_Loss: 0.3697868287563324 *\n",
      "Epoch: 14, Train_Loss: 0.3636983335018158, Test_Loss: 0.3661046624183655 *\n",
      "Epoch: 14, Train_Loss: 0.3614782989025116, Test_Loss: 0.40100929141044617\n",
      "Epoch: 14, Train_Loss: 0.35782021284103394, Test_Loss: 0.40336501598358154\n",
      "Epoch: 14, Train_Loss: 0.3552747070789337, Test_Loss: 0.4328294098377228\n",
      "Epoch: 14, Train_Loss: 0.3575265407562256, Test_Loss: 0.48304206132888794\n",
      "Epoch: 14, Train_Loss: 0.40600720047950745, Test_Loss: 0.37497442960739136 *\n",
      "Epoch: 14, Train_Loss: 0.37332627177238464, Test_Loss: 0.37319400906562805 *\n",
      "Epoch: 14, Train_Loss: 0.3684024512767792, Test_Loss: 0.3565134108066559 *\n",
      "Epoch: 14, Train_Loss: 0.3557190001010895, Test_Loss: 0.35596951842308044 *\n",
      "Epoch: 14, Train_Loss: 0.3545622229576111, Test_Loss: 0.3570865988731384\n",
      "Epoch: 14, Train_Loss: 0.3699732720851898, Test_Loss: 0.35716086626052856\n",
      "Epoch: 14, Train_Loss: 0.35607922077178955, Test_Loss: 0.3556261360645294 *\n",
      "Epoch: 14, Train_Loss: 0.3559412360191345, Test_Loss: 0.35826537013053894\n",
      "Epoch: 14, Train_Loss: 0.3746267557144165, Test_Loss: 0.3584240674972534\n",
      "Epoch: 14, Train_Loss: 0.39365875720977783, Test_Loss: 0.35546401143074036 *\n",
      "Epoch: 14, Train_Loss: 0.3725068271160126, Test_Loss: 0.3590793311595917\n",
      "Epoch: 14, Train_Loss: 0.3534907102584839, Test_Loss: 0.3651486933231354\n",
      "Epoch: 14, Train_Loss: 0.3539332449436188, Test_Loss: 0.36250925064086914 *\n",
      "Epoch: 14, Train_Loss: 0.43860918283462524, Test_Loss: 0.39850008487701416\n",
      "Epoch: 14, Train_Loss: 0.42738208174705505, Test_Loss: 0.3880068063735962 *\n",
      "Epoch: 14, Train_Loss: 0.4214237332344055, Test_Loss: 0.6312699317932129\n",
      "Epoch: 14, Train_Loss: 0.38102418184280396, Test_Loss: 0.6931985020637512\n",
      "Epoch: 14, Train_Loss: 0.40811318159103394, Test_Loss: 0.461292564868927 *\n",
      "Epoch: 14, Train_Loss: 0.41646191477775574, Test_Loss: 0.36429962515830994 *\n",
      "Epoch: 14, Train_Loss: 0.4093790650367737, Test_Loss: 0.3731369972229004\n",
      "Epoch: 14, Train_Loss: 0.3895668387413025, Test_Loss: 0.3798874020576477\n",
      "Epoch: 14, Train_Loss: 0.5205725431442261, Test_Loss: 0.5220644474029541\n",
      "Epoch: 14, Train_Loss: 0.3775347173213959, Test_Loss: 1.1593214273452759\n",
      "Epoch: 14, Train_Loss: 0.37494054436683655, Test_Loss: 0.9720205068588257 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train_Loss: 0.3539024293422699, Test_Loss: 0.40404650568962097 *\n",
      "Epoch: 14, Train_Loss: 0.35226115584373474, Test_Loss: 0.3904310166835785 *\n",
      "Epoch: 14, Train_Loss: 0.35226204991340637, Test_Loss: 0.371038556098938 *\n",
      "Epoch: 14, Train_Loss: 0.35281628370285034, Test_Loss: 0.3671155273914337 *\n",
      "Epoch: 14, Train_Loss: 0.40208354592323303, Test_Loss: 0.37398451566696167\n",
      "Epoch: 14, Train_Loss: 5.042111396789551, Test_Loss: 0.3716582655906677 *\n",
      "Epoch: 14, Train_Loss: 0.49411770701408386, Test_Loss: 0.40895769000053406\n",
      "Epoch: 14, Train_Loss: 0.35988783836364746, Test_Loss: 0.35562118887901306 *\n",
      "Epoch: 14, Train_Loss: 0.36609822511672974, Test_Loss: 0.37808293104171753\n",
      "Epoch: 14, Train_Loss: 0.35354772210121155, Test_Loss: 0.47287285327911377\n",
      "Epoch: 14, Train_Loss: 0.35211190581321716, Test_Loss: 0.712022066116333\n",
      "Epoch: 14, Train_Loss: 0.3523634672164917, Test_Loss: 0.6192834377288818 *\n",
      "Epoch: 14, Train_Loss: 0.35138991475105286, Test_Loss: 0.37899288535118103 *\n",
      "Epoch: 14, Train_Loss: 0.3518509566783905, Test_Loss: 0.3708454966545105 *\n",
      "Epoch: 14, Train_Loss: 0.3515206575393677, Test_Loss: 0.3706890940666199 *\n",
      "Epoch: 14, Train_Loss: 0.4043409824371338, Test_Loss: 0.3712663948535919\n",
      "Epoch: 14, Train_Loss: 0.40912824869155884, Test_Loss: 0.3736545741558075\n",
      "Epoch: 14, Train_Loss: 0.4296615421772003, Test_Loss: 0.9249664545059204\n",
      "Epoch: 14, Train_Loss: 0.3927622139453888, Test_Loss: 5.181853771209717\n",
      "Epoch: 14, Train_Loss: 0.3547627329826355, Test_Loss: 0.3897135853767395 *\n",
      "Epoch: 14, Train_Loss: 0.46937984228134155, Test_Loss: 0.36320048570632935 *\n",
      "Epoch: 14, Train_Loss: 0.5741589069366455, Test_Loss: 0.3574967384338379 *\n",
      "Epoch: 14, Train_Loss: 0.5751121640205383, Test_Loss: 0.36427152156829834\n",
      "Epoch: 14, Train_Loss: 0.5392576456069946, Test_Loss: 0.358318954706192 *\n",
      "Epoch: 14, Train_Loss: 0.352784126996994, Test_Loss: 0.35602253675460815 *\n",
      "Epoch: 14, Train_Loss: 0.3509182035923004, Test_Loss: 0.35974445939064026\n",
      "Model saved at location save_new\\model.ckpt at epoch 14\n",
      "Epoch: 14, Train_Loss: 0.3514513373374939, Test_Loss: 0.35227805376052856 *\n",
      "Epoch: 14, Train_Loss: 0.364492803812027, Test_Loss: 0.3534298539161682\n",
      "Epoch: 14, Train_Loss: 0.3674955666065216, Test_Loss: 0.35510948300361633\n",
      "Epoch: 14, Train_Loss: 0.36625343561172485, Test_Loss: 0.37230437994003296\n",
      "Epoch: 14, Train_Loss: 0.3570999205112457, Test_Loss: 0.36250606179237366 *\n",
      "Epoch: 14, Train_Loss: 0.3499608635902405, Test_Loss: 0.3563445210456848 *\n",
      "Epoch: 14, Train_Loss: 0.35682329535484314, Test_Loss: 0.3701518774032593\n",
      "Epoch: 14, Train_Loss: 0.37364834547042847, Test_Loss: 0.35303688049316406 *\n",
      "Epoch: 14, Train_Loss: 0.5098452568054199, Test_Loss: 0.35533785820007324\n",
      "Epoch: 14, Train_Loss: 0.4941766858100891, Test_Loss: 0.35443490743637085 *\n",
      "Epoch: 14, Train_Loss: 0.492461621761322, Test_Loss: 0.377928763628006\n",
      "Epoch: 14, Train_Loss: 0.386616975069046, Test_Loss: 0.3565134108066559 *\n",
      "Epoch: 14, Train_Loss: 0.4724900722503662, Test_Loss: 0.3546498417854309 *\n",
      "Epoch: 14, Train_Loss: 0.4616919457912445, Test_Loss: 0.3591400384902954\n",
      "Epoch: 14, Train_Loss: 0.4091608226299286, Test_Loss: 0.3992915749549866\n",
      "Epoch: 14, Train_Loss: 0.48114871978759766, Test_Loss: 0.3952706754207611 *\n",
      "Epoch: 14, Train_Loss: 0.5687354207038879, Test_Loss: 0.37199392914772034 *\n",
      "Epoch: 14, Train_Loss: 0.45112040638923645, Test_Loss: 0.359931081533432 *\n",
      "Epoch: 14, Train_Loss: 0.3564792573451996, Test_Loss: 0.37426942586898804\n",
      "Epoch: 14, Train_Loss: 2.4492852687835693, Test_Loss: 0.3614281117916107 *\n",
      "Epoch: 14, Train_Loss: 1.4258809089660645, Test_Loss: 0.361306369304657 *\n",
      "Epoch: 14, Train_Loss: 0.3859679698944092, Test_Loss: 0.38098427653312683\n",
      "Epoch: 14, Train_Loss: 0.3898511826992035, Test_Loss: 0.4124862849712372\n",
      "Epoch: 14, Train_Loss: 0.3799120783805847, Test_Loss: 3.194378614425659\n",
      "Epoch: 14, Train_Loss: 0.3801698386669159, Test_Loss: 2.9030566215515137 *\n",
      "Epoch: 14, Train_Loss: 0.352386474609375, Test_Loss: 0.3507140576839447 *\n",
      "Epoch: 14, Train_Loss: 0.38163894414901733, Test_Loss: 0.3497985601425171 *\n",
      "Epoch: 14, Train_Loss: 0.4836522042751312, Test_Loss: 0.40011316537857056\n",
      "Epoch: 14, Train_Loss: 0.43680834770202637, Test_Loss: 0.3750086724758148 *\n",
      "Epoch: 14, Train_Loss: 0.410552054643631, Test_Loss: 0.39823591709136963\n",
      "Epoch: 14, Train_Loss: 0.3932550847530365, Test_Loss: 0.39631280303001404 *\n",
      "Epoch: 14, Train_Loss: 0.3744318187236786, Test_Loss: 0.42848914861679077\n",
      "Epoch: 14, Train_Loss: 0.3647501468658447, Test_Loss: 0.35348039865493774 *\n",
      "Epoch: 14, Train_Loss: 0.36073288321495056, Test_Loss: 0.3692971467971802\n",
      "Epoch: 14, Train_Loss: 0.40145373344421387, Test_Loss: 0.36859017610549927 *\n",
      "Epoch: 14, Train_Loss: 0.3908509910106659, Test_Loss: 0.3703210651874542\n",
      "Epoch: 14, Train_Loss: 0.3619093596935272, Test_Loss: 0.35453367233276367 *\n",
      "Epoch: 14, Train_Loss: 0.3524893522262573, Test_Loss: 0.4224034547805786\n",
      "Epoch: 14, Train_Loss: 0.3721790015697479, Test_Loss: 0.41413506865501404 *\n",
      "Epoch: 14, Train_Loss: 0.379109650850296, Test_Loss: 0.41034621000289917 *\n",
      "Epoch: 14, Train_Loss: 0.35839974880218506, Test_Loss: 0.4201847016811371\n",
      "Epoch: 14, Train_Loss: 0.3495669960975647, Test_Loss: 0.3790060877799988 *\n",
      "Epoch: 14, Train_Loss: 0.3492896258831024, Test_Loss: 0.37824469804763794 *\n",
      "Epoch: 14, Train_Loss: 0.34762635827064514, Test_Loss: 0.3617697060108185 *\n",
      "Epoch: 14, Train_Loss: 0.3509121835231781, Test_Loss: 0.36209917068481445\n",
      "Epoch: 14, Train_Loss: 0.34920915961265564, Test_Loss: 0.3610045909881592 *\n",
      "Epoch: 14, Train_Loss: 0.35308194160461426, Test_Loss: 0.367876261472702\n",
      "Epoch: 14, Train_Loss: 0.35252493619918823, Test_Loss: 0.36567631363868713 *\n",
      "Epoch: 14, Train_Loss: 0.3477901220321655, Test_Loss: 0.35688015818595886 *\n",
      "Epoch: 14, Train_Loss: 0.3478523790836334, Test_Loss: 0.3636796474456787\n",
      "Epoch: 14, Train_Loss: 0.35210365056991577, Test_Loss: 0.3633437156677246 *\n",
      "Epoch: 14, Train_Loss: 0.36502179503440857, Test_Loss: 0.3614308834075928 *\n",
      "Epoch: 14, Train_Loss: 0.36512380838394165, Test_Loss: 0.35281264781951904 *\n",
      "Epoch: 14, Train_Loss: 0.3632161617279053, Test_Loss: 0.3821280300617218\n",
      "Epoch: 15, Train_Loss: 0.3719625174999237, Test_Loss: 0.41230565309524536 *\n",
      "Epoch: 15, Train_Loss: 0.35820382833480835, Test_Loss: 0.3933655023574829 *\n",
      "Epoch: 15, Train_Loss: 0.35652956366539, Test_Loss: 0.8047152757644653\n",
      "Epoch: 15, Train_Loss: 0.3500196635723114, Test_Loss: 0.7360635995864868 *\n",
      "Epoch: 15, Train_Loss: 0.35689759254455566, Test_Loss: 0.4651232957839966 *\n",
      "Epoch: 15, Train_Loss: 0.3775864243507385, Test_Loss: 0.36416253447532654 *\n",
      "Epoch: 15, Train_Loss: 0.35356903076171875, Test_Loss: 0.3686515986919403\n",
      "Epoch: 15, Train_Loss: 0.3518011271953583, Test_Loss: 0.3805001378059387\n",
      "Epoch: 15, Train_Loss: 0.3487556278705597, Test_Loss: 0.5537805557250977\n",
      "Epoch: 15, Train_Loss: 0.35858598351478577, Test_Loss: 0.8291014432907104\n",
      "Epoch: 15, Train_Loss: 0.3994182348251343, Test_Loss: 0.7483177781105042 *\n",
      "Epoch: 15, Train_Loss: 0.40777668356895447, Test_Loss: 0.42407453060150146 *\n",
      "Epoch: 15, Train_Loss: 0.3790273666381836, Test_Loss: 0.3769723176956177 *\n",
      "Epoch: 15, Train_Loss: 0.34640994668006897, Test_Loss: 0.35286372900009155 *\n",
      "Epoch: 15, Train_Loss: 0.4130646884441376, Test_Loss: 0.34668204188346863 *\n",
      "Epoch: 15, Train_Loss: 0.3621601462364197, Test_Loss: 0.35457053780555725\n",
      "Epoch: 15, Train_Loss: 0.3491826057434082, Test_Loss: 0.36162635684013367\n",
      "Epoch: 15, Train_Loss: 0.36631691455841064, Test_Loss: 0.3948444128036499\n",
      "Epoch: 15, Train_Loss: 0.36754244565963745, Test_Loss: 0.3466258645057678 *\n",
      "Epoch: 15, Train_Loss: 0.4641116261482239, Test_Loss: 0.40455788373947144\n",
      "Epoch: 15, Train_Loss: 0.41559770703315735, Test_Loss: 0.46609053015708923\n",
      "Epoch: 15, Train_Loss: 0.38886067271232605, Test_Loss: 0.7201061248779297\n",
      "Epoch: 15, Train_Loss: 0.3612464368343353, Test_Loss: 0.5877004861831665 *\n",
      "Epoch: 15, Train_Loss: 0.3522334694862366, Test_Loss: 0.36711668968200684 *\n",
      "Epoch: 15, Train_Loss: 0.36578434705734253, Test_Loss: 0.3597829341888428 *\n",
      "Epoch: 15, Train_Loss: 0.346769243478775, Test_Loss: 0.3594015836715698 *\n",
      "Epoch: 15, Train_Loss: 0.35270869731903076, Test_Loss: 0.3592643737792969 *\n",
      "Epoch: 15, Train_Loss: 0.35914137959480286, Test_Loss: 0.36692285537719727\n",
      "Epoch: 15, Train_Loss: 0.363097220659256, Test_Loss: 1.9725697040557861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train_Loss: 0.4484443664550781, Test_Loss: 4.177511215209961\n",
      "Epoch: 15, Train_Loss: 0.34769660234451294, Test_Loss: 0.3626503348350525 *\n",
      "Epoch: 15, Train_Loss: 0.4104674160480499, Test_Loss: 0.3494589626789093 *\n",
      "Epoch: 15, Train_Loss: 0.35881757736206055, Test_Loss: 0.3504578769207001\n",
      "Epoch: 15, Train_Loss: 0.37739261984825134, Test_Loss: 0.3538747727870941\n",
      "Epoch: 15, Train_Loss: 0.4215364158153534, Test_Loss: 0.3491372764110565 *\n",
      "Epoch: 15, Train_Loss: 0.5898593664169312, Test_Loss: 0.3560926616191864\n",
      "Epoch: 15, Train_Loss: 0.3602592349052429, Test_Loss: 0.35473576188087463 *\n",
      "Epoch: 15, Train_Loss: 0.3823978304862976, Test_Loss: 0.34615853428840637 *\n",
      "Epoch: 15, Train_Loss: 0.34381577372550964, Test_Loss: 0.35058653354644775\n",
      "Epoch: 15, Train_Loss: 0.34384647011756897, Test_Loss: 0.3515627980232239\n",
      "Epoch: 15, Train_Loss: 0.3443916440010071, Test_Loss: 0.37249135971069336\n",
      "Epoch: 15, Train_Loss: 0.3433959484100342, Test_Loss: 0.3524025082588196 *\n",
      "Epoch: 15, Train_Loss: 0.3539058268070221, Test_Loss: 0.346801221370697 *\n",
      "Epoch: 15, Train_Loss: 0.3526122272014618, Test_Loss: 0.3636401891708374\n",
      "Epoch: 15, Train_Loss: 0.3578229248523712, Test_Loss: 0.34481847286224365 *\n",
      "Epoch: 15, Train_Loss: 0.3487708568572998, Test_Loss: 0.34983038902282715\n",
      "Epoch: 15, Train_Loss: 0.35185402631759644, Test_Loss: 0.3486444354057312 *\n",
      "Epoch: 15, Train_Loss: 0.3533031642436981, Test_Loss: 0.3700051009654999\n",
      "Epoch: 15, Train_Loss: 0.34431174397468567, Test_Loss: 0.3470490574836731 *\n",
      "Epoch: 15, Train_Loss: 0.342335969209671, Test_Loss: 0.3454064726829529 *\n",
      "Epoch: 15, Train_Loss: 0.3652373254299164, Test_Loss: 0.35449832677841187\n",
      "Epoch: 15, Train_Loss: 0.37203559279441833, Test_Loss: 0.3802792727947235\n",
      "Epoch: 15, Train_Loss: 0.37747907638549805, Test_Loss: 0.3805740177631378\n",
      "Epoch: 15, Train_Loss: 0.3429040312767029, Test_Loss: 0.3532484471797943 *\n",
      "Epoch: 15, Train_Loss: 0.3967750668525696, Test_Loss: 0.3472999632358551 *\n",
      "Epoch: 15, Train_Loss: 0.39322227239608765, Test_Loss: 0.36187100410461426\n",
      "Epoch: 15, Train_Loss: 0.3765515685081482, Test_Loss: 0.35050320625305176 *\n",
      "Epoch: 15, Train_Loss: 0.3468664884567261, Test_Loss: 0.34947657585144043 *\n",
      "Epoch: 15, Train_Loss: 0.3794773519039154, Test_Loss: 0.40845316648483276\n",
      "Epoch: 15, Train_Loss: 0.3425104022026062, Test_Loss: 0.37700673937797546 *\n",
      "Epoch: 15, Train_Loss: 0.36251747608184814, Test_Loss: 4.267565727233887\n",
      "Epoch: 15, Train_Loss: 0.34996315836906433, Test_Loss: 1.9213348627090454 *\n",
      "Epoch: 15, Train_Loss: 0.3657362461090088, Test_Loss: 0.34326401352882385 *\n",
      "Epoch: 15, Train_Loss: 1.21586012840271, Test_Loss: 0.3512157201766968\n",
      "Epoch: 15, Train_Loss: 4.332629203796387, Test_Loss: 0.38243913650512695\n",
      "Epoch: 15, Train_Loss: 0.971423864364624, Test_Loss: 0.3813319206237793 *\n",
      "Epoch: 15, Train_Loss: 0.36149320006370544, Test_Loss: 0.3612121343612671 *\n",
      "Epoch: 15, Train_Loss: 0.3488435745239258, Test_Loss: 0.41417160630226135\n",
      "Epoch: 15, Train_Loss: 0.4849449396133423, Test_Loss: 0.4240056276321411\n",
      "Epoch: 15, Train_Loss: 0.4126172661781311, Test_Loss: 0.34424158930778503 *\n",
      "Epoch: 15, Train_Loss: 0.35486462712287903, Test_Loss: 0.37518107891082764\n",
      "Epoch: 15, Train_Loss: 0.34120917320251465, Test_Loss: 0.35962605476379395 *\n",
      "Epoch: 15, Train_Loss: 0.40037718415260315, Test_Loss: 0.35455214977264404 *\n",
      "Epoch: 15, Train_Loss: 0.35976582765579224, Test_Loss: 0.34517717361450195 *\n",
      "Epoch: 15, Train_Loss: 0.35471516847610474, Test_Loss: 0.40533965826034546\n",
      "Epoch: 15, Train_Loss: 0.623316764831543, Test_Loss: 0.38907289505004883 *\n",
      "Epoch: 15, Train_Loss: 1.426222324371338, Test_Loss: 0.4303332567214966\n",
      "Epoch: 15, Train_Loss: 1.4439311027526855, Test_Loss: 0.4033120572566986 *\n",
      "Epoch: 15, Train_Loss: 0.41779983043670654, Test_Loss: 0.37922415137290955 *\n",
      "Epoch: 15, Train_Loss: 0.4747582674026489, Test_Loss: 0.35611462593078613 *\n",
      "Epoch: 15, Train_Loss: 2.3169567584991455, Test_Loss: 0.3459826111793518 *\n",
      "Epoch: 15, Train_Loss: 1.1811060905456543, Test_Loss: 0.34770479798316956\n",
      "Epoch: 15, Train_Loss: 0.36985185742378235, Test_Loss: 0.3481736481189728\n",
      "Epoch: 15, Train_Loss: 0.34956440329551697, Test_Loss: 0.34914857149124146\n",
      "Epoch: 15, Train_Loss: 0.7835489511489868, Test_Loss: 0.34861433506011963 *\n",
      "Epoch: 15, Train_Loss: 0.9609195590019226, Test_Loss: 0.3439459502696991 *\n",
      "Epoch: 15, Train_Loss: 0.9019156694412231, Test_Loss: 0.3442891538143158\n",
      "Epoch: 15, Train_Loss: 0.35224649310112, Test_Loss: 0.3526323735713959\n",
      "Epoch: 15, Train_Loss: 0.37959179282188416, Test_Loss: 0.3857181668281555\n",
      "Epoch: 15, Train_Loss: 0.5301566123962402, Test_Loss: 0.4149945378303528\n",
      "Epoch: 15, Train_Loss: 0.5037030577659607, Test_Loss: 0.3624207675457001 *\n",
      "Epoch: 15, Train_Loss: 0.4071771502494812, Test_Loss: 0.3854112923145294\n",
      "Epoch: 15, Train_Loss: 0.4024931788444519, Test_Loss: 0.47752782702445984\n",
      "Epoch: 15, Train_Loss: 0.37869447469711304, Test_Loss: 0.49561166763305664\n",
      "Epoch: 15, Train_Loss: 0.36457157135009766, Test_Loss: 0.499911367893219\n",
      "Epoch: 15, Train_Loss: 0.5533854961395264, Test_Loss: 0.37762323021888733 *\n",
      "Epoch: 15, Train_Loss: 0.4216982126235962, Test_Loss: 0.36583495140075684 *\n",
      "Epoch: 15, Train_Loss: 0.37961316108703613, Test_Loss: 0.38807064294815063\n",
      "Epoch: 15, Train_Loss: 0.4440370202064514, Test_Loss: 0.4474944472312927\n",
      "Epoch: 15, Train_Loss: 0.39125674962997437, Test_Loss: 0.5182256698608398\n",
      "Model saved at location save_new\\model.ckpt at epoch 15\n",
      "Epoch: 15, Train_Loss: 0.4012695848941803, Test_Loss: 0.40394458174705505 *\n",
      "Epoch: 15, Train_Loss: 0.4427803158760071, Test_Loss: 0.7664482593536377\n",
      "Epoch: 15, Train_Loss: 0.49808168411254883, Test_Loss: 0.38232994079589844 *\n",
      "Epoch: 15, Train_Loss: 0.4570189416408539, Test_Loss: 0.38528358936309814\n",
      "Epoch: 15, Train_Loss: 0.651084840297699, Test_Loss: 0.3780185282230377 *\n",
      "Epoch: 15, Train_Loss: 0.44241559505462646, Test_Loss: 0.36830857396125793 *\n",
      "Epoch: 15, Train_Loss: 0.36929941177368164, Test_Loss: 0.4415104389190674\n",
      "Epoch: 15, Train_Loss: 0.340756893157959, Test_Loss: 0.3640599250793457 *\n",
      "Epoch: 15, Train_Loss: 0.3476136326789856, Test_Loss: 0.3757272958755493\n",
      "Epoch: 15, Train_Loss: 0.34569379687309265, Test_Loss: 0.4223625063896179\n",
      "Epoch: 15, Train_Loss: 0.34893864393234253, Test_Loss: 0.5120537281036377\n",
      "Epoch: 15, Train_Loss: 0.35627835988998413, Test_Loss: 0.5975835919380188\n",
      "Epoch: 15, Train_Loss: 0.34725046157836914, Test_Loss: 0.6256487965583801\n",
      "Epoch: 15, Train_Loss: 0.34877029061317444, Test_Loss: 1.073167324066162\n",
      "Epoch: 15, Train_Loss: 0.37475651502609253, Test_Loss: 0.41947633028030396 *\n",
      "Epoch: 15, Train_Loss: 0.5272759795188904, Test_Loss: 0.3399945795536041 *\n",
      "Epoch: 15, Train_Loss: 0.550540030002594, Test_Loss: 0.34138980507850647\n",
      "Epoch: 15, Train_Loss: 0.3705812394618988, Test_Loss: 0.34315964579582214\n",
      "Epoch: 15, Train_Loss: 0.3854186534881592, Test_Loss: 0.6121052503585815\n",
      "Epoch: 15, Train_Loss: 0.42424246668815613, Test_Loss: 3.193560838699341\n",
      "Epoch: 15, Train_Loss: 0.4149710536003113, Test_Loss: 2.3844029903411865 *\n",
      "Epoch: 15, Train_Loss: 0.3612816035747528, Test_Loss: 0.3647398352622986 *\n",
      "Epoch: 15, Train_Loss: 0.37408721446990967, Test_Loss: 0.36471137404441833 *\n",
      "Epoch: 15, Train_Loss: 0.6170423030853271, Test_Loss: 0.3568800389766693 *\n",
      "Epoch: 15, Train_Loss: 0.5474797487258911, Test_Loss: 0.3474321663379669 *\n",
      "Epoch: 15, Train_Loss: 0.4424145519733429, Test_Loss: 0.3799516260623932\n",
      "Epoch: 15, Train_Loss: 0.35770440101623535, Test_Loss: 0.3971436023712158\n",
      "Epoch: 15, Train_Loss: 0.37733355164527893, Test_Loss: 0.3610790967941284 *\n",
      "Epoch: 15, Train_Loss: 0.6245282888412476, Test_Loss: 0.35185450315475464 *\n",
      "Epoch: 15, Train_Loss: 0.79473876953125, Test_Loss: 0.3613622486591339\n",
      "Epoch: 15, Train_Loss: 0.5323038101196289, Test_Loss: 0.38132134079933167\n",
      "Epoch: 15, Train_Loss: 0.39733514189720154, Test_Loss: 0.4577599763870239\n",
      "Epoch: 15, Train_Loss: 0.35012099146842957, Test_Loss: 0.34673240780830383 *\n",
      "Epoch: 15, Train_Loss: 0.34313851594924927, Test_Loss: 0.3661317527294159\n",
      "Epoch: 15, Train_Loss: 0.699816107749939, Test_Loss: 0.3538811206817627 *\n",
      "Epoch: 15, Train_Loss: 0.4163568913936615, Test_Loss: 0.354931116104126\n",
      "Epoch: 15, Train_Loss: 0.36206957697868347, Test_Loss: 0.3716062009334564\n",
      "Epoch: 15, Train_Loss: 0.5834946632385254, Test_Loss: 0.4124261736869812\n",
      "Epoch: 15, Train_Loss: 0.3928029537200928, Test_Loss: 0.41698509454727173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train_Loss: 0.34874457120895386, Test_Loss: 0.34888729453086853 *\n",
      "Epoch: 15, Train_Loss: 0.36245813965797424, Test_Loss: 0.38246023654937744\n",
      "Epoch: 15, Train_Loss: 0.4885149896144867, Test_Loss: 0.3960809111595154\n",
      "Epoch: 15, Train_Loss: 0.3729763627052307, Test_Loss: 0.4434594511985779\n",
      "Epoch: 15, Train_Loss: 0.5150694847106934, Test_Loss: 0.4571358561515808\n",
      "Epoch: 15, Train_Loss: 0.3827160894870758, Test_Loss: 0.37771856784820557 *\n",
      "Epoch: 15, Train_Loss: 0.49267271161079407, Test_Loss: 0.36328908801078796 *\n",
      "Epoch: 15, Train_Loss: 0.39558833837509155, Test_Loss: 0.40576648712158203\n",
      "Epoch: 15, Train_Loss: 0.3559616506099701, Test_Loss: 0.374326229095459 *\n",
      "Epoch: 15, Train_Loss: 0.347454696893692, Test_Loss: 0.3508142828941345 *\n",
      "Epoch: 15, Train_Loss: 0.3854251801967621, Test_Loss: 0.5040781497955322\n",
      "Epoch: 15, Train_Loss: 0.6242768168449402, Test_Loss: 0.39158859848976135 *\n",
      "Epoch: 15, Train_Loss: 0.5888012051582336, Test_Loss: 5.477959632873535\n",
      "Epoch: 15, Train_Loss: 0.635936439037323, Test_Loss: 0.6669259071350098 *\n",
      "Epoch: 15, Train_Loss: 0.8006869554519653, Test_Loss: 0.36875787377357483 *\n",
      "Epoch: 15, Train_Loss: 0.5610484480857849, Test_Loss: 0.37553030252456665\n",
      "Epoch: 15, Train_Loss: 0.5359251499176025, Test_Loss: 0.342839777469635 *\n",
      "Epoch: 15, Train_Loss: 0.4110996425151825, Test_Loss: 0.35502102971076965\n",
      "Epoch: 15, Train_Loss: 0.3481180965900421, Test_Loss: 0.3472934663295746 *\n",
      "Epoch: 15, Train_Loss: 0.34717434644699097, Test_Loss: 0.39681577682495117\n",
      "Epoch: 15, Train_Loss: 0.3592279851436615, Test_Loss: 0.377697229385376 *\n",
      "Epoch: 15, Train_Loss: 0.5239992141723633, Test_Loss: 0.34300950169563293 *\n",
      "Epoch: 15, Train_Loss: 0.6474124193191528, Test_Loss: 0.3558062016963959\n",
      "Epoch: 15, Train_Loss: 0.6019638776779175, Test_Loss: 0.4032920002937317\n",
      "Epoch: 15, Train_Loss: 1.6703094244003296, Test_Loss: 0.3779589533805847 *\n",
      "Epoch: 15, Train_Loss: 0.9357354640960693, Test_Loss: 0.36308974027633667 *\n",
      "Epoch: 15, Train_Loss: 0.650781512260437, Test_Loss: 0.3407953381538391 *\n",
      "Epoch: 15, Train_Loss: 0.390484094619751, Test_Loss: 0.3989063501358032\n",
      "Epoch: 15, Train_Loss: 0.34326785802841187, Test_Loss: 0.36771973967552185 *\n",
      "Epoch: 15, Train_Loss: 0.5340410470962524, Test_Loss: 0.3747114837169647\n",
      "Epoch: 15, Train_Loss: 1.0947322845458984, Test_Loss: 0.43896931409835815\n",
      "Epoch: 15, Train_Loss: 0.8997852802276611, Test_Loss: 0.34935951232910156 *\n",
      "Epoch: 15, Train_Loss: 0.3791179358959198, Test_Loss: 0.35855770111083984\n",
      "Epoch: 15, Train_Loss: 0.38031429052352905, Test_Loss: 0.35858312249183655\n",
      "Epoch: 15, Train_Loss: 0.41717952489852905, Test_Loss: 0.3796495199203491\n",
      "Epoch: 15, Train_Loss: 0.6666405200958252, Test_Loss: 0.3646317720413208 *\n",
      "Epoch: 15, Train_Loss: 0.4892905354499817, Test_Loss: 0.3629363775253296 *\n",
      "Epoch: 15, Train_Loss: 0.43058261275291443, Test_Loss: 0.398617148399353\n",
      "Epoch: 15, Train_Loss: 0.46486079692840576, Test_Loss: 0.3637215197086334 *\n",
      "Epoch: 15, Train_Loss: 0.3772617280483246, Test_Loss: 0.3702051341533661\n",
      "Epoch: 15, Train_Loss: 0.3472210764884949, Test_Loss: 0.3903670907020569\n",
      "Epoch: 15, Train_Loss: 0.379465788602829, Test_Loss: 0.3936516344547272\n",
      "Epoch: 15, Train_Loss: 0.35259300470352173, Test_Loss: 0.3408600986003876 *\n",
      "Epoch: 15, Train_Loss: 0.3997519612312317, Test_Loss: 0.37224945425987244\n",
      "Epoch: 15, Train_Loss: 0.3658081889152527, Test_Loss: 0.46948328614234924\n",
      "Epoch: 15, Train_Loss: 0.8939707279205322, Test_Loss: 0.48646268248558044\n",
      "Epoch: 15, Train_Loss: 16.323230743408203, Test_Loss: 0.4463198184967041 *\n",
      "Epoch: 15, Train_Loss: 0.5021566152572632, Test_Loss: 0.381576269865036 *\n",
      "Epoch: 15, Train_Loss: 1.7914128303527832, Test_Loss: 0.34475257992744446 *\n",
      "Epoch: 15, Train_Loss: 0.9846511483192444, Test_Loss: 0.3459261953830719\n",
      "Epoch: 15, Train_Loss: 0.36833447217941284, Test_Loss: 0.3870956599712372\n",
      "Epoch: 15, Train_Loss: 0.4720441699028015, Test_Loss: 0.5427236557006836\n",
      "Epoch: 15, Train_Loss: 6.305200099945068, Test_Loss: 0.3975680470466614 *\n",
      "Epoch: 15, Train_Loss: 3.1820666790008545, Test_Loss: 0.5932128429412842\n",
      "Epoch: 15, Train_Loss: 0.38780543208122253, Test_Loss: 0.4078831076622009 *\n",
      "Epoch: 15, Train_Loss: 1.2498382329940796, Test_Loss: 0.4477772116661072\n",
      "Epoch: 15, Train_Loss: 5.063448429107666, Test_Loss: 0.5887740850448608\n",
      "Epoch: 15, Train_Loss: 0.5706931948661804, Test_Loss: 0.6220195293426514\n",
      "Epoch: 15, Train_Loss: 0.36345288157463074, Test_Loss: 0.6517472267150879\n",
      "Epoch: 15, Train_Loss: 0.37471315264701843, Test_Loss: 0.4043238162994385 *\n",
      "Epoch: 15, Train_Loss: 0.36866670846939087, Test_Loss: 0.45837801694869995\n",
      "Model saved at location save_new\\model.ckpt at epoch 15\n",
      "Epoch: 15, Train_Loss: 0.3869447410106659, Test_Loss: 0.5301951766014099\n",
      "Epoch: 15, Train_Loss: 0.3346502482891083, Test_Loss: 0.5654494166374207\n",
      "Epoch: 15, Train_Loss: 0.34690359234809875, Test_Loss: 1.200087070465088\n",
      "Epoch: 15, Train_Loss: 0.33471155166625977, Test_Loss: 0.44074374437332153 *\n",
      "Epoch: 15, Train_Loss: 0.34420478343963623, Test_Loss: 0.4367998540401459 *\n",
      "Epoch: 15, Train_Loss: 0.37199321389198303, Test_Loss: 0.5624414682388306\n",
      "Epoch: 15, Train_Loss: 0.36188745498657227, Test_Loss: 0.5674238204956055\n",
      "Epoch: 15, Train_Loss: 0.379963755607605, Test_Loss: 0.5096831917762756 *\n",
      "Epoch: 15, Train_Loss: 0.403111070394516, Test_Loss: 0.4285651445388794 *\n",
      "Epoch: 15, Train_Loss: 0.37225276231765747, Test_Loss: 0.35343578457832336 *\n",
      "Epoch: 15, Train_Loss: 0.34731876850128174, Test_Loss: 6.298872470855713\n",
      "Epoch: 15, Train_Loss: 0.36354511976242065, Test_Loss: 2.0059127807617188 *\n",
      "Epoch: 15, Train_Loss: 0.3384169936180115, Test_Loss: 0.6904160976409912 *\n",
      "Epoch: 15, Train_Loss: 0.3330689072608948, Test_Loss: 0.5446168184280396 *\n",
      "Epoch: 15, Train_Loss: 0.3346608579158783, Test_Loss: 0.5594217777252197\n",
      "Epoch: 15, Train_Loss: 0.3330036699771881, Test_Loss: 0.348420113325119 *\n",
      "Epoch: 15, Train_Loss: 0.33292293548583984, Test_Loss: 0.593614399433136\n",
      "Epoch: 15, Train_Loss: 0.33234721422195435, Test_Loss: 0.6895344257354736\n",
      "Epoch: 15, Train_Loss: 0.3353783190250397, Test_Loss: 0.459449827671051 *\n",
      "Epoch: 15, Train_Loss: 0.3339962959289551, Test_Loss: 0.47744983434677124\n",
      "Epoch: 15, Train_Loss: 0.33336853981018066, Test_Loss: 0.4821028411388397\n",
      "Epoch: 15, Train_Loss: 0.34682103991508484, Test_Loss: 0.5571834444999695\n",
      "Epoch: 15, Train_Loss: 0.3611763119697571, Test_Loss: 0.7341355681419373\n",
      "Epoch: 15, Train_Loss: 0.3894898295402527, Test_Loss: 0.6793797016143799 *\n",
      "Epoch: 15, Train_Loss: 0.3429858684539795, Test_Loss: 1.1785547733306885\n",
      "Epoch: 15, Train_Loss: 0.4057255983352661, Test_Loss: 0.6453458666801453 *\n",
      "Epoch: 15, Train_Loss: 6.338309288024902, Test_Loss: 0.44901925325393677 *\n",
      "Epoch: 15, Train_Loss: 2.6529223918914795, Test_Loss: 0.5435187816619873\n",
      "Epoch: 15, Train_Loss: 0.3499566912651062, Test_Loss: 0.6758154034614563\n",
      "Epoch: 15, Train_Loss: 0.39476722478866577, Test_Loss: 0.8588265180587769\n",
      "Epoch: 15, Train_Loss: 0.4710230827331543, Test_Loss: 0.5655789375305176 *\n",
      "Epoch: 15, Train_Loss: 0.3940877914428711, Test_Loss: 0.663150429725647\n",
      "Epoch: 15, Train_Loss: 0.426550030708313, Test_Loss: 0.8421317338943481\n",
      "Epoch: 15, Train_Loss: 0.4771856665611267, Test_Loss: 0.837887704372406 *\n",
      "Epoch: 15, Train_Loss: 0.44484972953796387, Test_Loss: 0.8086210489273071 *\n",
      "Epoch: 15, Train_Loss: 0.46994736790657043, Test_Loss: 0.49396461248397827 *\n",
      "Epoch: 15, Train_Loss: 0.43633270263671875, Test_Loss: 0.45821741223335266 *\n",
      "Epoch: 15, Train_Loss: 0.35234445333480835, Test_Loss: 0.5821396708488464\n",
      "Epoch: 15, Train_Loss: 0.3734093904495239, Test_Loss: 0.4860347509384155 *\n",
      "Epoch: 15, Train_Loss: 0.44261324405670166, Test_Loss: 0.3858473598957062 *\n",
      "Epoch: 15, Train_Loss: 0.5576477646827698, Test_Loss: 0.5401694178581238\n",
      "Epoch: 15, Train_Loss: 0.4560941159725189, Test_Loss: 0.5286142826080322 *\n",
      "Epoch: 15, Train_Loss: 0.40483346581459045, Test_Loss: 5.80153751373291\n",
      "Epoch: 15, Train_Loss: 0.3710050582885742, Test_Loss: 0.36485812067985535 *\n",
      "Epoch: 15, Train_Loss: 0.341009259223938, Test_Loss: 0.3312411606311798 *\n",
      "Epoch: 15, Train_Loss: 0.36887651681900024, Test_Loss: 0.35363131761550903\n",
      "Epoch: 15, Train_Loss: 0.3451916575431824, Test_Loss: 0.33591994643211365 *\n",
      "Epoch: 15, Train_Loss: 0.3599737584590912, Test_Loss: 0.3616613745689392\n",
      "Epoch: 15, Train_Loss: 0.34755176305770874, Test_Loss: 0.3367045819759369 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train_Loss: 0.3486931025981903, Test_Loss: 0.4454767405986786\n",
      "Epoch: 15, Train_Loss: 0.3639519214630127, Test_Loss: 0.3766540288925171 *\n",
      "Epoch: 15, Train_Loss: 5.109605312347412, Test_Loss: 0.33162206411361694 *\n",
      "Epoch: 15, Train_Loss: 1.0181269645690918, Test_Loss: 0.3707628846168518\n",
      "Epoch: 15, Train_Loss: 0.3299500346183777, Test_Loss: 0.3464711010456085 *\n",
      "Epoch: 15, Train_Loss: 0.3349333107471466, Test_Loss: 0.33920347690582275 *\n",
      "Epoch: 15, Train_Loss: 0.33734017610549927, Test_Loss: 0.36426952481269836\n",
      "Epoch: 15, Train_Loss: 0.33310753107070923, Test_Loss: 0.35654976963996887 *\n",
      "Epoch: 15, Train_Loss: 0.3314116895198822, Test_Loss: 0.38523152470588684\n",
      "Epoch: 15, Train_Loss: 0.3317398726940155, Test_Loss: 0.4481329321861267\n",
      "Epoch: 15, Train_Loss: 0.3704693019390106, Test_Loss: 0.377664178609848 *\n",
      "Epoch: 15, Train_Loss: 0.34934374690055847, Test_Loss: 0.3593786954879761 *\n",
      "Epoch: 15, Train_Loss: 0.3620874881744385, Test_Loss: 0.33401036262512207 *\n",
      "Epoch: 15, Train_Loss: 0.33015599846839905, Test_Loss: 0.33471381664276123\n",
      "Epoch: 15, Train_Loss: 0.32964181900024414, Test_Loss: 0.3338926136493683 *\n",
      "Epoch: 15, Train_Loss: 0.3427095413208008, Test_Loss: 0.3351675868034363\n",
      "Epoch: 15, Train_Loss: 0.3330722153186798, Test_Loss: 0.3324757516384125 *\n",
      "Epoch: 15, Train_Loss: 0.3305787146091461, Test_Loss: 0.3342474400997162\n",
      "Epoch: 15, Train_Loss: 0.33998364210128784, Test_Loss: 0.3421647548675537\n",
      "Epoch: 15, Train_Loss: 0.3635321259498596, Test_Loss: 0.332050085067749 *\n",
      "Epoch: 15, Train_Loss: 0.3527020514011383, Test_Loss: 0.33190423250198364 *\n",
      "Epoch: 15, Train_Loss: 0.3287539780139923, Test_Loss: 0.34225204586982727\n",
      "Epoch: 15, Train_Loss: 0.32921892404556274, Test_Loss: 0.3360324800014496 *\n",
      "Epoch: 15, Train_Loss: 0.3880196511745453, Test_Loss: 0.36619195342063904\n",
      "Epoch: 15, Train_Loss: 0.38583171367645264, Test_Loss: 0.3596670627593994 *\n",
      "Epoch: 15, Train_Loss: 0.36153942346572876, Test_Loss: 0.4985550045967102\n",
      "Epoch: 15, Train_Loss: 0.3634408116340637, Test_Loss: 0.6265548467636108\n",
      "Epoch: 15, Train_Loss: 0.41230887174606323, Test_Loss: 0.4412453770637512 *\n",
      "Epoch: 15, Train_Loss: 0.39235469698905945, Test_Loss: 0.3579167425632477 *\n",
      "Epoch: 15, Train_Loss: 0.3643166422843933, Test_Loss: 0.33635154366493225 *\n",
      "Epoch: 15, Train_Loss: 0.3806268870830536, Test_Loss: 0.3425006568431854\n",
      "Epoch: 15, Train_Loss: 0.5331031680107117, Test_Loss: 0.40452104806900024\n",
      "Epoch: 15, Train_Loss: 0.35329580307006836, Test_Loss: 0.7005313038825989\n",
      "Epoch: 15, Train_Loss: 0.3703386187553406, Test_Loss: 0.6935008764266968 *\n",
      "Epoch: 15, Train_Loss: 0.3303546607494354, Test_Loss: 0.4236721098423004 *\n",
      "Epoch: 15, Train_Loss: 0.3292234539985657, Test_Loss: 0.3732268214225769 *\n",
      "Epoch: 15, Train_Loss: 0.3286486268043518, Test_Loss: 0.3406165838241577 *\n",
      "Epoch: 15, Train_Loss: 0.32805752754211426, Test_Loss: 0.34770190715789795\n",
      "Epoch: 15, Train_Loss: 0.32831141352653503, Test_Loss: 0.343639612197876 *\n",
      "Epoch: 15, Train_Loss: 4.572669506072998, Test_Loss: 0.3507921099662781\n",
      "Epoch: 15, Train_Loss: 1.0864026546478271, Test_Loss: 0.3576081097126007\n",
      "Epoch: 15, Train_Loss: 0.3287160396575928, Test_Loss: 0.3533710241317749 *\n",
      "Epoch: 15, Train_Loss: 0.3424907326698303, Test_Loss: 0.3351069688796997 *\n",
      "Epoch: 15, Train_Loss: 0.331746906042099, Test_Loss: 0.4356512129306793\n",
      "Epoch: 15, Train_Loss: 0.32789281010627747, Test_Loss: 0.7199100852012634\n",
      "Epoch: 15, Train_Loss: 0.327675461769104, Test_Loss: 0.45118552446365356 *\n",
      "Epoch: 15, Train_Loss: 0.3269798159599304, Test_Loss: 0.4852175712585449\n",
      "Epoch: 15, Train_Loss: 0.32693198323249817, Test_Loss: 0.3429521918296814 *\n",
      "Epoch: 15, Train_Loss: 0.32807034254074097, Test_Loss: 0.34315305948257446\n",
      "Epoch: 15, Train_Loss: 0.35784170031547546, Test_Loss: 0.34331437945365906\n",
      "Epoch: 15, Train_Loss: 0.37921956181526184, Test_Loss: 0.3437550663948059\n",
      "Model saved at location save_new\\model.ckpt at epoch 15\n",
      "Epoch: 15, Train_Loss: 0.3926399350166321, Test_Loss: 0.37179887294769287\n",
      "Epoch: 15, Train_Loss: 0.3682163953781128, Test_Loss: 5.520560264587402\n",
      "Epoch: 15, Train_Loss: 0.3339364528656006, Test_Loss: 0.5827271938323975 *\n",
      "Epoch: 15, Train_Loss: 0.3806067705154419, Test_Loss: 0.34277814626693726 *\n",
      "Epoch: 15, Train_Loss: 0.55655837059021, Test_Loss: 0.33263280987739563 *\n",
      "Epoch: 15, Train_Loss: 0.5514332056045532, Test_Loss: 0.3358190357685089\n",
      "Epoch: 15, Train_Loss: 0.5392451286315918, Test_Loss: 0.33926936984062195\n",
      "Epoch: 15, Train_Loss: 0.35001105070114136, Test_Loss: 0.3299221694469452 *\n",
      "Epoch: 15, Train_Loss: 0.32659634947776794, Test_Loss: 0.33556467294692993\n",
      "Epoch: 15, Train_Loss: 0.3261883556842804, Test_Loss: 0.3283178508281708 *\n",
      "Epoch: 15, Train_Loss: 0.33589649200439453, Test_Loss: 0.32793548703193665 *\n",
      "Epoch: 15, Train_Loss: 0.3430561423301697, Test_Loss: 0.3327791094779968\n",
      "Epoch: 15, Train_Loss: 0.342820405960083, Test_Loss: 0.3495023548603058\n",
      "Epoch: 15, Train_Loss: 0.3398745656013489, Test_Loss: 0.3330362141132355 *\n",
      "Epoch: 15, Train_Loss: 0.32564860582351685, Test_Loss: 0.3367113471031189\n",
      "Epoch: 15, Train_Loss: 0.3285779356956482, Test_Loss: 0.34162968397140503\n",
      "Epoch: 15, Train_Loss: 0.3487367033958435, Test_Loss: 0.3301137685775757 *\n",
      "Epoch: 15, Train_Loss: 0.4522244930267334, Test_Loss: 0.3268328607082367 *\n",
      "Epoch: 15, Train_Loss: 0.508712649345398, Test_Loss: 0.3304140269756317\n",
      "Epoch: 15, Train_Loss: 0.4780065715312958, Test_Loss: 0.34026721119880676\n",
      "Epoch: 15, Train_Loss: 0.3969520926475525, Test_Loss: 0.33181697130203247 *\n",
      "Epoch: 15, Train_Loss: 0.4401598572731018, Test_Loss: 0.3313564658164978 *\n",
      "Epoch: 15, Train_Loss: 0.4525052309036255, Test_Loss: 0.3282861113548279 *\n",
      "Epoch: 15, Train_Loss: 0.3354335427284241, Test_Loss: 0.34893113374710083\n",
      "Epoch: 15, Train_Loss: 0.47676941752433777, Test_Loss: 0.35169467329978943\n",
      "Epoch: 15, Train_Loss: 0.42705458402633667, Test_Loss: 0.34232497215270996 *\n",
      "Epoch: 15, Train_Loss: 0.5641095042228699, Test_Loss: 0.3305736482143402 *\n",
      "Epoch: 15, Train_Loss: 0.3335398733615875, Test_Loss: 0.33407193422317505\n",
      "Epoch: 15, Train_Loss: 1.3830757141113281, Test_Loss: 0.33455127477645874\n",
      "Epoch: 15, Train_Loss: 2.250138759613037, Test_Loss: 0.33240506052970886 *\n",
      "Epoch: 15, Train_Loss: 0.36704087257385254, Test_Loss: 0.33404120802879333\n",
      "Epoch: 15, Train_Loss: 0.3834744989871979, Test_Loss: 0.3961070775985718\n",
      "Epoch: 15, Train_Loss: 0.35866212844848633, Test_Loss: 1.4079195261001587\n",
      "Epoch: 15, Train_Loss: 0.357188880443573, Test_Loss: 4.656813144683838\n",
      "Epoch: 15, Train_Loss: 0.32674792408943176, Test_Loss: 0.32998931407928467 *\n",
      "Epoch: 15, Train_Loss: 0.33653995394706726, Test_Loss: 0.32562997937202454 *\n",
      "Epoch: 15, Train_Loss: 0.4431575834751129, Test_Loss: 0.37286120653152466\n",
      "Epoch: 15, Train_Loss: 0.41345399618148804, Test_Loss: 0.35791316628456116 *\n",
      "Epoch: 15, Train_Loss: 0.3877517282962799, Test_Loss: 0.38258007168769836\n",
      "Epoch: 15, Train_Loss: 0.3485507071018219, Test_Loss: 0.3403691351413727 *\n",
      "Epoch: 15, Train_Loss: 0.34351786971092224, Test_Loss: 0.4226178526878357\n",
      "Epoch: 15, Train_Loss: 0.33801180124282837, Test_Loss: 0.3423488140106201 *\n",
      "Epoch: 15, Train_Loss: 0.3409388065338135, Test_Loss: 0.3340205252170563 *\n",
      "Epoch: 15, Train_Loss: 0.358388751745224, Test_Loss: 0.348760724067688\n",
      "Epoch: 15, Train_Loss: 0.3779933452606201, Test_Loss: 0.36398398876190186\n",
      "Epoch: 15, Train_Loss: 0.3444098234176636, Test_Loss: 0.3289649188518524 *\n",
      "Epoch: 15, Train_Loss: 0.3338385820388794, Test_Loss: 0.388113409280777\n",
      "Epoch: 15, Train_Loss: 0.3407897651195526, Test_Loss: 0.39826706051826477\n",
      "Epoch: 15, Train_Loss: 0.3482816517353058, Test_Loss: 0.37081462144851685 *\n",
      "Epoch: 15, Train_Loss: 0.33763939142227173, Test_Loss: 0.36948907375335693 *\n",
      "Epoch: 15, Train_Loss: 0.33194828033447266, Test_Loss: 0.353163480758667 *\n",
      "Epoch: 15, Train_Loss: 0.3255608379840851, Test_Loss: 0.3809982240200043\n",
      "Epoch: 15, Train_Loss: 0.32406625151634216, Test_Loss: 0.33161360025405884 *\n",
      "Epoch: 15, Train_Loss: 0.3314495086669922, Test_Loss: 0.3334789574146271\n",
      "Epoch: 15, Train_Loss: 0.32929569482803345, Test_Loss: 0.34092384576797485\n",
      "Epoch: 15, Train_Loss: 0.3334455192089081, Test_Loss: 0.3441172242164612\n",
      "Epoch: 15, Train_Loss: 0.337404727935791, Test_Loss: 0.3396959900856018 *\n",
      "Epoch: 15, Train_Loss: 0.32622742652893066, Test_Loss: 0.3374883234500885 *\n",
      "Epoch: 15, Train_Loss: 0.3266240656375885, Test_Loss: 0.3324677348136902 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train_Loss: 0.32885101437568665, Test_Loss: 0.33055803179740906 *\n",
      "Epoch: 15, Train_Loss: 0.3402625620365143, Test_Loss: 0.3349381685256958\n",
      "Epoch: 15, Train_Loss: 0.34501156210899353, Test_Loss: 0.3286205530166626 *\n",
      "Epoch: 15, Train_Loss: 0.3384361267089844, Test_Loss: 0.33995357155799866\n",
      "Epoch: 15, Train_Loss: 0.34822386503219604, Test_Loss: 0.4077432453632355\n",
      "Epoch: 15, Train_Loss: 0.3319820761680603, Test_Loss: 0.34169143438339233 *\n",
      "Epoch: 15, Train_Loss: 0.3419896364212036, Test_Loss: 0.6840428113937378\n",
      "Epoch: 15, Train_Loss: 0.32706141471862793, Test_Loss: 0.7197362780570984\n",
      "Epoch: 15, Train_Loss: 0.3270425796508789, Test_Loss: 0.4402901530265808 *\n",
      "Epoch: 15, Train_Loss: 0.3501792550086975, Test_Loss: 0.3409445583820343 *\n",
      "Epoch: 15, Train_Loss: 0.33567774295806885, Test_Loss: 0.3431767523288727\n",
      "Epoch: 15, Train_Loss: 0.3288103938102722, Test_Loss: 0.3357568085193634 *\n",
      "Epoch: 15, Train_Loss: 0.3253462016582489, Test_Loss: 0.40892136096954346\n",
      "Epoch: 15, Train_Loss: 0.33133015036582947, Test_Loss: 0.5591042041778564\n",
      "Epoch: 15, Train_Loss: 0.36435768008232117, Test_Loss: 0.6273478269577026\n",
      "Epoch: 15, Train_Loss: 0.3640677332878113, Test_Loss: 0.3763313889503479 *\n",
      "Epoch: 15, Train_Loss: 0.37243834137916565, Test_Loss: 0.38208216428756714\n",
      "Epoch: 15, Train_Loss: 0.3228210210800171, Test_Loss: 0.32600390911102295 *\n",
      "Epoch: 15, Train_Loss: 0.3715585768222809, Test_Loss: 0.3263257145881653\n",
      "Epoch: 15, Train_Loss: 0.35428738594055176, Test_Loss: 0.3301546573638916\n",
      "Epoch: 15, Train_Loss: 0.32669952511787415, Test_Loss: 0.3333607316017151\n",
      "Epoch: 15, Train_Loss: 0.3347269892692566, Test_Loss: 0.3673037588596344\n",
      "Epoch: 15, Train_Loss: 0.3499816656112671, Test_Loss: 0.3325733542442322 *\n",
      "Epoch: 15, Train_Loss: 0.4190811514854431, Test_Loss: 0.33492469787597656\n",
      "Epoch: 15, Train_Loss: 0.404289573431015, Test_Loss: 0.43511447310447693\n",
      "Epoch: 15, Train_Loss: 0.36414703726768494, Test_Loss: 0.7145667672157288\n",
      "Epoch: 15, Train_Loss: 0.35012349486351013, Test_Loss: 0.5580279231071472 *\n",
      "Epoch: 15, Train_Loss: 0.328806608915329, Test_Loss: 0.3521881103515625 *\n",
      "Epoch: 15, Train_Loss: 0.3440518081188202, Test_Loss: 0.333604633808136 *\n",
      "Epoch: 15, Train_Loss: 0.32196035981178284, Test_Loss: 0.3332146406173706 *\n",
      "Epoch: 15, Train_Loss: 0.3301360309123993, Test_Loss: 0.3335376977920532\n",
      "Epoch: 15, Train_Loss: 0.32923373579978943, Test_Loss: 0.3332875967025757 *\n",
      "Epoch: 15, Train_Loss: 0.33621105551719666, Test_Loss: 0.5854446887969971\n",
      "Epoch: 15, Train_Loss: 0.4191804528236389, Test_Loss: 5.691593647003174\n",
      "Epoch: 15, Train_Loss: 0.32142871618270874, Test_Loss: 0.3944513499736786 *\n",
      "Epoch: 15, Train_Loss: 0.3863511383533478, Test_Loss: 0.33936259150505066 *\n",
      "Epoch: 15, Train_Loss: 0.32694414258003235, Test_Loss: 0.33356475830078125 *\n",
      "Epoch: 15, Train_Loss: 0.3414272964000702, Test_Loss: 0.3279256820678711 *\n",
      "Epoch: 15, Train_Loss: 0.33939841389656067, Test_Loss: 0.32756876945495605 *\n",
      "Epoch: 15, Train_Loss: 0.612438440322876, Test_Loss: 0.3328976631164551\n",
      "Epoch: 15, Train_Loss: 0.3565341830253601, Test_Loss: 0.3451747000217438\n",
      "Model saved at location save_new\\model.ckpt at epoch 15\n",
      "Epoch: 15, Train_Loss: 0.3565133512020111, Test_Loss: 0.32411086559295654 *\n",
      "Epoch: 15, Train_Loss: 0.3219204545021057, Test_Loss: 0.32614901661872864\n",
      "Epoch: 15, Train_Loss: 0.3206644058227539, Test_Loss: 0.3441188335418701\n",
      "Epoch: 15, Train_Loss: 0.32197269797325134, Test_Loss: 0.3944990634918213\n",
      "Epoch: 15, Train_Loss: 0.3210052251815796, Test_Loss: 0.328958660364151 *\n",
      "Epoch: 15, Train_Loss: 0.32729342579841614, Test_Loss: 0.34180474281311035\n",
      "Epoch: 15, Train_Loss: 0.3266395926475525, Test_Loss: 0.3709244132041931\n",
      "Epoch: 15, Train_Loss: 0.33919376134872437, Test_Loss: 0.32613202929496765 *\n",
      "Epoch: 15, Train_Loss: 0.3305785655975342, Test_Loss: 0.32390862703323364 *\n",
      "Epoch: 15, Train_Loss: 0.3275951147079468, Test_Loss: 0.3204444944858551 *\n",
      "Epoch: 15, Train_Loss: 0.33040642738342285, Test_Loss: 0.34758156538009644\n",
      "Epoch: 15, Train_Loss: 0.3219163715839386, Test_Loss: 0.3221394717693329 *\n",
      "Epoch: 15, Train_Loss: 0.31929346919059753, Test_Loss: 0.3299643099308014\n",
      "Epoch: 15, Train_Loss: 0.3347820043563843, Test_Loss: 0.32211098074913025 *\n",
      "Epoch: 15, Train_Loss: 0.34489548206329346, Test_Loss: 0.34110477566719055\n",
      "Epoch: 15, Train_Loss: 0.3594174385070801, Test_Loss: 0.34708118438720703\n",
      "Epoch: 15, Train_Loss: 0.3201863467693329, Test_Loss: 0.3304964005947113 *\n",
      "Epoch: 15, Train_Loss: 0.3573704957962036, Test_Loss: 0.32178348302841187 *\n",
      "Epoch: 15, Train_Loss: 0.38457924127578735, Test_Loss: 0.33092808723449707\n",
      "Epoch: 15, Train_Loss: 0.3559330999851227, Test_Loss: 0.3265504539012909 *\n",
      "Epoch: 15, Train_Loss: 0.3195493519306183, Test_Loss: 0.32360076904296875 *\n",
      "Epoch: 15, Train_Loss: 0.35772955417633057, Test_Loss: 0.33317121863365173\n",
      "Epoch: 15, Train_Loss: 0.3218690752983093, Test_Loss: 0.39017385244369507\n",
      "Epoch: 15, Train_Loss: 0.3364466428756714, Test_Loss: 2.6472532749176025\n",
      "Epoch: 15, Train_Loss: 0.3199670612812042, Test_Loss: 3.530820369720459\n",
      "Epoch: 15, Train_Loss: 0.3447517156600952, Test_Loss: 0.3232288360595703 *\n",
      "Epoch: 15, Train_Loss: 0.38823872804641724, Test_Loss: 0.31887632608413696 *\n",
      "Epoch: 15, Train_Loss: 3.235656261444092, Test_Loss: 0.3694688379764557\n",
      "Epoch: 15, Train_Loss: 2.6884641647338867, Test_Loss: 0.3440614938735962 *\n",
      "Epoch: 15, Train_Loss: 0.34685608744621277, Test_Loss: 0.36606207489967346\n",
      "Epoch: 15, Train_Loss: 0.32170534133911133, Test_Loss: 0.36230501532554626 *\n",
      "Epoch: 15, Train_Loss: 0.43228885531425476, Test_Loss: 0.4295414388179779\n",
      "Epoch: 15, Train_Loss: 0.43301233649253845, Test_Loss: 0.32374367117881775 *\n",
      "Epoch: 15, Train_Loss: 0.33990612626075745, Test_Loss: 0.33718204498291016\n",
      "Epoch: 15, Train_Loss: 0.31887000799179077, Test_Loss: 0.34113284945487976\n",
      "Epoch: 15, Train_Loss: 0.3606196343898773, Test_Loss: 0.3396724462509155 *\n",
      "Epoch: 15, Train_Loss: 0.34846651554107666, Test_Loss: 0.326021283864975 *\n",
      "Epoch: 15, Train_Loss: 0.3276888430118561, Test_Loss: 0.38150736689567566\n",
      "Epoch: 15, Train_Loss: 0.42726439237594604, Test_Loss: 0.3801516890525818 *\n",
      "Epoch: 15, Train_Loss: 1.0609426498413086, Test_Loss: 0.3799399435520172 *\n",
      "Epoch: 15, Train_Loss: 1.4710640907287598, Test_Loss: 0.37201544642448425 *\n",
      "Epoch: 15, Train_Loss: 0.3926946520805359, Test_Loss: 0.34005388617515564 *\n",
      "Epoch: 15, Train_Loss: 0.3977595567703247, Test_Loss: 0.3633093535900116\n",
      "Epoch: 15, Train_Loss: 1.8204936981201172, Test_Loss: 0.32002660632133484 *\n",
      "Epoch: 15, Train_Loss: 1.4103078842163086, Test_Loss: 0.3223875164985657\n",
      "Epoch: 15, Train_Loss: 0.33123520016670227, Test_Loss: 0.322733998298645\n",
      "Epoch: 15, Train_Loss: 0.31902769207954407, Test_Loss: 0.3317521810531616\n",
      "Epoch: 15, Train_Loss: 0.6696059703826904, Test_Loss: 0.32469406723976135 *\n",
      "Epoch: 15, Train_Loss: 0.6664680242538452, Test_Loss: 0.3203348219394684 *\n",
      "Epoch: 15, Train_Loss: 0.897213339805603, Test_Loss: 0.3309394419193268\n",
      "Epoch: 15, Train_Loss: 0.3609977662563324, Test_Loss: 0.3631278872489929\n",
      "Epoch: 15, Train_Loss: 0.35191404819488525, Test_Loss: 0.4392070472240448\n",
      "Epoch: 15, Train_Loss: 0.41148126125335693, Test_Loss: 0.4655885100364685\n",
      "Epoch: 15, Train_Loss: 0.47223252058029175, Test_Loss: 0.38768208026885986 *\n",
      "Epoch: 16, Train_Loss: 0.3814423978328705, Test_Loss: 0.37401461601257324 *\n",
      "Epoch: 16, Train_Loss: 0.42822718620300293, Test_Loss: 0.408474862575531\n",
      "Epoch: 16, Train_Loss: 0.3634839355945587, Test_Loss: 0.5044354200363159\n",
      "Epoch: 16, Train_Loss: 0.32783016562461853, Test_Loss: 0.422799289226532 *\n",
      "Epoch: 16, Train_Loss: 0.44642189145088196, Test_Loss: 0.335479736328125 *\n",
      "Epoch: 16, Train_Loss: 0.4118592143058777, Test_Loss: 0.36741209030151367\n",
      "Epoch: 16, Train_Loss: 0.36545202136039734, Test_Loss: 0.3686712384223938\n",
      "Epoch: 16, Train_Loss: 0.3948659300804138, Test_Loss: 0.35881054401397705 *\n",
      "Epoch: 16, Train_Loss: 0.3960052728652954, Test_Loss: 0.4638988673686981\n",
      "Epoch: 16, Train_Loss: 0.3946646749973297, Test_Loss: 0.4723231792449951\n",
      "Epoch: 16, Train_Loss: 0.4779764413833618, Test_Loss: 0.6675090789794922\n",
      "Epoch: 16, Train_Loss: 0.4367741048336029, Test_Loss: 0.3994138240814209 *\n",
      "Epoch: 16, Train_Loss: 0.36247286200523376, Test_Loss: 0.38969358801841736 *\n",
      "Epoch: 16, Train_Loss: 0.6239780187606812, Test_Loss: 0.3672233819961548 *\n",
      "Epoch: 16, Train_Loss: 0.45146018266677856, Test_Loss: 0.3377421498298645 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train_Loss: 0.341805100440979, Test_Loss: 0.35147547721862793\n",
      "Epoch: 16, Train_Loss: 0.3261852562427521, Test_Loss: 0.35644328594207764\n",
      "Epoch: 16, Train_Loss: 0.3237321972846985, Test_Loss: 0.34373024106025696 *\n",
      "Epoch: 16, Train_Loss: 0.3262200951576233, Test_Loss: 0.36518049240112305\n",
      "Epoch: 16, Train_Loss: 0.32420045137405396, Test_Loss: 0.381344199180603\n",
      "Epoch: 16, Train_Loss: 0.32557886838912964, Test_Loss: 0.5225395560264587\n",
      "Epoch: 16, Train_Loss: 0.3430696129798889, Test_Loss: 0.6257199048995972\n",
      "Epoch: 16, Train_Loss: 0.34229910373687744, Test_Loss: 0.9675405025482178\n",
      "Epoch: 16, Train_Loss: 0.3537245988845825, Test_Loss: 0.4813855290412903 *\n",
      "Epoch: 16, Train_Loss: 0.4372212886810303, Test_Loss: 0.3230653703212738 *\n",
      "Epoch: 16, Train_Loss: 0.6243919134140015, Test_Loss: 0.32341787219047546\n",
      "Epoch: 16, Train_Loss: 0.33828169107437134, Test_Loss: 0.32590004801750183\n",
      "Epoch: 16, Train_Loss: 0.35195332765579224, Test_Loss: 0.5415456891059875\n",
      "Epoch: 16, Train_Loss: 0.40691938996315, Test_Loss: 1.3425663709640503\n",
      "Epoch: 16, Train_Loss: 0.38997358083724976, Test_Loss: 4.407598495483398\n",
      "Epoch: 16, Train_Loss: 0.49615105986595154, Test_Loss: 0.3690696954727173 *\n",
      "Epoch: 16, Train_Loss: 0.4095984697341919, Test_Loss: 0.35421568155288696 *\n",
      "Epoch: 16, Train_Loss: 0.5751702785491943, Test_Loss: 0.3395184874534607 *\n",
      "Epoch: 16, Train_Loss: 0.5103235244750977, Test_Loss: 0.32986927032470703 *\n",
      "Epoch: 16, Train_Loss: 0.44247955083847046, Test_Loss: 0.3343450725078583\n",
      "Epoch: 16, Train_Loss: 0.34029003977775574, Test_Loss: 0.36492693424224854\n",
      "Epoch: 16, Train_Loss: 0.3583850860595703, Test_Loss: 0.3487322926521301 *\n",
      "Epoch: 16, Train_Loss: 0.5021891593933105, Test_Loss: 0.31876084208488464 *\n",
      "Epoch: 16, Train_Loss: 0.9076764583587646, Test_Loss: 0.33388176560401917\n",
      "Epoch: 16, Train_Loss: 0.7068002223968506, Test_Loss: 0.33865809440612793\n",
      "Epoch: 16, Train_Loss: 0.35157185792922974, Test_Loss: 0.42163023352622986\n",
      "Epoch: 16, Train_Loss: 0.3334495723247528, Test_Loss: 0.3286091387271881 *\n",
      "Epoch: 16, Train_Loss: 0.32258039712905884, Test_Loss: 0.32675454020500183 *\n",
      "Epoch: 16, Train_Loss: 0.6015286445617676, Test_Loss: 0.3532693684101105\n",
      "Epoch: 16, Train_Loss: 0.5497199296951294, Test_Loss: 0.3387935757637024 *\n",
      "Epoch: 16, Train_Loss: 0.32172179222106934, Test_Loss: 0.33379119634628296 *\n",
      "Epoch: 16, Train_Loss: 0.4880717396736145, Test_Loss: 0.4351293444633484\n",
      "Epoch: 16, Train_Loss: 0.36343538761138916, Test_Loss: 0.4011617600917816 *\n",
      "Epoch: 16, Train_Loss: 0.33112454414367676, Test_Loss: 0.3250344693660736 *\n",
      "Epoch: 16, Train_Loss: 0.3641994297504425, Test_Loss: 0.36269092559814453\n",
      "Epoch: 16, Train_Loss: 0.4787416458129883, Test_Loss: 0.32587432861328125 *\n",
      "Epoch: 16, Train_Loss: 0.3585655987262726, Test_Loss: 0.3783184289932251\n",
      "Epoch: 16, Train_Loss: 0.4277327060699463, Test_Loss: 0.4002281129360199\n",
      "Epoch: 16, Train_Loss: 0.33921003341674805, Test_Loss: 0.3395758867263794 *\n",
      "Epoch: 16, Train_Loss: 0.4589953124523163, Test_Loss: 0.32162097096443176 *\n",
      "Epoch: 16, Train_Loss: 0.3393116593360901, Test_Loss: 0.36457082629203796\n",
      "Epoch: 16, Train_Loss: 0.33883804082870483, Test_Loss: 0.3616940379142761 *\n",
      "Epoch: 16, Train_Loss: 0.32065004110336304, Test_Loss: 0.3303905427455902 *\n",
      "Epoch: 16, Train_Loss: 0.3704378008842468, Test_Loss: 0.3890291750431061\n",
      "Epoch: 16, Train_Loss: 0.4176584482192993, Test_Loss: 0.41667911410331726\n",
      "Epoch: 16, Train_Loss: 0.51530921459198, Test_Loss: 3.5939900875091553\n",
      "Epoch: 16, Train_Loss: 0.497012197971344, Test_Loss: 2.236733913421631 *\n",
      "Epoch: 16, Train_Loss: 0.7570575475692749, Test_Loss: 0.34560343623161316 *\n",
      "Epoch: 16, Train_Loss: 0.6331854462623596, Test_Loss: 0.3385583758354187 *\n",
      "Epoch: 16, Train_Loss: 0.5206624865531921, Test_Loss: 0.36026692390441895\n",
      "Epoch: 16, Train_Loss: 0.3843177556991577, Test_Loss: 0.3207654654979706 *\n",
      "Epoch: 16, Train_Loss: 0.3336811065673828, Test_Loss: 0.3360782265663147\n",
      "Epoch: 16, Train_Loss: 0.3284558951854706, Test_Loss: 0.3788570165634155\n",
      "Epoch: 16, Train_Loss: 0.3251778185367584, Test_Loss: 0.35998424887657166 *\n",
      "Epoch: 16, Train_Loss: 0.4443322420120239, Test_Loss: 0.3241766095161438 *\n",
      "Epoch: 16, Train_Loss: 0.6088491678237915, Test_Loss: 0.34409475326538086\n",
      "Epoch: 16, Train_Loss: 0.6242297887802124, Test_Loss: 0.36317914724349976\n",
      "Epoch: 16, Train_Loss: 1.5153279304504395, Test_Loss: 0.4001528322696686\n",
      "Epoch: 16, Train_Loss: 1.1873635053634644, Test_Loss: 0.3364337086677551 *\n",
      "Epoch: 16, Train_Loss: 0.5388431549072266, Test_Loss: 0.35673364996910095\n",
      "Epoch: 16, Train_Loss: 0.4691163897514343, Test_Loss: 0.3665899932384491\n",
      "Epoch: 16, Train_Loss: 0.32088690996170044, Test_Loss: 0.37125465273857117\n",
      "Epoch: 16, Train_Loss: 0.41542842984199524, Test_Loss: 0.32495784759521484 *\n",
      "Epoch: 16, Train_Loss: 0.9193275570869446, Test_Loss: 0.39935487508773804\n",
      "Epoch: 16, Train_Loss: 1.0179256200790405, Test_Loss: 0.3616325855255127 *\n",
      "Epoch: 16, Train_Loss: 0.3376224637031555, Test_Loss: 0.3175579011440277 *\n",
      "Epoch: 16, Train_Loss: 0.3606298565864563, Test_Loss: 0.32676562666893005\n",
      "Epoch: 16, Train_Loss: 0.3979876637458801, Test_Loss: 0.33022207021713257\n",
      "Epoch: 16, Train_Loss: 0.58558189868927, Test_Loss: 0.32652175426483154 *\n",
      "Epoch: 16, Train_Loss: 0.43476444482803345, Test_Loss: 0.32828226685523987\n",
      "Epoch: 16, Train_Loss: 0.5028846263885498, Test_Loss: 0.32777488231658936 *\n",
      "Epoch: 16, Train_Loss: 0.45382463932037354, Test_Loss: 0.3348124921321869\n",
      "Epoch: 16, Train_Loss: 0.491932213306427, Test_Loss: 0.3437396287918091\n",
      "Epoch: 16, Train_Loss: 0.3251398503780365, Test_Loss: 0.3957674503326416\n",
      "Epoch: 16, Train_Loss: 0.34122616052627563, Test_Loss: 0.40272632241249084\n",
      "Epoch: 16, Train_Loss: 0.325863778591156, Test_Loss: 0.3300114870071411 *\n",
      "Epoch: 16, Train_Loss: 0.40899795293807983, Test_Loss: 0.34858688712120056\n",
      "Epoch: 16, Train_Loss: 0.331228643655777, Test_Loss: 0.4057527780532837\n",
      "Epoch: 16, Train_Loss: 0.3451882004737854, Test_Loss: 0.4452345371246338\n",
      "Epoch: 16, Train_Loss: 16.30167579650879, Test_Loss: 0.47775498032569885\n",
      "Epoch: 16, Train_Loss: 0.3390467166900635, Test_Loss: 0.3643593192100525 *\n",
      "Epoch: 16, Train_Loss: 1.6782732009887695, Test_Loss: 0.33572956919670105 *\n",
      "Epoch: 16, Train_Loss: 1.276249647140503, Test_Loss: 0.3456493616104126\n",
      "Epoch: 16, Train_Loss: 0.33966562151908875, Test_Loss: 0.34019726514816284 *\n",
      "Epoch: 16, Train_Loss: 0.5817439556121826, Test_Loss: 0.4790493845939636\n",
      "Model saved at location save_new\\model.ckpt at epoch 16\n",
      "Epoch: 16, Train_Loss: 3.4821603298187256, Test_Loss: 0.3829806447029114 *\n",
      "Epoch: 16, Train_Loss: 4.830814838409424, Test_Loss: 0.6975889205932617\n",
      "Epoch: 16, Train_Loss: 0.39606282114982605, Test_Loss: 0.47258615493774414 *\n",
      "Epoch: 16, Train_Loss: 0.5556077361106873, Test_Loss: 0.4251670837402344 *\n",
      "Epoch: 16, Train_Loss: 4.862270832061768, Test_Loss: 0.4552002251148224\n",
      "Epoch: 16, Train_Loss: 0.7800391912460327, Test_Loss: 0.4753984808921814\n",
      "Epoch: 16, Train_Loss: 0.43459293246269226, Test_Loss: 0.6380316019058228\n",
      "Epoch: 16, Train_Loss: 0.31800779700279236, Test_Loss: 0.33992454409599304 *\n",
      "Epoch: 16, Train_Loss: 0.3651065528392792, Test_Loss: 0.3972693681716919\n",
      "Epoch: 16, Train_Loss: 0.3483712375164032, Test_Loss: 0.3563970923423767 *\n",
      "Epoch: 16, Train_Loss: 0.3175548315048218, Test_Loss: 0.7015113234519958\n",
      "Epoch: 16, Train_Loss: 0.31548553705215454, Test_Loss: 1.5044629573822021\n",
      "Epoch: 16, Train_Loss: 0.3116692006587982, Test_Loss: 0.619888186454773 *\n",
      "Epoch: 16, Train_Loss: 0.31119677424430847, Test_Loss: 0.4363240897655487 *\n",
      "Epoch: 16, Train_Loss: 0.3357352018356323, Test_Loss: 1.038966178894043\n",
      "Epoch: 16, Train_Loss: 0.3233167827129364, Test_Loss: 1.2857722043991089\n",
      "Epoch: 16, Train_Loss: 0.43516284227371216, Test_Loss: 1.1100021600723267 *\n",
      "Epoch: 16, Train_Loss: 0.3749467730522156, Test_Loss: 0.816696047782898 *\n",
      "Epoch: 16, Train_Loss: 0.3406566381454468, Test_Loss: 0.3495340943336487 *\n",
      "Epoch: 16, Train_Loss: 0.3172825574874878, Test_Loss: 3.50536847114563\n",
      "Epoch: 16, Train_Loss: 0.3430302143096924, Test_Loss: 4.262099266052246\n",
      "Epoch: 16, Train_Loss: 0.31381890177726746, Test_Loss: 0.7438161373138428 *\n",
      "Epoch: 16, Train_Loss: 0.3130893409252167, Test_Loss: 0.49431920051574707 *\n",
      "Epoch: 16, Train_Loss: 0.3134714365005493, Test_Loss: 0.6754770874977112\n",
      "Epoch: 16, Train_Loss: 0.31114864349365234, Test_Loss: 0.419990599155426 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train_Loss: 0.3111785352230072, Test_Loss: 0.5345647931098938\n",
      "Epoch: 16, Train_Loss: 0.3101955056190491, Test_Loss: 0.8138421773910522\n",
      "Epoch: 16, Train_Loss: 0.3123822808265686, Test_Loss: 0.5604835748672485 *\n",
      "Epoch: 16, Train_Loss: 0.31126096844673157, Test_Loss: 0.4171925485134125 *\n",
      "Epoch: 16, Train_Loss: 0.3109530806541443, Test_Loss: 0.5243455171585083\n",
      "Epoch: 16, Train_Loss: 0.3208320140838623, Test_Loss: 0.42273062467575073 *\n",
      "Epoch: 16, Train_Loss: 0.3444526493549347, Test_Loss: 0.8165379762649536\n",
      "Epoch: 16, Train_Loss: 0.3581344187259674, Test_Loss: 0.6148215532302856 *\n",
      "Epoch: 16, Train_Loss: 0.359891414642334, Test_Loss: 1.1019397974014282\n",
      "Epoch: 16, Train_Loss: 0.45033958554267883, Test_Loss: 0.8088363409042358 *\n",
      "Epoch: 16, Train_Loss: 2.9297924041748047, Test_Loss: 0.3877967298030853 *\n",
      "Epoch: 16, Train_Loss: 5.951170921325684, Test_Loss: 0.4979347288608551\n",
      "Epoch: 16, Train_Loss: 0.3229411542415619, Test_Loss: 0.5683942437171936\n",
      "Epoch: 16, Train_Loss: 0.35645678639411926, Test_Loss: 0.95732581615448\n",
      "Epoch: 16, Train_Loss: 0.39389699697494507, Test_Loss: 0.5018835663795471 *\n",
      "Epoch: 16, Train_Loss: 0.38476741313934326, Test_Loss: 0.541458010673523\n",
      "Epoch: 16, Train_Loss: 0.34647390246391296, Test_Loss: 0.7105634212493896\n",
      "Epoch: 16, Train_Loss: 0.53624427318573, Test_Loss: 0.932115912437439\n",
      "Epoch: 16, Train_Loss: 0.43084171414375305, Test_Loss: 1.0812779664993286\n",
      "Epoch: 16, Train_Loss: 0.4502573609352112, Test_Loss: 0.5287021994590759 *\n",
      "Epoch: 16, Train_Loss: 0.42060956358909607, Test_Loss: 0.4405944347381592 *\n",
      "Epoch: 16, Train_Loss: 0.3545883893966675, Test_Loss: 0.6604964733123779\n",
      "Epoch: 16, Train_Loss: 0.33708861470222473, Test_Loss: 0.5235860347747803 *\n",
      "Epoch: 16, Train_Loss: 0.4676364064216614, Test_Loss: 0.4388696253299713 *\n",
      "Epoch: 16, Train_Loss: 0.47965845465660095, Test_Loss: 0.6552697420120239\n",
      "Epoch: 16, Train_Loss: 0.5420377254486084, Test_Loss: 0.36648911237716675 *\n",
      "Epoch: 16, Train_Loss: 0.35343852639198303, Test_Loss: 5.159328937530518\n",
      "Epoch: 16, Train_Loss: 0.3437291085720062, Test_Loss: 1.4513094425201416 *\n",
      "Epoch: 16, Train_Loss: 0.31349408626556396, Test_Loss: 0.3132455050945282 *\n",
      "Epoch: 16, Train_Loss: 0.3225550651550293, Test_Loss: 0.32647234201431274\n",
      "Epoch: 16, Train_Loss: 0.3302639424800873, Test_Loss: 0.3133799433708191 *\n",
      "Epoch: 16, Train_Loss: 0.3730892539024353, Test_Loss: 0.3200353980064392\n",
      "Epoch: 16, Train_Loss: 0.3331799805164337, Test_Loss: 0.31931525468826294 *\n",
      "Epoch: 16, Train_Loss: 0.3152715563774109, Test_Loss: 0.3990527093410492\n",
      "Epoch: 16, Train_Loss: 0.343961626291275, Test_Loss: 0.3895307779312134 *\n",
      "Epoch: 16, Train_Loss: 2.5037126541137695, Test_Loss: 0.31086939573287964 *\n",
      "Epoch: 16, Train_Loss: 3.4782795906066895, Test_Loss: 0.3442418873310089\n",
      "Epoch: 16, Train_Loss: 0.3106549382209778, Test_Loss: 0.32563459873199463 *\n",
      "Epoch: 16, Train_Loss: 0.31224527955055237, Test_Loss: 0.32054102420806885 *\n",
      "Epoch: 16, Train_Loss: 0.31695038080215454, Test_Loss: 0.3181074857711792 *\n",
      "Epoch: 16, Train_Loss: 0.3112926781177521, Test_Loss: 0.3165397346019745 *\n",
      "Epoch: 16, Train_Loss: 0.31358546018600464, Test_Loss: 0.35652148723602295\n",
      "Epoch: 16, Train_Loss: 0.30979123711586, Test_Loss: 0.41837841272354126\n",
      "Epoch: 16, Train_Loss: 0.3281734585762024, Test_Loss: 0.3708791434764862 *\n",
      "Epoch: 16, Train_Loss: 0.32492151856422424, Test_Loss: 0.3425733745098114 *\n",
      "Epoch: 16, Train_Loss: 0.3600222170352936, Test_Loss: 0.3199191689491272 *\n",
      "Epoch: 16, Train_Loss: 0.3095318078994751, Test_Loss: 0.3210715353488922\n",
      "Epoch: 16, Train_Loss: 0.3084808588027954, Test_Loss: 0.317295104265213 *\n",
      "Epoch: 16, Train_Loss: 0.3124193251132965, Test_Loss: 0.3206685185432434\n",
      "Epoch: 16, Train_Loss: 0.32121771574020386, Test_Loss: 0.3147869408130646 *\n",
      "Epoch: 16, Train_Loss: 0.3107599914073944, Test_Loss: 0.3178384006023407\n",
      "Epoch: 16, Train_Loss: 0.3119520843029022, Test_Loss: 0.3320070803165436\n",
      "Epoch: 16, Train_Loss: 0.3351379632949829, Test_Loss: 0.3179933726787567 *\n",
      "Epoch: 16, Train_Loss: 0.33565330505371094, Test_Loss: 0.3116726875305176 *\n",
      "Epoch: 16, Train_Loss: 0.31216961145401, Test_Loss: 0.32160815596580505\n",
      "Epoch: 16, Train_Loss: 0.3098328113555908, Test_Loss: 0.3298012316226959\n",
      "Epoch: 16, Train_Loss: 0.3364558219909668, Test_Loss: 0.32141587138175964 *\n",
      "Epoch: 16, Train_Loss: 0.341350793838501, Test_Loss: 0.33206844329833984\n",
      "Epoch: 16, Train_Loss: 0.32906630635261536, Test_Loss: 0.43805089592933655\n",
      "Epoch: 16, Train_Loss: 0.35463517904281616, Test_Loss: 0.5519794821739197\n",
      "Epoch: 16, Train_Loss: 0.4591251611709595, Test_Loss: 0.4385661780834198 *\n",
      "Epoch: 16, Train_Loss: 0.4046200215816498, Test_Loss: 0.35531121492385864 *\n",
      "Epoch: 16, Train_Loss: 0.33371204137802124, Test_Loss: 0.3199186623096466 *\n",
      "Epoch: 16, Train_Loss: 0.3563390076160431, Test_Loss: 0.32804471254348755\n",
      "Epoch: 16, Train_Loss: 0.43353426456451416, Test_Loss: 0.36375999450683594\n",
      "Epoch: 16, Train_Loss: 0.42440739274024963, Test_Loss: 0.6266001462936401\n",
      "Epoch: 16, Train_Loss: 0.3516744077205658, Test_Loss: 0.5848445892333984 *\n",
      "Epoch: 16, Train_Loss: 0.3099133372306824, Test_Loss: 0.5969181060791016\n",
      "Epoch: 16, Train_Loss: 0.3149106204509735, Test_Loss: 0.3761848509311676 *\n",
      "Epoch: 16, Train_Loss: 0.31300944089889526, Test_Loss: 0.32773342728614807 *\n",
      "Epoch: 16, Train_Loss: 0.31283655762672424, Test_Loss: 0.3287813067436218\n",
      "Epoch: 16, Train_Loss: 0.3074507415294647, Test_Loss: 0.31947481632232666 *\n",
      "Epoch: 16, Train_Loss: 3.0122756958007812, Test_Loss: 0.3325117826461792\n",
      "Epoch: 16, Train_Loss: 2.4196205139160156, Test_Loss: 0.32696446776390076 *\n",
      "Epoch: 16, Train_Loss: 0.3066091537475586, Test_Loss: 0.34198787808418274\n",
      "Model saved at location save_new\\model.ckpt at epoch 16\n",
      "Epoch: 16, Train_Loss: 0.317374587059021, Test_Loss: 0.3116176128387451 *\n",
      "Epoch: 16, Train_Loss: 0.31246671080589294, Test_Loss: 0.41724663972854614\n",
      "Epoch: 16, Train_Loss: 0.30767926573753357, Test_Loss: 0.637839674949646\n",
      "Epoch: 16, Train_Loss: 0.30661293864250183, Test_Loss: 0.44470852613449097 *\n",
      "Epoch: 16, Train_Loss: 0.30670031905174255, Test_Loss: 0.5365894436836243\n",
      "Epoch: 16, Train_Loss: 0.30577296018600464, Test_Loss: 0.3261309266090393 *\n",
      "Epoch: 16, Train_Loss: 0.30660250782966614, Test_Loss: 0.3253277838230133 *\n",
      "Epoch: 16, Train_Loss: 0.3190976083278656, Test_Loss: 0.32579344511032104\n",
      "Epoch: 16, Train_Loss: 0.35632461309432983, Test_Loss: 0.3254775106906891 *\n",
      "Epoch: 16, Train_Loss: 0.35368406772613525, Test_Loss: 0.34609609842300415\n",
      "Epoch: 16, Train_Loss: 0.36033859848976135, Test_Loss: 3.841153383255005\n",
      "Epoch: 16, Train_Loss: 0.329507976770401, Test_Loss: 2.0426127910614014 *\n",
      "Epoch: 16, Train_Loss: 0.31792837381362915, Test_Loss: 0.3212417960166931 *\n",
      "Epoch: 16, Train_Loss: 0.5355179309844971, Test_Loss: 0.3143116533756256 *\n",
      "Epoch: 16, Train_Loss: 0.5411133170127869, Test_Loss: 0.3128304183483124 *\n",
      "Epoch: 16, Train_Loss: 0.5233796834945679, Test_Loss: 0.32478564977645874\n",
      "Epoch: 16, Train_Loss: 0.38468465209007263, Test_Loss: 0.30960413813591003 *\n",
      "Epoch: 16, Train_Loss: 0.3058713376522064, Test_Loss: 0.31562793254852295\n",
      "Epoch: 16, Train_Loss: 0.3050667941570282, Test_Loss: 0.30655863881111145 *\n",
      "Epoch: 16, Train_Loss: 0.3108174502849579, Test_Loss: 0.306741863489151\n",
      "Epoch: 16, Train_Loss: 0.3199363052845001, Test_Loss: 0.30907371640205383\n",
      "Epoch: 16, Train_Loss: 0.3261975347995758, Test_Loss: 0.30993688106536865\n",
      "Epoch: 16, Train_Loss: 0.3200574219226837, Test_Loss: 0.3213101625442505\n",
      "Epoch: 16, Train_Loss: 0.30477747321128845, Test_Loss: 0.32365530729293823\n",
      "Epoch: 16, Train_Loss: 0.3050813674926758, Test_Loss: 0.31282946467399597 *\n",
      "Epoch: 16, Train_Loss: 0.32358625531196594, Test_Loss: 0.31194931268692017 *\n",
      "Epoch: 16, Train_Loss: 0.39432498812675476, Test_Loss: 0.30553460121154785 *\n",
      "Epoch: 16, Train_Loss: 0.5240933299064636, Test_Loss: 0.30740436911582947\n",
      "Epoch: 16, Train_Loss: 0.4714275002479553, Test_Loss: 0.3129221498966217\n",
      "Epoch: 16, Train_Loss: 0.41828763484954834, Test_Loss: 0.3138437569141388\n",
      "Epoch: 16, Train_Loss: 0.3941783308982849, Test_Loss: 0.30840715765953064 *\n",
      "Epoch: 16, Train_Loss: 0.43248993158340454, Test_Loss: 0.30518007278442383 *\n",
      "Epoch: 16, Train_Loss: 0.3278188109397888, Test_Loss: 0.3168545663356781\n",
      "Epoch: 16, Train_Loss: 0.43626248836517334, Test_Loss: 0.3261559307575226\n",
      "Epoch: 16, Train_Loss: 0.4010734558105469, Test_Loss: 0.32501351833343506 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train_Loss: 0.5661348104476929, Test_Loss: 0.3097299039363861 *\n",
      "Epoch: 16, Train_Loss: 0.3136720359325409, Test_Loss: 0.30688774585723877 *\n",
      "Epoch: 16, Train_Loss: 0.5527623891830444, Test_Loss: 0.3119727075099945\n",
      "Epoch: 16, Train_Loss: 3.0203640460968018, Test_Loss: 0.31128162145614624 *\n",
      "Epoch: 16, Train_Loss: 0.3853875994682312, Test_Loss: 0.30786770582199097 *\n",
      "Epoch: 16, Train_Loss: 0.36600950360298157, Test_Loss: 0.3703717589378357\n",
      "Epoch: 16, Train_Loss: 0.341626912355423, Test_Loss: 0.34169483184814453 *\n",
      "Epoch: 16, Train_Loss: 0.3278951644897461, Test_Loss: 5.607426643371582\n",
      "Epoch: 16, Train_Loss: 0.308903306722641, Test_Loss: 0.4252656400203705 *\n",
      "Epoch: 16, Train_Loss: 0.31242746114730835, Test_Loss: 0.3055141866207123 *\n",
      "Epoch: 16, Train_Loss: 0.398712158203125, Test_Loss: 0.33401063084602356\n",
      "Epoch: 16, Train_Loss: 0.4225490689277649, Test_Loss: 0.3450677692890167\n",
      "Epoch: 16, Train_Loss: 0.387032151222229, Test_Loss: 0.35855942964553833\n",
      "Epoch: 16, Train_Loss: 0.34082460403442383, Test_Loss: 0.30967801809310913 *\n",
      "Epoch: 16, Train_Loss: 0.3282916843891144, Test_Loss: 0.39147812128067017\n",
      "Epoch: 16, Train_Loss: 0.32359689474105835, Test_Loss: 0.3515135943889618 *\n",
      "Epoch: 16, Train_Loss: 0.32408037781715393, Test_Loss: 0.3080809414386749 *\n",
      "Epoch: 16, Train_Loss: 0.3158576190471649, Test_Loss: 0.32185861468315125\n",
      "Epoch: 16, Train_Loss: 0.3562767207622528, Test_Loss: 0.3387686014175415\n",
      "Epoch: 16, Train_Loss: 0.32577359676361084, Test_Loss: 0.30832022428512573 *\n",
      "Epoch: 16, Train_Loss: 0.3073551654815674, Test_Loss: 0.33409953117370605\n",
      "Epoch: 16, Train_Loss: 0.32185888290405273, Test_Loss: 0.3873140513896942\n",
      "Epoch: 16, Train_Loss: 0.3369561731815338, Test_Loss: 0.3405262231826782 *\n",
      "Epoch: 16, Train_Loss: 0.3213922381401062, Test_Loss: 0.3543136715888977\n",
      "Epoch: 16, Train_Loss: 0.3093656599521637, Test_Loss: 0.33538374304771423 *\n",
      "Epoch: 16, Train_Loss: 0.3054455816745758, Test_Loss: 0.37071293592453003\n",
      "Epoch: 16, Train_Loss: 0.30402061343193054, Test_Loss: 0.3168536126613617 *\n",
      "Epoch: 16, Train_Loss: 0.30695340037345886, Test_Loss: 0.3177192807197571\n",
      "Epoch: 16, Train_Loss: 0.3160507380962372, Test_Loss: 0.3216366469860077\n",
      "Epoch: 16, Train_Loss: 0.30659475922584534, Test_Loss: 0.3233901560306549\n",
      "Epoch: 16, Train_Loss: 0.3164355754852295, Test_Loss: 0.3229590058326721 *\n",
      "Epoch: 16, Train_Loss: 0.3059251308441162, Test_Loss: 0.32117289304733276 *\n",
      "Epoch: 16, Train_Loss: 0.30410492420196533, Test_Loss: 0.3106684684753418 *\n",
      "Epoch: 16, Train_Loss: 0.30698439478874207, Test_Loss: 0.320555180311203\n",
      "Epoch: 16, Train_Loss: 0.3168961703777313, Test_Loss: 0.31723135709762573 *\n",
      "Epoch: 16, Train_Loss: 0.3196662664413452, Test_Loss: 0.3099864721298218 *\n",
      "Epoch: 16, Train_Loss: 0.32776525616645813, Test_Loss: 0.3158864676952362\n",
      "Epoch: 16, Train_Loss: 0.32998648285865784, Test_Loss: 0.3764009475708008\n",
      "Epoch: 16, Train_Loss: 0.3119881749153137, Test_Loss: 0.3374216854572296 *\n",
      "Epoch: 16, Train_Loss: 0.32833823561668396, Test_Loss: 0.5693577527999878\n",
      "Epoch: 16, Train_Loss: 0.3091089427471161, Test_Loss: 0.7391208410263062\n",
      "Epoch: 16, Train_Loss: 0.3064684569835663, Test_Loss: 0.4791547656059265 *\n",
      "Epoch: 16, Train_Loss: 0.3260103464126587, Test_Loss: 0.35422900319099426 *\n",
      "Epoch: 16, Train_Loss: 0.3256130814552307, Test_Loss: 0.3085417151451111 *\n",
      "Epoch: 16, Train_Loss: 0.30599167943000793, Test_Loss: 0.30946215987205505\n",
      "Epoch: 16, Train_Loss: 0.3051202595233917, Test_Loss: 0.3637330234050751\n",
      "Epoch: 16, Train_Loss: 0.30799099802970886, Test_Loss: 0.670447826385498\n",
      "Epoch: 16, Train_Loss: 0.33792179822921753, Test_Loss: 0.5968122482299805 *\n",
      "Epoch: 16, Train_Loss: 0.3367358446121216, Test_Loss: 0.44710999727249146 *\n",
      "Epoch: 16, Train_Loss: 0.35882437229156494, Test_Loss: 0.38381823897361755 *\n",
      "Epoch: 16, Train_Loss: 0.30778902769088745, Test_Loss: 0.30440255999565125 *\n",
      "Epoch: 16, Train_Loss: 0.3197437524795532, Test_Loss: 0.308242529630661\n",
      "Epoch: 16, Train_Loss: 0.3678605556488037, Test_Loss: 0.30642056465148926 *\n",
      "Epoch: 16, Train_Loss: 0.30456459522247314, Test_Loss: 0.31841057538986206\n",
      "Epoch: 16, Train_Loss: 0.3115314841270447, Test_Loss: 0.3250007927417755\n",
      "Epoch: 16, Train_Loss: 0.3209134042263031, Test_Loss: 0.33114930987358093\n",
      "Epoch: 16, Train_Loss: 0.36310797929763794, Test_Loss: 0.30595290660858154 *\n",
      "Epoch: 16, Train_Loss: 0.39216727018356323, Test_Loss: 0.4254762828350067\n",
      "Epoch: 16, Train_Loss: 0.34896552562713623, Test_Loss: 0.6932744979858398\n",
      "Epoch: 16, Train_Loss: 0.3291628956794739, Test_Loss: 0.4006027579307556 *\n",
      "Epoch: 16, Train_Loss: 0.30607980489730835, Test_Loss: 0.48174241185188293\n",
      "Epoch: 16, Train_Loss: 0.3266439735889435, Test_Loss: 0.31485652923583984 *\n",
      "Epoch: 16, Train_Loss: 0.3022255599498749, Test_Loss: 0.3146449327468872 *\n",
      "Epoch: 16, Train_Loss: 0.3066991865634918, Test_Loss: 0.31485164165496826\n",
      "Epoch: 16, Train_Loss: 0.3088344931602478, Test_Loss: 0.31383249163627625 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 16\n",
      "Epoch: 16, Train_Loss: 0.3206515312194824, Test_Loss: 0.33241650462150574\n",
      "Epoch: 16, Train_Loss: 0.37380897998809814, Test_Loss: 5.1413984298706055\n",
      "Epoch: 16, Train_Loss: 0.3412426710128784, Test_Loss: 0.9124173521995544 *\n",
      "Epoch: 16, Train_Loss: 0.3524973392486572, Test_Loss: 0.31559988856315613 *\n",
      "Epoch: 16, Train_Loss: 0.31835854053497314, Test_Loss: 0.3064661920070648 *\n",
      "Epoch: 16, Train_Loss: 0.32296884059906006, Test_Loss: 0.30771753191947937\n",
      "Epoch: 16, Train_Loss: 0.31316253542900085, Test_Loss: 0.31127437949180603\n",
      "Epoch: 16, Train_Loss: 0.5033451318740845, Test_Loss: 0.3073592185974121 *\n",
      "Epoch: 16, Train_Loss: 0.4387199878692627, Test_Loss: 0.3175108730792999\n",
      "Epoch: 16, Train_Loss: 0.30946674942970276, Test_Loss: 0.3031165301799774 *\n",
      "Epoch: 16, Train_Loss: 0.3325699269771576, Test_Loss: 0.30300843715667725 *\n",
      "Epoch: 16, Train_Loss: 0.3002665340900421, Test_Loss: 0.31233781576156616\n",
      "Epoch: 16, Train_Loss: 0.30063360929489136, Test_Loss: 0.33850324153900146\n",
      "Epoch: 16, Train_Loss: 0.3008160889148712, Test_Loss: 0.3104954957962036 *\n",
      "Epoch: 16, Train_Loss: 0.3028697669506073, Test_Loss: 0.31663060188293457\n",
      "Epoch: 16, Train_Loss: 0.30666306614875793, Test_Loss: 0.3286828100681305\n",
      "Epoch: 16, Train_Loss: 0.3202081322669983, Test_Loss: 0.31069040298461914 *\n",
      "Epoch: 16, Train_Loss: 0.3068104684352875, Test_Loss: 0.3007049262523651 *\n",
      "Epoch: 16, Train_Loss: 0.3095550239086151, Test_Loss: 0.3021024167537689\n",
      "Epoch: 16, Train_Loss: 0.311207115650177, Test_Loss: 0.3136862814426422\n",
      "Epoch: 16, Train_Loss: 0.3004680573940277, Test_Loss: 0.30702975392341614 *\n",
      "Epoch: 16, Train_Loss: 0.30080509185791016, Test_Loss: 0.3032780587673187 *\n",
      "Epoch: 16, Train_Loss: 0.3012146055698395, Test_Loss: 0.30116671323776245 *\n",
      "Epoch: 16, Train_Loss: 0.33416062593460083, Test_Loss: 0.31051337718963623\n",
      "Epoch: 16, Train_Loss: 0.3338306248188019, Test_Loss: 0.315881609916687\n",
      "Epoch: 16, Train_Loss: 0.3099817931652069, Test_Loss: 0.3147276043891907 *\n",
      "Epoch: 16, Train_Loss: 0.3218386173248291, Test_Loss: 0.3016500174999237 *\n",
      "Epoch: 16, Train_Loss: 0.3650622069835663, Test_Loss: 0.30094319581985474 *\n",
      "Epoch: 16, Train_Loss: 0.3391464948654175, Test_Loss: 0.3036123812198639\n",
      "Epoch: 16, Train_Loss: 0.3050655722618103, Test_Loss: 0.30313241481781006 *\n",
      "Epoch: 16, Train_Loss: 0.32715728878974915, Test_Loss: 0.30369263887405396\n",
      "Epoch: 16, Train_Loss: 0.3130105137825012, Test_Loss: 0.3720727860927582\n",
      "Epoch: 16, Train_Loss: 0.3115089535713196, Test_Loss: 0.7853618860244751\n",
      "Epoch: 16, Train_Loss: 0.30407342314720154, Test_Loss: 5.3179030418396\n",
      "Epoch: 16, Train_Loss: 0.3286381959915161, Test_Loss: 0.30636298656463623 *\n",
      "Epoch: 16, Train_Loss: 0.3562532067298889, Test_Loss: 0.29962992668151855 *\n",
      "Epoch: 16, Train_Loss: 2.588989734649658, Test_Loss: 0.33685433864593506\n",
      "Epoch: 16, Train_Loss: 3.331695079803467, Test_Loss: 0.33125409483909607 *\n",
      "Epoch: 16, Train_Loss: 0.32360196113586426, Test_Loss: 0.34955617785453796\n",
      "Epoch: 16, Train_Loss: 0.30196431279182434, Test_Loss: 0.30551832914352417 *\n",
      "Epoch: 16, Train_Loss: 0.36829137802124023, Test_Loss: 0.4209724962711334\n",
      "Epoch: 16, Train_Loss: 0.4457441568374634, Test_Loss: 0.3300130069255829 *\n",
      "Epoch: 16, Train_Loss: 0.3239438533782959, Test_Loss: 0.30176883935928345 *\n",
      "Epoch: 16, Train_Loss: 0.3019537627696991, Test_Loss: 0.33195358514785767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train_Loss: 0.3160104751586914, Test_Loss: 0.3195166289806366 *\n",
      "Epoch: 16, Train_Loss: 0.3507244884967804, Test_Loss: 0.3036971092224121 *\n",
      "Epoch: 16, Train_Loss: 0.31074008345603943, Test_Loss: 0.3479217290878296\n",
      "Epoch: 16, Train_Loss: 0.30847716331481934, Test_Loss: 0.35221484303474426\n",
      "Epoch: 16, Train_Loss: 1.02921724319458, Test_Loss: 0.35270801186561584\n",
      "Epoch: 16, Train_Loss: 1.371244192123413, Test_Loss: 0.3571920096874237\n",
      "Epoch: 16, Train_Loss: 0.6650429964065552, Test_Loss: 0.3303035497665405 *\n",
      "Epoch: 16, Train_Loss: 0.3918699622154236, Test_Loss: 0.35195261240005493\n",
      "Epoch: 16, Train_Loss: 1.3447580337524414, Test_Loss: 0.3012791872024536 *\n",
      "Epoch: 16, Train_Loss: 1.9087165594100952, Test_Loss: 0.29998233914375305 *\n",
      "Epoch: 16, Train_Loss: 0.40353602170944214, Test_Loss: 0.3062170147895813\n",
      "Epoch: 16, Train_Loss: 0.30261775851249695, Test_Loss: 0.311117947101593\n",
      "Epoch: 16, Train_Loss: 0.40256109833717346, Test_Loss: 0.3063943088054657 *\n",
      "Epoch: 16, Train_Loss: 0.8770964741706848, Test_Loss: 0.30315232276916504 *\n",
      "Epoch: 16, Train_Loss: 1.0093001127243042, Test_Loss: 0.3056866526603699\n",
      "Epoch: 16, Train_Loss: 0.3267851173877716, Test_Loss: 0.30551809072494507 *\n",
      "Epoch: 16, Train_Loss: 0.31550994515419006, Test_Loss: 0.32623291015625\n",
      "Epoch: 16, Train_Loss: 0.3198126554489136, Test_Loss: 0.3807670474052429\n",
      "Epoch: 16, Train_Loss: 0.47680211067199707, Test_Loss: 0.3212970197200775 *\n",
      "Epoch: 16, Train_Loss: 0.35077330470085144, Test_Loss: 0.304524302482605 *\n",
      "Epoch: 16, Train_Loss: 0.41510942578315735, Test_Loss: 0.447889119386673\n",
      "Epoch: 16, Train_Loss: 0.3446175158023834, Test_Loss: 0.5168471336364746\n",
      "Epoch: 16, Train_Loss: 0.3112584054470062, Test_Loss: 0.3482939600944519 *\n",
      "Epoch: 16, Train_Loss: 0.38012364506721497, Test_Loss: 0.3200938403606415 *\n",
      "Epoch: 16, Train_Loss: 0.4487799108028412, Test_Loss: 0.37865185737609863\n",
      "Epoch: 16, Train_Loss: 0.3289506435394287, Test_Loss: 0.35838866233825684 *\n",
      "Epoch: 16, Train_Loss: 0.37583887577056885, Test_Loss: 0.3085763454437256 *\n",
      "Epoch: 16, Train_Loss: 0.3662688434123993, Test_Loss: 0.3223707675933838\n",
      "Epoch: 16, Train_Loss: 0.35400497913360596, Test_Loss: 0.40993252396583557\n",
      "Epoch: 16, Train_Loss: 0.3751319646835327, Test_Loss: 0.41774773597717285\n",
      "Epoch: 16, Train_Loss: 0.39297792315483093, Test_Loss: 0.3922754228115082 *\n",
      "Epoch: 16, Train_Loss: 0.35541197657585144, Test_Loss: 0.43180516362190247\n",
      "Epoch: 16, Train_Loss: 0.3419407904148102, Test_Loss: 0.3595316410064697 *\n",
      "Epoch: 16, Train_Loss: 0.3998975455760956, Test_Loss: 0.3252675235271454 *\n",
      "Epoch: 16, Train_Loss: 0.3343045115470886, Test_Loss: 0.34229591488838196\n",
      "Epoch: 16, Train_Loss: 0.32979005575180054, Test_Loss: 0.3819178342819214\n",
      "Epoch: 16, Train_Loss: 0.29995372891426086, Test_Loss: 0.3154199421405792 *\n",
      "Epoch: 16, Train_Loss: 0.2998180389404297, Test_Loss: 0.33342212438583374\n",
      "Epoch: 16, Train_Loss: 0.2995610535144806, Test_Loss: 0.3285834491252899 *\n",
      "Epoch: 16, Train_Loss: 0.3109646737575531, Test_Loss: 0.5004544258117676\n",
      "Epoch: 16, Train_Loss: 0.31949225068092346, Test_Loss: 0.6370503902435303\n",
      "Epoch: 16, Train_Loss: 0.31454890966415405, Test_Loss: 0.6247678995132446 *\n",
      "Epoch: 16, Train_Loss: 0.3146071434020996, Test_Loss: 0.6914242506027222\n",
      "Epoch: 16, Train_Loss: 0.5033910870552063, Test_Loss: 0.4099035859107971 *\n",
      "Epoch: 16, Train_Loss: 0.48053795099258423, Test_Loss: 0.4066808521747589 *\n",
      "Epoch: 16, Train_Loss: 0.30535373091697693, Test_Loss: 0.40541741251945496 *\n",
      "Epoch: 16, Train_Loss: 0.34162309765815735, Test_Loss: 0.44846850633621216\n",
      "Epoch: 16, Train_Loss: 0.36584675312042236, Test_Loss: 0.43893808126449585 *\n",
      "Epoch: 16, Train_Loss: 0.3802424669265747, Test_Loss: 5.39832878112793\n",
      "Epoch: 16, Train_Loss: 0.50767982006073, Test_Loss: 0.4879630208015442 *\n",
      "Epoch: 16, Train_Loss: 0.40711089968681335, Test_Loss: 0.3819674849510193 *\n",
      "Epoch: 16, Train_Loss: 0.47934097051620483, Test_Loss: 0.34370148181915283 *\n",
      "Epoch: 16, Train_Loss: 0.3875831365585327, Test_Loss: 0.3267936706542969 *\n",
      "Epoch: 16, Train_Loss: 0.37562495470046997, Test_Loss: 0.3173757791519165 *\n",
      "Epoch: 16, Train_Loss: 0.3245798349380493, Test_Loss: 0.4214198589324951\n",
      "Epoch: 16, Train_Loss: 0.31924399733543396, Test_Loss: 0.4139028489589691 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 16\n",
      "Epoch: 16, Train_Loss: 0.40539997816085815, Test_Loss: 0.3055513799190521 *\n",
      "Epoch: 16, Train_Loss: 0.7723852396011353, Test_Loss: 0.33798739314079285\n",
      "Epoch: 16, Train_Loss: 0.6774279475212097, Test_Loss: 0.3431277871131897\n",
      "Epoch: 16, Train_Loss: 0.3353021442890167, Test_Loss: 0.5303698182106018\n",
      "Epoch: 16, Train_Loss: 0.3441193997859955, Test_Loss: 0.37666743993759155 *\n",
      "Epoch: 16, Train_Loss: 0.3032928705215454, Test_Loss: 0.3342079818248749 *\n",
      "Epoch: 16, Train_Loss: 0.4180663824081421, Test_Loss: 0.34735792875289917\n",
      "Epoch: 16, Train_Loss: 0.6353614330291748, Test_Loss: 0.3421660363674164 *\n",
      "Epoch: 16, Train_Loss: 0.3025471866130829, Test_Loss: 0.32265645265579224 *\n",
      "Epoch: 16, Train_Loss: 0.421203076839447, Test_Loss: 0.45612484216690063\n",
      "Epoch: 16, Train_Loss: 0.34548717737197876, Test_Loss: 0.4563955068588257\n",
      "Epoch: 16, Train_Loss: 0.3529190123081207, Test_Loss: 0.32730045914649963 *\n",
      "Epoch: 16, Train_Loss: 0.3596377372741699, Test_Loss: 0.3560178279876709\n",
      "Epoch: 16, Train_Loss: 0.38575851917266846, Test_Loss: 0.31678229570388794 *\n",
      "Epoch: 16, Train_Loss: 0.3758600354194641, Test_Loss: 0.36058783531188965\n",
      "Epoch: 16, Train_Loss: 0.36169183254241943, Test_Loss: 0.37431126832962036\n",
      "Epoch: 16, Train_Loss: 0.3098776638507843, Test_Loss: 0.3639671206474304 *\n",
      "Epoch: 16, Train_Loss: 0.3699690103530884, Test_Loss: 0.30542662739753723 *\n",
      "Epoch: 16, Train_Loss: 0.3282419741153717, Test_Loss: 0.3303389847278595\n",
      "Epoch: 16, Train_Loss: 0.3279477059841156, Test_Loss: 0.346732497215271\n",
      "Epoch: 16, Train_Loss: 0.3136449158191681, Test_Loss: 0.3273085355758667 *\n",
      "Epoch: 16, Train_Loss: 0.32030993700027466, Test_Loss: 0.31803518533706665 *\n",
      "Epoch: 16, Train_Loss: 0.38401514291763306, Test_Loss: 0.4346049129962921\n",
      "Epoch: 16, Train_Loss: 0.5015755891799927, Test_Loss: 2.0335166454315186\n",
      "Epoch: 16, Train_Loss: 0.5217516422271729, Test_Loss: 3.749302864074707\n",
      "Epoch: 16, Train_Loss: 0.6608459949493408, Test_Loss: 0.31936705112457275 *\n",
      "Epoch: 16, Train_Loss: 0.5185294151306152, Test_Loss: 0.320939302444458\n",
      "Epoch: 16, Train_Loss: 0.49921858310699463, Test_Loss: 0.36042797565460205\n",
      "Epoch: 16, Train_Loss: 0.3833889365196228, Test_Loss: 0.3010272979736328 *\n",
      "Epoch: 16, Train_Loss: 0.3454006612300873, Test_Loss: 0.31760331988334656\n",
      "Epoch: 16, Train_Loss: 0.31363150477409363, Test_Loss: 0.350504994392395\n",
      "Epoch: 16, Train_Loss: 0.30896133184432983, Test_Loss: 0.37153422832489014\n",
      "Epoch: 16, Train_Loss: 0.3881806433200836, Test_Loss: 0.3084075152873993 *\n",
      "Epoch: 16, Train_Loss: 0.5962095856666565, Test_Loss: 0.320552259683609\n",
      "Epoch: 16, Train_Loss: 0.7148258090019226, Test_Loss: 0.33638980984687805\n",
      "Epoch: 16, Train_Loss: 1.0683643817901611, Test_Loss: 0.399933785200119\n",
      "Epoch: 16, Train_Loss: 1.3480244874954224, Test_Loss: 0.3326525390148163 *\n",
      "Epoch: 16, Train_Loss: 0.46064984798431396, Test_Loss: 0.3668729066848755\n",
      "Epoch: 16, Train_Loss: 0.5650739669799805, Test_Loss: 0.3290445804595947 *\n",
      "Epoch: 16, Train_Loss: 0.2983652949333191, Test_Loss: 0.3834230899810791\n",
      "Epoch: 16, Train_Loss: 0.3320850729942322, Test_Loss: 0.3029664158821106 *\n",
      "Epoch: 16, Train_Loss: 0.6456280946731567, Test_Loss: 0.35730621218681335\n",
      "Epoch: 16, Train_Loss: 0.9917734861373901, Test_Loss: 0.3974957764148712\n",
      "Epoch: 16, Train_Loss: 0.3474421203136444, Test_Loss: 0.31119340658187866 *\n",
      "Epoch: 16, Train_Loss: 0.3320096731185913, Test_Loss: 0.3315008878707886\n",
      "Epoch: 16, Train_Loss: 0.3414866328239441, Test_Loss: 0.33469486236572266\n",
      "Epoch: 16, Train_Loss: 0.4718652069568634, Test_Loss: 0.32541948556900024 *\n",
      "Epoch: 16, Train_Loss: 0.5397642850875854, Test_Loss: 0.32202205061912537 *\n",
      "Epoch: 16, Train_Loss: 0.5420805811882019, Test_Loss: 0.32832071185112\n",
      "Epoch: 16, Train_Loss: 0.44539761543273926, Test_Loss: 0.36219295859336853\n",
      "Epoch: 16, Train_Loss: 0.5818921327590942, Test_Loss: 0.41279661655426025\n",
      "Epoch: 16, Train_Loss: 0.3019731342792511, Test_Loss: 0.4414253830909729\n",
      "Epoch: 16, Train_Loss: 0.3299807608127594, Test_Loss: 0.48319900035858154\n",
      "Epoch: 16, Train_Loss: 0.32332003116607666, Test_Loss: 0.36427515745162964 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train_Loss: 0.3680092394351959, Test_Loss: 0.3123915195465088 *\n",
      "Epoch: 17, Train_Loss: 0.3043330907821655, Test_Loss: 0.38029733300209045\n",
      "Epoch: 17, Train_Loss: 0.33494865894317627, Test_Loss: 0.44079774618148804\n",
      "Epoch: 17, Train_Loss: 15.982284545898438, Test_Loss: 0.37969836592674255 *\n",
      "Epoch: 17, Train_Loss: 0.3867054879665375, Test_Loss: 0.3218551576137543 *\n",
      "Epoch: 17, Train_Loss: 1.6626341342926025, Test_Loss: 0.3553205132484436\n",
      "Epoch: 17, Train_Loss: 1.3318082094192505, Test_Loss: 0.3585561513900757\n",
      "Epoch: 17, Train_Loss: 0.3275151550769806, Test_Loss: 0.3059181869029999 *\n",
      "Epoch: 17, Train_Loss: 0.4349111318588257, Test_Loss: 0.36060601472854614\n",
      "Epoch: 17, Train_Loss: 2.0533230304718018, Test_Loss: 0.4836428761482239\n",
      "Epoch: 17, Train_Loss: 5.533475875854492, Test_Loss: 0.5661213397979736\n",
      "Epoch: 17, Train_Loss: 0.4073749780654907, Test_Loss: 0.49184244871139526 *\n",
      "Epoch: 17, Train_Loss: 0.3866952061653137, Test_Loss: 0.5375546216964722\n",
      "Epoch: 17, Train_Loss: 4.690953731536865, Test_Loss: 0.41850459575653076 *\n",
      "Epoch: 17, Train_Loss: 0.6708438396453857, Test_Loss: 0.382880300283432 *\n",
      "Epoch: 17, Train_Loss: 0.5560333728790283, Test_Loss: 0.6616458892822266\n",
      "Epoch: 17, Train_Loss: 0.3005809187889099, Test_Loss: 0.4065098762512207 *\n",
      "Epoch: 17, Train_Loss: 0.31766262650489807, Test_Loss: 0.3622497320175171 *\n",
      "Epoch: 17, Train_Loss: 0.321490615606308, Test_Loss: 0.3108215928077698 *\n",
      "Epoch: 17, Train_Loss: 0.2936813235282898, Test_Loss: 0.7202685475349426\n",
      "Epoch: 17, Train_Loss: 0.3031863570213318, Test_Loss: 1.092522144317627\n",
      "Epoch: 17, Train_Loss: 0.29206132888793945, Test_Loss: 0.906013011932373 *\n",
      "Epoch: 17, Train_Loss: 0.29190221428871155, Test_Loss: 0.5925755500793457 *\n",
      "Epoch: 17, Train_Loss: 0.29346853494644165, Test_Loss: 0.6058145761489868\n",
      "Epoch: 17, Train_Loss: 0.3078381419181824, Test_Loss: 1.2713954448699951\n",
      "Epoch: 17, Train_Loss: 0.4346467852592468, Test_Loss: 1.2508208751678467 *\n",
      "Epoch: 17, Train_Loss: 0.3789503276348114, Test_Loss: 1.1357879638671875 *\n",
      "Epoch: 17, Train_Loss: 0.3195425271987915, Test_Loss: 0.6272677183151245 *\n",
      "Epoch: 17, Train_Loss: 0.302707701921463, Test_Loss: 0.9840683937072754\n",
      "Epoch: 17, Train_Loss: 0.34015053510665894, Test_Loss: 7.14825963973999\n",
      "Epoch: 17, Train_Loss: 0.29452911019325256, Test_Loss: 0.5741003751754761 *\n",
      "Epoch: 17, Train_Loss: 0.2987651824951172, Test_Loss: 1.3935041427612305\n",
      "Epoch: 17, Train_Loss: 0.29479342699050903, Test_Loss: 1.1791616678237915 *\n",
      "Epoch: 17, Train_Loss: 0.29197630286216736, Test_Loss: 0.7928104400634766 *\n",
      "Epoch: 17, Train_Loss: 0.29185420274734497, Test_Loss: 0.812921404838562\n",
      "Epoch: 17, Train_Loss: 0.29175814986228943, Test_Loss: 1.964818000793457\n",
      "Epoch: 17, Train_Loss: 0.292981892824173, Test_Loss: 1.6614563465118408 *\n",
      "Epoch: 17, Train_Loss: 0.2921999990940094, Test_Loss: 0.7981463670730591 *\n",
      "Epoch: 17, Train_Loss: 0.2918204069137573, Test_Loss: 1.475212574005127\n",
      "Epoch: 17, Train_Loss: 0.2979790270328522, Test_Loss: 1.120566487312317 *\n",
      "Epoch: 17, Train_Loss: 0.32161104679107666, Test_Loss: 2.307342290878296\n",
      "Epoch: 17, Train_Loss: 0.3393319547176361, Test_Loss: 1.699746012687683 *\n",
      "Epoch: 17, Train_Loss: 0.38708919286727905, Test_Loss: 2.4004244804382324\n",
      "Epoch: 17, Train_Loss: 0.3976402282714844, Test_Loss: 2.061793565750122 *\n",
      "Epoch: 17, Train_Loss: 0.7937218546867371, Test_Loss: 0.5639958381652832 *\n",
      "Epoch: 17, Train_Loss: 5.99254846572876, Test_Loss: 0.6824079155921936\n",
      "Epoch: 17, Train_Loss: 0.3242799639701843, Test_Loss: 0.45229631662368774 *\n",
      "Epoch: 17, Train_Loss: 0.4194772243499756, Test_Loss: 1.785630464553833\n",
      "Epoch: 17, Train_Loss: 0.400112122297287, Test_Loss: 0.8665341138839722 *\n",
      "Epoch: 17, Train_Loss: 0.4891231656074524, Test_Loss: 1.200330376625061\n",
      "Epoch: 17, Train_Loss: 0.394794225692749, Test_Loss: 0.6889430284500122 *\n",
      "Epoch: 17, Train_Loss: 0.6215819120407104, Test_Loss: 1.3883634805679321\n",
      "Epoch: 17, Train_Loss: 0.48540130257606506, Test_Loss: 1.3363404273986816 *\n",
      "Epoch: 17, Train_Loss: 0.40838807821273804, Test_Loss: 0.6717865467071533 *\n",
      "Epoch: 17, Train_Loss: 0.3926278054714203, Test_Loss: 0.40657904744148254 *\n",
      "Epoch: 17, Train_Loss: 0.3506040573120117, Test_Loss: 0.6100167036056519\n",
      "Epoch: 17, Train_Loss: 0.29608920216560364, Test_Loss: 0.5276262164115906 *\n",
      "Epoch: 17, Train_Loss: 0.44134488701820374, Test_Loss: 0.4296897053718567 *\n",
      "Epoch: 17, Train_Loss: 0.35069864988327026, Test_Loss: 0.4118178188800812 *\n",
      "Epoch: 17, Train_Loss: 0.5343766212463379, Test_Loss: 0.4233683943748474\n",
      "Epoch: 17, Train_Loss: 0.3575512170791626, Test_Loss: 3.1207048892974854\n",
      "Epoch: 17, Train_Loss: 0.3207128643989563, Test_Loss: 2.945704936981201 *\n",
      "Epoch: 17, Train_Loss: 0.29899266362190247, Test_Loss: 0.29304563999176025 *\n",
      "Epoch: 17, Train_Loss: 0.33269333839416504, Test_Loss: 0.2911493182182312 *\n",
      "Epoch: 17, Train_Loss: 0.35242199897766113, Test_Loss: 0.32147204875946045\n",
      "Epoch: 17, Train_Loss: 0.31632131338119507, Test_Loss: 0.2993347942829132 *\n",
      "Epoch: 17, Train_Loss: 0.29591354727745056, Test_Loss: 0.3409253656864166\n",
      "Epoch: 17, Train_Loss: 0.29205650091171265, Test_Loss: 0.3378782868385315 *\n",
      "Epoch: 17, Train_Loss: 0.31828662753105164, Test_Loss: 0.3776171803474426\n",
      "Epoch: 17, Train_Loss: 1.3030879497528076, Test_Loss: 0.29389652609825134 *\n",
      "Epoch: 17, Train_Loss: 5.134170055389404, Test_Loss: 0.31275486946105957\n",
      "Epoch: 17, Train_Loss: 0.29344797134399414, Test_Loss: 0.31034091114997864 *\n",
      "Epoch: 17, Train_Loss: 0.29090210795402527, Test_Loss: 0.3085591197013855 *\n",
      "Epoch: 17, Train_Loss: 0.2952921986579895, Test_Loss: 0.2975975275039673 *\n",
      "Epoch: 17, Train_Loss: 0.2931559383869171, Test_Loss: 0.32931047677993774\n",
      "Epoch: 17, Train_Loss: 0.29088500142097473, Test_Loss: 0.3356657326221466\n",
      "Epoch: 17, Train_Loss: 0.28999558091163635, Test_Loss: 0.37571388483047485\n",
      "Epoch: 17, Train_Loss: 0.3003349304199219, Test_Loss: 0.39597591757774353\n",
      "Epoch: 17, Train_Loss: 0.3154069781303406, Test_Loss: 0.3097035884857178 *\n",
      "Epoch: 17, Train_Loss: 0.31566664576530457, Test_Loss: 0.30010056495666504 *\n",
      "Epoch: 17, Train_Loss: 0.30041229724884033, Test_Loss: 0.29232802987098694 *\n",
      "Epoch: 17, Train_Loss: 0.29044538736343384, Test_Loss: 0.29383477568626404\n",
      "Epoch: 17, Train_Loss: 0.28970733284950256, Test_Loss: 0.2927667796611786 *\n",
      "Epoch: 17, Train_Loss: 0.306363970041275, Test_Loss: 0.29460155963897705\n",
      "Epoch: 17, Train_Loss: 0.2929924428462982, Test_Loss: 0.2936844229698181 *\n",
      "Epoch: 17, Train_Loss: 0.2933865487575531, Test_Loss: 0.29276415705680847 *\n",
      "Epoch: 17, Train_Loss: 0.31937921047210693, Test_Loss: 0.2969394624233246\n",
      "Epoch: 17, Train_Loss: 0.32192379236221313, Test_Loss: 0.29037949442863464 *\n",
      "Epoch: 17, Train_Loss: 0.2989872992038727, Test_Loss: 0.29306671023368835\n",
      "Epoch: 17, Train_Loss: 0.28904032707214355, Test_Loss: 0.30373847484588623\n",
      "Epoch: 17, Train_Loss: 0.30236586928367615, Test_Loss: 0.29644066095352173 *\n",
      "Epoch: 17, Train_Loss: 0.34294331073760986, Test_Loss: 0.31923580169677734\n",
      "Epoch: 17, Train_Loss: 0.31396931409835815, Test_Loss: 0.3386251628398895\n",
      "Epoch: 17, Train_Loss: 0.3576889634132385, Test_Loss: 0.5837737917900085\n",
      "Epoch: 17, Train_Loss: 0.3113259971141815, Test_Loss: 0.5533539056777954 *\n",
      "Epoch: 17, Train_Loss: 0.41261377930641174, Test_Loss: 0.36409637331962585 *\n",
      "Epoch: 17, Train_Loss: 0.3136403262615204, Test_Loss: 0.2948915362358093 *\n",
      "Epoch: 17, Train_Loss: 0.3423372805118561, Test_Loss: 0.31228160858154297\n",
      "Epoch: 17, Train_Loss: 0.3176965117454529, Test_Loss: 0.3255159556865692\n",
      "Epoch: 17, Train_Loss: 0.4598284363746643, Test_Loss: 0.5059006810188293\n",
      "Model saved at location save_new\\model.ckpt at epoch 17\n",
      "Epoch: 17, Train_Loss: 0.3775283694267273, Test_Loss: 0.8306718468666077\n",
      "Epoch: 17, Train_Loss: 0.31122151017189026, Test_Loss: 0.6976684331893921 *\n",
      "Epoch: 17, Train_Loss: 0.29736894369125366, Test_Loss: 0.3602023124694824 *\n",
      "Epoch: 17, Train_Loss: 0.29390689730644226, Test_Loss: 0.32849809527397156 *\n",
      "Epoch: 17, Train_Loss: 0.2921161651611328, Test_Loss: 0.31382501125335693 *\n",
      "Epoch: 17, Train_Loss: 0.28956642746925354, Test_Loss: 0.30020466446876526 *\n",
      "Epoch: 17, Train_Loss: 1.3054625988006592, Test_Loss: 0.3055395781993866\n",
      "Epoch: 17, Train_Loss: 4.064418315887451, Test_Loss: 0.3030393719673157 *\n",
      "Epoch: 17, Train_Loss: 0.2923722267150879, Test_Loss: 0.3307918310165405\n",
      "Epoch: 17, Train_Loss: 0.2962492108345032, Test_Loss: 0.2913229167461395 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train_Loss: 0.29579633474349976, Test_Loss: 0.34699344635009766\n",
      "Epoch: 17, Train_Loss: 0.2896665334701538, Test_Loss: 0.4093182384967804\n",
      "Epoch: 17, Train_Loss: 0.29023605585098267, Test_Loss: 0.656376838684082\n",
      "Epoch: 17, Train_Loss: 0.2889043092727661, Test_Loss: 0.5599842667579651 *\n",
      "Epoch: 17, Train_Loss: 0.28773531317710876, Test_Loss: 0.3296028971672058 *\n",
      "Epoch: 17, Train_Loss: 0.28867846727371216, Test_Loss: 0.3170662820339203 *\n",
      "Epoch: 17, Train_Loss: 0.2905676066875458, Test_Loss: 0.31725865602493286\n",
      "Epoch: 17, Train_Loss: 0.3393464684486389, Test_Loss: 0.3187516927719116\n",
      "Epoch: 17, Train_Loss: 0.31465810537338257, Test_Loss: 0.32630929350852966\n",
      "Epoch: 17, Train_Loss: 0.3455950915813446, Test_Loss: 1.7271748781204224\n",
      "Epoch: 17, Train_Loss: 0.3179650604724884, Test_Loss: 4.099636077880859\n",
      "Epoch: 17, Train_Loss: 0.2959538698196411, Test_Loss: 0.312700092792511 *\n",
      "Epoch: 17, Train_Loss: 0.48997926712036133, Test_Loss: 0.298167884349823 *\n",
      "Epoch: 17, Train_Loss: 0.5388327240943909, Test_Loss: 0.2966974973678589 *\n",
      "Epoch: 17, Train_Loss: 0.5218591690063477, Test_Loss: 0.30312395095825195\n",
      "Epoch: 17, Train_Loss: 0.4172482490539551, Test_Loss: 0.2936176061630249 *\n",
      "Epoch: 17, Train_Loss: 0.2885705530643463, Test_Loss: 0.2945510745048523\n",
      "Epoch: 17, Train_Loss: 0.28718844056129456, Test_Loss: 0.29027774930000305 *\n",
      "Epoch: 17, Train_Loss: 0.2905197739601135, Test_Loss: 0.28821319341659546 *\n",
      "Epoch: 17, Train_Loss: 0.30350908637046814, Test_Loss: 0.28884458541870117\n",
      "Epoch: 17, Train_Loss: 0.30815887451171875, Test_Loss: 0.28783273696899414 *\n",
      "Epoch: 17, Train_Loss: 0.30561503767967224, Test_Loss: 0.30154934525489807\n",
      "Epoch: 17, Train_Loss: 0.2904697358608246, Test_Loss: 0.301563024520874\n",
      "Epoch: 17, Train_Loss: 0.2868008017539978, Test_Loss: 0.2919834554195404 *\n",
      "Epoch: 17, Train_Loss: 0.30221807956695557, Test_Loss: 0.3167615830898285\n",
      "Epoch: 17, Train_Loss: 0.33702901005744934, Test_Loss: 0.28824326395988464 *\n",
      "Epoch: 17, Train_Loss: 0.47541654109954834, Test_Loss: 0.29094555974006653\n",
      "Epoch: 17, Train_Loss: 0.4071948230266571, Test_Loss: 0.29010501503944397 *\n",
      "Epoch: 17, Train_Loss: 0.4178481996059418, Test_Loss: 0.31213635206222534\n",
      "Epoch: 17, Train_Loss: 0.3452710807323456, Test_Loss: 0.29309359192848206 *\n",
      "Epoch: 17, Train_Loss: 0.39424240589141846, Test_Loss: 0.29157665371894836 *\n",
      "Epoch: 17, Train_Loss: 0.3591228425502777, Test_Loss: 0.30062335729599\n",
      "Epoch: 17, Train_Loss: 0.3928282558917999, Test_Loss: 0.32838165760040283\n",
      "Epoch: 17, Train_Loss: 0.3961265981197357, Test_Loss: 0.32676592469215393 *\n",
      "Epoch: 17, Train_Loss: 0.5658172965049744, Test_Loss: 0.29986873269081116 *\n",
      "Epoch: 17, Train_Loss: 0.2987196743488312, Test_Loss: 0.2937408685684204 *\n",
      "Epoch: 17, Train_Loss: 0.29127824306488037, Test_Loss: 0.31243085861206055\n",
      "Epoch: 17, Train_Loss: 3.115065574645996, Test_Loss: 0.3023434579372406 *\n",
      "Epoch: 17, Train_Loss: 0.5840563774108887, Test_Loss: 0.2946174442768097 *\n",
      "Epoch: 17, Train_Loss: 0.33932679891586304, Test_Loss: 0.3588721454143524\n",
      "Epoch: 17, Train_Loss: 0.3334220051765442, Test_Loss: 0.32478415966033936 *\n",
      "Epoch: 17, Train_Loss: 0.31168168783187866, Test_Loss: 4.046929359436035\n",
      "Epoch: 17, Train_Loss: 0.30343350768089294, Test_Loss: 1.9310150146484375 *\n",
      "Epoch: 17, Train_Loss: 0.2959305942058563, Test_Loss: 0.28884774446487427 *\n",
      "Epoch: 17, Train_Loss: 0.35788214206695557, Test_Loss: 0.2953838109970093\n",
      "Epoch: 17, Train_Loss: 0.43658071756362915, Test_Loss: 0.33405372500419617\n",
      "Epoch: 17, Train_Loss: 0.3669162690639496, Test_Loss: 0.3255104720592499 *\n",
      "Epoch: 17, Train_Loss: 0.3325481712818146, Test_Loss: 0.30953454971313477 *\n",
      "Epoch: 17, Train_Loss: 0.3013134002685547, Test_Loss: 0.34851810336112976\n",
      "Epoch: 17, Train_Loss: 0.3020002543926239, Test_Loss: 0.35851696133613586\n",
      "Epoch: 17, Train_Loss: 0.29807671904563904, Test_Loss: 0.29018542170524597 *\n",
      "Epoch: 17, Train_Loss: 0.29747360944747925, Test_Loss: 0.3100931644439697\n",
      "Epoch: 17, Train_Loss: 0.37860023975372314, Test_Loss: 0.3111772835254669\n",
      "Epoch: 17, Train_Loss: 0.32230907678604126, Test_Loss: 0.30536505579948425 *\n",
      "Epoch: 17, Train_Loss: 0.2968599498271942, Test_Loss: 0.2907262444496155 *\n",
      "Epoch: 17, Train_Loss: 0.29453760385513306, Test_Loss: 0.36069798469543457\n",
      "Epoch: 17, Train_Loss: 0.3104512393474579, Test_Loss: 0.33612093329429626 *\n",
      "Epoch: 17, Train_Loss: 0.3098197877407074, Test_Loss: 0.3609571158885956\n",
      "Epoch: 17, Train_Loss: 0.2917373478412628, Test_Loss: 0.32745563983917236 *\n",
      "Epoch: 17, Train_Loss: 0.28689098358154297, Test_Loss: 0.3286074995994568\n",
      "Epoch: 17, Train_Loss: 0.2864908277988434, Test_Loss: 0.30351129174232483 *\n",
      "Epoch: 17, Train_Loss: 0.2863464057445526, Test_Loss: 0.3006599247455597 *\n",
      "Epoch: 17, Train_Loss: 0.2992458641529083, Test_Loss: 0.302056223154068\n",
      "Epoch: 17, Train_Loss: 0.28731051087379456, Test_Loss: 0.30280011892318726\n",
      "Epoch: 17, Train_Loss: 0.29243865609169006, Test_Loss: 0.3058391809463501\n",
      "Epoch: 17, Train_Loss: 0.2884846329689026, Test_Loss: 0.302984356880188 *\n",
      "Epoch: 17, Train_Loss: 0.2867186367511749, Test_Loss: 0.2948788106441498 *\n",
      "Epoch: 17, Train_Loss: 0.28561487793922424, Test_Loss: 0.3006765842437744\n",
      "Epoch: 17, Train_Loss: 0.2888679802417755, Test_Loss: 0.2984660565853119 *\n",
      "Epoch: 17, Train_Loss: 0.30048227310180664, Test_Loss: 0.29369306564331055 *\n",
      "Epoch: 17, Train_Loss: 0.30485472083091736, Test_Loss: 0.2935889959335327 *\n",
      "Epoch: 17, Train_Loss: 0.3001413643360138, Test_Loss: 0.32309645414352417\n",
      "Epoch: 17, Train_Loss: 0.30199310183525085, Test_Loss: 0.34217002987861633\n",
      "Epoch: 17, Train_Loss: 0.30287283658981323, Test_Loss: 0.40914738178253174\n",
      "Epoch: 17, Train_Loss: 0.29093751311302185, Test_Loss: 0.7529761791229248\n",
      "Epoch: 17, Train_Loss: 0.28672999143600464, Test_Loss: 0.5878064632415771 *\n",
      "Epoch: 17, Train_Loss: 0.3023757338523865, Test_Loss: 0.3790549039840698 *\n",
      "Epoch: 17, Train_Loss: 0.30958595871925354, Test_Loss: 0.29509463906288147 *\n",
      "Epoch: 17, Train_Loss: 0.2853420674800873, Test_Loss: 0.3008885681629181\n",
      "Epoch: 17, Train_Loss: 0.2877574563026428, Test_Loss: 0.33764344453811646\n",
      "Epoch: 17, Train_Loss: 0.2871958017349243, Test_Loss: 0.6125774383544922\n",
      "Epoch: 17, Train_Loss: 0.31516391038894653, Test_Loss: 0.7985593676567078\n",
      "Epoch: 17, Train_Loss: 0.33009856939315796, Test_Loss: 0.6401196718215942 *\n",
      "Epoch: 17, Train_Loss: 0.3405945897102356, Test_Loss: 0.3789435625076294 *\n",
      "Epoch: 17, Train_Loss: 0.30127447843551636, Test_Loss: 0.2985573410987854 *\n",
      "Epoch: 17, Train_Loss: 0.28690916299819946, Test_Loss: 0.29096201062202454 *\n",
      "Epoch: 17, Train_Loss: 0.35947585105895996, Test_Loss: 0.2851508855819702 *\n",
      "Epoch: 17, Train_Loss: 0.2898957133293152, Test_Loss: 0.2962636351585388\n",
      "Epoch: 17, Train_Loss: 0.29275280237197876, Test_Loss: 0.3054048717021942\n",
      "Epoch: 17, Train_Loss: 0.3060988783836365, Test_Loss: 0.323514848947525\n",
      "Model saved at location save_new\\model.ckpt at epoch 17\n",
      "Epoch: 17, Train_Loss: 0.2973063886165619, Test_Loss: 0.28589460253715515 *\n",
      "Epoch: 17, Train_Loss: 0.40312305092811584, Test_Loss: 0.3753361105918884\n",
      "Epoch: 17, Train_Loss: 0.3372621536254883, Test_Loss: 0.5240280032157898\n",
      "Epoch: 17, Train_Loss: 0.31363213062286377, Test_Loss: 0.5307343006134033\n",
      "Epoch: 17, Train_Loss: 0.2925128936767578, Test_Loss: 0.5154985189437866 *\n",
      "Epoch: 17, Train_Loss: 0.3059372901916504, Test_Loss: 0.3030044734477997 *\n",
      "Epoch: 17, Train_Loss: 0.288512259721756, Test_Loss: 0.301391065120697 *\n",
      "Epoch: 17, Train_Loss: 0.28746622800827026, Test_Loss: 0.3011675179004669 *\n",
      "Epoch: 17, Train_Loss: 0.2957700490951538, Test_Loss: 0.3005402088165283 *\n",
      "Epoch: 17, Train_Loss: 0.30103030800819397, Test_Loss: 0.3113081753253937\n",
      "Epoch: 17, Train_Loss: 0.3274953365325928, Test_Loss: 3.13252854347229\n",
      "Epoch: 17, Train_Loss: 0.3686344623565674, Test_Loss: 2.828085422515869 *\n",
      "Epoch: 17, Train_Loss: 0.3053519129753113, Test_Loss: 0.30311694741249084 *\n",
      "Epoch: 17, Train_Loss: 0.33144503831863403, Test_Loss: 0.29040834307670593 *\n",
      "Epoch: 17, Train_Loss: 0.3077005445957184, Test_Loss: 0.29158642888069153\n",
      "Epoch: 17, Train_Loss: 0.306580126285553, Test_Loss: 0.29708045721054077\n",
      "Epoch: 17, Train_Loss: 0.381477028131485, Test_Loss: 0.28606683015823364 *\n",
      "Epoch: 17, Train_Loss: 0.5263733267784119, Test_Loss: 0.2927554249763489\n",
      "Epoch: 17, Train_Loss: 0.2915382385253906, Test_Loss: 0.28458842635154724 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train_Loss: 0.32475048303604126, Test_Loss: 0.2838130593299866 *\n",
      "Epoch: 17, Train_Loss: 0.2826900780200958, Test_Loss: 0.28547871112823486\n",
      "Epoch: 17, Train_Loss: 0.2829172909259796, Test_Loss: 0.28385812044143677 *\n",
      "Epoch: 17, Train_Loss: 0.2832987606525421, Test_Loss: 0.2994874119758606\n",
      "Epoch: 17, Train_Loss: 0.2833682596683502, Test_Loss: 0.2977102994918823 *\n",
      "Epoch: 17, Train_Loss: 0.29405051469802856, Test_Loss: 0.2906275987625122 *\n",
      "Epoch: 17, Train_Loss: 0.2993926405906677, Test_Loss: 0.30006471276283264\n",
      "Epoch: 17, Train_Loss: 0.2902791500091553, Test_Loss: 0.28272247314453125 *\n",
      "Epoch: 17, Train_Loss: 0.28800836205482483, Test_Loss: 0.28579777479171753\n",
      "Epoch: 17, Train_Loss: 0.2935428321361542, Test_Loss: 0.29079923033714294\n",
      "Epoch: 17, Train_Loss: 0.28420567512512207, Test_Loss: 0.29698872566223145\n",
      "Epoch: 17, Train_Loss: 0.2835400402545929, Test_Loss: 0.2860482633113861 *\n",
      "Epoch: 17, Train_Loss: 0.28173500299453735, Test_Loss: 0.28413936495780945 *\n",
      "Epoch: 17, Train_Loss: 0.3129870593547821, Test_Loss: 0.30016961693763733\n",
      "Epoch: 17, Train_Loss: 0.315848708152771, Test_Loss: 0.3108168840408325\n",
      "Epoch: 17, Train_Loss: 0.30677530169487, Test_Loss: 0.30671507120132446 *\n",
      "Epoch: 17, Train_Loss: 0.290731281042099, Test_Loss: 0.28716740012168884 *\n",
      "Epoch: 17, Train_Loss: 0.34553200006484985, Test_Loss: 0.2862100303173065 *\n",
      "Epoch: 17, Train_Loss: 0.3211480379104614, Test_Loss: 0.2999652028083801\n",
      "Epoch: 17, Train_Loss: 0.3031870424747467, Test_Loss: 0.291853129863739 *\n",
      "Epoch: 17, Train_Loss: 0.29737961292266846, Test_Loss: 0.28528738021850586 *\n",
      "Epoch: 17, Train_Loss: 0.3124789297580719, Test_Loss: 0.3497018814086914\n",
      "Epoch: 17, Train_Loss: 0.2867898941040039, Test_Loss: 0.3144439160823822 *\n",
      "Epoch: 17, Train_Loss: 0.2946782112121582, Test_Loss: 5.2287983894348145\n",
      "Epoch: 17, Train_Loss: 0.3096572160720825, Test_Loss: 0.7811291217803955 *\n",
      "Epoch: 17, Train_Loss: 0.32838985323905945, Test_Loss: 0.28246867656707764 *\n",
      "Epoch: 17, Train_Loss: 2.3344690799713135, Test_Loss: 0.3039141893386841\n",
      "Epoch: 17, Train_Loss: 3.6419410705566406, Test_Loss: 0.3265380263328552\n",
      "Epoch: 17, Train_Loss: 0.2884712815284729, Test_Loss: 0.3311265707015991\n",
      "Epoch: 17, Train_Loss: 0.29161491990089417, Test_Loss: 0.2898995280265808 *\n",
      "Epoch: 17, Train_Loss: 0.3138430416584015, Test_Loss: 0.3685663938522339\n",
      "Epoch: 17, Train_Loss: 0.4160291850566864, Test_Loss: 0.3492332398891449 *\n",
      "Epoch: 17, Train_Loss: 0.3039323389530182, Test_Loss: 0.2828725278377533 *\n",
      "Epoch: 17, Train_Loss: 0.28640854358673096, Test_Loss: 0.31830230355262756\n",
      "Epoch: 17, Train_Loss: 0.2829134464263916, Test_Loss: 0.2973863184452057 *\n",
      "Epoch: 17, Train_Loss: 0.33906662464141846, Test_Loss: 0.2909838557243347 *\n",
      "Epoch: 17, Train_Loss: 0.294849693775177, Test_Loss: 0.28936368227005005 *\n",
      "Epoch: 17, Train_Loss: 0.29645270109176636, Test_Loss: 0.3494555652141571\n",
      "Epoch: 17, Train_Loss: 0.8470947742462158, Test_Loss: 0.32042959332466125 *\n",
      "Epoch: 17, Train_Loss: 1.367387294769287, Test_Loss: 0.36806249618530273\n",
      "Epoch: 17, Train_Loss: 0.9484862089157104, Test_Loss: 0.3186025619506836 *\n",
      "Epoch: 17, Train_Loss: 0.34949034452438354, Test_Loss: 0.3327981233596802\n",
      "Epoch: 17, Train_Loss: 0.7788523435592651, Test_Loss: 0.2934981882572174 *\n",
      "Epoch: 17, Train_Loss: 2.130302667617798, Test_Loss: 0.28585293889045715 *\n",
      "Epoch: 17, Train_Loss: 0.7204100489616394, Test_Loss: 0.29210153222084045\n",
      "Epoch: 17, Train_Loss: 0.3085097372531891, Test_Loss: 0.28888171911239624 *\n",
      "Epoch: 17, Train_Loss: 0.31166067719459534, Test_Loss: 0.2927064895629883\n",
      "Epoch: 17, Train_Loss: 0.9206575155258179, Test_Loss: 0.28856325149536133 *\n",
      "Epoch: 17, Train_Loss: 1.065320611000061, Test_Loss: 0.28954529762268066\n",
      "Epoch: 17, Train_Loss: 0.535629153251648, Test_Loss: 0.285554975271225 *\n",
      "Epoch: 17, Train_Loss: 0.28319084644317627, Test_Loss: 0.31011131405830383\n",
      "Epoch: 17, Train_Loss: 0.2946268320083618, Test_Loss: 0.3435507118701935\n",
      "Epoch: 17, Train_Loss: 0.621889054775238, Test_Loss: 0.34623825550079346\n",
      "Epoch: 17, Train_Loss: 0.3824945390224457, Test_Loss: 0.29859864711761475 *\n",
      "Epoch: 17, Train_Loss: 0.3544202744960785, Test_Loss: 0.33888864517211914\n",
      "Epoch: 17, Train_Loss: 0.3501746654510498, Test_Loss: 0.4779854118824005\n",
      "Epoch: 17, Train_Loss: 0.29819369316101074, Test_Loss: 0.4443671405315399 *\n",
      "Epoch: 17, Train_Loss: 0.3059453070163727, Test_Loss: 0.3439035713672638 *\n",
      "Epoch: 17, Train_Loss: 0.4152463674545288, Test_Loss: 0.32513824105262756 *\n",
      "Epoch: 17, Train_Loss: 0.33881089091300964, Test_Loss: 0.3379032015800476\n",
      "Epoch: 17, Train_Loss: 0.29789412021636963, Test_Loss: 0.34373095631599426\n",
      "Epoch: 17, Train_Loss: 0.39043280482292175, Test_Loss: 0.3532211184501648\n",
      "Epoch: 17, Train_Loss: 0.32734599709510803, Test_Loss: 0.481353223323822\n",
      "Epoch: 17, Train_Loss: 0.332902193069458, Test_Loss: 0.35741758346557617 *\n",
      "Epoch: 17, Train_Loss: 0.3614085912704468, Test_Loss: 0.587844729423523\n",
      "Epoch: 17, Train_Loss: 0.4294750392436981, Test_Loss: 0.34142133593559265 *\n",
      "Epoch: 17, Train_Loss: 0.33441027998924255, Test_Loss: 0.33991727232933044 *\n",
      "Epoch: 17, Train_Loss: 0.3631961941719055, Test_Loss: 0.3000394105911255 *\n",
      "Epoch: 17, Train_Loss: 0.318259596824646, Test_Loss: 0.2918453812599182 *\n",
      "Epoch: 17, Train_Loss: 0.3202894330024719, Test_Loss: 0.38857340812683105\n",
      "Epoch: 17, Train_Loss: 0.28229549527168274, Test_Loss: 0.30256351828575134 *\n",
      "Epoch: 17, Train_Loss: 0.28400102257728577, Test_Loss: 0.32618245482444763\n",
      "Epoch: 17, Train_Loss: 0.2807694971561432, Test_Loss: 0.32667872309684753\n",
      "Epoch: 17, Train_Loss: 0.2896123230457306, Test_Loss: 0.6730338931083679\n",
      "Epoch: 17, Train_Loss: 0.3011362850666046, Test_Loss: 0.6589462757110596 *\n",
      "Epoch: 17, Train_Loss: 0.2974596917629242, Test_Loss: 0.5205342769622803 *\n",
      "Epoch: 17, Train_Loss: 0.28728172183036804, Test_Loss: 0.9170396327972412\n",
      "Epoch: 17, Train_Loss: 0.4009406268596649, Test_Loss: 0.4237601161003113 *\n",
      "Epoch: 17, Train_Loss: 0.5590856075286865, Test_Loss: 0.3911935091018677 *\n",
      "Epoch: 17, Train_Loss: 0.2963417172431946, Test_Loss: 0.39512887597084045\n",
      "Epoch: 17, Train_Loss: 0.3339359760284424, Test_Loss: 0.3928968906402588 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 17\n",
      "Epoch: 17, Train_Loss: 0.31630560755729675, Test_Loss: 0.5055695176124573\n",
      "Epoch: 17, Train_Loss: 0.35761743783950806, Test_Loss: 4.219333648681641\n",
      "Epoch: 17, Train_Loss: 0.46477049589157104, Test_Loss: 1.2536473274230957 *\n",
      "Epoch: 17, Train_Loss: 0.34577715396881104, Test_Loss: 0.348795622587204 *\n",
      "Epoch: 17, Train_Loss: 0.4099650979042053, Test_Loss: 0.3470518887042999 *\n",
      "Epoch: 17, Train_Loss: 0.4401085376739502, Test_Loss: 0.3164784014225006 *\n",
      "Epoch: 17, Train_Loss: 0.3912699818611145, Test_Loss: 0.2944439947605133 *\n",
      "Epoch: 17, Train_Loss: 0.33896493911743164, Test_Loss: 0.327653706073761\n",
      "Epoch: 17, Train_Loss: 0.29687410593032837, Test_Loss: 0.3644096851348877\n",
      "Epoch: 17, Train_Loss: 0.366361528635025, Test_Loss: 0.30321839451789856 *\n",
      "Epoch: 17, Train_Loss: 0.6408326625823975, Test_Loss: 0.3058724105358124\n",
      "Epoch: 17, Train_Loss: 0.660341739654541, Test_Loss: 0.35616302490234375\n",
      "Epoch: 17, Train_Loss: 0.3303196430206299, Test_Loss: 0.4024132192134857\n",
      "Epoch: 17, Train_Loss: 0.3125292658805847, Test_Loss: 0.4498021900653839\n",
      "Epoch: 17, Train_Loss: 0.29992976784706116, Test_Loss: 0.3708592355251312 *\n",
      "Epoch: 17, Train_Loss: 0.2980523407459259, Test_Loss: 0.4343217611312866\n",
      "Epoch: 17, Train_Loss: 0.608323335647583, Test_Loss: 0.36335110664367676 *\n",
      "Epoch: 17, Train_Loss: 0.2969907522201538, Test_Loss: 0.3116709589958191 *\n",
      "Epoch: 17, Train_Loss: 0.35918521881103516, Test_Loss: 0.38375839591026306\n",
      "Epoch: 17, Train_Loss: 0.3533383905887604, Test_Loss: 0.38299429416656494 *\n",
      "Epoch: 17, Train_Loss: 0.3457163870334625, Test_Loss: 0.362711101770401 *\n",
      "Epoch: 17, Train_Loss: 0.3061218559741974, Test_Loss: 0.30382269620895386 *\n",
      "Epoch: 17, Train_Loss: 0.3229706287384033, Test_Loss: 0.33727627992630005\n",
      "Epoch: 17, Train_Loss: 0.35262584686279297, Test_Loss: 0.3498673141002655\n",
      "Epoch: 17, Train_Loss: 0.31076380610466003, Test_Loss: 0.37540942430496216\n",
      "Epoch: 17, Train_Loss: 0.30990302562713623, Test_Loss: 0.38201630115509033\n",
      "Epoch: 17, Train_Loss: 0.3044719696044922, Test_Loss: 0.3035491108894348 *\n",
      "Epoch: 17, Train_Loss: 0.3932152986526489, Test_Loss: 0.3085338771343231\n",
      "Epoch: 17, Train_Loss: 0.325125515460968, Test_Loss: 0.3523441553115845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train_Loss: 0.31177204847335815, Test_Loss: 0.3135257661342621 *\n",
      "Epoch: 17, Train_Loss: 0.28774189949035645, Test_Loss: 0.2952805161476135 *\n",
      "Epoch: 17, Train_Loss: 0.3749253749847412, Test_Loss: 0.4284258484840393\n",
      "Epoch: 17, Train_Loss: 0.5065031051635742, Test_Loss: 0.4179830253124237 *\n",
      "Epoch: 17, Train_Loss: 0.5620836019515991, Test_Loss: 6.052574157714844\n",
      "Epoch: 17, Train_Loss: 0.6941046118736267, Test_Loss: 0.3104667067527771 *\n",
      "Epoch: 17, Train_Loss: 0.5830956697463989, Test_Loss: 0.3320445716381073\n",
      "Epoch: 17, Train_Loss: 0.4696517586708069, Test_Loss: 0.35589247941970825\n",
      "Epoch: 17, Train_Loss: 0.4193468689918518, Test_Loss: 0.2870837450027466 *\n",
      "Epoch: 17, Train_Loss: 0.3410091996192932, Test_Loss: 0.29376840591430664\n",
      "Epoch: 17, Train_Loss: 0.28810417652130127, Test_Loss: 0.29900607466697693\n",
      "Epoch: 17, Train_Loss: 0.2914668917655945, Test_Loss: 0.36344724893569946\n",
      "Epoch: 17, Train_Loss: 0.3192051947116852, Test_Loss: 0.3124510943889618 *\n",
      "Epoch: 17, Train_Loss: 0.5439401865005493, Test_Loss: 0.28928056359291077 *\n",
      "Epoch: 17, Train_Loss: 0.6534737348556519, Test_Loss: 0.2980991005897522\n",
      "Epoch: 17, Train_Loss: 0.787757158279419, Test_Loss: 0.37710121273994446\n",
      "Epoch: 17, Train_Loss: 1.5331145524978638, Test_Loss: 0.3231266140937805 *\n",
      "Epoch: 17, Train_Loss: 0.5310928821563721, Test_Loss: 0.34247612953186035\n",
      "Epoch: 17, Train_Loss: 0.5214056372642517, Test_Loss: 0.30231478810310364 *\n",
      "Epoch: 17, Train_Loss: 0.28927096724510193, Test_Loss: 0.3961060643196106\n",
      "Epoch: 17, Train_Loss: 0.28637346625328064, Test_Loss: 0.29501447081565857 *\n",
      "Epoch: 17, Train_Loss: 0.5678907632827759, Test_Loss: 0.34198063611984253\n",
      "Epoch: 17, Train_Loss: 1.064960241317749, Test_Loss: 0.42107105255126953\n",
      "Epoch: 17, Train_Loss: 0.4387018084526062, Test_Loss: 0.2965362071990967 *\n",
      "Epoch: 17, Train_Loss: 0.34478288888931274, Test_Loss: 0.32495999336242676\n",
      "Epoch: 17, Train_Loss: 0.32662078738212585, Test_Loss: 0.3226689100265503 *\n",
      "Epoch: 17, Train_Loss: 0.36378660798072815, Test_Loss: 0.336223304271698\n",
      "Epoch: 17, Train_Loss: 0.5886896848678589, Test_Loss: 0.31796595454216003 *\n",
      "Epoch: 17, Train_Loss: 0.44475406408309937, Test_Loss: 0.3171171545982361 *\n",
      "Epoch: 17, Train_Loss: 0.454315185546875, Test_Loss: 0.37471744418144226\n",
      "Epoch: 17, Train_Loss: 0.5859774947166443, Test_Loss: 0.3487389087677002 *\n",
      "Epoch: 17, Train_Loss: 0.29882684350013733, Test_Loss: 0.4129480719566345\n",
      "Epoch: 17, Train_Loss: 0.30421796441078186, Test_Loss: 0.4816693067550659\n",
      "Epoch: 17, Train_Loss: 0.335850328207016, Test_Loss: 0.44252321124076843 *\n",
      "Epoch: 17, Train_Loss: 0.3597186803817749, Test_Loss: 0.3327026963233948 *\n",
      "Epoch: 17, Train_Loss: 0.33793848752975464, Test_Loss: 0.4165930151939392\n",
      "Epoch: 17, Train_Loss: 0.30780744552612305, Test_Loss: 0.3883086144924164 *\n",
      "Epoch: 17, Train_Loss: 13.34717845916748, Test_Loss: 0.33532801270484924 *\n",
      "Epoch: 17, Train_Loss: 3.0449299812316895, Test_Loss: 0.2999715805053711 *\n",
      "Epoch: 17, Train_Loss: 1.4536073207855225, Test_Loss: 0.34856683015823364\n",
      "Epoch: 17, Train_Loss: 1.3932175636291504, Test_Loss: 0.34949028491973877\n",
      "Epoch: 17, Train_Loss: 0.39646807312965393, Test_Loss: 0.30660128593444824 *\n",
      "Epoch: 17, Train_Loss: 0.3814323842525482, Test_Loss: 0.3423202931880951\n",
      "Epoch: 17, Train_Loss: 1.1223039627075195, Test_Loss: 0.48456645011901855\n",
      "Epoch: 17, Train_Loss: 6.284058094024658, Test_Loss: 0.4106235206127167 *\n",
      "Epoch: 17, Train_Loss: 0.9780561923980713, Test_Loss: 0.5282412767410278\n",
      "Epoch: 17, Train_Loss: 0.354306697845459, Test_Loss: 0.4581509828567505 *\n",
      "Epoch: 17, Train_Loss: 3.694004774093628, Test_Loss: 0.38370341062545776 *\n",
      "Epoch: 17, Train_Loss: 1.5857406854629517, Test_Loss: 0.45406949520111084\n",
      "Epoch: 17, Train_Loss: 0.7580052614212036, Test_Loss: 0.5068057775497437\n",
      "Epoch: 17, Train_Loss: 0.28564271330833435, Test_Loss: 0.5383478999137878\n",
      "Epoch: 17, Train_Loss: 0.35024482011795044, Test_Loss: 0.33740150928497314 *\n",
      "Epoch: 17, Train_Loss: 0.3533531129360199, Test_Loss: 0.3722657859325409\n",
      "Epoch: 17, Train_Loss: 0.29470154643058777, Test_Loss: 0.710234522819519\n",
      "Epoch: 17, Train_Loss: 0.288693904876709, Test_Loss: 0.6177545785903931 *\n",
      "Epoch: 17, Train_Loss: 0.27558377385139465, Test_Loss: 1.0486135482788086\n",
      "Epoch: 17, Train_Loss: 0.27553796768188477, Test_Loss: 0.3557606637477875 *\n",
      "Epoch: 17, Train_Loss: 0.27766916155815125, Test_Loss: 0.846619725227356\n",
      "Epoch: 17, Train_Loss: 0.2941366136074066, Test_Loss: 1.4214509725570679\n",
      "Epoch: 17, Train_Loss: 0.3644339442253113, Test_Loss: 1.1821315288543701 *\n",
      "Epoch: 17, Train_Loss: 0.3525782823562622, Test_Loss: 0.8961515426635742 *\n",
      "Epoch: 17, Train_Loss: 0.3527300953865051, Test_Loss: 0.4949600100517273 *\n",
      "Epoch: 17, Train_Loss: 0.2977815568447113, Test_Loss: 0.3333923816680908 *\n",
      "Epoch: 17, Train_Loss: 0.29007938504219055, Test_Loss: 8.18505573272705\n",
      "Epoch: 17, Train_Loss: 0.281319797039032, Test_Loss: 0.6358976364135742 *\n",
      "Epoch: 17, Train_Loss: 0.28054574131965637, Test_Loss: 0.6238823533058167 *\n",
      "Epoch: 17, Train_Loss: 0.2784116268157959, Test_Loss: 0.46071529388427734 *\n",
      "Epoch: 17, Train_Loss: 0.2770337164402008, Test_Loss: 0.4528476595878601 *\n",
      "Epoch: 17, Train_Loss: 0.27550581097602844, Test_Loss: 0.28894656896591187 *\n",
      "Epoch: 17, Train_Loss: 0.2758077085018158, Test_Loss: 0.5980590581893921\n",
      "Epoch: 17, Train_Loss: 0.27561432123184204, Test_Loss: 0.574612021446228 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 17\n",
      "Epoch: 17, Train_Loss: 0.2762072682380676, Test_Loss: 0.36875420808792114 *\n",
      "Epoch: 17, Train_Loss: 0.2756289839744568, Test_Loss: 0.4047608971595764\n",
      "Epoch: 17, Train_Loss: 0.2778683006763458, Test_Loss: 0.417591392993927\n",
      "Epoch: 17, Train_Loss: 0.3005535900592804, Test_Loss: 0.654813289642334\n",
      "Epoch: 17, Train_Loss: 0.30387362837791443, Test_Loss: 0.4639303684234619 *\n",
      "Epoch: 17, Train_Loss: 0.3308132588863373, Test_Loss: 0.6285676956176758\n",
      "Epoch: 17, Train_Loss: 0.31106871366500854, Test_Loss: 0.9408476948738098\n",
      "Epoch: 17, Train_Loss: 0.33948275446891785, Test_Loss: 0.37326276302337646 *\n",
      "Epoch: 17, Train_Loss: 8.021523475646973, Test_Loss: 0.3551575541496277 *\n",
      "Epoch: 17, Train_Loss: 0.4870259165763855, Test_Loss: 0.49156302213668823\n",
      "Epoch: 17, Train_Loss: 0.2844793498516083, Test_Loss: 0.7460149526596069\n",
      "Epoch: 17, Train_Loss: 0.29577895998954773, Test_Loss: 0.6461831331253052 *\n",
      "Epoch: 17, Train_Loss: 0.3415333032608032, Test_Loss: 0.5302296876907349 *\n",
      "Epoch: 17, Train_Loss: 0.28114694356918335, Test_Loss: 0.5244625806808472 *\n",
      "Epoch: 17, Train_Loss: 0.3692881464958191, Test_Loss: 1.03691565990448\n",
      "Epoch: 17, Train_Loss: 0.44741860032081604, Test_Loss: 1.0395739078521729\n",
      "Epoch: 17, Train_Loss: 0.46232277154922485, Test_Loss: 0.9319866895675659 *\n",
      "Epoch: 17, Train_Loss: 0.3997473418712616, Test_Loss: 0.520375669002533 *\n",
      "Epoch: 17, Train_Loss: 0.3696916103363037, Test_Loss: 0.6043171882629395\n",
      "Epoch: 17, Train_Loss: 0.2760721445083618, Test_Loss: 0.7692464590072632\n",
      "Epoch: 17, Train_Loss: 0.3945615291595459, Test_Loss: 0.5732236504554749 *\n",
      "Epoch: 17, Train_Loss: 0.3247554302215576, Test_Loss: 0.424621045589447 *\n",
      "Epoch: 17, Train_Loss: 0.6151444911956787, Test_Loss: 0.6474059820175171\n",
      "Epoch: 17, Train_Loss: 0.3484606146812439, Test_Loss: 1.4753520488739014\n",
      "Epoch: 17, Train_Loss: 0.3203827738761902, Test_Loss: 5.149755001068115\n",
      "Epoch: 17, Train_Loss: 0.2988806664943695, Test_Loss: 0.3033323585987091 *\n",
      "Epoch: 17, Train_Loss: 0.29922664165496826, Test_Loss: 0.2758899927139282 *\n",
      "Epoch: 17, Train_Loss: 0.302927702665329, Test_Loss: 0.2966034710407257\n",
      "Epoch: 17, Train_Loss: 0.2995398938655853, Test_Loss: 0.2909117341041565 *\n",
      "Epoch: 17, Train_Loss: 0.36457139253616333, Test_Loss: 0.2940758466720581\n",
      "Epoch: 17, Train_Loss: 0.32882118225097656, Test_Loss: 0.3042502701282501\n",
      "Epoch: 17, Train_Loss: 0.32055339217185974, Test_Loss: 0.3986886441707611\n",
      "Epoch: 17, Train_Loss: 0.3865375518798828, Test_Loss: 0.2972998023033142 *\n",
      "Epoch: 17, Train_Loss: 5.290606498718262, Test_Loss: 0.2805831730365753 *\n",
      "Epoch: 17, Train_Loss: 0.2831892967224121, Test_Loss: 0.3048868179321289\n",
      "Epoch: 17, Train_Loss: 0.27966171503067017, Test_Loss: 0.29713061451911926 *\n",
      "Epoch: 17, Train_Loss: 0.2915809750556946, Test_Loss: 0.28078174591064453 *\n",
      "Epoch: 17, Train_Loss: 0.28161510825157166, Test_Loss: 0.33170634508132935\n",
      "Epoch: 17, Train_Loss: 0.2757898271083832, Test_Loss: 0.2899518311023712 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train_Loss: 0.2775765061378479, Test_Loss: 0.36007314920425415\n",
      "Epoch: 17, Train_Loss: 0.2797483503818512, Test_Loss: 0.3396703898906708 *\n",
      "Epoch: 17, Train_Loss: 0.29461002349853516, Test_Loss: 0.30361810326576233 *\n",
      "Epoch: 17, Train_Loss: 0.29567548632621765, Test_Loss: 0.3072628080844879\n",
      "Epoch: 17, Train_Loss: 0.31317463517189026, Test_Loss: 0.29479047656059265 *\n",
      "Epoch: 17, Train_Loss: 0.27574390172958374, Test_Loss: 0.30546215176582336\n",
      "Epoch: 17, Train_Loss: 0.27474090456962585, Test_Loss: 0.29190996289253235 *\n",
      "Epoch: 17, Train_Loss: 0.28846701979637146, Test_Loss: 0.2904376983642578 *\n",
      "Epoch: 17, Train_Loss: 0.2782421112060547, Test_Loss: 0.28588879108428955 *\n",
      "Epoch: 17, Train_Loss: 0.27794748544692993, Test_Loss: 0.2904583215713501\n",
      "Epoch: 17, Train_Loss: 0.28810450434684753, Test_Loss: 0.3126140236854553\n",
      "Epoch: 17, Train_Loss: 0.3008217215538025, Test_Loss: 0.2872258424758911 *\n",
      "Epoch: 17, Train_Loss: 0.28842341899871826, Test_Loss: 0.2808813750743866 *\n",
      "Epoch: 17, Train_Loss: 0.2752036154270172, Test_Loss: 0.31796908378601074\n",
      "Epoch: 17, Train_Loss: 0.27522197365760803, Test_Loss: 0.27989476919174194 *\n",
      "Epoch: 18, Train_Loss: 0.3142297863960266, Test_Loss: 0.29229044914245605 *\n",
      "Epoch: 18, Train_Loss: 0.2914830446243286, Test_Loss: 0.3868059515953064\n",
      "Epoch: 18, Train_Loss: 0.3239597678184509, Test_Loss: 0.42781293392181396\n",
      "Epoch: 18, Train_Loss: 0.3205290734767914, Test_Loss: 0.46696823835372925\n",
      "Epoch: 18, Train_Loss: 0.3940277397632599, Test_Loss: 0.3490009903907776 *\n",
      "Epoch: 18, Train_Loss: 0.2989439070224762, Test_Loss: 0.29689720273017883 *\n",
      "Epoch: 18, Train_Loss: 0.3131467401981354, Test_Loss: 0.29798170924186707\n",
      "Epoch: 18, Train_Loss: 0.31768539547920227, Test_Loss: 0.28294336795806885 *\n",
      "Epoch: 18, Train_Loss: 0.5040900707244873, Test_Loss: 0.34311217069625854\n",
      "Epoch: 18, Train_Loss: 0.3211423456668854, Test_Loss: 0.5716922283172607\n",
      "Epoch: 18, Train_Loss: 0.29199039936065674, Test_Loss: 0.6045637130737305\n",
      "Epoch: 18, Train_Loss: 0.28210803866386414, Test_Loss: 0.3294582664966583 *\n",
      "Epoch: 18, Train_Loss: 0.29311877489089966, Test_Loss: 0.3212001621723175 *\n",
      "Epoch: 18, Train_Loss: 0.28311294317245483, Test_Loss: 0.28495174646377563 *\n",
      "Epoch: 18, Train_Loss: 0.27667123079299927, Test_Loss: 0.28117597103118896 *\n",
      "Epoch: 18, Train_Loss: 0.3602634072303772, Test_Loss: 0.2873387336730957\n",
      "Epoch: 18, Train_Loss: 4.737064838409424, Test_Loss: 0.29122981429100037\n",
      "Epoch: 18, Train_Loss: 0.3592996597290039, Test_Loss: 0.3084906339645386\n",
      "Epoch: 18, Train_Loss: 0.27812686562538147, Test_Loss: 0.2867048382759094 *\n",
      "Epoch: 18, Train_Loss: 0.280710369348526, Test_Loss: 0.2856100797653198 *\n",
      "Epoch: 18, Train_Loss: 0.27409589290618896, Test_Loss: 0.3811332583427429\n",
      "Epoch: 18, Train_Loss: 0.2755594849586487, Test_Loss: 0.6624014973640442\n",
      "Epoch: 18, Train_Loss: 0.27412253618240356, Test_Loss: 0.526411771774292 *\n",
      "Epoch: 18, Train_Loss: 0.272340327501297, Test_Loss: 0.33643001317977905 *\n",
      "Epoch: 18, Train_Loss: 0.2750968337059021, Test_Loss: 0.301308810710907 *\n",
      "Epoch: 18, Train_Loss: 0.27339792251586914, Test_Loss: 0.3008021116256714 *\n",
      "Epoch: 18, Train_Loss: 0.3111400902271271, Test_Loss: 0.3029785752296448\n",
      "Epoch: 18, Train_Loss: 0.2958984375, Test_Loss: 0.3024447560310364 *\n",
      "Epoch: 18, Train_Loss: 0.3389131724834442, Test_Loss: 0.4913942515850067\n",
      "Epoch: 18, Train_Loss: 0.29677629470825195, Test_Loss: 5.393763542175293\n",
      "Epoch: 18, Train_Loss: 0.2768474817276001, Test_Loss: 0.34808850288391113 *\n",
      "Epoch: 18, Train_Loss: 0.4022924602031708, Test_Loss: 0.29358527064323425 *\n",
      "Epoch: 18, Train_Loss: 0.4866673946380615, Test_Loss: 0.28219926357269287 *\n",
      "Epoch: 18, Train_Loss: 0.4987124800682068, Test_Loss: 0.283644437789917\n",
      "Epoch: 18, Train_Loss: 0.4396999180316925, Test_Loss: 0.2813684344291687 *\n",
      "Epoch: 18, Train_Loss: 0.27430295944213867, Test_Loss: 0.28015196323394775 *\n",
      "Epoch: 18, Train_Loss: 0.2721673846244812, Test_Loss: 0.28674638271331787\n",
      "Epoch: 18, Train_Loss: 0.27284133434295654, Test_Loss: 0.2733529508113861 *\n",
      "Epoch: 18, Train_Loss: 0.2852185070514679, Test_Loss: 0.27504047751426697\n",
      "Epoch: 18, Train_Loss: 0.2918846309185028, Test_Loss: 0.2782055139541626\n",
      "Epoch: 18, Train_Loss: 0.29051268100738525, Test_Loss: 0.3156854212284088\n",
      "Epoch: 18, Train_Loss: 0.27968424558639526, Test_Loss: 0.2783251702785492 *\n",
      "Epoch: 18, Train_Loss: 0.2715071737766266, Test_Loss: 0.27693524956703186 *\n",
      "Epoch: 18, Train_Loss: 0.28065648674964905, Test_Loss: 0.31023523211479187\n",
      "Epoch: 18, Train_Loss: 0.2982112467288971, Test_Loss: 0.27387887239456177 *\n",
      "Epoch: 18, Train_Loss: 0.41492438316345215, Test_Loss: 0.2787131667137146\n",
      "Epoch: 18, Train_Loss: 0.40476083755493164, Test_Loss: 0.2783690094947815 *\n",
      "Epoch: 18, Train_Loss: 0.3984484076499939, Test_Loss: 0.3095342516899109\n",
      "Epoch: 18, Train_Loss: 0.3044055104255676, Test_Loss: 0.2795000970363617 *\n",
      "Epoch: 18, Train_Loss: 0.3816419243812561, Test_Loss: 0.28711703419685364\n",
      "Epoch: 18, Train_Loss: 0.3777832090854645, Test_Loss: 0.28003907203674316 *\n",
      "Epoch: 18, Train_Loss: 0.33891940116882324, Test_Loss: 0.33544832468032837\n",
      "Epoch: 18, Train_Loss: 0.39612889289855957, Test_Loss: 0.33187004923820496 *\n",
      "Epoch: 18, Train_Loss: 0.506641685962677, Test_Loss: 0.31193807721138 *\n",
      "Epoch: 18, Train_Loss: 0.3427749276161194, Test_Loss: 0.28254780173301697 *\n",
      "Epoch: 18, Train_Loss: 0.27558577060699463, Test_Loss: 0.30139198899269104\n",
      "Epoch: 18, Train_Loss: 2.496852159500122, Test_Loss: 0.309911847114563\n",
      "Epoch: 18, Train_Loss: 1.0474443435668945, Test_Loss: 0.2906154692173004 *\n",
      "Epoch: 18, Train_Loss: 0.3159603476524353, Test_Loss: 0.29124295711517334\n",
      "Epoch: 18, Train_Loss: 0.3199611306190491, Test_Loss: 0.35958537459373474\n",
      "Epoch: 18, Train_Loss: 0.28502538800239563, Test_Loss: 2.453740358352661\n",
      "Epoch: 18, Train_Loss: 0.2940792739391327, Test_Loss: 3.4254813194274902\n",
      "Epoch: 18, Train_Loss: 0.2796126902103424, Test_Loss: 0.27946773171424866 *\n",
      "Epoch: 18, Train_Loss: 0.31676483154296875, Test_Loss: 0.27348142862319946 *\n",
      "Epoch: 18, Train_Loss: 0.4156847596168518, Test_Loss: 0.3220973312854767\n",
      "Epoch: 18, Train_Loss: 0.3421529531478882, Test_Loss: 0.2833322584629059 *\n",
      "Epoch: 18, Train_Loss: 0.32072213292121887, Test_Loss: 0.3200545907020569\n",
      "Epoch: 18, Train_Loss: 0.280915766954422, Test_Loss: 0.3076643943786621 *\n",
      "Epoch: 18, Train_Loss: 0.29168936610221863, Test_Loss: 0.3518611788749695\n",
      "Epoch: 18, Train_Loss: 0.2826560437679291, Test_Loss: 0.2804540693759918 *\n",
      "Epoch: 18, Train_Loss: 0.28456446528434753, Test_Loss: 0.2943788766860962\n",
      "Epoch: 18, Train_Loss: 0.33217623829841614, Test_Loss: 0.29791155457496643\n",
      "Epoch: 18, Train_Loss: 0.30800747871398926, Test_Loss: 0.33374232053756714\n",
      "Epoch: 18, Train_Loss: 0.2818422019481659, Test_Loss: 0.2872142791748047 *\n",
      "Epoch: 18, Train_Loss: 0.2762192189693451, Test_Loss: 0.33651864528656006\n",
      "Epoch: 18, Train_Loss: 0.28627172112464905, Test_Loss: 0.3224520683288574 *\n",
      "Epoch: 18, Train_Loss: 0.2905394732952118, Test_Loss: 0.32134848833084106 *\n",
      "Epoch: 18, Train_Loss: 0.2813485562801361, Test_Loss: 0.28757306933403015 *\n",
      "Epoch: 18, Train_Loss: 0.2725631594657898, Test_Loss: 0.31087225675582886\n",
      "Epoch: 18, Train_Loss: 0.27197974920272827, Test_Loss: 0.3294898569583893\n",
      "Epoch: 18, Train_Loss: 0.2704022228717804, Test_Loss: 0.27666962146759033 *\n",
      "Epoch: 18, Train_Loss: 0.2793025076389313, Test_Loss: 0.28116634488105774\n",
      "Epoch: 18, Train_Loss: 0.27321657538414, Test_Loss: 0.2824302017688751\n",
      "Epoch: 18, Train_Loss: 0.2797550857067108, Test_Loss: 0.29631227254867554\n",
      "Epoch: 18, Train_Loss: 0.2839029133319855, Test_Loss: 0.28701305389404297 *\n",
      "Epoch: 18, Train_Loss: 0.2717874050140381, Test_Loss: 0.28047460317611694 *\n",
      "Epoch: 18, Train_Loss: 0.2706577479839325, Test_Loss: 0.278670996427536 *\n",
      "Epoch: 18, Train_Loss: 0.27619555592536926, Test_Loss: 0.2758568823337555 *\n",
      "Epoch: 18, Train_Loss: 0.28626906871795654, Test_Loss: 0.27352964878082275 *\n",
      "Epoch: 18, Train_Loss: 0.28773126006126404, Test_Loss: 0.2764618396759033\n",
      "Epoch: 18, Train_Loss: 0.2862294316291809, Test_Loss: 0.29001447558403015\n",
      "Epoch: 18, Train_Loss: 0.2934128940105438, Test_Loss: 0.3215941786766052\n",
      "Epoch: 18, Train_Loss: 0.2972274422645569, Test_Loss: 0.2959139049053192 *\n",
      "Epoch: 18, Train_Loss: 0.30083757638931274, Test_Loss: 0.6349674463272095\n",
      "Epoch: 18, Train_Loss: 0.28067344427108765, Test_Loss: 0.6168792247772217 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train_Loss: 0.29470276832580566, Test_Loss: 0.372563898563385 *\n",
      "Epoch: 18, Train_Loss: 0.30156219005584717, Test_Loss: 0.28363606333732605 *\n",
      "Epoch: 18, Train_Loss: 0.27858003973960876, Test_Loss: 0.2941804826259613\n",
      "Epoch: 18, Train_Loss: 0.27752676606178284, Test_Loss: 0.2877354025840759 *\n",
      "Epoch: 18, Train_Loss: 0.2795058786869049, Test_Loss: 0.41962963342666626\n",
      "Model saved at location save_new\\model.ckpt at epoch 18\n",
      "Epoch: 18, Train_Loss: 0.2842651903629303, Test_Loss: 0.5842260122299194\n",
      "Epoch: 18, Train_Loss: 0.31793299317359924, Test_Loss: 0.6404393911361694\n",
      "Epoch: 18, Train_Loss: 0.3300658166408539, Test_Loss: 0.3340400457382202 *\n",
      "Epoch: 18, Train_Loss: 0.298177570104599, Test_Loss: 0.31789612770080566 *\n",
      "Epoch: 18, Train_Loss: 0.2707141041755676, Test_Loss: 0.27542081475257874 *\n",
      "Epoch: 18, Train_Loss: 0.3426443934440613, Test_Loss: 0.2726069688796997 *\n",
      "Epoch: 18, Train_Loss: 0.2830018103122711, Test_Loss: 0.27853095531463623\n",
      "Epoch: 18, Train_Loss: 0.2730943560600281, Test_Loss: 0.2785434424877167\n",
      "Epoch: 18, Train_Loss: 0.28298449516296387, Test_Loss: 0.3155243992805481\n",
      "Epoch: 18, Train_Loss: 0.2907920181751251, Test_Loss: 0.2730511426925659 *\n",
      "Epoch: 18, Train_Loss: 0.37948089838027954, Test_Loss: 0.30621835589408875\n",
      "Epoch: 18, Train_Loss: 0.3424897789955139, Test_Loss: 0.4008636474609375\n",
      "Epoch: 18, Train_Loss: 0.3106922209262848, Test_Loss: 0.6206004619598389\n",
      "Epoch: 18, Train_Loss: 0.28489211201667786, Test_Loss: 0.5172612071037292 *\n",
      "Epoch: 18, Train_Loss: 0.2778022587299347, Test_Loss: 0.2963421940803528 *\n",
      "Epoch: 18, Train_Loss: 0.2861536145210266, Test_Loss: 0.2866034507751465 *\n",
      "Epoch: 18, Train_Loss: 0.27101564407348633, Test_Loss: 0.2862272560596466 *\n",
      "Epoch: 18, Train_Loss: 0.2795262038707733, Test_Loss: 0.2866014540195465\n",
      "Epoch: 18, Train_Loss: 0.27995312213897705, Test_Loss: 0.2908480167388916\n",
      "Epoch: 18, Train_Loss: 0.2874508500099182, Test_Loss: 1.1416113376617432\n",
      "Epoch: 18, Train_Loss: 0.36846354603767395, Test_Loss: 4.956289291381836\n",
      "Epoch: 18, Train_Loss: 0.27221009135246277, Test_Loss: 0.2997984290122986 *\n",
      "Epoch: 18, Train_Loss: 0.32989877462387085, Test_Loss: 0.2830987572669983 *\n",
      "Epoch: 18, Train_Loss: 0.2851845622062683, Test_Loss: 0.2797304391860962 *\n",
      "Epoch: 18, Train_Loss: 0.2999621331691742, Test_Loss: 0.27730792760849 *\n",
      "Epoch: 18, Train_Loss: 0.35559624433517456, Test_Loss: 0.27390000224113464 *\n",
      "Epoch: 18, Train_Loss: 0.4957064986228943, Test_Loss: 0.2816621661186218\n",
      "Epoch: 18, Train_Loss: 0.28360286355018616, Test_Loss: 0.28090745210647583 *\n",
      "Epoch: 18, Train_Loss: 0.3085295855998993, Test_Loss: 0.2698456943035126 *\n",
      "Epoch: 18, Train_Loss: 0.2681308686733246, Test_Loss: 0.2724638283252716\n",
      "Epoch: 18, Train_Loss: 0.26798132061958313, Test_Loss: 0.27837854623794556\n",
      "Epoch: 18, Train_Loss: 0.2689918577671051, Test_Loss: 0.320065438747406\n",
      "Epoch: 18, Train_Loss: 0.2688426673412323, Test_Loss: 0.2760927975177765 *\n",
      "Epoch: 18, Train_Loss: 0.2763462960720062, Test_Loss: 0.2814141511917114\n",
      "Epoch: 18, Train_Loss: 0.27651315927505493, Test_Loss: 0.3137461245059967\n",
      "Epoch: 18, Train_Loss: 0.27973178029060364, Test_Loss: 0.27043378353118896 *\n",
      "Epoch: 18, Train_Loss: 0.2734324634075165, Test_Loss: 0.2755310535430908\n",
      "Epoch: 18, Train_Loss: 0.2770287096500397, Test_Loss: 0.27093052864074707 *\n",
      "Epoch: 18, Train_Loss: 0.27630913257598877, Test_Loss: 0.31306344270706177\n",
      "Epoch: 18, Train_Loss: 0.26878663897514343, Test_Loss: 0.2727764844894409 *\n",
      "Epoch: 18, Train_Loss: 0.2672053277492523, Test_Loss: 0.2811225652694702\n",
      "Epoch: 18, Train_Loss: 0.2893025875091553, Test_Loss: 0.2748441994190216 *\n",
      "Epoch: 18, Train_Loss: 0.29305386543273926, Test_Loss: 0.3147834241390228\n",
      "Epoch: 18, Train_Loss: 0.3024892210960388, Test_Loss: 0.3205685317516327\n",
      "Epoch: 18, Train_Loss: 0.2680889368057251, Test_Loss: 0.2828332781791687 *\n",
      "Epoch: 18, Train_Loss: 0.3127862215042114, Test_Loss: 0.272921085357666 *\n",
      "Epoch: 18, Train_Loss: 0.3060537278652191, Test_Loss: 0.30273938179016113\n",
      "Epoch: 18, Train_Loss: 0.2981742024421692, Test_Loss: 0.28657734394073486 *\n",
      "Epoch: 18, Train_Loss: 0.2734297513961792, Test_Loss: 0.2762993574142456 *\n",
      "Epoch: 18, Train_Loss: 0.30583781003952026, Test_Loss: 0.3128606975078583\n",
      "Epoch: 18, Train_Loss: 0.2671782970428467, Test_Loss: 0.32949119806289673\n",
      "Epoch: 18, Train_Loss: 0.28586670756340027, Test_Loss: 3.5162322521209717\n",
      "Epoch: 18, Train_Loss: 0.277969628572464, Test_Loss: 2.5914835929870605 *\n",
      "Epoch: 18, Train_Loss: 0.29272547364234924, Test_Loss: 0.26970985531806946 *\n",
      "Epoch: 18, Train_Loss: 1.3416471481323242, Test_Loss: 0.26962313055992126 *\n",
      "Epoch: 18, Train_Loss: 4.315093994140625, Test_Loss: 0.31065598130226135\n",
      "Epoch: 18, Train_Loss: 0.6440149545669556, Test_Loss: 0.2887108027935028 *\n",
      "Epoch: 18, Train_Loss: 0.28368204832077026, Test_Loss: 0.29781967401504517\n",
      "Epoch: 18, Train_Loss: 0.274657666683197, Test_Loss: 0.33189335465431213\n",
      "Epoch: 18, Train_Loss: 0.3819620609283447, Test_Loss: 0.3611745238304138\n",
      "Epoch: 18, Train_Loss: 0.32277387380599976, Test_Loss: 0.2700307369232178 *\n",
      "Epoch: 18, Train_Loss: 0.27746501564979553, Test_Loss: 0.29328814148902893\n",
      "Epoch: 18, Train_Loss: 0.2665669620037079, Test_Loss: 0.28650471568107605 *\n",
      "Epoch: 18, Train_Loss: 0.3209802806377411, Test_Loss: 0.2844434082508087 *\n",
      "Epoch: 18, Train_Loss: 0.2861340641975403, Test_Loss: 0.2726336717605591 *\n",
      "Epoch: 18, Train_Loss: 0.2766133248806, Test_Loss: 0.32375314831733704\n",
      "Epoch: 18, Train_Loss: 0.5734992623329163, Test_Loss: 0.3173843026161194 *\n",
      "Epoch: 18, Train_Loss: 1.1608142852783203, Test_Loss: 0.34826231002807617\n",
      "Epoch: 18, Train_Loss: 1.2794722318649292, Test_Loss: 0.3289593458175659 *\n",
      "Epoch: 18, Train_Loss: 0.33134177327156067, Test_Loss: 0.29696938395500183 *\n",
      "Epoch: 18, Train_Loss: 0.43231040239334106, Test_Loss: 0.28909218311309814 *\n",
      "Epoch: 18, Train_Loss: 1.954446792602539, Test_Loss: 0.26917046308517456 *\n",
      "Epoch: 18, Train_Loss: 1.048136591911316, Test_Loss: 0.273811936378479\n",
      "Epoch: 18, Train_Loss: 0.290508508682251, Test_Loss: 0.27478525042533875\n",
      "Epoch: 18, Train_Loss: 0.27789178490638733, Test_Loss: 0.28127866983413696\n",
      "Epoch: 18, Train_Loss: 0.7254399061203003, Test_Loss: 0.2767076790332794 *\n",
      "Epoch: 18, Train_Loss: 0.918372392654419, Test_Loss: 0.2675163745880127 *\n",
      "Epoch: 18, Train_Loss: 0.8699719905853271, Test_Loss: 0.2711908519268036\n",
      "Epoch: 18, Train_Loss: 0.27310776710510254, Test_Loss: 0.2711922824382782\n",
      "Epoch: 18, Train_Loss: 0.29555943608283997, Test_Loss: 0.3062440752983093\n",
      "Epoch: 18, Train_Loss: 0.4893339276313782, Test_Loss: 0.31196048855781555\n",
      "Epoch: 18, Train_Loss: 0.4503171443939209, Test_Loss: 0.27258679270744324 *\n",
      "Epoch: 18, Train_Loss: 0.3021208643913269, Test_Loss: 0.29833075404167175\n",
      "Epoch: 18, Train_Loss: 0.327114999294281, Test_Loss: 0.34878212213516235\n",
      "Epoch: 18, Train_Loss: 0.3040185570716858, Test_Loss: 0.4751013517379761\n",
      "Epoch: 18, Train_Loss: 0.3017561137676239, Test_Loss: 0.4360494613647461 *\n",
      "Epoch: 18, Train_Loss: 0.3833652138710022, Test_Loss: 0.3148312568664551 *\n",
      "Epoch: 18, Train_Loss: 0.3817117512226105, Test_Loss: 0.31038013100624084 *\n",
      "Epoch: 18, Train_Loss: 0.29512739181518555, Test_Loss: 0.3224025070667267\n",
      "Epoch: 18, Train_Loss: 0.3701002597808838, Test_Loss: 0.2850060760974884 *\n",
      "Epoch: 18, Train_Loss: 0.36192232370376587, Test_Loss: 0.4101008474826813\n",
      "Epoch: 18, Train_Loss: 0.32970184087753296, Test_Loss: 0.3048546314239502 *\n",
      "Epoch: 18, Train_Loss: 0.36521071195602417, Test_Loss: 0.7033052444458008\n",
      "Epoch: 18, Train_Loss: 0.5256853103637695, Test_Loss: 0.3780220150947571 *\n",
      "Epoch: 18, Train_Loss: 0.29387590289115906, Test_Loss: 0.3207630217075348 *\n",
      "Epoch: 18, Train_Loss: 0.40127119421958923, Test_Loss: 0.2871990501880646 *\n",
      "Epoch: 18, Train_Loss: 0.31813690066337585, Test_Loss: 0.2738783359527588 *\n",
      "Epoch: 18, Train_Loss: 0.30909058451652527, Test_Loss: 0.32162198424339294\n",
      "Epoch: 18, Train_Loss: 0.2674187123775482, Test_Loss: 0.3125734329223633 *\n",
      "Epoch: 18, Train_Loss: 0.27010148763656616, Test_Loss: 0.28272807598114014 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 18\n",
      "Epoch: 18, Train_Loss: 0.2670331597328186, Test_Loss: 0.33874133229255676\n",
      "Epoch: 18, Train_Loss: 0.2735193967819214, Test_Loss: 0.44904106855392456\n",
      "Epoch: 18, Train_Loss: 0.27655965089797974, Test_Loss: 0.4471452832221985 *\n",
      "Epoch: 18, Train_Loss: 0.29275450110435486, Test_Loss: 0.6569818258285522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train_Loss: 0.28407004475593567, Test_Loss: 0.8925390243530273\n",
      "Epoch: 18, Train_Loss: 0.32941532135009766, Test_Loss: 0.46274179220199585 *\n",
      "Epoch: 18, Train_Loss: 0.4212350845336914, Test_Loss: 0.3457120954990387 *\n",
      "Epoch: 18, Train_Loss: 0.4339827001094818, Test_Loss: 0.34860992431640625\n",
      "Epoch: 18, Train_Loss: 0.28988009691238403, Test_Loss: 0.3456539809703827 *\n",
      "Epoch: 18, Train_Loss: 0.30734217166900635, Test_Loss: 0.5084625482559204\n",
      "Epoch: 18, Train_Loss: 0.35530605912208557, Test_Loss: 2.407055139541626\n",
      "Epoch: 18, Train_Loss: 0.3580359220504761, Test_Loss: 3.0992751121520996\n",
      "Epoch: 18, Train_Loss: 0.4007781445980072, Test_Loss: 0.3452516794204712 *\n",
      "Epoch: 18, Train_Loss: 0.4313991665840149, Test_Loss: 0.3042808771133423 *\n",
      "Epoch: 18, Train_Loss: 0.39408501982688904, Test_Loss: 0.3030676245689392 *\n",
      "Epoch: 18, Train_Loss: 0.3722158372402191, Test_Loss: 0.2844034433364868 *\n",
      "Epoch: 18, Train_Loss: 0.37718552350997925, Test_Loss: 0.2929275631904602\n",
      "Epoch: 18, Train_Loss: 0.2866641879081726, Test_Loss: 0.33672428131103516\n",
      "Epoch: 18, Train_Loss: 0.3162182569503784, Test_Loss: 0.29714474081993103 *\n",
      "Epoch: 18, Train_Loss: 0.5930194854736328, Test_Loss: 0.2727706730365753 *\n",
      "Epoch: 18, Train_Loss: 0.5492268204689026, Test_Loss: 0.28517860174179077\n",
      "Epoch: 18, Train_Loss: 0.41823744773864746, Test_Loss: 0.3002365827560425\n",
      "Epoch: 18, Train_Loss: 0.3093188405036926, Test_Loss: 0.3851052522659302\n",
      "Epoch: 18, Train_Loss: 0.29520711302757263, Test_Loss: 0.279949426651001 *\n",
      "Epoch: 18, Train_Loss: 0.2757781147956848, Test_Loss: 0.31229379773139954\n",
      "Epoch: 18, Train_Loss: 0.5984815359115601, Test_Loss: 0.3104209899902344 *\n",
      "Epoch: 18, Train_Loss: 0.3279516100883484, Test_Loss: 0.2966338098049164 *\n",
      "Epoch: 18, Train_Loss: 0.2973233461380005, Test_Loss: 0.3147256076335907\n",
      "Epoch: 18, Train_Loss: 0.44751012325286865, Test_Loss: 0.35872378945350647\n",
      "Epoch: 18, Train_Loss: 0.30514565110206604, Test_Loss: 0.3391442894935608 *\n",
      "Epoch: 18, Train_Loss: 0.2795594334602356, Test_Loss: 0.2891363799571991 *\n",
      "Epoch: 18, Train_Loss: 0.294974148273468, Test_Loss: 0.31018850207328796\n",
      "Epoch: 18, Train_Loss: 0.3939729332923889, Test_Loss: 0.2953869104385376 *\n",
      "Epoch: 18, Train_Loss: 0.32196563482284546, Test_Loss: 0.3504396677017212\n",
      "Epoch: 18, Train_Loss: 0.4034808278083801, Test_Loss: 0.363879919052124\n",
      "Epoch: 18, Train_Loss: 0.29695019125938416, Test_Loss: 0.2937992811203003 *\n",
      "Epoch: 18, Train_Loss: 0.3728444576263428, Test_Loss: 0.27675458788871765 *\n",
      "Epoch: 18, Train_Loss: 0.31659185886383057, Test_Loss: 0.3163985311985016\n",
      "Epoch: 18, Train_Loss: 0.2821581959724426, Test_Loss: 0.3027624785900116 *\n",
      "Epoch: 18, Train_Loss: 0.27095890045166016, Test_Loss: 0.27425676584243774 *\n",
      "Epoch: 18, Train_Loss: 0.3504934012889862, Test_Loss: 0.4035816192626953\n",
      "Epoch: 18, Train_Loss: 0.5229034423828125, Test_Loss: 0.3359379172325134 *\n",
      "Epoch: 18, Train_Loss: 0.5057673454284668, Test_Loss: 4.965284824371338\n",
      "Epoch: 18, Train_Loss: 0.5612231492996216, Test_Loss: 1.2898902893066406 *\n",
      "Epoch: 18, Train_Loss: 0.7049851417541504, Test_Loss: 0.2954852283000946 *\n",
      "Epoch: 18, Train_Loss: 0.4820402264595032, Test_Loss: 0.29843413829803467\n",
      "Epoch: 18, Train_Loss: 0.40875834226608276, Test_Loss: 0.28840699791908264 *\n",
      "Epoch: 18, Train_Loss: 0.33185654878616333, Test_Loss: 0.2728065848350525 *\n",
      "Epoch: 18, Train_Loss: 0.274797260761261, Test_Loss: 0.280056893825531\n",
      "Epoch: 18, Train_Loss: 0.2773643434047699, Test_Loss: 0.34153199195861816\n",
      "Epoch: 18, Train_Loss: 0.2833089828491211, Test_Loss: 0.3042389750480652 *\n",
      "Epoch: 18, Train_Loss: 0.48017609119415283, Test_Loss: 0.27429839968681335 *\n",
      "Epoch: 18, Train_Loss: 0.5390908122062683, Test_Loss: 0.2903065085411072\n",
      "Epoch: 18, Train_Loss: 0.5645197629928589, Test_Loss: 0.3131745159626007\n",
      "Epoch: 18, Train_Loss: 1.4560874700546265, Test_Loss: 0.3273518979549408\n",
      "Epoch: 18, Train_Loss: 0.8558906316757202, Test_Loss: 0.2814796268939972 *\n",
      "Epoch: 18, Train_Loss: 0.45579099655151367, Test_Loss: 0.28186893463134766\n",
      "Epoch: 18, Train_Loss: 0.30363625288009644, Test_Loss: 0.3435152769088745\n",
      "Epoch: 18, Train_Loss: 0.27211570739746094, Test_Loss: 0.3042031526565552 *\n",
      "Epoch: 18, Train_Loss: 0.46167245507240295, Test_Loss: 0.286689817905426 *\n",
      "Epoch: 18, Train_Loss: 0.9070315957069397, Test_Loss: 0.35442614555358887\n",
      "Epoch: 18, Train_Loss: 0.6466135382652283, Test_Loss: 0.30337461829185486 *\n",
      "Epoch: 18, Train_Loss: 0.30472591519355774, Test_Loss: 0.286355197429657 *\n",
      "Epoch: 18, Train_Loss: 0.292704313993454, Test_Loss: 0.29614707827568054\n",
      "Epoch: 18, Train_Loss: 0.34465762972831726, Test_Loss: 0.3142234981060028\n",
      "Epoch: 18, Train_Loss: 0.5463665723800659, Test_Loss: 0.28796136379241943 *\n",
      "Epoch: 18, Train_Loss: 0.4036473035812378, Test_Loss: 0.2924271821975708\n",
      "Epoch: 18, Train_Loss: 0.3316771388053894, Test_Loss: 0.3178533911705017\n",
      "Epoch: 18, Train_Loss: 0.39551448822021484, Test_Loss: 0.28991836309432983 *\n",
      "Epoch: 18, Train_Loss: 0.2974209487438202, Test_Loss: 0.3336333632469177\n",
      "Epoch: 18, Train_Loss: 0.2779904901981354, Test_Loss: 0.37041646242141724\n",
      "Epoch: 18, Train_Loss: 0.32144683599472046, Test_Loss: 0.3789849579334259\n",
      "Epoch: 18, Train_Loss: 0.27772122621536255, Test_Loss: 0.3022143840789795 *\n",
      "Epoch: 18, Train_Loss: 0.3628102242946625, Test_Loss: 0.32041624188423157\n",
      "Epoch: 18, Train_Loss: 0.3177920877933502, Test_Loss: 0.39025789499282837\n",
      "Epoch: 18, Train_Loss: 1.483935832977295, Test_Loss: 0.37025076150894165 *\n",
      "Epoch: 18, Train_Loss: 14.941108703613281, Test_Loss: 0.3552447557449341 *\n",
      "Epoch: 18, Train_Loss: 0.6492239236831665, Test_Loss: 0.30717724561691284 *\n",
      "Epoch: 18, Train_Loss: 1.370803952217102, Test_Loss: 0.32031992077827454\n",
      "Epoch: 18, Train_Loss: 0.8678562641143799, Test_Loss: 0.29237937927246094 *\n",
      "Epoch: 18, Train_Loss: 0.320806086063385, Test_Loss: 0.30749964714050293\n",
      "Epoch: 18, Train_Loss: 0.5668779611587524, Test_Loss: 0.48769810795783997\n",
      "Epoch: 18, Train_Loss: 5.521903991699219, Test_Loss: 0.34397080540657043 *\n",
      "Epoch: 18, Train_Loss: 2.4395546913146973, Test_Loss: 0.6793195009231567\n",
      "Epoch: 18, Train_Loss: 0.32742398977279663, Test_Loss: 0.3962446451187134 *\n",
      "Epoch: 18, Train_Loss: 1.4437843561172485, Test_Loss: 0.3283423185348511 *\n",
      "Epoch: 18, Train_Loss: 3.6876845359802246, Test_Loss: 0.49825766682624817\n",
      "Epoch: 18, Train_Loss: 0.8203116655349731, Test_Loss: 0.5224984288215637\n",
      "Epoch: 18, Train_Loss: 0.337239146232605, Test_Loss: 0.6072442531585693\n",
      "Epoch: 18, Train_Loss: 0.43336760997772217, Test_Loss: 0.35447561740875244 *\n",
      "Epoch: 18, Train_Loss: 0.464152455329895, Test_Loss: 0.53128981590271\n",
      "Epoch: 18, Train_Loss: 0.3537985384464264, Test_Loss: 0.5654014945030212\n",
      "Epoch: 18, Train_Loss: 0.27812135219573975, Test_Loss: 0.4734199047088623 *\n",
      "Epoch: 18, Train_Loss: 0.26738861203193665, Test_Loss: 0.9292258620262146\n",
      "Epoch: 18, Train_Loss: 0.2616848647594452, Test_Loss: 0.3753041625022888 *\n",
      "Epoch: 18, Train_Loss: 0.2701822817325592, Test_Loss: 0.3922653794288635\n",
      "Epoch: 18, Train_Loss: 0.306946337223053, Test_Loss: 0.729977548122406\n",
      "Epoch: 18, Train_Loss: 0.2907112240791321, Test_Loss: 0.8139632344245911\n",
      "Epoch: 18, Train_Loss: 0.35968002676963806, Test_Loss: 0.7763952016830444 *\n",
      "Epoch: 18, Train_Loss: 0.4890912175178528, Test_Loss: 0.7279343605041504 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 18\n",
      "Epoch: 18, Train_Loss: 0.3201533555984497, Test_Loss: 0.3164047598838806 *\n",
      "Epoch: 18, Train_Loss: 0.27303647994995117, Test_Loss: 4.7776055335998535\n",
      "Epoch: 18, Train_Loss: 0.2798159718513489, Test_Loss: 2.279142379760742 *\n",
      "Epoch: 18, Train_Loss: 0.27026987075805664, Test_Loss: 0.5509735345840454 *\n",
      "Epoch: 18, Train_Loss: 0.2665737271308899, Test_Loss: 0.5045368671417236 *\n",
      "Epoch: 18, Train_Loss: 0.2680535316467285, Test_Loss: 0.560282826423645\n",
      "Epoch: 18, Train_Loss: 0.2633662223815918, Test_Loss: 0.28406989574432373 *\n",
      "Epoch: 18, Train_Loss: 0.26295942068099976, Test_Loss: 0.5773111581802368\n",
      "Epoch: 18, Train_Loss: 0.26331329345703125, Test_Loss: 0.8002517223358154\n",
      "Epoch: 18, Train_Loss: 0.2649454176425934, Test_Loss: 0.4832072854042053 *\n",
      "Epoch: 18, Train_Loss: 0.26318326592445374, Test_Loss: 0.4346933364868164 *\n",
      "Epoch: 18, Train_Loss: 0.265932559967041, Test_Loss: 0.60686194896698\n",
      "Epoch: 18, Train_Loss: 0.2921721637248993, Test_Loss: 0.6220074892044067\n",
      "Epoch: 18, Train_Loss: 0.28548189997673035, Test_Loss: 0.9673616886138916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train_Loss: 0.37136149406433105, Test_Loss: 0.5848327875137329 *\n",
      "Epoch: 18, Train_Loss: 0.2941156029701233, Test_Loss: 0.8749070167541504\n",
      "Epoch: 18, Train_Loss: 0.3299807012081146, Test_Loss: 0.5101190209388733 *\n",
      "Epoch: 18, Train_Loss: 7.0039262771606445, Test_Loss: 0.27201640605926514 *\n",
      "Epoch: 18, Train_Loss: 1.9962210655212402, Test_Loss: 0.3226760923862457\n",
      "Epoch: 18, Train_Loss: 0.2838754653930664, Test_Loss: 0.40588366985321045\n",
      "Epoch: 18, Train_Loss: 0.3100231885910034, Test_Loss: 0.4716399908065796\n",
      "Epoch: 18, Train_Loss: 0.36833322048187256, Test_Loss: 0.2984536588191986 *\n",
      "Epoch: 18, Train_Loss: 0.2946547865867615, Test_Loss: 0.3982943892478943\n",
      "Epoch: 18, Train_Loss: 0.29136162996292114, Test_Loss: 0.4462755024433136\n",
      "Epoch: 18, Train_Loss: 0.3065241873264313, Test_Loss: 0.4468578100204468\n",
      "Epoch: 18, Train_Loss: 0.3388281762599945, Test_Loss: 0.4974665641784668\n",
      "Epoch: 18, Train_Loss: 0.37442705035209656, Test_Loss: 0.3184901177883148 *\n",
      "Epoch: 18, Train_Loss: 0.3341027796268463, Test_Loss: 0.3557845950126648\n",
      "Epoch: 18, Train_Loss: 0.2743894159793854, Test_Loss: 0.3635883331298828\n",
      "Epoch: 18, Train_Loss: 0.2822139263153076, Test_Loss: 0.3400028347969055 *\n",
      "Epoch: 18, Train_Loss: 0.26553037762641907, Test_Loss: 0.2950029969215393 *\n",
      "Epoch: 18, Train_Loss: 0.36110448837280273, Test_Loss: 0.46292752027511597\n",
      "Epoch: 18, Train_Loss: 0.2685207426548004, Test_Loss: 0.35468611121177673 *\n",
      "Epoch: 18, Train_Loss: 0.29580676555633545, Test_Loss: 5.9453816413879395\n",
      "Epoch: 18, Train_Loss: 0.29551219940185547, Test_Loss: 0.5083358287811279 *\n",
      "Epoch: 18, Train_Loss: 0.27637046575546265, Test_Loss: 0.26244789361953735 *\n",
      "Epoch: 18, Train_Loss: 0.28679633140563965, Test_Loss: 0.2814878225326538\n",
      "Epoch: 18, Train_Loss: 0.27425527572631836, Test_Loss: 0.26609474420547485 *\n",
      "Epoch: 18, Train_Loss: 0.29596325755119324, Test_Loss: 0.2727503776550293\n",
      "Epoch: 18, Train_Loss: 0.28678613901138306, Test_Loss: 0.2737460136413574\n",
      "Epoch: 18, Train_Loss: 0.2708478569984436, Test_Loss: 0.37983259558677673\n",
      "Epoch: 18, Train_Loss: 0.3389682173728943, Test_Loss: 0.31542932987213135 *\n",
      "Epoch: 18, Train_Loss: 4.5809736251831055, Test_Loss: 0.2633064389228821 *\n",
      "Epoch: 18, Train_Loss: 0.6693712472915649, Test_Loss: 0.2923712432384491\n",
      "Epoch: 18, Train_Loss: 0.2621983587741852, Test_Loss: 0.27885571122169495 *\n",
      "Epoch: 18, Train_Loss: 0.2860001027584076, Test_Loss: 0.2685813307762146 *\n",
      "Epoch: 18, Train_Loss: 0.2655479311943054, Test_Loss: 0.2812301218509674\n",
      "Epoch: 18, Train_Loss: 0.2617000639438629, Test_Loss: 0.2687092423439026 *\n",
      "Epoch: 18, Train_Loss: 0.27137115597724915, Test_Loss: 0.31406328082084656\n",
      "Epoch: 18, Train_Loss: 0.26689136028289795, Test_Loss: 0.36718469858169556\n",
      "Epoch: 18, Train_Loss: 0.29455217719078064, Test_Loss: 0.324194073677063 *\n",
      "Epoch: 18, Train_Loss: 0.273904025554657, Test_Loss: 0.29252681136131287 *\n",
      "Epoch: 18, Train_Loss: 0.32873401045799255, Test_Loss: 0.28497618436813354 *\n",
      "Epoch: 18, Train_Loss: 0.2607974112033844, Test_Loss: 0.28986093401908875\n",
      "Epoch: 18, Train_Loss: 0.2603535056114197, Test_Loss: 0.27497217059135437 *\n",
      "Epoch: 18, Train_Loss: 0.27083373069763184, Test_Loss: 0.27865535020828247\n",
      "Epoch: 18, Train_Loss: 0.26633456349372864, Test_Loss: 0.27338099479675293 *\n",
      "Epoch: 18, Train_Loss: 0.2618671953678131, Test_Loss: 0.2738831639289856\n",
      "Epoch: 18, Train_Loss: 0.27113857865333557, Test_Loss: 0.30131590366363525\n",
      "Epoch: 18, Train_Loss: 0.28744935989379883, Test_Loss: 0.2836577296257019 *\n",
      "Epoch: 18, Train_Loss: 0.2800275981426239, Test_Loss: 0.2713468074798584 *\n",
      "Epoch: 18, Train_Loss: 0.2600114345550537, Test_Loss: 0.2900952100753784\n",
      "Epoch: 18, Train_Loss: 0.26108676195144653, Test_Loss: 0.27456963062286377 *\n",
      "Epoch: 18, Train_Loss: 0.30889636278152466, Test_Loss: 0.28434932231903076\n",
      "Epoch: 18, Train_Loss: 0.2765987515449524, Test_Loss: 0.28935977816581726\n",
      "Epoch: 18, Train_Loss: 0.2887760400772095, Test_Loss: 0.3957538902759552\n",
      "Epoch: 18, Train_Loss: 0.3357353210449219, Test_Loss: 0.4086199700832367\n",
      "Epoch: 18, Train_Loss: 0.38320407271385193, Test_Loss: 0.3388029634952545 *\n",
      "Epoch: 18, Train_Loss: 0.3076515197753906, Test_Loss: 0.3016033172607422 *\n",
      "Epoch: 18, Train_Loss: 0.2899201512336731, Test_Loss: 0.299480676651001 *\n",
      "Epoch: 18, Train_Loss: 0.31102681159973145, Test_Loss: 0.2805372476577759 *\n",
      "Epoch: 18, Train_Loss: 0.4453275799751282, Test_Loss: 0.3339092433452606\n",
      "Epoch: 18, Train_Loss: 0.29053032398223877, Test_Loss: 0.6481055617332458\n",
      "Epoch: 18, Train_Loss: 0.3007148504257202, Test_Loss: 0.5975771546363831 *\n",
      "Epoch: 18, Train_Loss: 0.2590372860431671, Test_Loss: 0.4349382817745209 *\n",
      "Epoch: 18, Train_Loss: 0.2590208351612091, Test_Loss: 0.3078002333641052 *\n",
      "Epoch: 18, Train_Loss: 0.2589241862297058, Test_Loss: 0.29287242889404297 *\n",
      "Epoch: 18, Train_Loss: 0.2588594853878021, Test_Loss: 0.2927152216434479 *\n",
      "Epoch: 18, Train_Loss: 0.2599492371082306, Test_Loss: 0.2813945412635803 *\n",
      "Epoch: 18, Train_Loss: 4.786538600921631, Test_Loss: 0.2881489396095276\n",
      "Epoch: 18, Train_Loss: 0.8918429017066956, Test_Loss: 0.2853392958641052 *\n",
      "Epoch: 18, Train_Loss: 0.26409339904785156, Test_Loss: 0.29074615240097046\n",
      "Epoch: 18, Train_Loss: 0.28153085708618164, Test_Loss: 0.26738855242729187 *\n",
      "Epoch: 18, Train_Loss: 0.2632467746734619, Test_Loss: 0.36841535568237305\n",
      "Epoch: 18, Train_Loss: 0.2593998908996582, Test_Loss: 0.6272670030593872\n",
      "Epoch: 18, Train_Loss: 0.26032766699790955, Test_Loss: 0.3615787923336029 *\n",
      "Epoch: 18, Train_Loss: 0.2586754858493805, Test_Loss: 0.4558890461921692\n",
      "Epoch: 18, Train_Loss: 0.2588309049606323, Test_Loss: 0.2812330722808838 *\n",
      "Epoch: 18, Train_Loss: 0.259575754404068, Test_Loss: 0.2822432518005371\n",
      "Epoch: 18, Train_Loss: 0.2918730080127716, Test_Loss: 0.28272518515586853\n",
      "Epoch: 18, Train_Loss: 0.3193807005882263, Test_Loss: 0.2822110950946808 *\n",
      "Epoch: 18, Train_Loss: 0.3314087390899658, Test_Loss: 0.2973850667476654\n",
      "Epoch: 18, Train_Loss: 0.3065718412399292, Test_Loss: 4.953365325927734\n",
      "Epoch: 18, Train_Loss: 0.2633529305458069, Test_Loss: 0.961556613445282 *\n",
      "Epoch: 18, Train_Loss: 0.32422104477882385, Test_Loss: 0.2779281735420227 *\n",
      "Epoch: 18, Train_Loss: 0.4978625178337097, Test_Loss: 0.2666429579257965 *\n",
      "Epoch: 18, Train_Loss: 0.48902231454849243, Test_Loss: 0.2718781530857086\n",
      "Epoch: 18, Train_Loss: 0.48369652032852173, Test_Loss: 0.2763616740703583\n",
      "Epoch: 18, Train_Loss: 0.27367210388183594, Test_Loss: 0.2642129957675934 *\n",
      "Epoch: 18, Train_Loss: 0.2589423656463623, Test_Loss: 0.2634028196334839 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 18\n",
      "Epoch: 18, Train_Loss: 0.2585170269012451, Test_Loss: 0.25974681973457336 *\n",
      "Epoch: 18, Train_Loss: 0.2675393521785736, Test_Loss: 0.2593581974506378 *\n",
      "Epoch: 18, Train_Loss: 0.27486661076545715, Test_Loss: 0.2623269855976105\n",
      "Epoch: 18, Train_Loss: 0.275989830493927, Test_Loss: 0.27265042066574097\n",
      "Epoch: 18, Train_Loss: 0.27147242426872253, Test_Loss: 0.2665620744228363 *\n",
      "Epoch: 18, Train_Loss: 0.2579132616519928, Test_Loss: 0.28263092041015625\n",
      "Epoch: 18, Train_Loss: 0.26226338744163513, Test_Loss: 0.2746922969818115 *\n",
      "Epoch: 18, Train_Loss: 0.2806839644908905, Test_Loss: 0.25967642664909363 *\n",
      "Epoch: 18, Train_Loss: 0.3997229337692261, Test_Loss: 0.2589229345321655 *\n",
      "Epoch: 18, Train_Loss: 0.43403661251068115, Test_Loss: 0.2629495859146118\n",
      "Epoch: 18, Train_Loss: 0.39446157217025757, Test_Loss: 0.27604225277900696\n",
      "Epoch: 18, Train_Loss: 0.3253025412559509, Test_Loss: 0.26581138372421265 *\n",
      "Epoch: 18, Train_Loss: 0.3653426766395569, Test_Loss: 0.26356011629104614 *\n",
      "Epoch: 18, Train_Loss: 0.3716973066329956, Test_Loss: 0.25929710268974304 *\n",
      "Epoch: 18, Train_Loss: 0.26998239755630493, Test_Loss: 0.2760528028011322\n",
      "Epoch: 18, Train_Loss: 0.38935884833335876, Test_Loss: 0.28218337893486023\n",
      "Epoch: 18, Train_Loss: 0.34552791714668274, Test_Loss: 0.2827264964580536\n",
      "Epoch: 18, Train_Loss: 0.4870185852050781, Test_Loss: 0.2635749578475952 *\n",
      "Epoch: 18, Train_Loss: 0.2651562988758087, Test_Loss: 0.26388534903526306\n",
      "Epoch: 18, Train_Loss: 1.4762930870056152, Test_Loss: 0.2721257507801056\n",
      "Epoch: 18, Train_Loss: 2.0789690017700195, Test_Loss: 0.26652973890304565 *\n",
      "Epoch: 18, Train_Loss: 0.3007362484931946, Test_Loss: 0.2622253894805908 *\n",
      "Epoch: 18, Train_Loss: 0.3182654082775116, Test_Loss: 0.33523017168045044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train_Loss: 0.28529980778694153, Test_Loss: 0.635313093662262\n",
      "Epoch: 18, Train_Loss: 0.2845214605331421, Test_Loss: 5.23298454284668\n",
      "Epoch: 18, Train_Loss: 0.2596439719200134, Test_Loss: 0.2657015919685364 *\n",
      "Epoch: 18, Train_Loss: 0.2718829810619354, Test_Loss: 0.2596522569656372 *\n",
      "Epoch: 18, Train_Loss: 0.37991103529930115, Test_Loss: 0.2989610433578491\n",
      "Epoch: 18, Train_Loss: 0.33633288741111755, Test_Loss: 0.30176666378974915\n",
      "Epoch: 18, Train_Loss: 0.32300281524658203, Test_Loss: 0.31665781140327454\n",
      "Epoch: 18, Train_Loss: 0.28738081455230713, Test_Loss: 0.2621806859970093 *\n",
      "Epoch: 18, Train_Loss: 0.2834564745426178, Test_Loss: 0.3577215075492859\n",
      "Epoch: 18, Train_Loss: 0.27205905318260193, Test_Loss: 0.28800493478775024 *\n",
      "Epoch: 18, Train_Loss: 0.27376821637153625, Test_Loss: 0.2630806863307953 *\n",
      "Epoch: 18, Train_Loss: 0.28955942392349243, Test_Loss: 0.2842719852924347\n",
      "Epoch: 18, Train_Loss: 0.2971721589565277, Test_Loss: 0.2865853011608124\n",
      "Epoch: 18, Train_Loss: 0.27053502202033997, Test_Loss: 0.2602952718734741 *\n",
      "Epoch: 18, Train_Loss: 0.25933530926704407, Test_Loss: 0.31319716572761536\n",
      "Epoch: 18, Train_Loss: 0.2793149948120117, Test_Loss: 0.3388573229312897\n",
      "Epoch: 18, Train_Loss: 0.28354552388191223, Test_Loss: 0.302104651927948 *\n",
      "Epoch: 18, Train_Loss: 0.2720336318016052, Test_Loss: 0.3296804428100586\n",
      "Epoch: 18, Train_Loss: 0.26083454489707947, Test_Loss: 0.2897050380706787 *\n",
      "Epoch: 18, Train_Loss: 0.25864163041114807, Test_Loss: 0.3208347260951996\n",
      "Epoch: 18, Train_Loss: 0.25691497325897217, Test_Loss: 0.26801228523254395 *\n",
      "Epoch: 18, Train_Loss: 0.25976863503456116, Test_Loss: 0.2693910300731659\n",
      "Epoch: 18, Train_Loss: 0.25824272632598877, Test_Loss: 0.2782231867313385\n",
      "Epoch: 18, Train_Loss: 0.2635883092880249, Test_Loss: 0.2807510793209076\n",
      "Epoch: 18, Train_Loss: 0.2648651599884033, Test_Loss: 0.2774849534034729 *\n",
      "Epoch: 18, Train_Loss: 0.25809815526008606, Test_Loss: 0.27525174617767334 *\n",
      "Epoch: 18, Train_Loss: 0.2572679817676544, Test_Loss: 0.2668219804763794 *\n",
      "Epoch: 18, Train_Loss: 0.2605428397655487, Test_Loss: 0.26745694875717163\n",
      "Epoch: 18, Train_Loss: 0.2700551152229309, Test_Loss: 0.2718021869659424\n",
      "Epoch: 18, Train_Loss: 0.27382388710975647, Test_Loss: 0.2632264792919159 *\n",
      "Epoch: 18, Train_Loss: 0.2703891694545746, Test_Loss: 0.27778586745262146\n",
      "Epoch: 19, Train_Loss: 0.2864466905593872, Test_Loss: 0.3521297574043274 *\n",
      "Epoch: 19, Train_Loss: 0.2665965259075165, Test_Loss: 0.28147226572036743 *\n",
      "Epoch: 19, Train_Loss: 0.2679905295372009, Test_Loss: 0.5870006680488586\n",
      "Epoch: 19, Train_Loss: 0.26130208373069763, Test_Loss: 0.6595058441162109\n",
      "Epoch: 19, Train_Loss: 0.26176154613494873, Test_Loss: 0.4068042039871216 *\n",
      "Epoch: 19, Train_Loss: 0.2887493669986725, Test_Loss: 0.28766578435897827 *\n",
      "Epoch: 19, Train_Loss: 0.2707782983779907, Test_Loss: 0.2750895023345947 *\n",
      "Epoch: 19, Train_Loss: 0.2622883915901184, Test_Loss: 0.2646010220050812 *\n",
      "Epoch: 19, Train_Loss: 0.26122790575027466, Test_Loss: 0.31923791766166687\n",
      "Epoch: 19, Train_Loss: 0.26603278517723083, Test_Loss: 0.6099304556846619\n",
      "Epoch: 19, Train_Loss: 0.29661229252815247, Test_Loss: 0.6066203117370605 *\n",
      "Epoch: 19, Train_Loss: 0.31210070848464966, Test_Loss: 0.3234628736972809 *\n",
      "Epoch: 19, Train_Loss: 0.31001734733581543, Test_Loss: 0.32892537117004395\n",
      "Epoch: 19, Train_Loss: 0.2578367590904236, Test_Loss: 0.25971612334251404 *\n",
      "Epoch: 19, Train_Loss: 0.3133648633956909, Test_Loss: 0.26063042879104614\n",
      "Epoch: 19, Train_Loss: 0.2869032323360443, Test_Loss: 0.2641756236553192\n",
      "Epoch: 19, Train_Loss: 0.25931745767593384, Test_Loss: 0.2668224275112152\n",
      "Epoch: 19, Train_Loss: 0.26786139607429504, Test_Loss: 0.28302082419395447\n",
      "Epoch: 19, Train_Loss: 0.280502587556839, Test_Loss: 0.27422457933425903 *\n",
      "Epoch: 19, Train_Loss: 0.34689152240753174, Test_Loss: 0.26105231046676636 *\n",
      "Epoch: 19, Train_Loss: 0.3294869363307953, Test_Loss: 0.37604597210884094\n",
      "Epoch: 19, Train_Loss: 0.29442811012268066, Test_Loss: 0.6590709686279297\n",
      "Epoch: 19, Train_Loss: 0.27606379985809326, Test_Loss: 0.41423261165618896 *\n",
      "Epoch: 19, Train_Loss: 0.2666485905647278, Test_Loss: 0.3808692693710327 *\n",
      "Epoch: 19, Train_Loss: 0.27342841029167175, Test_Loss: 0.27851611375808716 *\n",
      "Epoch: 19, Train_Loss: 0.25628194212913513, Test_Loss: 0.2783356308937073 *\n",
      "Epoch: 19, Train_Loss: 0.2610931992530823, Test_Loss: 0.278876930475235\n",
      "Epoch: 19, Train_Loss: 0.2717214822769165, Test_Loss: 0.27621641755104065 *\n",
      "Epoch: 19, Train_Loss: 0.2761234641075134, Test_Loss: 0.33463433384895325\n",
      "Epoch: 19, Train_Loss: 0.36135751008987427, Test_Loss: 5.7675933837890625\n",
      "Epoch: 19, Train_Loss: 0.2555505335330963, Test_Loss: 0.41837894916534424 *\n",
      "Epoch: 19, Train_Loss: 0.31439337134361267, Test_Loss: 0.27570515871047974 *\n",
      "Epoch: 19, Train_Loss: 0.2625810503959656, Test_Loss: 0.2616632878780365 *\n",
      "Epoch: 19, Train_Loss: 0.29017502069473267, Test_Loss: 0.27168774604797363\n",
      "Epoch: 19, Train_Loss: 0.28063151240348816, Test_Loss: 0.265392541885376 *\n",
      "Epoch: 19, Train_Loss: 0.5495932102203369, Test_Loss: 0.26055535674095154 *\n",
      "Epoch: 19, Train_Loss: 0.2868300974369049, Test_Loss: 0.26690641045570374\n",
      "Epoch: 19, Train_Loss: 0.2933693528175354, Test_Loss: 0.2591399550437927 *\n",
      "Epoch: 19, Train_Loss: 0.2555413842201233, Test_Loss: 0.25635892152786255 *\n",
      "Epoch: 19, Train_Loss: 0.2552046775817871, Test_Loss: 0.2652904987335205\n",
      "Epoch: 19, Train_Loss: 0.2567139267921448, Test_Loss: 0.29435503482818604\n",
      "Epoch: 19, Train_Loss: 0.2572425901889801, Test_Loss: 0.265510618686676 *\n",
      "Epoch: 19, Train_Loss: 0.26064929366111755, Test_Loss: 0.265322208404541 *\n",
      "Epoch: 19, Train_Loss: 0.25901171565055847, Test_Loss: 0.29027485847473145\n",
      "Epoch: 19, Train_Loss: 0.26963555812835693, Test_Loss: 0.25614267587661743 *\n",
      "Epoch: 19, Train_Loss: 0.26008841395378113, Test_Loss: 0.2585258185863495\n",
      "Epoch: 19, Train_Loss: 0.25881800055503845, Test_Loss: 0.2590208351612091\n",
      "Epoch: 19, Train_Loss: 0.26310786604881287, Test_Loss: 0.2895628809928894\n",
      "Epoch: 19, Train_Loss: 0.25724565982818604, Test_Loss: 0.2612932324409485 *\n",
      "Epoch: 19, Train_Loss: 0.25485485792160034, Test_Loss: 0.26382967829704285\n",
      "Epoch: 19, Train_Loss: 0.27079758048057556, Test_Loss: 0.2565477788448334 *\n",
      "Epoch: 19, Train_Loss: 0.28065934777259827, Test_Loss: 0.27942314743995667\n",
      "Epoch: 19, Train_Loss: 0.2966403663158417, Test_Loss: 0.27898722887039185 *\n",
      "Epoch: 19, Train_Loss: 0.2557995915412903, Test_Loss: 0.27670297026634216 *\n",
      "Epoch: 19, Train_Loss: 0.28392621874809265, Test_Loss: 0.2573695778846741 *\n",
      "Epoch: 19, Train_Loss: 0.2995019555091858, Test_Loss: 0.2663505971431732\n",
      "Epoch: 19, Train_Loss: 0.28784680366516113, Test_Loss: 0.2660842835903168 *\n",
      "Epoch: 19, Train_Loss: 0.2547963261604309, Test_Loss: 0.2612172067165375 *\n",
      "Epoch: 19, Train_Loss: 0.2932795584201813, Test_Loss: 0.2614772319793701\n",
      "Epoch: 19, Train_Loss: 0.25661730766296387, Test_Loss: 0.33145394921302795\n",
      "Epoch: 19, Train_Loss: 0.2699124813079834, Test_Loss: 1.832878589630127\n",
      "Epoch: 19, Train_Loss: 0.25541821122169495, Test_Loss: 4.176793575286865\n",
      "Epoch: 19, Train_Loss: 0.2816469669342041, Test_Loss: 0.26208701729774475 *\n",
      "Epoch: 19, Train_Loss: 0.3484426736831665, Test_Loss: 0.25422847270965576 *\n",
      "Epoch: 19, Train_Loss: 3.3871262073516846, Test_Loss: 0.29949814081192017\n",
      "Epoch: 19, Train_Loss: 2.508882761001587, Test_Loss: 0.2812424302101135 *\n",
      "Epoch: 19, Train_Loss: 0.2792617678642273, Test_Loss: 0.3012790083885193\n",
      "Epoch: 19, Train_Loss: 0.25568562746047974, Test_Loss: 0.2839960753917694 *\n",
      "Epoch: 19, Train_Loss: 0.3600671589374542, Test_Loss: 0.3762926459312439\n",
      "Epoch: 19, Train_Loss: 0.3511018753051758, Test_Loss: 0.26590660214424133 *\n",
      "Epoch: 19, Train_Loss: 0.26979100704193115, Test_Loss: 0.26460781693458557 *\n",
      "Epoch: 19, Train_Loss: 0.2535850405693054, Test_Loss: 0.2842741012573242\n",
      "Epoch: 19, Train_Loss: 0.29564523696899414, Test_Loss: 0.2734493017196655 *\n",
      "Epoch: 19, Train_Loss: 0.2802989184856415, Test_Loss: 0.2616615295410156 *\n",
      "Epoch: 19, Train_Loss: 0.26250556111335754, Test_Loss: 0.311935156583786\n",
      "Epoch: 19, Train_Loss: 0.387969970703125, Test_Loss: 0.31943783164024353\n",
      "Epoch: 19, Train_Loss: 0.9878504872322083, Test_Loss: 0.3181075155735016 *\n",
      "Epoch: 19, Train_Loss: 1.3333150148391724, Test_Loss: 0.33466851711273193\n",
      "Epoch: 19, Train_Loss: 0.3368874192237854, Test_Loss: 0.2777709364891052 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train_Loss: 0.3429834842681885, Test_Loss: 0.30742698907852173\n",
      "Epoch: 19, Train_Loss: 1.7648478746414185, Test_Loss: 0.2566031217575073 *\n",
      "Epoch: 19, Train_Loss: 1.3902413845062256, Test_Loss: 0.2565547227859497 *\n",
      "Epoch: 19, Train_Loss: 0.2747275233268738, Test_Loss: 0.26317885518074036\n",
      "Epoch: 19, Train_Loss: 0.25903892517089844, Test_Loss: 0.27030208706855774\n",
      "Epoch: 19, Train_Loss: 0.5695842504501343, Test_Loss: 0.25940635800361633 *\n",
      "Epoch: 19, Train_Loss: 0.8148130178451538, Test_Loss: 0.2586389482021332 *\n",
      "Epoch: 19, Train_Loss: 0.9465268850326538, Test_Loss: 0.2592903971672058\n",
      "Epoch: 19, Train_Loss: 0.258149653673172, Test_Loss: 0.25721028447151184 *\n",
      "Epoch: 19, Train_Loss: 0.28278908133506775, Test_Loss: 0.25984975695610046\n",
      "Epoch: 19, Train_Loss: 0.3580899238586426, Test_Loss: 0.2883686423301697\n",
      "Epoch: 19, Train_Loss: 0.5472617149353027, Test_Loss: 0.2551504075527191 *\n",
      "Epoch: 19, Train_Loss: 0.26725736260414124, Test_Loss: 0.269867867231369\n",
      "Epoch: 19, Train_Loss: 0.3054977357387543, Test_Loss: 0.31137704849243164\n",
      "Epoch: 19, Train_Loss: 0.2887870967388153, Test_Loss: 0.4987627863883972\n",
      "Epoch: 19, Train_Loss: 0.269603431224823, Test_Loss: 0.4240001142024994 *\n",
      "Epoch: 19, Train_Loss: 0.3493841290473938, Test_Loss: 0.29281577467918396 *\n",
      "Epoch: 19, Train_Loss: 0.38323402404785156, Test_Loss: 0.30209407210350037\n",
      "Epoch: 19, Train_Loss: 0.29106417298316956, Test_Loss: 0.30800074338912964\n",
      "Epoch: 19, Train_Loss: 0.3274693489074707, Test_Loss: 0.26082509756088257 *\n",
      "Epoch: 19, Train_Loss: 0.2966333329677582, Test_Loss: 0.2918040454387665\n",
      "Model saved at location save_new\\model.ckpt at epoch 19\n",
      "Epoch: 19, Train_Loss: 0.28630197048187256, Test_Loss: 0.31186050176620483\n",
      "Epoch: 19, Train_Loss: 0.3351978063583374, Test_Loss: 0.4996379613876343\n",
      "Epoch: 19, Train_Loss: 0.4291512966156006, Test_Loss: 0.3664025366306305 *\n",
      "Epoch: 19, Train_Loss: 0.28701159358024597, Test_Loss: 0.3543379306793213 *\n",
      "Epoch: 19, Train_Loss: 0.38824331760406494, Test_Loss: 0.28642624616622925 *\n",
      "Epoch: 19, Train_Loss: 0.3132748007774353, Test_Loss: 0.2587350904941559 *\n",
      "Epoch: 19, Train_Loss: 0.310657262802124, Test_Loss: 0.272117018699646\n",
      "Epoch: 19, Train_Loss: 0.2677812874317169, Test_Loss: 0.29958027601242065\n",
      "Epoch: 19, Train_Loss: 0.25655171275138855, Test_Loss: 0.27267393469810486 *\n",
      "Epoch: 19, Train_Loss: 0.2552545964717865, Test_Loss: 0.2857483923435211\n",
      "Epoch: 19, Train_Loss: 0.2566097378730774, Test_Loss: 0.3085970878601074\n",
      "Epoch: 19, Train_Loss: 0.2664448916912079, Test_Loss: 0.45984917879104614\n",
      "Epoch: 19, Train_Loss: 0.2744426429271698, Test_Loss: 0.5633018016815186\n",
      "Epoch: 19, Train_Loss: 0.2698395848274231, Test_Loss: 0.7697087526321411\n",
      "Epoch: 19, Train_Loss: 0.268738716840744, Test_Loss: 0.5327392816543579 *\n",
      "Epoch: 19, Train_Loss: 0.38392654061317444, Test_Loss: 0.49462398886680603 *\n",
      "Epoch: 19, Train_Loss: 0.5313962697982788, Test_Loss: 0.49632567167282104\n",
      "Epoch: 19, Train_Loss: 0.2737038731575012, Test_Loss: 0.5022231340408325\n",
      "Epoch: 19, Train_Loss: 0.3074081540107727, Test_Loss: 0.4833787679672241 *\n",
      "Epoch: 19, Train_Loss: 0.2934514880180359, Test_Loss: 0.7148745059967041\n",
      "Epoch: 19, Train_Loss: 0.2935829758644104, Test_Loss: 5.226676940917969\n",
      "Epoch: 19, Train_Loss: 0.4420246183872223, Test_Loss: 0.3389925956726074 *\n",
      "Epoch: 19, Train_Loss: 0.4235549569129944, Test_Loss: 0.3378696143627167 *\n",
      "Epoch: 19, Train_Loss: 0.49873650074005127, Test_Loss: 0.3129082918167114 *\n",
      "Epoch: 19, Train_Loss: 0.34617510437965393, Test_Loss: 0.2744709849357605 *\n",
      "Epoch: 19, Train_Loss: 0.37417274713516235, Test_Loss: 0.2646440863609314 *\n",
      "Epoch: 19, Train_Loss: 0.27373361587524414, Test_Loss: 0.3458472490310669\n",
      "Epoch: 19, Train_Loss: 0.29316380620002747, Test_Loss: 0.331132709980011 *\n",
      "Epoch: 19, Train_Loss: 0.4443127512931824, Test_Loss: 0.25979122519493103 *\n",
      "Epoch: 19, Train_Loss: 0.8640641570091248, Test_Loss: 0.2705961763858795\n",
      "Epoch: 19, Train_Loss: 0.6023091673851013, Test_Loss: 0.29646989703178406\n",
      "Epoch: 19, Train_Loss: 0.2945134937763214, Test_Loss: 0.46950429677963257\n",
      "Epoch: 19, Train_Loss: 0.2718547582626343, Test_Loss: 0.28302040696144104 *\n",
      "Epoch: 19, Train_Loss: 0.2609488368034363, Test_Loss: 0.26769891381263733 *\n",
      "Epoch: 19, Train_Loss: 0.5430949926376343, Test_Loss: 0.2923966348171234\n",
      "Epoch: 19, Train_Loss: 0.47183123230934143, Test_Loss: 0.2816437780857086 *\n",
      "Epoch: 19, Train_Loss: 0.26079896092414856, Test_Loss: 0.2657172977924347 *\n",
      "Epoch: 19, Train_Loss: 0.42397791147232056, Test_Loss: 0.31197234988212585\n",
      "Epoch: 19, Train_Loss: 0.27571946382522583, Test_Loss: 0.34244075417518616\n",
      "Epoch: 19, Train_Loss: 0.2691984176635742, Test_Loss: 0.262959361076355 *\n",
      "Epoch: 19, Train_Loss: 0.29623180627822876, Test_Loss: 0.28772804141044617\n",
      "Epoch: 19, Train_Loss: 0.3720448315143585, Test_Loss: 0.2592168152332306 *\n",
      "Epoch: 19, Train_Loss: 0.2965574562549591, Test_Loss: 0.2911512553691864\n",
      "Epoch: 19, Train_Loss: 0.3512146770954132, Test_Loss: 0.31673410534858704\n",
      "Epoch: 19, Train_Loss: 0.2612602412700653, Test_Loss: 0.28509095311164856 *\n",
      "Epoch: 19, Train_Loss: 0.358458936214447, Test_Loss: 0.2557433843612671 *\n",
      "Epoch: 19, Train_Loss: 0.27537214756011963, Test_Loss: 0.28235557675361633\n",
      "Epoch: 19, Train_Loss: 0.2742806375026703, Test_Loss: 0.2830767035484314\n",
      "Epoch: 19, Train_Loss: 0.2602626383304596, Test_Loss: 0.27500632405281067 *\n",
      "Epoch: 19, Train_Loss: 0.32419952750205994, Test_Loss: 0.2776675820350647\n",
      "Epoch: 19, Train_Loss: 0.3745890259742737, Test_Loss: 0.37596356868743896\n",
      "Epoch: 19, Train_Loss: 0.4715297222137451, Test_Loss: 2.8715744018554688\n",
      "Epoch: 19, Train_Loss: 0.4529654383659363, Test_Loss: 2.6500885486602783 *\n",
      "Epoch: 19, Train_Loss: 0.6845375299453735, Test_Loss: 0.27778488397598267 *\n",
      "Epoch: 19, Train_Loss: 0.5484944581985474, Test_Loss: 0.26802462339401245 *\n",
      "Epoch: 19, Train_Loss: 0.45058658719062805, Test_Loss: 0.30460992455482483\n",
      "Epoch: 19, Train_Loss: 0.31496134400367737, Test_Loss: 0.2551780343055725 *\n",
      "Epoch: 19, Train_Loss: 0.27150267362594604, Test_Loss: 0.2729129195213318\n",
      "Epoch: 19, Train_Loss: 0.26339951157569885, Test_Loss: 0.32298293709754944\n",
      "Epoch: 19, Train_Loss: 0.2639244496822357, Test_Loss: 0.29817408323287964 *\n",
      "Epoch: 19, Train_Loss: 0.39767614006996155, Test_Loss: 0.26097092032432556 *\n",
      "Epoch: 19, Train_Loss: 0.5184158682823181, Test_Loss: 0.2812129259109497\n",
      "Epoch: 19, Train_Loss: 0.5315266251564026, Test_Loss: 0.2852989733219147\n",
      "Epoch: 19, Train_Loss: 1.3428170680999756, Test_Loss: 0.35669630765914917\n",
      "Epoch: 19, Train_Loss: 1.1200093030929565, Test_Loss: 0.2811773717403412 *\n",
      "Epoch: 19, Train_Loss: 0.4055177569389343, Test_Loss: 0.30507513880729675\n",
      "Epoch: 19, Train_Loss: 0.37418466806411743, Test_Loss: 0.30276456475257874 *\n",
      "Epoch: 19, Train_Loss: 0.258282870054245, Test_Loss: 0.33169835805892944\n",
      "Epoch: 19, Train_Loss: 0.3588750958442688, Test_Loss: 0.2596237361431122 *\n",
      "Epoch: 19, Train_Loss: 0.77940434217453, Test_Loss: 0.3239325284957886\n",
      "Epoch: 19, Train_Loss: 0.8709431886672974, Test_Loss: 0.3565102219581604\n",
      "Epoch: 19, Train_Loss: 0.27571189403533936, Test_Loss: 0.2623642683029175 *\n",
      "Epoch: 19, Train_Loss: 0.28648945689201355, Test_Loss: 0.2892197370529175\n",
      "Epoch: 19, Train_Loss: 0.33580347895622253, Test_Loss: 0.2884669005870819 *\n",
      "Epoch: 19, Train_Loss: 0.542371928691864, Test_Loss: 0.2786315083503723 *\n",
      "Epoch: 19, Train_Loss: 0.36571305990219116, Test_Loss: 0.2748587429523468 *\n",
      "Epoch: 19, Train_Loss: 0.3989381790161133, Test_Loss: 0.2705608904361725 *\n",
      "Epoch: 19, Train_Loss: 0.3904554843902588, Test_Loss: 0.29027995467185974\n",
      "Epoch: 19, Train_Loss: 0.38293027877807617, Test_Loss: 0.2918798625469208\n",
      "Epoch: 19, Train_Loss: 0.26222702860832214, Test_Loss: 0.3326534628868103\n",
      "Epoch: 19, Train_Loss: 0.28437110781669617, Test_Loss: 0.34993496537208557\n",
      "Epoch: 19, Train_Loss: 0.25999370217323303, Test_Loss: 0.2738766074180603 *\n",
      "Epoch: 19, Train_Loss: 0.3206436336040497, Test_Loss: 0.2691638171672821 *\n",
      "Epoch: 19, Train_Loss: 0.28665876388549805, Test_Loss: 0.3063649535179138\n",
      "Epoch: 19, Train_Loss: 0.3035559356212616, Test_Loss: 0.3873152732849121\n",
      "Epoch: 19, Train_Loss: 15.709142684936523, Test_Loss: 0.3744056224822998 *\n",
      "Epoch: 19, Train_Loss: 0.32112568616867065, Test_Loss: 0.28266772627830505 *\n",
      "Epoch: 19, Train_Loss: 1.2709436416625977, Test_Loss: 0.2991875410079956\n",
      "Epoch: 19, Train_Loss: 1.0982924699783325, Test_Loss: 0.3102627098560333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train_Loss: 0.30385100841522217, Test_Loss: 0.27502816915512085 *\n",
      "Epoch: 19, Train_Loss: 0.5118308663368225, Test_Loss: 0.3789794445037842\n",
      "Epoch: 19, Train_Loss: 3.261613607406616, Test_Loss: 0.3234245777130127 *\n",
      "Epoch: 19, Train_Loss: 4.266456127166748, Test_Loss: 0.6220846772193909\n",
      "Epoch: 19, Train_Loss: 0.30695584416389465, Test_Loss: 0.4545963406562805 *\n",
      "Epoch: 19, Train_Loss: 0.6027289032936096, Test_Loss: 0.38782015442848206 *\n",
      "Epoch: 19, Train_Loss: 4.337047100067139, Test_Loss: 0.37107202410697937 *\n",
      "Epoch: 19, Train_Loss: 0.7848621010780334, Test_Loss: 0.37444743514060974\n",
      "Epoch: 19, Train_Loss: 0.2686831057071686, Test_Loss: 0.5523684620857239\n",
      "Epoch: 19, Train_Loss: 0.2671778202056885, Test_Loss: 0.3088729977607727 *\n",
      "Epoch: 19, Train_Loss: 0.36882561445236206, Test_Loss: 0.5495648980140686\n",
      "Model saved at location save_new\\model.ckpt at epoch 19\n",
      "Epoch: 19, Train_Loss: 0.32320839166641235, Test_Loss: 0.35131385922431946 *\n",
      "Epoch: 19, Train_Loss: 0.25171712040901184, Test_Loss: 0.849534809589386\n",
      "Epoch: 19, Train_Loss: 0.27199000120162964, Test_Loss: 1.1523499488830566\n",
      "Epoch: 19, Train_Loss: 0.24940335750579834, Test_Loss: 0.5915738344192505 *\n",
      "Epoch: 19, Train_Loss: 0.2602314352989197, Test_Loss: 0.37334129214286804 *\n",
      "Epoch: 19, Train_Loss: 0.3296352028846741, Test_Loss: 0.8230609893798828\n",
      "Epoch: 19, Train_Loss: 0.2750389873981476, Test_Loss: 0.9112555384635925\n",
      "Epoch: 19, Train_Loss: 0.3706052005290985, Test_Loss: 0.7502512335777283 *\n",
      "Epoch: 19, Train_Loss: 0.3943125605583191, Test_Loss: 0.5714950561523438 *\n",
      "Epoch: 19, Train_Loss: 0.30545830726623535, Test_Loss: 0.31393542885780334 *\n",
      "Epoch: 19, Train_Loss: 0.2603219151496887, Test_Loss: 2.206120729446411\n",
      "Epoch: 19, Train_Loss: 0.26634839177131653, Test_Loss: 4.745818614959717\n",
      "Epoch: 19, Train_Loss: 0.255326509475708, Test_Loss: 0.4401592016220093 *\n",
      "Epoch: 19, Train_Loss: 0.2541118860244751, Test_Loss: 0.45943349599838257\n",
      "Epoch: 19, Train_Loss: 0.2541913688182831, Test_Loss: 0.512837290763855\n",
      "Epoch: 19, Train_Loss: 0.24998055398464203, Test_Loss: 0.30586978793144226 *\n",
      "Epoch: 19, Train_Loss: 0.2503085136413574, Test_Loss: 0.3886524736881256\n",
      "Epoch: 19, Train_Loss: 0.2500493824481964, Test_Loss: 0.7889409065246582\n",
      "Epoch: 19, Train_Loss: 0.2525081932544708, Test_Loss: 0.5509499311447144 *\n",
      "Epoch: 19, Train_Loss: 0.2503427565097809, Test_Loss: 0.28954753279685974 *\n",
      "Epoch: 19, Train_Loss: 0.25056809186935425, Test_Loss: 0.45473045110702515\n",
      "Epoch: 19, Train_Loss: 0.2787153720855713, Test_Loss: 0.4379155933856964 *\n",
      "Epoch: 19, Train_Loss: 0.2891334295272827, Test_Loss: 0.8863275051116943\n",
      "Epoch: 19, Train_Loss: 0.34371310472488403, Test_Loss: 0.5741170644760132 *\n",
      "Epoch: 19, Train_Loss: 0.3569810092449188, Test_Loss: 0.8434612154960632\n",
      "Epoch: 19, Train_Loss: 0.4153125286102295, Test_Loss: 0.6449651718139648 *\n",
      "Epoch: 19, Train_Loss: 3.2936556339263916, Test_Loss: 0.2880965769290924 *\n",
      "Epoch: 19, Train_Loss: 5.197391510009766, Test_Loss: 0.351578027009964\n",
      "Epoch: 19, Train_Loss: 0.2757345139980316, Test_Loss: 0.32743120193481445 *\n",
      "Epoch: 19, Train_Loss: 0.33254873752593994, Test_Loss: 0.8592423796653748\n",
      "Epoch: 19, Train_Loss: 0.3781440854072571, Test_Loss: 0.3428587317466736 *\n",
      "Epoch: 19, Train_Loss: 0.39140546321868896, Test_Loss: 0.47534650564193726\n",
      "Epoch: 19, Train_Loss: 0.3213399052619934, Test_Loss: 0.39585983753204346 *\n",
      "Epoch: 19, Train_Loss: 0.4029373526573181, Test_Loss: 0.6249697208404541\n",
      "Epoch: 19, Train_Loss: 0.3202771544456482, Test_Loss: 0.7848328351974487\n",
      "Epoch: 19, Train_Loss: 0.3722320795059204, Test_Loss: 0.3912729024887085 *\n",
      "Epoch: 19, Train_Loss: 0.34199708700180054, Test_Loss: 0.31580787897109985 *\n",
      "Epoch: 19, Train_Loss: 0.27782997488975525, Test_Loss: 0.5214351415634155\n",
      "Epoch: 19, Train_Loss: 0.2663577198982239, Test_Loss: 0.4219256043434143 *\n",
      "Epoch: 19, Train_Loss: 0.2728748321533203, Test_Loss: 0.34816843271255493 *\n",
      "Epoch: 19, Train_Loss: 0.3689793646335602, Test_Loss: 0.49117958545684814\n",
      "Epoch: 19, Train_Loss: 0.3492876887321472, Test_Loss: 0.3945559859275818 *\n",
      "Epoch: 19, Train_Loss: 0.27633652091026306, Test_Loss: 4.630043983459473\n",
      "Epoch: 19, Train_Loss: 0.2910090386867523, Test_Loss: 2.340304374694824 *\n",
      "Epoch: 19, Train_Loss: 0.2546108067035675, Test_Loss: 0.2633328437805176 *\n",
      "Epoch: 19, Train_Loss: 0.26443368196487427, Test_Loss: 0.26335805654525757\n",
      "Epoch: 19, Train_Loss: 0.278513103723526, Test_Loss: 0.25753629207611084 *\n",
      "Epoch: 19, Train_Loss: 0.28633254766464233, Test_Loss: 0.26750287413597107\n",
      "Epoch: 19, Train_Loss: 0.25746235251426697, Test_Loss: 0.2692212760448456\n",
      "Epoch: 19, Train_Loss: 0.24859042465686798, Test_Loss: 0.37390464544296265\n",
      "Epoch: 19, Train_Loss: 0.2677682042121887, Test_Loss: 0.39216160774230957\n",
      "Epoch: 19, Train_Loss: 2.4889986515045166, Test_Loss: 0.2511674463748932 *\n",
      "Epoch: 19, Train_Loss: 2.9729268550872803, Test_Loss: 0.2869294285774231\n",
      "Epoch: 19, Train_Loss: 0.25165292620658875, Test_Loss: 0.2677404582500458 *\n",
      "Epoch: 19, Train_Loss: 0.2494044005870819, Test_Loss: 0.2673724889755249 *\n",
      "Epoch: 19, Train_Loss: 0.2539820671081543, Test_Loss: 0.26310113072395325 *\n",
      "Epoch: 19, Train_Loss: 0.24914640188217163, Test_Loss: 0.2825811505317688\n",
      "Epoch: 19, Train_Loss: 0.261545866727829, Test_Loss: 0.31494563817977905\n",
      "Epoch: 19, Train_Loss: 0.2498847097158432, Test_Loss: 0.39682334661483765\n",
      "Epoch: 19, Train_Loss: 0.2698899805545807, Test_Loss: 0.4145212173461914\n",
      "Epoch: 19, Train_Loss: 0.26141461730003357, Test_Loss: 0.26688897609710693 *\n",
      "Epoch: 19, Train_Loss: 0.3166670799255371, Test_Loss: 0.2808416485786438\n",
      "Epoch: 19, Train_Loss: 0.24952305853366852, Test_Loss: 0.31465959548950195\n",
      "Epoch: 19, Train_Loss: 0.24865034222602844, Test_Loss: 0.28933480381965637 *\n",
      "Epoch: 19, Train_Loss: 0.2556345760822296, Test_Loss: 0.28811389207839966 *\n",
      "Epoch: 19, Train_Loss: 0.25929176807403564, Test_Loss: 0.271822065114975 *\n",
      "Epoch: 19, Train_Loss: 0.24976898729801178, Test_Loss: 0.2755703330039978\n",
      "Epoch: 19, Train_Loss: 0.25172412395477295, Test_Loss: 0.3087966740131378\n",
      "Epoch: 19, Train_Loss: 0.27396002411842346, Test_Loss: 0.30687353014945984 *\n",
      "Epoch: 19, Train_Loss: 0.271718829870224, Test_Loss: 0.29643118381500244 *\n",
      "Epoch: 19, Train_Loss: 0.2488510012626648, Test_Loss: 0.2790432572364807 *\n",
      "Epoch: 19, Train_Loss: 0.24920205771923065, Test_Loss: 0.2867434322834015\n",
      "Epoch: 19, Train_Loss: 0.2824644446372986, Test_Loss: 0.2528010904788971 *\n",
      "Epoch: 19, Train_Loss: 0.26888507604599, Test_Loss: 0.2776276469230652\n",
      "Epoch: 19, Train_Loss: 0.2815110683441162, Test_Loss: 0.35044950246810913\n",
      "Epoch: 19, Train_Loss: 0.28856709599494934, Test_Loss: 0.3738672733306885\n",
      "Epoch: 19, Train_Loss: 0.3517928719520569, Test_Loss: 0.3597753643989563 *\n",
      "Epoch: 19, Train_Loss: 0.31585684418678284, Test_Loss: 0.29672181606292725 *\n",
      "Epoch: 19, Train_Loss: 0.2661456763744354, Test_Loss: 0.29849326610565186\n",
      "Epoch: 19, Train_Loss: 0.30034562945365906, Test_Loss: 0.28077611327171326 *\n",
      "Epoch: 19, Train_Loss: 0.35621070861816406, Test_Loss: 0.30575910210609436\n",
      "Epoch: 19, Train_Loss: 0.32877710461616516, Test_Loss: 0.5410206317901611\n",
      "Epoch: 19, Train_Loss: 0.3275958299636841, Test_Loss: 0.5693813562393188\n",
      "Epoch: 19, Train_Loss: 0.2482646256685257, Test_Loss: 0.6279473900794983\n",
      "Epoch: 19, Train_Loss: 0.2504560351371765, Test_Loss: 0.31430792808532715 *\n",
      "Epoch: 19, Train_Loss: 0.2510954737663269, Test_Loss: 0.29287323355674744 *\n",
      "Epoch: 19, Train_Loss: 0.2521238923072815, Test_Loss: 0.2939567565917969\n",
      "Epoch: 19, Train_Loss: 0.2503919303417206, Test_Loss: 0.26643773913383484 *\n",
      "Epoch: 19, Train_Loss: 3.3538007736206055, Test_Loss: 0.2691275477409363\n",
      "Epoch: 19, Train_Loss: 2.0828065872192383, Test_Loss: 0.2647086977958679 *\n",
      "Epoch: 19, Train_Loss: 0.24770508706569672, Test_Loss: 0.28134098649024963\n",
      "Epoch: 19, Train_Loss: 0.2601056694984436, Test_Loss: 0.24999192357063293 *\n",
      "Epoch: 19, Train_Loss: 0.2517491579055786, Test_Loss: 0.3381959795951843\n",
      "Epoch: 19, Train_Loss: 0.24888719618320465, Test_Loss: 0.4698371887207031\n",
      "Epoch: 19, Train_Loss: 0.2489975541830063, Test_Loss: 0.4938007593154907\n",
      "Epoch: 19, Train_Loss: 0.2487613558769226, Test_Loss: 0.49305427074432373 *\n",
      "Epoch: 19, Train_Loss: 0.24710969626903534, Test_Loss: 0.27799880504608154 *\n",
      "Epoch: 19, Train_Loss: 0.24920961260795593, Test_Loss: 0.2772718667984009 *\n",
      "Epoch: 19, Train_Loss: 0.26129814982414246, Test_Loss: 0.27726006507873535 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train_Loss: 0.29786112904548645, Test_Loss: 0.27732765674591064\n",
      "Model saved at location save_new\\model.ckpt at epoch 19\n",
      "Epoch: 19, Train_Loss: 0.2996445894241333, Test_Loss: 0.28412336111068726\n",
      "Epoch: 19, Train_Loss: 0.2942088544368744, Test_Loss: 2.96362042427063\n",
      "Epoch: 19, Train_Loss: 0.2672785818576813, Test_Loss: 2.9767532348632812\n",
      "Epoch: 19, Train_Loss: 0.2655717730522156, Test_Loss: 0.2774491310119629 *\n",
      "Epoch: 19, Train_Loss: 0.49722617864608765, Test_Loss: 0.2605748176574707 *\n",
      "Epoch: 19, Train_Loss: 0.48992276191711426, Test_Loss: 0.26459935307502747\n",
      "Epoch: 19, Train_Loss: 0.4849647283554077, Test_Loss: 0.26940444111824036\n",
      "Epoch: 19, Train_Loss: 0.3223356306552887, Test_Loss: 0.25181981921195984 *\n",
      "Epoch: 19, Train_Loss: 0.24783489108085632, Test_Loss: 0.2594534754753113\n",
      "Epoch: 19, Train_Loss: 0.24610048532485962, Test_Loss: 0.2482566386461258 *\n",
      "Epoch: 19, Train_Loss: 0.25222280621528625, Test_Loss: 0.24656884372234344 *\n",
      "Epoch: 19, Train_Loss: 0.2624770402908325, Test_Loss: 0.24936971068382263\n",
      "Epoch: 19, Train_Loss: 0.2688678801059723, Test_Loss: 0.24835160374641418 *\n",
      "Epoch: 19, Train_Loss: 0.26489055156707764, Test_Loss: 0.2677450478076935\n",
      "Epoch: 19, Train_Loss: 0.2463209480047226, Test_Loss: 0.27238044142723083\n",
      "Epoch: 19, Train_Loss: 0.24724401533603668, Test_Loss: 0.2682015895843506 *\n",
      "Epoch: 19, Train_Loss: 0.2673751711845398, Test_Loss: 0.25494518876075745 *\n",
      "Epoch: 19, Train_Loss: 0.340040385723114, Test_Loss: 0.2479247897863388 *\n",
      "Epoch: 19, Train_Loss: 0.4264545142650604, Test_Loss: 0.2526158392429352\n",
      "Epoch: 19, Train_Loss: 0.3410630226135254, Test_Loss: 0.2803945243358612\n",
      "Epoch: 19, Train_Loss: 0.34602090716362, Test_Loss: 0.2694966197013855 *\n",
      "Epoch: 19, Train_Loss: 0.3285273015499115, Test_Loss: 0.2534281313419342 *\n",
      "Epoch: 19, Train_Loss: 0.35932645201683044, Test_Loss: 0.24823550879955292 *\n",
      "Epoch: 19, Train_Loss: 0.2583967447280884, Test_Loss: 0.26476749777793884\n",
      "Epoch: 19, Train_Loss: 0.34925079345703125, Test_Loss: 0.2682613134384155\n",
      "Epoch: 19, Train_Loss: 0.30722445249557495, Test_Loss: 0.29296720027923584\n",
      "Epoch: 19, Train_Loss: 0.4934425354003906, Test_Loss: 0.2516094744205475 *\n",
      "Epoch: 19, Train_Loss: 0.2542555630207062, Test_Loss: 0.2510247528553009 *\n",
      "Epoch: 19, Train_Loss: 0.5935399532318115, Test_Loss: 0.2712906301021576\n",
      "Epoch: 19, Train_Loss: 2.8339433670043945, Test_Loss: 0.2628616988658905 *\n",
      "Epoch: 19, Train_Loss: 0.2983696460723877, Test_Loss: 0.24829266965389252 *\n",
      "Epoch: 19, Train_Loss: 0.30867522954940796, Test_Loss: 0.32880687713623047\n",
      "Epoch: 19, Train_Loss: 0.2804696261882782, Test_Loss: 0.2876253128051758 *\n",
      "Epoch: 19, Train_Loss: 0.2633739113807678, Test_Loss: 5.015240669250488\n",
      "Epoch: 19, Train_Loss: 0.24895888566970825, Test_Loss: 0.872663676738739 *\n",
      "Epoch: 19, Train_Loss: 0.2521780729293823, Test_Loss: 0.24809296429157257 *\n",
      "Epoch: 19, Train_Loss: 0.3379290699958801, Test_Loss: 0.26919469237327576\n",
      "Epoch: 19, Train_Loss: 0.3364512324333191, Test_Loss: 0.29817941784858704\n",
      "Epoch: 19, Train_Loss: 0.3078683018684387, Test_Loss: 0.29196029901504517 *\n",
      "Epoch: 19, Train_Loss: 0.2703776955604553, Test_Loss: 0.25457391142845154 *\n",
      "Epoch: 19, Train_Loss: 0.25896820425987244, Test_Loss: 0.3307972550392151\n",
      "Epoch: 19, Train_Loss: 0.25600460171699524, Test_Loss: 0.3019099831581116 *\n",
      "Epoch: 19, Train_Loss: 0.26354411244392395, Test_Loss: 0.25140562653541565 *\n",
      "Epoch: 19, Train_Loss: 0.2644959092140198, Test_Loss: 0.2698005437850952\n",
      "Epoch: 19, Train_Loss: 0.29273301362991333, Test_Loss: 0.27127426862716675\n",
      "Epoch: 19, Train_Loss: 0.26211676001548767, Test_Loss: 0.25557810068130493 *\n",
      "Epoch: 19, Train_Loss: 0.2467874139547348, Test_Loss: 0.25732460618019104\n",
      "Epoch: 19, Train_Loss: 0.2634299695491791, Test_Loss: 0.34639424085617065\n",
      "Epoch: 19, Train_Loss: 0.2648642957210541, Test_Loss: 0.2864779531955719 *\n",
      "Epoch: 19, Train_Loss: 0.26127174496650696, Test_Loss: 0.3218768537044525\n",
      "Epoch: 19, Train_Loss: 0.25006040930747986, Test_Loss: 0.2831951379776001 *\n",
      "Epoch: 19, Train_Loss: 0.2470627874135971, Test_Loss: 0.31507349014282227\n",
      "Epoch: 19, Train_Loss: 0.24523422122001648, Test_Loss: 0.2706540822982788 *\n",
      "Epoch: 19, Train_Loss: 0.24692249298095703, Test_Loss: 0.2682496905326843 *\n",
      "Epoch: 19, Train_Loss: 0.2480648159980774, Test_Loss: 0.27703675627708435\n",
      "Epoch: 19, Train_Loss: 0.25010669231414795, Test_Loss: 0.2779303193092346\n",
      "Epoch: 19, Train_Loss: 0.2566995918750763, Test_Loss: 0.27866634726524353\n",
      "Epoch: 19, Train_Loss: 0.247161865234375, Test_Loss: 0.2731541693210602 *\n",
      "Epoch: 19, Train_Loss: 0.2457454651594162, Test_Loss: 0.2585529088973999 *\n",
      "Epoch: 19, Train_Loss: 0.24698489904403687, Test_Loss: 0.2600301504135132\n",
      "Epoch: 19, Train_Loss: 0.2532375156879425, Test_Loss: 0.25593048334121704 *\n",
      "Epoch: 19, Train_Loss: 0.25632086396217346, Test_Loss: 0.25338566303253174 *\n",
      "Epoch: 19, Train_Loss: 0.25872352719306946, Test_Loss: 0.25832828879356384\n",
      "Epoch: 19, Train_Loss: 0.27305203676223755, Test_Loss: 0.32590171694755554\n",
      "Epoch: 19, Train_Loss: 0.2540861964225769, Test_Loss: 0.31767165660858154 *\n",
      "Epoch: 19, Train_Loss: 0.26171645522117615, Test_Loss: 0.49427539110183716\n",
      "Epoch: 19, Train_Loss: 0.2511742115020752, Test_Loss: 0.6928732991218567\n",
      "Epoch: 19, Train_Loss: 0.24811764061450958, Test_Loss: 0.46400749683380127 *\n",
      "Epoch: 19, Train_Loss: 0.26844581961631775, Test_Loss: 0.30957895517349243 *\n",
      "Epoch: 19, Train_Loss: 0.265543669462204, Test_Loss: 0.25838202238082886 *\n",
      "Epoch: 19, Train_Loss: 0.24847646057605743, Test_Loss: 0.25422272086143494 *\n",
      "Epoch: 19, Train_Loss: 0.25026795268058777, Test_Loss: 0.2997857928276062\n",
      "Epoch: 19, Train_Loss: 0.2562391757965088, Test_Loss: 0.5807127952575684\n",
      "Epoch: 19, Train_Loss: 0.26782724261283875, Test_Loss: 0.4974108338356018 *\n",
      "Epoch: 19, Train_Loss: 0.2914939820766449, Test_Loss: 0.4779183864593506 *\n",
      "Epoch: 19, Train_Loss: 0.3151851296424866, Test_Loss: 0.347217857837677 *\n",
      "Epoch: 19, Train_Loss: 0.24891112744808197, Test_Loss: 0.255394846200943 *\n",
      "Epoch: 19, Train_Loss: 0.2715406119823456, Test_Loss: 0.25179430842399597 *\n",
      "Epoch: 19, Train_Loss: 0.3126524090766907, Test_Loss: 0.24511802196502686 *\n",
      "Epoch: 19, Train_Loss: 0.24799582362174988, Test_Loss: 0.261589378118515\n",
      "Epoch: 19, Train_Loss: 0.253368616104126, Test_Loss: 0.2581309974193573 *\n",
      "Epoch: 19, Train_Loss: 0.25763407349586487, Test_Loss: 0.2701036334037781\n",
      "Epoch: 19, Train_Loss: 0.3091750741004944, Test_Loss: 0.24795812368392944 *\n",
      "Epoch: 19, Train_Loss: 0.3240201771259308, Test_Loss: 0.3945857882499695\n",
      "Epoch: 19, Train_Loss: 0.28406161069869995, Test_Loss: 0.6065329909324646\n",
      "Epoch: 19, Train_Loss: 0.2629953920841217, Test_Loss: 0.3549230098724365 *\n",
      "Epoch: 19, Train_Loss: 0.25339633226394653, Test_Loss: 0.46416521072387695\n",
      "Epoch: 19, Train_Loss: 0.26618897914886475, Test_Loss: 0.27063828706741333 *\n",
      "Epoch: 19, Train_Loss: 0.24497266113758087, Test_Loss: 0.27103856205940247\n",
      "Epoch: 19, Train_Loss: 0.2479855716228485, Test_Loss: 0.27080169320106506 *\n",
      "Epoch: 19, Train_Loss: 0.2577528953552246, Test_Loss: 0.26851218938827515 *\n",
      "Epoch: 19, Train_Loss: 0.26910755038261414, Test_Loss: 0.2763114869594574\n",
      "Epoch: 19, Train_Loss: 0.3202233612537384, Test_Loss: 4.435281753540039\n",
      "Epoch: 19, Train_Loss: 0.2766447067260742, Test_Loss: 1.6047841310501099 *\n",
      "Epoch: 19, Train_Loss: 0.2925281524658203, Test_Loss: 0.27194830775260925 *\n",
      "Epoch: 19, Train_Loss: 0.2560388445854187, Test_Loss: 0.25807464122772217 *\n",
      "Epoch: 19, Train_Loss: 0.27179211378097534, Test_Loss: 0.2625716030597687\n",
      "Epoch: 19, Train_Loss: 0.2551736533641815, Test_Loss: 0.261566698551178 *\n",
      "Epoch: 19, Train_Loss: 0.47195297479629517, Test_Loss: 0.2536945044994354 *\n",
      "Epoch: 19, Train_Loss: 0.35419878363609314, Test_Loss: 0.26016828417778015\n",
      "Model saved at location save_new\\model.ckpt at epoch 19\n",
      "Epoch: 19, Train_Loss: 0.25660911202430725, Test_Loss: 0.24680829048156738 *\n",
      "Epoch: 19, Train_Loss: 0.2704257071018219, Test_Loss: 0.24554117023944855 *\n",
      "Epoch: 19, Train_Loss: 0.24393510818481445, Test_Loss: 0.2528368830680847\n",
      "Epoch: 19, Train_Loss: 0.2452453076839447, Test_Loss: 0.2630809247493744\n",
      "Epoch: 19, Train_Loss: 0.24647438526153564, Test_Loss: 0.2687138319015503\n",
      "Epoch: 19, Train_Loss: 0.24599123001098633, Test_Loss: 0.2703254520893097\n",
      "Epoch: 19, Train_Loss: 0.24650788307189941, Test_Loss: 0.2715389132499695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train_Loss: 0.2611842155456543, Test_Loss: 0.24731506407260895 *\n",
      "Epoch: 19, Train_Loss: 0.2482876032590866, Test_Loss: 0.24604031443595886 *\n",
      "Epoch: 19, Train_Loss: 0.24917326867580414, Test_Loss: 0.2518075108528137\n",
      "Epoch: 19, Train_Loss: 0.2519350051879883, Test_Loss: 0.28815674781799316\n",
      "Epoch: 19, Train_Loss: 0.2445729821920395, Test_Loss: 0.26063111424446106 *\n",
      "Epoch: 19, Train_Loss: 0.24426674842834473, Test_Loss: 0.2477395087480545 *\n",
      "Epoch: 19, Train_Loss: 0.24676111340522766, Test_Loss: 0.2500550448894501\n",
      "Epoch: 19, Train_Loss: 0.27479287981987, Test_Loss: 0.2580468952655792\n",
      "Epoch: 19, Train_Loss: 0.27635544538497925, Test_Loss: 0.2604791224002838\n",
      "Epoch: 19, Train_Loss: 0.2539732754230499, Test_Loss: 0.2786807715892792\n",
      "Epoch: 19, Train_Loss: 0.2589983642101288, Test_Loss: 0.24491649866104126 *\n",
      "Epoch: 19, Train_Loss: 0.28139588236808777, Test_Loss: 0.2502327561378479\n",
      "Epoch: 19, Train_Loss: 0.2750539183616638, Test_Loss: 0.25875067710876465\n",
      "Epoch: 19, Train_Loss: 0.24547463655471802, Test_Loss: 0.2558896243572235 *\n",
      "Epoch: 19, Train_Loss: 0.26923006772994995, Test_Loss: 0.24451880156993866 *\n",
      "Epoch: 19, Train_Loss: 0.2574042081832886, Test_Loss: 0.33107149600982666\n",
      "Epoch: 19, Train_Loss: 0.2539260685443878, Test_Loss: 0.3549732565879822\n",
      "Epoch: 19, Train_Loss: 0.2452828288078308, Test_Loss: 5.615091800689697\n",
      "Epoch: 19, Train_Loss: 0.2714593708515167, Test_Loss: 0.2971753478050232 *\n",
      "Epoch: 19, Train_Loss: 0.30623018741607666, Test_Loss: 0.24346137046813965 *\n",
      "Epoch: 19, Train_Loss: 2.5123846530914307, Test_Loss: 0.27195969223976135\n",
      "Epoch: 19, Train_Loss: 3.3350582122802734, Test_Loss: 0.2824273705482483\n",
      "Epoch: 19, Train_Loss: 0.2762939929962158, Test_Loss: 0.27931731939315796 *\n",
      "Epoch: 19, Train_Loss: 0.2457238733768463, Test_Loss: 0.24855799973011017 *\n",
      "Epoch: 19, Train_Loss: 0.29681551456451416, Test_Loss: 0.3714587688446045\n",
      "Epoch: 19, Train_Loss: 0.3659396767616272, Test_Loss: 0.29128119349479675 *\n",
      "Epoch: 19, Train_Loss: 0.26612550020217896, Test_Loss: 0.24394597113132477 *\n",
      "Epoch: 19, Train_Loss: 0.245158851146698, Test_Loss: 0.2755925953388214\n",
      "Epoch: 19, Train_Loss: 0.26284080743789673, Test_Loss: 0.2609286904335022 *\n",
      "Epoch: 19, Train_Loss: 0.29627010226249695, Test_Loss: 0.24678517878055573 *\n",
      "Epoch: 19, Train_Loss: 0.25540149211883545, Test_Loss: 0.2744377851486206\n",
      "Epoch: 19, Train_Loss: 0.2534065842628479, Test_Loss: 0.29442018270492554\n",
      "Epoch: 19, Train_Loss: 0.8301208019256592, Test_Loss: 0.29072147607803345 *\n",
      "Epoch: 19, Train_Loss: 0.9541593790054321, Test_Loss: 0.3224811553955078\n",
      "Epoch: 19, Train_Loss: 0.5357758402824402, Test_Loss: 0.2833673357963562 *\n",
      "Epoch: 19, Train_Loss: 0.3552822768688202, Test_Loss: 0.3116729259490967\n",
      "Epoch: 19, Train_Loss: 1.2255054712295532, Test_Loss: 0.2530028522014618 *\n",
      "Epoch: 19, Train_Loss: 1.4634534120559692, Test_Loss: 0.24478912353515625 *\n",
      "Epoch: 19, Train_Loss: 0.30516767501831055, Test_Loss: 0.25475141406059265\n",
      "Epoch: 19, Train_Loss: 0.24483351409435272, Test_Loss: 0.25796476006507874\n",
      "Epoch: 19, Train_Loss: 0.3810303509235382, Test_Loss: 0.25366514921188354 *\n",
      "Epoch: 19, Train_Loss: 0.6965810060501099, Test_Loss: 0.24810157716274261 *\n",
      "Epoch: 19, Train_Loss: 0.7727006077766418, Test_Loss: 0.25401780009269714\n",
      "Epoch: 19, Train_Loss: 0.25269100069999695, Test_Loss: 0.24819044768810272 *\n",
      "Epoch: 19, Train_Loss: 0.2674103081226349, Test_Loss: 0.2507665753364563\n",
      "Epoch: 19, Train_Loss: 0.2696007490158081, Test_Loss: 0.2847437262535095\n",
      "Epoch: 19, Train_Loss: 0.4269194006919861, Test_Loss: 0.2529318332672119 *\n",
      "Epoch: 20, Train_Loss: 0.2704375982284546, Test_Loss: 0.25995320081710815 *\n",
      "Epoch: 20, Train_Loss: 0.31892699003219604, Test_Loss: 0.3026883006095886\n",
      "Epoch: 20, Train_Loss: 0.2725497782230377, Test_Loss: 0.45959335565567017\n",
      "Epoch: 20, Train_Loss: 0.2635592222213745, Test_Loss: 0.3765223026275635 *\n",
      "Epoch: 20, Train_Loss: 0.3456845283508301, Test_Loss: 0.2976764738559723 *\n",
      "Epoch: 20, Train_Loss: 0.36714860796928406, Test_Loss: 0.2996092438697815\n",
      "Epoch: 20, Train_Loss: 0.2771712839603424, Test_Loss: 0.27733248472213745 *\n",
      "Epoch: 20, Train_Loss: 0.3032320439815521, Test_Loss: 0.24656283855438232 *\n",
      "Epoch: 20, Train_Loss: 0.30836939811706543, Test_Loss: 0.2691444754600525\n",
      "Epoch: 20, Train_Loss: 0.26965826749801636, Test_Loss: 0.42550432682037354\n",
      "Epoch: 20, Train_Loss: 0.29769784212112427, Test_Loss: 0.2935081124305725 *\n",
      "Epoch: 20, Train_Loss: 0.3319241404533386, Test_Loss: 0.39538535475730896\n",
      "Epoch: 20, Train_Loss: 0.27982693910598755, Test_Loss: 0.32685065269470215 *\n",
      "Epoch: 20, Train_Loss: 0.2878866493701935, Test_Loss: 0.2851406931877136 *\n",
      "Epoch: 20, Train_Loss: 0.32743197679519653, Test_Loss: 0.2589178681373596 *\n",
      "Epoch: 20, Train_Loss: 0.27269893884658813, Test_Loss: 0.2575226426124573 *\n",
      "Epoch: 20, Train_Loss: 0.2632134258747101, Test_Loss: 0.3080686330795288\n",
      "Epoch: 20, Train_Loss: 0.24334761500358582, Test_Loss: 0.25496912002563477 *\n",
      "Epoch: 20, Train_Loss: 0.24497956037521362, Test_Loss: 0.26897644996643066\n",
      "Epoch: 20, Train_Loss: 0.24507564306259155, Test_Loss: 0.25816619396209717 *\n",
      "Epoch: 20, Train_Loss: 0.2519274950027466, Test_Loss: 0.5102338790893555\n",
      "Epoch: 20, Train_Loss: 0.26565754413604736, Test_Loss: 0.6000076532363892\n",
      "Epoch: 20, Train_Loss: 0.2604384124279022, Test_Loss: 0.4225490093231201 *\n",
      "Epoch: 20, Train_Loss: 0.25653067231178284, Test_Loss: 0.8526419997215271\n",
      "Epoch: 20, Train_Loss: 0.42643120884895325, Test_Loss: 0.5895906686782837 *\n",
      "Epoch: 20, Train_Loss: 0.44126349687576294, Test_Loss: 0.5897507071495056\n",
      "Epoch: 20, Train_Loss: 0.2521173059940338, Test_Loss: 0.5870205163955688 *\n",
      "Epoch: 20, Train_Loss: 0.2898598909378052, Test_Loss: 0.5281574726104736 *\n",
      "Epoch: 20, Train_Loss: 0.2804492115974426, Test_Loss: 0.3568789064884186 *\n",
      "Epoch: 20, Train_Loss: 0.2823698818683624, Test_Loss: 5.573596477508545\n",
      "Epoch: 20, Train_Loss: 0.44126036763191223, Test_Loss: 0.5879826545715332 *\n",
      "Epoch: 20, Train_Loss: 0.406083345413208, Test_Loss: 0.33137059211730957 *\n",
      "Epoch: 20, Train_Loss: 0.46233344078063965, Test_Loss: 0.30007418990135193 *\n",
      "Epoch: 20, Train_Loss: 0.2892790138721466, Test_Loss: 0.2671721577644348 *\n",
      "Epoch: 20, Train_Loss: 0.3317258059978485, Test_Loss: 0.254738450050354 *\n",
      "Epoch: 20, Train_Loss: 0.2594268023967743, Test_Loss: 0.331939697265625\n",
      "Epoch: 20, Train_Loss: 0.2589867115020752, Test_Loss: 0.32892635464668274 *\n",
      "Epoch: 20, Train_Loss: 0.34794896841049194, Test_Loss: 0.26212015748023987 *\n",
      "Epoch: 20, Train_Loss: 0.7597713470458984, Test_Loss: 0.24549859762191772 *\n",
      "Epoch: 20, Train_Loss: 0.632548987865448, Test_Loss: 0.25744518637657166\n",
      "Epoch: 20, Train_Loss: 0.2653495669364929, Test_Loss: 0.4100158214569092\n",
      "Epoch: 20, Train_Loss: 0.29260993003845215, Test_Loss: 0.31515997648239136 *\n",
      "Epoch: 20, Train_Loss: 0.24820463359355927, Test_Loss: 0.25500521063804626 *\n",
      "Epoch: 20, Train_Loss: 0.3903600573539734, Test_Loss: 0.27318283915519714\n",
      "Epoch: 20, Train_Loss: 0.5877817869186401, Test_Loss: 0.26282113790512085 *\n",
      "Epoch: 20, Train_Loss: 0.24857427179813385, Test_Loss: 0.2678544819355011\n",
      "Epoch: 20, Train_Loss: 0.3914225697517395, Test_Loss: 0.2920857071876526\n",
      "Epoch: 20, Train_Loss: 0.26253607869148254, Test_Loss: 0.3341600298881531\n",
      "Epoch: 20, Train_Loss: 0.27998536825180054, Test_Loss: 0.2641696035861969 *\n",
      "Epoch: 20, Train_Loss: 0.30156296491622925, Test_Loss: 0.2656804323196411\n",
      "Epoch: 20, Train_Loss: 0.3306923806667328, Test_Loss: 0.2638663947582245 *\n",
      "Epoch: 20, Train_Loss: 0.32673874497413635, Test_Loss: 0.26422590017318726\n",
      "Epoch: 20, Train_Loss: 0.2986084818840027, Test_Loss: 0.2757856249809265\n",
      "Epoch: 20, Train_Loss: 0.25357821583747864, Test_Loss: 0.3003394901752472\n",
      "Epoch: 20, Train_Loss: 0.32894113659858704, Test_Loss: 0.24315817654132843 *\n",
      "Epoch: 20, Train_Loss: 0.2813783586025238, Test_Loss: 0.2574925422668457\n",
      "Epoch: 20, Train_Loss: 0.2636767625808716, Test_Loss: 0.2726561427116394\n",
      "Epoch: 20, Train_Loss: 0.2572791576385498, Test_Loss: 0.26675429940223694 *\n",
      "Epoch: 20, Train_Loss: 0.2607424855232239, Test_Loss: 0.24390113353729248 *\n",
      "Epoch: 20, Train_Loss: 0.3142438530921936, Test_Loss: 0.37573373317718506\n",
      "Epoch: 20, Train_Loss: 0.39761480689048767, Test_Loss: 1.0214064121246338\n",
      "Epoch: 20, Train_Loss: 0.45565882325172424, Test_Loss: 4.310495376586914\n",
      "Epoch: 20, Train_Loss: 0.6259972453117371, Test_Loss: 0.2528291344642639 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train_Loss: 0.5134351253509521, Test_Loss: 0.2575925588607788\n",
      "Epoch: 20, Train_Loss: 0.4441642165184021, Test_Loss: 0.29304465651512146\n",
      "Epoch: 20, Train_Loss: 0.3198852837085724, Test_Loss: 0.2495633363723755 *\n",
      "Epoch: 20, Train_Loss: 0.2846388518810272, Test_Loss: 0.2560412883758545\n",
      "Epoch: 20, Train_Loss: 0.2551044821739197, Test_Loss: 0.2750597298145294\n",
      "Epoch: 20, Train_Loss: 0.253097265958786, Test_Loss: 0.30749231576919556\n",
      "Epoch: 20, Train_Loss: 0.34883445501327515, Test_Loss: 0.2627984285354614 *\n",
      "Epoch: 20, Train_Loss: 0.5510176420211792, Test_Loss: 0.24752719700336456 *\n",
      "Epoch: 20, Train_Loss: 0.6444000005722046, Test_Loss: 0.26653575897216797\n",
      "Epoch: 20, Train_Loss: 1.0716789960861206, Test_Loss: 0.31219521164894104\n",
      "Epoch: 20, Train_Loss: 1.237300157546997, Test_Loss: 0.26611417531967163 *\n",
      "Epoch: 20, Train_Loss: 0.40698349475860596, Test_Loss: 0.3068266212940216\n",
      "Epoch: 20, Train_Loss: 0.4726720154285431, Test_Loss: 0.27646154165267944 *\n",
      "Epoch: 20, Train_Loss: 0.24273142218589783, Test_Loss: 0.3459804654121399\n",
      "Epoch: 20, Train_Loss: 0.29052406549453735, Test_Loss: 0.25651785731315613 *\n",
      "Epoch: 20, Train_Loss: 0.5895530581474304, Test_Loss: 0.28876742720603943\n",
      "Epoch: 20, Train_Loss: 0.906260073184967, Test_Loss: 0.3549346625804901\n",
      "Epoch: 20, Train_Loss: 0.2849186658859253, Test_Loss: 0.25125813484191895 *\n",
      "Epoch: 20, Train_Loss: 0.2721766233444214, Test_Loss: 0.3109363615512848\n",
      "Epoch: 20, Train_Loss: 0.28676819801330566, Test_Loss: 0.2923806607723236 *\n",
      "Epoch: 20, Train_Loss: 0.45164257287979126, Test_Loss: 0.304099977016449\n",
      "Epoch: 20, Train_Loss: 0.45679590106010437, Test_Loss: 0.27583369612693787 *\n",
      "Epoch: 20, Train_Loss: 0.48148903250694275, Test_Loss: 0.2708140015602112 *\n",
      "Epoch: 20, Train_Loss: 0.392469584941864, Test_Loss: 0.31247156858444214\n",
      "Epoch: 20, Train_Loss: 0.5269001722335815, Test_Loss: 0.2981718182563782 *\n",
      "Epoch: 20, Train_Loss: 0.2469562292098999, Test_Loss: 0.3082258999347687\n",
      "Epoch: 20, Train_Loss: 0.26291704177856445, Test_Loss: 0.3749791085720062\n",
      "Epoch: 20, Train_Loss: 0.25196754932403564, Test_Loss: 0.2807645797729492 *\n",
      "Epoch: 20, Train_Loss: 0.3067725896835327, Test_Loss: 0.25011828541755676 *\n",
      "Epoch: 20, Train_Loss: 0.2667710781097412, Test_Loss: 0.31721556186676025\n",
      "Epoch: 20, Train_Loss: 0.29294276237487793, Test_Loss: 0.41405636072158813\n",
      "Epoch: 20, Train_Loss: 15.726641654968262, Test_Loss: 0.3224700391292572 *\n",
      "Epoch: 20, Train_Loss: 0.32445192337036133, Test_Loss: 0.2707105576992035 *\n",
      "Epoch: 20, Train_Loss: 1.3904392719268799, Test_Loss: 0.28954869508743286\n",
      "Epoch: 20, Train_Loss: 1.147670865058899, Test_Loss: 0.28235068917274475 *\n",
      "Epoch: 20, Train_Loss: 0.29800134897232056, Test_Loss: 0.245525062084198 *\n",
      "Epoch: 20, Train_Loss: 0.30690085887908936, Test_Loss: 0.28995218873023987\n",
      "Model saved at location save_new\\model.ckpt at epoch 20\n",
      "Epoch: 20, Train_Loss: 2.0589118003845215, Test_Loss: 0.3941909670829773\n",
      "Epoch: 20, Train_Loss: 5.070196151733398, Test_Loss: 0.46532750129699707\n",
      "Epoch: 20, Train_Loss: 0.34054118394851685, Test_Loss: 0.3798316717147827 *\n",
      "Epoch: 20, Train_Loss: 0.3856292963027954, Test_Loss: 0.45733442902565\n",
      "Epoch: 20, Train_Loss: 4.35645866394043, Test_Loss: 0.2991618812084198 *\n",
      "Epoch: 20, Train_Loss: 0.5200141072273254, Test_Loss: 0.26633313298225403 *\n",
      "Epoch: 20, Train_Loss: 0.4470457136631012, Test_Loss: 0.35928645730018616\n",
      "Epoch: 20, Train_Loss: 0.25267767906188965, Test_Loss: 0.3726140260696411\n",
      "Epoch: 20, Train_Loss: 0.28391870856285095, Test_Loss: 0.27937978506088257 *\n",
      "Epoch: 20, Train_Loss: 0.3020794093608856, Test_Loss: 0.26085299253463745 *\n",
      "Epoch: 20, Train_Loss: 0.24221086502075195, Test_Loss: 0.6041540503501892\n",
      "Epoch: 20, Train_Loss: 0.2518600821495056, Test_Loss: 0.6294859647750854\n",
      "Epoch: 20, Train_Loss: 0.2381606101989746, Test_Loss: 0.9546095132827759\n",
      "Epoch: 20, Train_Loss: 0.23825880885124207, Test_Loss: 0.37344539165496826 *\n",
      "Epoch: 20, Train_Loss: 0.2429741472005844, Test_Loss: 0.3467872440814972 *\n",
      "Epoch: 20, Train_Loss: 0.2642688751220703, Test_Loss: 0.7240500450134277\n",
      "Epoch: 20, Train_Loss: 0.3514634370803833, Test_Loss: 0.7196955680847168 *\n",
      "Epoch: 20, Train_Loss: 0.37537944316864014, Test_Loss: 0.6221433281898499 *\n",
      "Epoch: 20, Train_Loss: 0.2974741756916046, Test_Loss: 0.4562399387359619 *\n",
      "Epoch: 20, Train_Loss: 0.25488537549972534, Test_Loss: 0.5390714406967163\n",
      "Epoch: 20, Train_Loss: 0.2726603150367737, Test_Loss: 8.34329891204834\n",
      "Epoch: 20, Train_Loss: 0.2425576150417328, Test_Loss: 0.44534632563591003 *\n",
      "Epoch: 20, Train_Loss: 0.2596161961555481, Test_Loss: 1.1510553359985352\n",
      "Epoch: 20, Train_Loss: 0.24113915860652924, Test_Loss: 1.0427393913269043 *\n",
      "Epoch: 20, Train_Loss: 0.2382812201976776, Test_Loss: 0.7835479378700256 *\n",
      "Epoch: 20, Train_Loss: 0.23801322281360626, Test_Loss: 0.48348087072372437 *\n",
      "Epoch: 20, Train_Loss: 0.2387084662914276, Test_Loss: 1.7244845628738403\n",
      "Epoch: 20, Train_Loss: 0.23913706839084625, Test_Loss: 1.8018192052841187\n",
      "Epoch: 20, Train_Loss: 0.23803800344467163, Test_Loss: 0.8329355120658875 *\n",
      "Epoch: 20, Train_Loss: 0.23803089559078217, Test_Loss: 1.195114016532898\n",
      "Epoch: 20, Train_Loss: 0.2515415847301483, Test_Loss: 1.3124173879623413\n",
      "Epoch: 20, Train_Loss: 0.27135318517684937, Test_Loss: 2.204216957092285\n",
      "Epoch: 20, Train_Loss: 0.28483131527900696, Test_Loss: 1.4512617588043213 *\n",
      "Epoch: 20, Train_Loss: 0.38462644815444946, Test_Loss: 1.9024298191070557\n",
      "Epoch: 20, Train_Loss: 0.36343836784362793, Test_Loss: 1.870036244392395 *\n",
      "Epoch: 20, Train_Loss: 0.983849823474884, Test_Loss: 0.40197136998176575 *\n",
      "Epoch: 20, Train_Loss: 5.731863975524902, Test_Loss: 0.33271241188049316 *\n",
      "Epoch: 20, Train_Loss: 0.27807456254959106, Test_Loss: 0.2877292335033417 *\n",
      "Epoch: 20, Train_Loss: 0.34061750769615173, Test_Loss: 0.9281235337257385\n",
      "Epoch: 20, Train_Loss: 0.3297978937625885, Test_Loss: 0.5151572227478027 *\n",
      "Epoch: 20, Train_Loss: 0.44074541330337524, Test_Loss: 0.7771502733230591\n",
      "Epoch: 20, Train_Loss: 0.38866013288497925, Test_Loss: 0.3920784592628479 *\n",
      "Epoch: 20, Train_Loss: 0.6016156673431396, Test_Loss: 0.7417693138122559\n",
      "Epoch: 20, Train_Loss: 0.3478288948535919, Test_Loss: 0.7944580316543579\n",
      "Epoch: 20, Train_Loss: 0.3134881854057312, Test_Loss: 0.6080435514450073 *\n",
      "Epoch: 20, Train_Loss: 0.2913353741168976, Test_Loss: 0.32931795716285706 *\n",
      "Epoch: 20, Train_Loss: 0.26358479261398315, Test_Loss: 0.49103739857673645\n",
      "Epoch: 20, Train_Loss: 0.24893590807914734, Test_Loss: 0.6152456402778625\n",
      "Epoch: 20, Train_Loss: 0.30045512318611145, Test_Loss: 0.45546549558639526 *\n",
      "Epoch: 20, Train_Loss: 0.26426705718040466, Test_Loss: 0.339389830827713 *\n",
      "Epoch: 20, Train_Loss: 0.5307415127754211, Test_Loss: 0.5701674222946167\n",
      "Epoch: 20, Train_Loss: 0.27726295590400696, Test_Loss: 2.5237624645233154\n",
      "Epoch: 20, Train_Loss: 0.26868075132369995, Test_Loss: 3.64607572555542\n",
      "Epoch: 20, Train_Loss: 0.24479879438877106, Test_Loss: 0.2526290714740753 *\n",
      "Epoch: 20, Train_Loss: 0.2728653848171234, Test_Loss: 0.24020060896873474 *\n",
      "Epoch: 20, Train_Loss: 0.2880856394767761, Test_Loss: 0.2677931785583496\n",
      "Epoch: 20, Train_Loss: 0.2722773551940918, Test_Loss: 0.24731414020061493 *\n",
      "Epoch: 20, Train_Loss: 0.2570532560348511, Test_Loss: 0.28711387515068054\n",
      "Epoch: 20, Train_Loss: 0.2535874545574188, Test_Loss: 0.2760997712612152 *\n",
      "Epoch: 20, Train_Loss: 0.2550528645515442, Test_Loss: 0.33864089846611023\n",
      "Epoch: 20, Train_Loss: 1.3919590711593628, Test_Loss: 0.24427753686904907 *\n",
      "Epoch: 20, Train_Loss: 4.811611652374268, Test_Loss: 0.2550908327102661\n",
      "Epoch: 20, Train_Loss: 0.24119041860103607, Test_Loss: 0.266063392162323\n",
      "Epoch: 20, Train_Loss: 0.23861512541770935, Test_Loss: 0.2497035264968872 *\n",
      "Epoch: 20, Train_Loss: 0.24418553709983826, Test_Loss: 0.2566174268722534\n",
      "Epoch: 20, Train_Loss: 0.24390992522239685, Test_Loss: 0.2916741967201233\n",
      "Epoch: 20, Train_Loss: 0.2441149801015854, Test_Loss: 0.26660585403442383 *\n",
      "Epoch: 20, Train_Loss: 0.24406269192695618, Test_Loss: 0.3714892566204071\n",
      "Epoch: 20, Train_Loss: 0.24957701563835144, Test_Loss: 0.5000280141830444\n",
      "Epoch: 20, Train_Loss: 0.25065934658050537, Test_Loss: 0.2607560455799103 *\n",
      "Epoch: 20, Train_Loss: 0.2698703706264496, Test_Loss: 0.2542998194694519 *\n",
      "Epoch: 20, Train_Loss: 0.24967995285987854, Test_Loss: 0.27884119749069214\n",
      "Epoch: 20, Train_Loss: 0.2414967268705368, Test_Loss: 0.27502739429473877 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train_Loss: 0.24288177490234375, Test_Loss: 0.27424418926239014 *\n",
      "Epoch: 20, Train_Loss: 0.2553386986255646, Test_Loss: 0.2547086179256439 *\n",
      "Epoch: 20, Train_Loss: 0.23895786702632904, Test_Loss: 0.2638847529888153\n",
      "Epoch: 20, Train_Loss: 0.23936060070991516, Test_Loss: 0.2741701602935791\n",
      "Epoch: 20, Train_Loss: 0.27884986996650696, Test_Loss: 0.28928637504577637\n",
      "Epoch: 20, Train_Loss: 0.2841484546661377, Test_Loss: 0.25809022784233093 *\n",
      "Epoch: 20, Train_Loss: 0.2527478039264679, Test_Loss: 0.2489762306213379 *\n",
      "Epoch: 20, Train_Loss: 0.24054580926895142, Test_Loss: 0.27226775884628296\n",
      "Epoch: 20, Train_Loss: 0.2619766294956207, Test_Loss: 0.23936477303504944 *\n",
      "Epoch: 20, Train_Loss: 0.26512306928634644, Test_Loss: 0.2622516453266144\n",
      "Epoch: 20, Train_Loss: 0.24881048500537872, Test_Loss: 0.29048821330070496\n",
      "Epoch: 20, Train_Loss: 0.2742507755756378, Test_Loss: 0.4206429123878479\n",
      "Epoch: 20, Train_Loss: 0.26399967074394226, Test_Loss: 0.4251885414123535\n",
      "Epoch: 20, Train_Loss: 0.33796972036361694, Test_Loss: 0.2774021029472351 *\n",
      "Epoch: 20, Train_Loss: 0.26980629563331604, Test_Loss: 0.2523251175880432 *\n",
      "Epoch: 20, Train_Loss: 0.29823845624923706, Test_Loss: 0.2697564959526062\n",
      "Epoch: 20, Train_Loss: 0.2556563913822174, Test_Loss: 0.28081783652305603\n",
      "Epoch: 20, Train_Loss: 0.3794648349285126, Test_Loss: 0.46704626083374023\n",
      "Epoch: 20, Train_Loss: 0.31202301383018494, Test_Loss: 1.2582449913024902\n",
      "Epoch: 20, Train_Loss: 0.23748259246349335, Test_Loss: 0.8466047048568726 *\n",
      "Epoch: 20, Train_Loss: 0.23632292449474335, Test_Loss: 0.29281535744667053 *\n",
      "Epoch: 20, Train_Loss: 0.23603449761867523, Test_Loss: 0.2863416075706482 *\n",
      "Epoch: 20, Train_Loss: 0.23610015213489532, Test_Loss: 0.2609800100326538 *\n",
      "Epoch: 20, Train_Loss: 0.23606029152870178, Test_Loss: 0.2469305545091629 *\n",
      "Epoch: 20, Train_Loss: 1.4810093641281128, Test_Loss: 0.2511679232120514\n",
      "Epoch: 20, Train_Loss: 3.865640878677368, Test_Loss: 0.2576231062412262\n",
      "Epoch: 20, Train_Loss: 0.2393796592950821, Test_Loss: 0.28928250074386597\n",
      "Model saved at location save_new\\model.ckpt at epoch 20\n",
      "Epoch: 20, Train_Loss: 0.24921338260173798, Test_Loss: 0.23870767652988434 *\n",
      "Epoch: 20, Train_Loss: 0.2467328906059265, Test_Loss: 0.26929008960723877\n",
      "Epoch: 20, Train_Loss: 0.23635123670101166, Test_Loss: 0.3680074214935303\n",
      "Epoch: 20, Train_Loss: 0.23660549521446228, Test_Loss: 0.6140744686126709\n",
      "Epoch: 20, Train_Loss: 0.23691429197788239, Test_Loss: 0.49152079224586487 *\n",
      "Epoch: 20, Train_Loss: 0.2358296662569046, Test_Loss: 0.2669612169265747 *\n",
      "Epoch: 20, Train_Loss: 0.23629041016101837, Test_Loss: 0.260730504989624 *\n",
      "Epoch: 20, Train_Loss: 0.23981973528862, Test_Loss: 0.2610374987125397\n",
      "Epoch: 20, Train_Loss: 0.30429428815841675, Test_Loss: 0.2621433734893799\n",
      "Epoch: 20, Train_Loss: 0.29784858226776123, Test_Loss: 0.2629484534263611\n",
      "Epoch: 20, Train_Loss: 0.31102871894836426, Test_Loss: 0.9914133548736572\n",
      "Epoch: 20, Train_Loss: 0.27039289474487305, Test_Loss: 4.797347068786621\n",
      "Epoch: 20, Train_Loss: 0.238637775182724, Test_Loss: 0.2647344768047333 *\n",
      "Epoch: 20, Train_Loss: 0.4393857717514038, Test_Loss: 0.2495788335800171 *\n",
      "Epoch: 20, Train_Loss: 0.48693540692329407, Test_Loss: 0.24634498357772827 *\n",
      "Epoch: 20, Train_Loss: 0.47239744663238525, Test_Loss: 0.254294216632843\n",
      "Epoch: 20, Train_Loss: 0.37177953124046326, Test_Loss: 0.24588213860988617 *\n",
      "Epoch: 20, Train_Loss: 0.2370230257511139, Test_Loss: 0.24288217723369598 *\n",
      "Epoch: 20, Train_Loss: 0.23581711947917938, Test_Loss: 0.23684489727020264 *\n",
      "Epoch: 20, Train_Loss: 0.23983590304851532, Test_Loss: 0.2371341586112976\n",
      "Epoch: 20, Train_Loss: 0.2524900734424591, Test_Loss: 0.23701205849647522 *\n",
      "Epoch: 20, Train_Loss: 0.2568437159061432, Test_Loss: 0.23614658415317535 *\n",
      "Epoch: 20, Train_Loss: 0.25536879897117615, Test_Loss: 0.2412797063589096\n",
      "Epoch: 20, Train_Loss: 0.2383825182914734, Test_Loss: 0.2708629071712494\n",
      "Epoch: 20, Train_Loss: 0.23576946556568146, Test_Loss: 0.27440503239631653\n",
      "Epoch: 20, Train_Loss: 0.2524843215942383, Test_Loss: 0.2534082531929016 *\n",
      "Epoch: 20, Train_Loss: 0.2919554114341736, Test_Loss: 0.2356836497783661 *\n",
      "Epoch: 20, Train_Loss: 0.4531990885734558, Test_Loss: 0.2367282658815384\n",
      "Epoch: 20, Train_Loss: 0.4109188914299011, Test_Loss: 0.23690417408943176\n",
      "Epoch: 20, Train_Loss: 0.38279181718826294, Test_Loss: 0.25182798504829407\n",
      "Epoch: 20, Train_Loss: 0.29790550470352173, Test_Loss: 0.23638449609279633 *\n",
      "Epoch: 20, Train_Loss: 0.35041671991348267, Test_Loss: 0.2363353669643402 *\n",
      "Epoch: 20, Train_Loss: 0.29201361536979675, Test_Loss: 0.23543183505535126 *\n",
      "Epoch: 20, Train_Loss: 0.3344232439994812, Test_Loss: 0.24677850306034088\n",
      "Epoch: 20, Train_Loss: 0.31958818435668945, Test_Loss: 0.24632799625396729 *\n",
      "Epoch: 20, Train_Loss: 0.494297057390213, Test_Loss: 0.2387252002954483 *\n",
      "Epoch: 20, Train_Loss: 0.24630574882030487, Test_Loss: 0.2362404465675354 *\n",
      "Epoch: 20, Train_Loss: 0.24599793553352356, Test_Loss: 0.24088317155838013\n",
      "Epoch: 20, Train_Loss: 3.0876569747924805, Test_Loss: 0.23700715601444244 *\n",
      "Epoch: 20, Train_Loss: 0.4965553879737854, Test_Loss: 0.23644523322582245 *\n",
      "Epoch: 20, Train_Loss: 0.29181966185569763, Test_Loss: 0.25292274355888367\n",
      "Epoch: 20, Train_Loss: 0.29780375957489014, Test_Loss: 0.28937119245529175\n",
      "Epoch: 20, Train_Loss: 0.2912467420101166, Test_Loss: 3.2914133071899414\n",
      "Epoch: 20, Train_Loss: 0.25353580713272095, Test_Loss: 2.564929246902466 *\n",
      "Epoch: 20, Train_Loss: 0.23686227202415466, Test_Loss: 0.23647958040237427 *\n",
      "Epoch: 20, Train_Loss: 0.2951793670654297, Test_Loss: 0.23727670311927795\n",
      "Epoch: 20, Train_Loss: 0.35052967071533203, Test_Loss: 0.30512410402297974\n",
      "Epoch: 20, Train_Loss: 0.3162020742893219, Test_Loss: 0.3041783273220062 *\n",
      "Epoch: 20, Train_Loss: 0.2886357009410858, Test_Loss: 0.282856285572052 *\n",
      "Epoch: 20, Train_Loss: 0.28091421723365784, Test_Loss: 0.288228839635849\n",
      "Epoch: 20, Train_Loss: 0.25194525718688965, Test_Loss: 0.32498499751091003\n",
      "Epoch: 20, Train_Loss: 0.2592693567276001, Test_Loss: 0.23954874277114868 *\n",
      "Epoch: 20, Train_Loss: 0.24439637362957, Test_Loss: 0.2586749792098999\n",
      "Epoch: 20, Train_Loss: 0.28938886523246765, Test_Loss: 0.25544577836990356 *\n",
      "Epoch: 20, Train_Loss: 0.2645517587661743, Test_Loss: 0.2535627782344818 *\n",
      "Epoch: 20, Train_Loss: 0.23626163601875305, Test_Loss: 0.24072587490081787 *\n",
      "Epoch: 20, Train_Loss: 0.24628067016601562, Test_Loss: 0.3491685390472412\n",
      "Epoch: 20, Train_Loss: 0.2664571702480316, Test_Loss: 0.3161974549293518 *\n",
      "Epoch: 20, Train_Loss: 0.2641909718513489, Test_Loss: 0.3153776526451111 *\n",
      "Epoch: 20, Train_Loss: 0.23805664479732513, Test_Loss: 0.31858718395233154\n",
      "Epoch: 20, Train_Loss: 0.23544609546661377, Test_Loss: 0.26735836267471313 *\n",
      "Epoch: 20, Train_Loss: 0.235198512673378, Test_Loss: 0.26950696110725403\n",
      "Epoch: 20, Train_Loss: 0.23464399576187134, Test_Loss: 0.2584405839443207 *\n",
      "Epoch: 20, Train_Loss: 0.23575440049171448, Test_Loss: 0.2602907121181488\n",
      "Epoch: 20, Train_Loss: 0.2362358123064041, Test_Loss: 0.26034513115882874\n",
      "Epoch: 20, Train_Loss: 0.23875543475151062, Test_Loss: 0.26199331879615784\n",
      "Epoch: 20, Train_Loss: 0.23619911074638367, Test_Loss: 0.259786456823349 *\n",
      "Epoch: 20, Train_Loss: 0.23488089442253113, Test_Loss: 0.25571224093437195 *\n",
      "Epoch: 20, Train_Loss: 0.23524433374404907, Test_Loss: 0.2642297148704529\n",
      "Epoch: 20, Train_Loss: 0.2367827445268631, Test_Loss: 0.26362597942352295 *\n",
      "Epoch: 20, Train_Loss: 0.24774062633514404, Test_Loss: 0.258851557970047 *\n",
      "Epoch: 20, Train_Loss: 0.25061431527137756, Test_Loss: 0.2421027421951294 *\n",
      "Epoch: 20, Train_Loss: 0.25273996591567993, Test_Loss: 0.28827258944511414\n",
      "Epoch: 20, Train_Loss: 0.2511034905910492, Test_Loss: 0.32801398634910583\n",
      "Epoch: 20, Train_Loss: 0.24484391510486603, Test_Loss: 0.31195011734962463 *\n",
      "Epoch: 20, Train_Loss: 0.23551702499389648, Test_Loss: 0.7840482592582703\n",
      "Epoch: 20, Train_Loss: 0.23615486919879913, Test_Loss: 0.6483007073402405 *\n",
      "Epoch: 20, Train_Loss: 0.2526218891143799, Test_Loss: 0.34863346815109253 *\n",
      "Epoch: 20, Train_Loss: 0.261214941740036, Test_Loss: 0.25471222400665283 *\n",
      "Epoch: 20, Train_Loss: 0.23584520816802979, Test_Loss: 0.25463587045669556 *\n",
      "Epoch: 20, Train_Loss: 0.2387377917766571, Test_Loss: 0.27470484375953674\n",
      "Epoch: 20, Train_Loss: 0.23891036212444305, Test_Loss: 0.5436925292015076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train_Loss: 0.2695472240447998, Test_Loss: 1.0020129680633545\n",
      "Epoch: 20, Train_Loss: 0.27850398421287537, Test_Loss: 0.6666811108589172 *\n",
      "Epoch: 20, Train_Loss: 0.2970372140407562, Test_Loss: 0.3402118682861328 *\n",
      "Epoch: 20, Train_Loss: 0.25161823630332947, Test_Loss: 0.26362067461013794 *\n",
      "Epoch: 20, Train_Loss: 0.23911775648593903, Test_Loss: 0.24131198227405548 *\n",
      "Epoch: 20, Train_Loss: 0.31549787521362305, Test_Loss: 0.233841672539711 *\n",
      "Epoch: 20, Train_Loss: 0.2370705008506775, Test_Loss: 0.24307024478912354\n",
      "Epoch: 20, Train_Loss: 0.24317097663879395, Test_Loss: 0.2507302463054657\n",
      "Epoch: 20, Train_Loss: 0.2557185888290405, Test_Loss: 0.27282461524009705\n",
      "Epoch: 20, Train_Loss: 0.2506706416606903, Test_Loss: 0.23614656925201416 *\n",
      "Epoch: 20, Train_Loss: 0.361261785030365, Test_Loss: 0.3055303692817688\n",
      "Epoch: 20, Train_Loss: 0.2885475754737854, Test_Loss: 0.36702096462249756\n",
      "Epoch: 20, Train_Loss: 0.2543373107910156, Test_Loss: 0.6082483530044556\n",
      "Epoch: 20, Train_Loss: 0.2410391867160797, Test_Loss: 0.4776289463043213 *\n",
      "Epoch: 20, Train_Loss: 0.2565111219882965, Test_Loss: 0.262672483921051 *\n",
      "Epoch: 20, Train_Loss: 0.23695608973503113, Test_Loss: 0.2578097879886627 *\n",
      "Epoch: 20, Train_Loss: 0.23710119724273682, Test_Loss: 0.25752562284469604 *\n",
      "Epoch: 20, Train_Loss: 0.2481352537870407, Test_Loss: 0.2574650049209595 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 20\n",
      "Epoch: 20, Train_Loss: 0.2553585469722748, Test_Loss: 0.26151975989341736\n",
      "Epoch: 20, Train_Loss: 0.28634580969810486, Test_Loss: 2.102483034133911\n",
      "Epoch: 20, Train_Loss: 0.31576743721961975, Test_Loss: 3.764026165008545\n",
      "Epoch: 20, Train_Loss: 0.25672823190689087, Test_Loss: 0.25672194361686707 *\n",
      "Epoch: 20, Train_Loss: 0.2720654010772705, Test_Loss: 0.24040283262729645 *\n",
      "Epoch: 20, Train_Loss: 0.2653907239437103, Test_Loss: 0.24210667610168457\n",
      "Epoch: 20, Train_Loss: 0.2528759837150574, Test_Loss: 0.24779365956783295\n",
      "Epoch: 20, Train_Loss: 0.33538782596588135, Test_Loss: 0.2382843941450119 *\n",
      "Epoch: 20, Train_Loss: 0.4717646837234497, Test_Loss: 0.24083039164543152\n",
      "Epoch: 20, Train_Loss: 0.23915420472621918, Test_Loss: 0.23432859778404236 *\n",
      "Epoch: 20, Train_Loss: 0.27272433042526245, Test_Loss: 0.2336254119873047 *\n",
      "Epoch: 20, Train_Loss: 0.23292440176010132, Test_Loss: 0.2343013435602188\n",
      "Epoch: 20, Train_Loss: 0.23314650356769562, Test_Loss: 0.23323489725589752 *\n",
      "Epoch: 20, Train_Loss: 0.2338712364435196, Test_Loss: 0.24398638308048248\n",
      "Epoch: 20, Train_Loss: 0.2332673966884613, Test_Loss: 0.26733967661857605\n",
      "Epoch: 20, Train_Loss: 0.24138477444648743, Test_Loss: 0.2632204294204712 *\n",
      "Epoch: 20, Train_Loss: 0.2497137039899826, Test_Loss: 0.24173079431056976 *\n",
      "Epoch: 20, Train_Loss: 0.2396455705165863, Test_Loss: 0.2330823689699173 *\n",
      "Epoch: 20, Train_Loss: 0.24185261130332947, Test_Loss: 0.23447464406490326\n",
      "Epoch: 20, Train_Loss: 0.2389192134141922, Test_Loss: 0.2516492009162903\n",
      "Epoch: 20, Train_Loss: 0.23430025577545166, Test_Loss: 0.24663658440113068 *\n",
      "Epoch: 20, Train_Loss: 0.23412451148033142, Test_Loss: 0.23490560054779053 *\n",
      "Epoch: 20, Train_Loss: 0.23225118219852448, Test_Loss: 0.23317603766918182 *\n",
      "Epoch: 20, Train_Loss: 0.2652856707572937, Test_Loss: 0.2373054027557373\n",
      "Epoch: 20, Train_Loss: 0.2705214321613312, Test_Loss: 0.24274304509162903\n",
      "Epoch: 20, Train_Loss: 0.2574826776981354, Test_Loss: 0.24767881631851196\n",
      "Epoch: 20, Train_Loss: 0.24117183685302734, Test_Loss: 0.23375722765922546 *\n",
      "Epoch: 20, Train_Loss: 0.28684094548225403, Test_Loss: 0.23310182988643646 *\n",
      "Epoch: 20, Train_Loss: 0.26602283120155334, Test_Loss: 0.23908783495426178\n",
      "Epoch: 20, Train_Loss: 0.25054678320884705, Test_Loss: 0.23552930355072021 *\n",
      "Epoch: 20, Train_Loss: 0.24930471181869507, Test_Loss: 0.23366884887218475 *\n",
      "Epoch: 20, Train_Loss: 0.2624565660953522, Test_Loss: 0.2771907448768616\n",
      "Epoch: 20, Train_Loss: 0.23675896227359772, Test_Loss: 0.26652854681015015 *\n",
      "Epoch: 20, Train_Loss: 0.24288786947727203, Test_Loss: 4.402624607086182\n",
      "Epoch: 20, Train_Loss: 0.263033390045166, Test_Loss: 1.5359536409378052 *\n",
      "Epoch: 20, Train_Loss: 0.28820380568504333, Test_Loss: 0.23328320682048798 *\n",
      "Epoch: 20, Train_Loss: 2.3354504108428955, Test_Loss: 0.24504248797893524\n",
      "Epoch: 20, Train_Loss: 3.5481503009796143, Test_Loss: 0.2950544059276581\n",
      "Epoch: 20, Train_Loss: 0.2410641610622406, Test_Loss: 0.3015543222427368\n",
      "Epoch: 20, Train_Loss: 0.24116085469722748, Test_Loss: 0.24998635053634644 *\n",
      "Epoch: 20, Train_Loss: 0.2665984332561493, Test_Loss: 0.3112363815307617\n",
      "Epoch: 20, Train_Loss: 0.38134199380874634, Test_Loss: 0.3162655532360077\n",
      "Epoch: 20, Train_Loss: 0.25269725918769836, Test_Loss: 0.2346538007259369 *\n",
      "Epoch: 20, Train_Loss: 0.2368374913930893, Test_Loss: 0.26766279339790344\n",
      "Epoch: 20, Train_Loss: 0.2350938320159912, Test_Loss: 0.24892975389957428 *\n",
      "Epoch: 20, Train_Loss: 0.29561108350753784, Test_Loss: 0.2444162666797638 *\n",
      "Epoch: 20, Train_Loss: 0.24257446825504303, Test_Loss: 0.23584097623825073 *\n",
      "Epoch: 20, Train_Loss: 0.24788038432598114, Test_Loss: 0.3455342650413513\n",
      "Epoch: 20, Train_Loss: 0.8712581396102905, Test_Loss: 0.2788046598434448 *\n",
      "Epoch: 20, Train_Loss: 1.3471179008483887, Test_Loss: 0.3416348397731781\n",
      "Epoch: 20, Train_Loss: 0.8540223836898804, Test_Loss: 0.29594820737838745 *\n",
      "Epoch: 20, Train_Loss: 0.3318343758583069, Test_Loss: 0.2748381495475769 *\n",
      "Epoch: 20, Train_Loss: 0.8925084471702576, Test_Loss: 0.25327354669570923 *\n",
      "Epoch: 20, Train_Loss: 2.185192823410034, Test_Loss: 0.2515331506729126 *\n",
      "Epoch: 20, Train_Loss: 0.6386303305625916, Test_Loss: 0.2546175420284271\n",
      "Epoch: 20, Train_Loss: 0.2605435848236084, Test_Loss: 0.2550174295902252\n",
      "Epoch: 20, Train_Loss: 0.26146572828292847, Test_Loss: 0.2574050724506378\n",
      "Epoch: 20, Train_Loss: 0.964847981929779, Test_Loss: 0.2511039972305298 *\n",
      "Epoch: 20, Train_Loss: 0.9761241674423218, Test_Loss: 0.24184399843215942 *\n",
      "Epoch: 20, Train_Loss: 0.4330163598060608, Test_Loss: 0.23815646767616272 *\n",
      "Epoch: 20, Train_Loss: 0.2603908181190491, Test_Loss: 0.23906023800373077\n",
      "Epoch: 20, Train_Loss: 0.2585352063179016, Test_Loss: 0.23996414244174957\n",
      "Epoch: 20, Train_Loss: 0.6618658304214478, Test_Loss: 0.24264664947986603\n",
      "Epoch: 20, Train_Loss: 0.3468206226825714, Test_Loss: 0.2772504687309265\n",
      "Epoch: 20, Train_Loss: 0.2819107472896576, Test_Loss: 0.3251252770423889\n",
      "Epoch: 20, Train_Loss: 0.2810230851173401, Test_Loss: 0.4340112805366516\n",
      "Epoch: 20, Train_Loss: 0.2666943371295929, Test_Loss: 0.4955155551433563\n",
      "Epoch: 20, Train_Loss: 0.3213409185409546, Test_Loss: 0.3979076147079468 *\n",
      "Epoch: 20, Train_Loss: 0.368466854095459, Test_Loss: 0.28098738193511963 *\n",
      "Epoch: 20, Train_Loss: 0.33774107694625854, Test_Loss: 0.27516356110572815 *\n",
      "Epoch: 20, Train_Loss: 0.2427993267774582, Test_Loss: 0.25869515538215637 *\n",
      "Epoch: 20, Train_Loss: 0.325886070728302, Test_Loss: 0.26730576157569885\n",
      "Epoch: 20, Train_Loss: 0.2863427400588989, Test_Loss: 0.4587832987308502\n",
      "Epoch: 20, Train_Loss: 0.32586929202079773, Test_Loss: 0.36177682876586914 *\n",
      "Epoch: 20, Train_Loss: 0.29766571521759033, Test_Loss: 0.5552088022232056\n",
      "Epoch: 20, Train_Loss: 0.3339096009731293, Test_Loss: 0.30086737871170044 *\n",
      "Epoch: 20, Train_Loss: 0.284904807806015, Test_Loss: 0.2829882502555847 *\n",
      "Epoch: 20, Train_Loss: 0.2663308382034302, Test_Loss: 0.2695092558860779 *\n",
      "Epoch: 20, Train_Loss: 0.2660551071166992, Test_Loss: 0.23941513895988464 *\n",
      "Epoch: 20, Train_Loss: 0.25626760721206665, Test_Loss: 0.2844214141368866\n",
      "Epoch: 20, Train_Loss: 0.2322176843881607, Test_Loss: 0.24341276288032532 *\n",
      "Epoch: 20, Train_Loss: 0.2340267449617386, Test_Loss: 0.25258153676986694\n",
      "Epoch: 20, Train_Loss: 0.2355080395936966, Test_Loss: 0.24976754188537598 *\n",
      "Epoch: 20, Train_Loss: 0.23989006876945496, Test_Loss: 0.4981774687767029\n",
      "Epoch: 20, Train_Loss: 0.25782573223114014, Test_Loss: 0.5699979066848755\n",
      "Epoch: 20, Train_Loss: 0.2644731402397156, Test_Loss: 0.4155055284500122 *\n",
      "Epoch: 20, Train_Loss: 0.2432376742362976, Test_Loss: 0.6360402703285217\n",
      "Epoch: 20, Train_Loss: 0.43449652194976807, Test_Loss: 0.3723148703575134 *\n",
      "Epoch: 20, Train_Loss: 0.3709179162979126, Test_Loss: 0.37112438678741455 *\n",
      "Epoch: 20, Train_Loss: 0.24975083768367767, Test_Loss: 0.3707962930202484 *\n",
      "Epoch: 20, Train_Loss: 0.2627606689929962, Test_Loss: 0.3588718771934509 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train_Loss: 0.299562007188797, Test_Loss: 0.37761276960372925\n",
      "Epoch: 20, Train_Loss: 0.3275038003921509, Test_Loss: 3.848785638809204\n",
      "Epoch: 20, Train_Loss: 0.42266517877578735, Test_Loss: 2.135605573654175 *\n",
      "Epoch: 20, Train_Loss: 0.33478501439094543, Test_Loss: 0.3223927319049835 *\n",
      "Epoch: 20, Train_Loss: 0.43778300285339355, Test_Loss: 0.28352200984954834 *\n",
      "Epoch: 20, Train_Loss: 0.3142772614955902, Test_Loss: 0.2773864269256592 *\n",
      "Epoch: 20, Train_Loss: 0.28546836972236633, Test_Loss: 0.25365856289863586 *\n",
      "Epoch: 20, Train_Loss: 0.31097838282585144, Test_Loss: 0.25500744581222534\n",
      "Epoch: 20, Train_Loss: 0.24746128916740417, Test_Loss: 0.3728206753730774\n",
      "Model saved at location save_new\\model.ckpt at epoch 20\n",
      "Epoch: 20, Train_Loss: 0.2609785795211792, Test_Loss: 0.2792052924633026 *\n",
      "Epoch: 20, Train_Loss: 0.6941007971763611, Test_Loss: 0.23888647556304932 *\n",
      "Epoch: 20, Train_Loss: 0.6017911434173584, Test_Loss: 0.2778400182723999\n",
      "Epoch: 20, Train_Loss: 0.2660154402256012, Test_Loss: 0.306237131357193\n",
      "Epoch: 20, Train_Loss: 0.2836138606071472, Test_Loss: 0.488533079624176\n",
      "Epoch: 20, Train_Loss: 0.26566949486732483, Test_Loss: 0.2755838930606842 *\n",
      "Epoch: 20, Train_Loss: 0.2697572410106659, Test_Loss: 0.30218279361724854\n",
      "Epoch: 20, Train_Loss: 0.5120173692703247, Test_Loss: 0.26681533455848694 *\n",
      "Epoch: 20, Train_Loss: 0.23885908722877502, Test_Loss: 0.27662041783332825\n",
      "Epoch: 20, Train_Loss: 0.3312010169029236, Test_Loss: 0.31223157048225403\n",
      "Epoch: 20, Train_Loss: 0.29774239659309387, Test_Loss: 0.36975157260894775\n",
      "Epoch: 20, Train_Loss: 0.27201440930366516, Test_Loss: 0.2909358739852905 *\n",
      "Epoch: 20, Train_Loss: 0.27785202860832214, Test_Loss: 0.2573848068714142 *\n",
      "Epoch: 20, Train_Loss: 0.312213659286499, Test_Loss: 0.26724758744239807\n",
      "Epoch: 20, Train_Loss: 0.318046510219574, Test_Loss: 0.26996007561683655\n",
      "Epoch: 20, Train_Loss: 0.2643720507621765, Test_Loss: 0.30185946822166443\n",
      "Epoch: 20, Train_Loss: 0.25483736395835876, Test_Loss: 0.34580421447753906\n",
      "Epoch: 20, Train_Loss: 0.26178812980651855, Test_Loss: 0.240465447306633 *\n",
      "Epoch: 20, Train_Loss: 0.2662212550640106, Test_Loss: 0.25541436672210693\n",
      "Epoch: 20, Train_Loss: 0.27579832077026367, Test_Loss: 0.2798590660095215\n",
      "Epoch: 20, Train_Loss: 0.25359398126602173, Test_Loss: 0.2758585214614868 *\n",
      "Epoch: 20, Train_Loss: 0.24014106392860413, Test_Loss: 0.23557418584823608 *\n",
      "Epoch: 20, Train_Loss: 0.3165495693683624, Test_Loss: 0.4078494906425476\n",
      "Epoch: 20, Train_Loss: 0.4700247049331665, Test_Loss: 0.3061498701572418 *\n",
      "Epoch: 20, Train_Loss: 0.5111536383628845, Test_Loss: 5.628050327301025\n",
      "Epoch: 20, Train_Loss: 0.6441707611083984, Test_Loss: 0.43436485528945923 *\n",
      "Epoch: 20, Train_Loss: 0.45001477003097534, Test_Loss: 0.27714478969573975 *\n",
      "Epoch: 20, Train_Loss: 0.4382007122039795, Test_Loss: 0.2971798777580261\n",
      "Epoch: 20, Train_Loss: 0.32733315229415894, Test_Loss: 0.24342910945415497 *\n",
      "Epoch: 20, Train_Loss: 0.2872987389564514, Test_Loss: 0.24340374767780304 *\n",
      "Epoch: 20, Train_Loss: 0.2464262694120407, Test_Loss: 0.24655769765377045\n",
      "Epoch: 20, Train_Loss: 0.24699097871780396, Test_Loss: 0.2975531816482544\n",
      "Epoch: 20, Train_Loss: 0.28940266370773315, Test_Loss: 0.27058273553848267 *\n",
      "Epoch: 20, Train_Loss: 0.5044822096824646, Test_Loss: 0.2383148968219757 *\n",
      "Epoch: 20, Train_Loss: 0.5986249446868896, Test_Loss: 0.24890929460525513\n",
      "Epoch: 20, Train_Loss: 0.7832211256027222, Test_Loss: 0.2974083125591278\n",
      "Epoch: 20, Train_Loss: 1.248471736907959, Test_Loss: 0.2876009941101074 *\n",
      "Epoch: 20, Train_Loss: 0.453238844871521, Test_Loss: 0.26617202162742615 *\n",
      "Epoch: 20, Train_Loss: 0.4362160861492157, Test_Loss: 0.2508799135684967 *\n",
      "Epoch: 20, Train_Loss: 0.2334190011024475, Test_Loss: 0.3161415159702301\n",
      "Epoch: 20, Train_Loss: 0.24654409289360046, Test_Loss: 0.2621941566467285 *\n",
      "Epoch: 20, Train_Loss: 0.46843892335891724, Test_Loss: 0.2699154317378998\n",
      "Epoch: 20, Train_Loss: 0.8510839939117432, Test_Loss: 0.3530462980270386\n",
      "Epoch: 20, Train_Loss: 0.32384324073791504, Test_Loss: 0.24995461106300354 *\n",
      "Epoch: 20, Train_Loss: 0.2680501937866211, Test_Loss: 0.28460007905960083\n",
      "Epoch: 20, Train_Loss: 0.2594919800758362, Test_Loss: 0.2829444110393524 *\n",
      "Epoch: 20, Train_Loss: 0.32855498790740967, Test_Loss: 0.3126068413257599\n",
      "Epoch: 20, Train_Loss: 0.5476305484771729, Test_Loss: 0.2714758813381195 *\n",
      "Epoch: 20, Train_Loss: 0.42961639165878296, Test_Loss: 0.26428526639938354 *\n",
      "Epoch: 20, Train_Loss: 0.3542184829711914, Test_Loss: 0.33309218287467957\n",
      "Epoch: 20, Train_Loss: 0.5138915777206421, Test_Loss: 0.29005303978919983 *\n",
      "Epoch: 20, Train_Loss: 0.2504161596298218, Test_Loss: 0.30581533908843994\n",
      "Epoch: 20, Train_Loss: 0.24249519407749176, Test_Loss: 0.2980433404445648 *\n",
      "Epoch: 20, Train_Loss: 0.2678455114364624, Test_Loss: 0.2777913510799408 *\n",
      "Epoch: 21, Train_Loss: 0.30870574712753296, Test_Loss: 0.23615813255310059 *\n",
      "Epoch: 21, Train_Loss: 0.30941322445869446, Test_Loss: 0.2671465277671814\n",
      "Epoch: 21, Train_Loss: 0.278989315032959, Test_Loss: 0.3816717863082886\n",
      "Epoch: 21, Train_Loss: 13.956637382507324, Test_Loss: 0.28995418548583984 *\n",
      "Epoch: 21, Train_Loss: 1.9427295923233032, Test_Loss: 0.27874934673309326 *\n",
      "Epoch: 21, Train_Loss: 1.2811230421066284, Test_Loss: 0.2865348756313324\n",
      "Epoch: 21, Train_Loss: 0.9406406879425049, Test_Loss: 0.3067742586135864\n",
      "Epoch: 21, Train_Loss: 0.31270256638526917, Test_Loss: 0.2404695600271225 *\n",
      "Epoch: 21, Train_Loss: 0.2832053303718567, Test_Loss: 0.26301151514053345\n",
      "Epoch: 21, Train_Loss: 1.2647809982299805, Test_Loss: 0.45029330253601074\n",
      "Epoch: 21, Train_Loss: 5.761504173278809, Test_Loss: 0.31473639607429504 *\n",
      "Epoch: 21, Train_Loss: 0.7314509153366089, Test_Loss: 0.4812242090702057\n",
      "Epoch: 21, Train_Loss: 0.3154904842376709, Test_Loss: 0.34463825821876526 *\n",
      "Epoch: 21, Train_Loss: 3.99511456489563, Test_Loss: 0.2690562307834625 *\n",
      "Epoch: 21, Train_Loss: 1.2090506553649902, Test_Loss: 0.3365396559238434\n",
      "Epoch: 21, Train_Loss: 0.588501513004303, Test_Loss: 0.3055104911327362 *\n",
      "Epoch: 21, Train_Loss: 0.2336406707763672, Test_Loss: 0.3844436705112457\n",
      "Epoch: 21, Train_Loss: 0.2676336169242859, Test_Loss: 0.24679625034332275 *\n",
      "Epoch: 21, Train_Loss: 0.29839569330215454, Test_Loss: 0.2606051564216614\n",
      "Epoch: 21, Train_Loss: 0.24220913648605347, Test_Loss: 0.3584512174129486\n",
      "Epoch: 21, Train_Loss: 0.2526029348373413, Test_Loss: 0.44898685812950134\n",
      "Epoch: 21, Train_Loss: 0.22857320308685303, Test_Loss: 0.9700901508331299\n",
      "Epoch: 21, Train_Loss: 0.22881585359573364, Test_Loss: 0.3002176880836487 *\n",
      "Epoch: 21, Train_Loss: 0.23075425624847412, Test_Loss: 0.4263586103916168\n",
      "Epoch: 21, Train_Loss: 0.2500426173210144, Test_Loss: 1.2350338697433472\n",
      "Epoch: 21, Train_Loss: 0.3359487056732178, Test_Loss: 1.2836424112319946\n",
      "Epoch: 21, Train_Loss: 0.3419378995895386, Test_Loss: 1.241499900817871 *\n",
      "Epoch: 21, Train_Loss: 0.3168048560619354, Test_Loss: 1.0888230800628662 *\n",
      "Epoch: 21, Train_Loss: 0.2634113132953644, Test_Loss: 0.28560465574264526 *\n",
      "Epoch: 21, Train_Loss: 0.25477135181427, Test_Loss: 8.477097511291504\n",
      "Epoch: 21, Train_Loss: 0.23329199850559235, Test_Loss: 1.4119497537612915 *\n",
      "Epoch: 21, Train_Loss: 0.2386942207813263, Test_Loss: 0.8308985829353333 *\n",
      "Epoch: 21, Train_Loss: 0.23387128114700317, Test_Loss: 0.7509755492210388 *\n",
      "Epoch: 21, Train_Loss: 0.2295655757188797, Test_Loss: 0.701593816280365 *\n",
      "Epoch: 21, Train_Loss: 0.22834740579128265, Test_Loss: 0.338044673204422 *\n",
      "Epoch: 21, Train_Loss: 0.2291589081287384, Test_Loss: 1.0335705280303955\n",
      "Epoch: 21, Train_Loss: 0.2297905683517456, Test_Loss: 1.2922282218933105\n",
      "Epoch: 21, Train_Loss: 0.2291678488254547, Test_Loss: 0.8167539834976196 *\n",
      "Epoch: 21, Train_Loss: 0.22866183519363403, Test_Loss: 0.9636691212654114\n",
      "Epoch: 21, Train_Loss: 0.23593999445438385, Test_Loss: 1.0929137468338013\n",
      "Epoch: 21, Train_Loss: 0.2490919828414917, Test_Loss: 1.469219446182251\n",
      "Epoch: 21, Train_Loss: 0.2530559301376343, Test_Loss: 1.6104397773742676\n",
      "Epoch: 21, Train_Loss: 0.32848575711250305, Test_Loss: 1.3630273342132568 *\n",
      "Epoch: 21, Train_Loss: 0.2652800679206848, Test_Loss: 1.6415760517120361\n",
      "Epoch: 21, Train_Loss: 0.4075278639793396, Test_Loss: 0.7378191947937012 *\n",
      "Epoch: 21, Train_Loss: 6.757808685302734, Test_Loss: 0.2664697766304016 *\n",
      "Epoch: 21, Train_Loss: 0.31925904750823975, Test_Loss: 0.3202058970928192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train_Loss: 0.28753378987312317, Test_Loss: 0.5459000468254089\n",
      "Epoch: 21, Train_Loss: 0.306528240442276, Test_Loss: 0.5566757321357727\n",
      "Epoch: 21, Train_Loss: 0.42029303312301636, Test_Loss: 0.5456483364105225 *\n",
      "Epoch: 21, Train_Loss: 0.35301655530929565, Test_Loss: 0.6228155493736267\n",
      "Epoch: 21, Train_Loss: 0.3761812746524811, Test_Loss: 0.5375428795814514 *\n",
      "Epoch: 21, Train_Loss: 0.43935081362724304, Test_Loss: 0.6373760104179382\n",
      "Epoch: 21, Train_Loss: 0.3912627100944519, Test_Loss: 0.5833097100257874 *\n",
      "Epoch: 21, Train_Loss: 0.30827248096466064, Test_Loss: 0.3228708505630493 *\n",
      "Epoch: 21, Train_Loss: 0.26545119285583496, Test_Loss: 0.354053795337677\n",
      "Epoch: 21, Train_Loss: 0.23523235321044922, Test_Loss: 0.45333367586135864\n",
      "Epoch: 21, Train_Loss: 0.2685597538948059, Test_Loss: 0.36928653717041016 *\n",
      "Epoch: 21, Train_Loss: 0.23127305507659912, Test_Loss: 0.25892308354377747 *\n",
      "Epoch: 21, Train_Loss: 0.48605024814605713, Test_Loss: 0.5851225852966309\n",
      "Epoch: 21, Train_Loss: 0.28321075439453125, Test_Loss: 0.6080060601234436\n",
      "Epoch: 21, Train_Loss: 0.272970050573349, Test_Loss: 5.679296970367432\n",
      "Epoch: 21, Train_Loss: 0.25034669041633606, Test_Loss: 0.25014418363571167 *\n",
      "Epoch: 21, Train_Loss: 0.252763956785202, Test_Loss: 0.23001894354820251 *\n",
      "Epoch: 21, Train_Loss: 0.2602992355823517, Test_Loss: 0.253166526556015\n",
      "Epoch: 21, Train_Loss: 0.25130170583724976, Test_Loss: 0.23926801979541779 *\n",
      "Epoch: 21, Train_Loss: 0.24569912254810333, Test_Loss: 0.2454759180545807\n",
      "Epoch: 21, Train_Loss: 0.23004932701587677, Test_Loss: 0.24638304114341736\n",
      "Epoch: 21, Train_Loss: 0.2501331865787506, Test_Loss: 0.33177101612091064\n",
      "Epoch: 21, Train_Loss: 0.3901556730270386, Test_Loss: 0.2685040533542633 *\n",
      "Epoch: 21, Train_Loss: 5.024853706359863, Test_Loss: 0.2297980636358261 *\n",
      "Epoch: 21, Train_Loss: 0.24208475649356842, Test_Loss: 0.258537232875824\n",
      "Epoch: 21, Train_Loss: 0.23040235042572021, Test_Loss: 0.24663834273815155 *\n",
      "Epoch: 21, Train_Loss: 0.23011311888694763, Test_Loss: 0.23447443544864655 *\n",
      "Epoch: 21, Train_Loss: 0.2303798645734787, Test_Loss: 0.27679207921028137\n",
      "Epoch: 21, Train_Loss: 0.22796760499477386, Test_Loss: 0.2504332959651947 *\n",
      "Epoch: 21, Train_Loss: 0.23129397630691528, Test_Loss: 0.32485371828079224\n",
      "Epoch: 21, Train_Loss: 0.2325213998556137, Test_Loss: 0.3319116234779358\n",
      "Epoch: 21, Train_Loss: 0.25739431381225586, Test_Loss: 0.2688511610031128 *\n",
      "Epoch: 21, Train_Loss: 0.24817906320095062, Test_Loss: 0.248186856508255 *\n",
      "Epoch: 21, Train_Loss: 0.24565015733242035, Test_Loss: 0.2517417073249817\n",
      "Epoch: 21, Train_Loss: 0.22799654304981232, Test_Loss: 0.2707163393497467\n",
      "Epoch: 21, Train_Loss: 0.22833290696144104, Test_Loss: 0.2558310031890869 *\n",
      "Epoch: 21, Train_Loss: 0.24209120869636536, Test_Loss: 0.2591651976108551\n",
      "Epoch: 21, Train_Loss: 0.23705318570137024, Test_Loss: 0.24856175482273102 *\n",
      "Epoch: 21, Train_Loss: 0.23487526178359985, Test_Loss: 0.2507662773132324\n",
      "Epoch: 21, Train_Loss: 0.2423822432756424, Test_Loss: 0.28714051842689514\n",
      "Epoch: 21, Train_Loss: 0.25603368878364563, Test_Loss: 0.2489086091518402 *\n",
      "Epoch: 21, Train_Loss: 0.2433682382106781, Test_Loss: 0.2406364381313324 *\n",
      "Epoch: 21, Train_Loss: 0.22859996557235718, Test_Loss: 0.2880678176879883\n",
      "Epoch: 21, Train_Loss: 0.2328415811061859, Test_Loss: 0.24253399670124054 *\n",
      "Epoch: 21, Train_Loss: 0.2519474923610687, Test_Loss: 0.2354656606912613 *\n",
      "Epoch: 21, Train_Loss: 0.253217488527298, Test_Loss: 0.31122952699661255\n",
      "Epoch: 21, Train_Loss: 0.26798805594444275, Test_Loss: 0.3469947576522827\n",
      "Epoch: 21, Train_Loss: 0.2561998665332794, Test_Loss: 0.35504353046417236\n",
      "Epoch: 21, Train_Loss: 0.31478625535964966, Test_Loss: 0.29281121492385864 *\n",
      "Epoch: 21, Train_Loss: 0.25241708755493164, Test_Loss: 0.26838475465774536 *\n",
      "Epoch: 21, Train_Loss: 0.263629674911499, Test_Loss: 0.24700365960597992 *\n",
      "Epoch: 21, Train_Loss: 0.268043577671051, Test_Loss: 0.24186943471431732 *\n",
      "Epoch: 21, Train_Loss: 0.44464415311813354, Test_Loss: 0.2897651493549347\n",
      "Model saved at location save_new\\model.ckpt at epoch 21\n",
      "Epoch: 21, Train_Loss: 0.3299088180065155, Test_Loss: 0.5077251195907593\n",
      "Epoch: 21, Train_Loss: 0.24640320241451263, Test_Loss: 0.479484498500824 *\n",
      "Epoch: 21, Train_Loss: 0.2301207035779953, Test_Loss: 0.30841580033302307 *\n",
      "Epoch: 21, Train_Loss: 0.23763346672058105, Test_Loss: 0.27137550711631775 *\n",
      "Epoch: 21, Train_Loss: 0.2351263463497162, Test_Loss: 0.23785246908664703 *\n",
      "Epoch: 21, Train_Loss: 0.23169857263565063, Test_Loss: 0.24015212059020996\n",
      "Epoch: 21, Train_Loss: 0.38223427534103394, Test_Loss: 0.23905207216739655 *\n",
      "Epoch: 21, Train_Loss: 4.657417297363281, Test_Loss: 0.24673889577388763\n",
      "Epoch: 21, Train_Loss: 0.28106796741485596, Test_Loss: 0.25307613611221313\n",
      "Epoch: 21, Train_Loss: 0.23423880338668823, Test_Loss: 0.24649129807949066 *\n",
      "Epoch: 21, Train_Loss: 0.23704692721366882, Test_Loss: 0.23329928517341614 *\n",
      "Epoch: 21, Train_Loss: 0.2275901585817337, Test_Loss: 0.3428378999233246\n",
      "Epoch: 21, Train_Loss: 0.23055769503116608, Test_Loss: 0.6193878054618835\n",
      "Epoch: 21, Train_Loss: 0.2291989028453827, Test_Loss: 0.37635505199432373 *\n",
      "Epoch: 21, Train_Loss: 0.2266775667667389, Test_Loss: 0.3988227844238281\n",
      "Epoch: 21, Train_Loss: 0.22872091829776764, Test_Loss: 0.26868054270744324 *\n",
      "Epoch: 21, Train_Loss: 0.2289029061794281, Test_Loss: 0.2703784704208374\n",
      "Epoch: 21, Train_Loss: 0.26241230964660645, Test_Loss: 0.2718839943408966\n",
      "Epoch: 21, Train_Loss: 0.2551940679550171, Test_Loss: 0.26556843519210815 *\n",
      "Epoch: 21, Train_Loss: 0.2899089455604553, Test_Loss: 0.2954491078853607\n",
      "Epoch: 21, Train_Loss: 0.24760916829109192, Test_Loss: 5.369800090789795\n",
      "Epoch: 21, Train_Loss: 0.2328512817621231, Test_Loss: 0.4038429260253906 *\n",
      "Epoch: 21, Train_Loss: 0.37637442350387573, Test_Loss: 0.25130024552345276 *\n",
      "Epoch: 21, Train_Loss: 0.4566623270511627, Test_Loss: 0.23798811435699463 *\n",
      "Epoch: 21, Train_Loss: 0.4603095054626465, Test_Loss: 0.24039122462272644\n",
      "Epoch: 21, Train_Loss: 0.40045616030693054, Test_Loss: 0.24010303616523743 *\n",
      "Epoch: 21, Train_Loss: 0.22835500538349152, Test_Loss: 0.23154200613498688 *\n",
      "Epoch: 21, Train_Loss: 0.2268662303686142, Test_Loss: 0.2318664938211441\n",
      "Epoch: 21, Train_Loss: 0.22749195992946625, Test_Loss: 0.22823671996593475 *\n",
      "Epoch: 21, Train_Loss: 0.23758189380168915, Test_Loss: 0.22801998257637024 *\n",
      "Epoch: 21, Train_Loss: 0.241499662399292, Test_Loss: 0.23004253208637238\n",
      "Epoch: 21, Train_Loss: 0.2469601333141327, Test_Loss: 0.25999560952186584\n",
      "Epoch: 21, Train_Loss: 0.23513592779636383, Test_Loss: 0.2365008145570755 *\n",
      "Epoch: 21, Train_Loss: 0.22643162310123444, Test_Loss: 0.23809437453746796\n",
      "Epoch: 21, Train_Loss: 0.24003389477729797, Test_Loss: 0.2537708282470703\n",
      "Epoch: 21, Train_Loss: 0.25859519839286804, Test_Loss: 0.22744381427764893 *\n",
      "Epoch: 21, Train_Loss: 0.4065893590450287, Test_Loss: 0.22715723514556885 *\n",
      "Epoch: 21, Train_Loss: 0.3693900406360626, Test_Loss: 0.2282666116952896\n",
      "Epoch: 21, Train_Loss: 0.3631781339645386, Test_Loss: 0.23856239020824432\n",
      "Epoch: 21, Train_Loss: 0.25569242238998413, Test_Loss: 0.2284412831068039 *\n",
      "Epoch: 21, Train_Loss: 0.32618945837020874, Test_Loss: 0.2289440780878067\n",
      "Epoch: 21, Train_Loss: 0.313807874917984, Test_Loss: 0.2273445725440979 *\n",
      "Epoch: 21, Train_Loss: 0.2933964729309082, Test_Loss: 0.23958301544189453\n",
      "Epoch: 21, Train_Loss: 0.3120401203632355, Test_Loss: 0.2429073452949524\n",
      "Epoch: 21, Train_Loss: 0.4413362145423889, Test_Loss: 0.2398556023836136 *\n",
      "Epoch: 21, Train_Loss: 0.26274174451828003, Test_Loss: 0.22922323644161224 *\n",
      "Epoch: 21, Train_Loss: 0.2305518239736557, Test_Loss: 0.23184901475906372\n",
      "Epoch: 21, Train_Loss: 2.530883550643921, Test_Loss: 0.23042362928390503 *\n",
      "Epoch: 21, Train_Loss: 0.9272449016571045, Test_Loss: 0.22990545630455017 *\n",
      "Epoch: 21, Train_Loss: 0.2707825005054474, Test_Loss: 0.2306147813796997\n",
      "Epoch: 21, Train_Loss: 0.27149349451065063, Test_Loss: 0.30116763710975647\n",
      "Epoch: 21, Train_Loss: 0.24352361261844635, Test_Loss: 1.5855822563171387\n",
      "Epoch: 21, Train_Loss: 0.24992413818836212, Test_Loss: 4.0503950119018555\n",
      "Epoch: 21, Train_Loss: 0.23459269106388092, Test_Loss: 0.23005381226539612 *\n",
      "Epoch: 21, Train_Loss: 0.270851731300354, Test_Loss: 0.2281060814857483 *\n",
      "Epoch: 21, Train_Loss: 0.3473016023635864, Test_Loss: 0.28139328956604004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train_Loss: 0.2928440570831299, Test_Loss: 0.2618805170059204 *\n",
      "Epoch: 21, Train_Loss: 0.2622373402118683, Test_Loss: 0.2842389941215515\n",
      "Epoch: 21, Train_Loss: 0.23261956870555878, Test_Loss: 0.24800020456314087 *\n",
      "Epoch: 21, Train_Loss: 0.24269118905067444, Test_Loss: 0.310451865196228\n",
      "Epoch: 21, Train_Loss: 0.2336229383945465, Test_Loss: 0.24019142985343933 *\n",
      "Epoch: 21, Train_Loss: 0.2379302829504013, Test_Loss: 0.2389046549797058 *\n",
      "Epoch: 21, Train_Loss: 0.28608986735343933, Test_Loss: 0.24994544684886932\n",
      "Epoch: 21, Train_Loss: 0.26088371872901917, Test_Loss: 0.27126970887184143\n",
      "Epoch: 21, Train_Loss: 0.23377935588359833, Test_Loss: 0.23438911139965057 *\n",
      "Epoch: 21, Train_Loss: 0.2308892011642456, Test_Loss: 0.30582812428474426\n",
      "Epoch: 21, Train_Loss: 0.24316775798797607, Test_Loss: 0.30205217003822327 *\n",
      "Epoch: 21, Train_Loss: 0.25000590085983276, Test_Loss: 0.27843722701072693 *\n",
      "Epoch: 21, Train_Loss: 0.23368127644062042, Test_Loss: 0.262389600276947 *\n",
      "Epoch: 21, Train_Loss: 0.2265835404396057, Test_Loss: 0.25745993852615356 *\n",
      "Epoch: 21, Train_Loss: 0.22677330672740936, Test_Loss: 0.3141587972640991\n",
      "Epoch: 21, Train_Loss: 0.22564586997032166, Test_Loss: 0.23944412171840668 *\n",
      "Epoch: 21, Train_Loss: 0.23454785346984863, Test_Loss: 0.251907616853714\n",
      "Epoch: 21, Train_Loss: 0.22776691615581512, Test_Loss: 0.256644606590271\n",
      "Epoch: 21, Train_Loss: 0.2331291139125824, Test_Loss: 0.261117160320282\n",
      "Epoch: 21, Train_Loss: 0.23225528001785278, Test_Loss: 0.2525007128715515 *\n",
      "Epoch: 21, Train_Loss: 0.2269328534603119, Test_Loss: 0.24548476934432983 *\n",
      "Epoch: 21, Train_Loss: 0.22582393884658813, Test_Loss: 0.2396073043346405 *\n",
      "Epoch: 21, Train_Loss: 0.2282874882221222, Test_Loss: 0.2396475076675415\n",
      "Epoch: 21, Train_Loss: 0.2405099719762802, Test_Loss: 0.23763996362686157 *\n",
      "Epoch: 21, Train_Loss: 0.23784582316875458, Test_Loss: 0.2310895472764969 *\n",
      "Epoch: 21, Train_Loss: 0.2381667196750641, Test_Loss: 0.25227996706962585\n",
      "Epoch: 21, Train_Loss: 0.24981699883937836, Test_Loss: 0.33090561628341675\n",
      "Epoch: 21, Train_Loss: 0.2505640983581543, Test_Loss: 0.24565228819847107 *\n",
      "Epoch: 21, Train_Loss: 0.2487800121307373, Test_Loss: 0.636598527431488\n",
      "Epoch: 21, Train_Loss: 0.2395576685667038, Test_Loss: 0.5919975638389587 *\n",
      "Epoch: 21, Train_Loss: 0.2532939314842224, Test_Loss: 0.33036893606185913 *\n",
      "Epoch: 21, Train_Loss: 0.26160097122192383, Test_Loss: 0.24896708130836487 *\n",
      "Epoch: 21, Train_Loss: 0.2319224327802658, Test_Loss: 0.24854205548763275 *\n",
      "Epoch: 21, Train_Loss: 0.2324678897857666, Test_Loss: 0.2364712804555893 *\n",
      "Epoch: 21, Train_Loss: 0.23653969168663025, Test_Loss: 0.31153416633605957\n",
      "Epoch: 21, Train_Loss: 0.23740258812904358, Test_Loss: 0.4453718662261963\n",
      "Epoch: 21, Train_Loss: 0.26980677247047424, Test_Loss: 0.5273796319961548\n",
      "Epoch: 21, Train_Loss: 0.2903176248073578, Test_Loss: 0.2830214500427246 *\n",
      "Epoch: 21, Train_Loss: 0.2490232139825821, Test_Loss: 0.2857843041419983\n",
      "Epoch: 21, Train_Loss: 0.2249145209789276, Test_Loss: 0.22882352769374847 *\n",
      "Epoch: 21, Train_Loss: 0.29013216495513916, Test_Loss: 0.22767852246761322 *\n",
      "Epoch: 21, Train_Loss: 0.2380368709564209, Test_Loss: 0.23280268907546997\n",
      "Epoch: 21, Train_Loss: 0.22935505211353302, Test_Loss: 0.23496229946613312\n",
      "Epoch: 21, Train_Loss: 0.23775756359100342, Test_Loss: 0.2648429870605469\n",
      "Model saved at location save_new\\model.ckpt at epoch 21\n",
      "Epoch: 21, Train_Loss: 0.2490915060043335, Test_Loss: 0.23269523680210114 *\n",
      "Epoch: 21, Train_Loss: 0.33061179518699646, Test_Loss: 0.24183809757232666\n",
      "Epoch: 21, Train_Loss: 0.27660202980041504, Test_Loss: 0.3461918830871582\n",
      "Epoch: 21, Train_Loss: 0.2539900839328766, Test_Loss: 0.6047760248184204\n",
      "Epoch: 21, Train_Loss: 0.2366383820772171, Test_Loss: 0.4620674252510071 *\n",
      "Epoch: 21, Train_Loss: 0.23563414812088013, Test_Loss: 0.2539602518081665 *\n",
      "Epoch: 21, Train_Loss: 0.24060973525047302, Test_Loss: 0.24721993505954742 *\n",
      "Epoch: 21, Train_Loss: 0.22704236209392548, Test_Loss: 0.24679270386695862 *\n",
      "Epoch: 21, Train_Loss: 0.2341039925813675, Test_Loss: 0.24735623598098755\n",
      "Epoch: 21, Train_Loss: 0.24013113975524902, Test_Loss: 0.24120089411735535 *\n",
      "Epoch: 21, Train_Loss: 0.24790027737617493, Test_Loss: 0.5981398224830627\n",
      "Epoch: 21, Train_Loss: 0.3240938186645508, Test_Loss: 5.435980319976807\n",
      "Epoch: 21, Train_Loss: 0.22848662734031677, Test_Loss: 0.28352606296539307 *\n",
      "Epoch: 21, Train_Loss: 0.2816086709499359, Test_Loss: 0.25181353092193604 *\n",
      "Epoch: 21, Train_Loss: 0.2412039041519165, Test_Loss: 0.23790526390075684 *\n",
      "Epoch: 21, Train_Loss: 0.24798768758773804, Test_Loss: 0.2323508858680725 *\n",
      "Epoch: 21, Train_Loss: 0.31707948446273804, Test_Loss: 0.22983689606189728 *\n",
      "Epoch: 21, Train_Loss: 0.4415348172187805, Test_Loss: 0.2385539412498474\n",
      "Epoch: 21, Train_Loss: 0.23611389100551605, Test_Loss: 0.2360440492630005 *\n",
      "Epoch: 21, Train_Loss: 0.25537770986557007, Test_Loss: 0.22580192983150482 *\n",
      "Epoch: 21, Train_Loss: 0.22436879575252533, Test_Loss: 0.22701334953308105\n",
      "Epoch: 21, Train_Loss: 0.22386708855628967, Test_Loss: 0.23741310834884644\n",
      "Epoch: 21, Train_Loss: 0.22521406412124634, Test_Loss: 0.3155508041381836\n",
      "Epoch: 21, Train_Loss: 0.22440288960933685, Test_Loss: 0.23968006670475006 *\n",
      "Epoch: 21, Train_Loss: 0.2329699695110321, Test_Loss: 0.24014432728290558\n",
      "Epoch: 21, Train_Loss: 0.23428507149219513, Test_Loss: 0.2725105285644531\n",
      "Epoch: 21, Train_Loss: 0.23461349308490753, Test_Loss: 0.22558927536010742 *\n",
      "Epoch: 21, Train_Loss: 0.22871460020542145, Test_Loss: 0.22551411390304565 *\n",
      "Epoch: 21, Train_Loss: 0.22957570850849152, Test_Loss: 0.22743482887744904\n",
      "Epoch: 21, Train_Loss: 0.2309834063053131, Test_Loss: 0.2485031932592392\n",
      "Epoch: 21, Train_Loss: 0.22485844790935516, Test_Loss: 0.22475090622901917 *\n",
      "Epoch: 21, Train_Loss: 0.22328749299049377, Test_Loss: 0.22632509469985962\n",
      "Epoch: 21, Train_Loss: 0.24297365546226501, Test_Loss: 0.2241126149892807 *\n",
      "Epoch: 21, Train_Loss: 0.24670995771884918, Test_Loss: 0.23162248730659485\n",
      "Epoch: 21, Train_Loss: 0.2504119873046875, Test_Loss: 0.23671954870224\n",
      "Epoch: 21, Train_Loss: 0.2254406362771988, Test_Loss: 0.22945550084114075 *\n",
      "Epoch: 21, Train_Loss: 0.26253560185432434, Test_Loss: 0.22471420466899872 *\n",
      "Epoch: 21, Train_Loss: 0.25423547625541687, Test_Loss: 0.22821928560733795\n",
      "Epoch: 21, Train_Loss: 0.2532612979412079, Test_Loss: 0.22599734365940094 *\n",
      "Epoch: 21, Train_Loss: 0.2304050177335739, Test_Loss: 0.2254398614168167 *\n",
      "Epoch: 21, Train_Loss: 0.2541756331920624, Test_Loss: 0.23516368865966797\n",
      "Epoch: 21, Train_Loss: 0.22358709573745728, Test_Loss: 0.296575665473938\n",
      "Epoch: 21, Train_Loss: 0.24454814195632935, Test_Loss: 2.821524143218994\n",
      "Epoch: 21, Train_Loss: 0.23557361960411072, Test_Loss: 3.177690029144287\n",
      "Epoch: 21, Train_Loss: 0.24774160981178284, Test_Loss: 0.2269638180732727 *\n",
      "Epoch: 21, Train_Loss: 1.5671972036361694, Test_Loss: 0.22364374995231628 *\n",
      "Epoch: 21, Train_Loss: 4.141185760498047, Test_Loss: 0.27040794491767883\n",
      "Epoch: 21, Train_Loss: 0.4208162724971771, Test_Loss: 0.25158533453941345 *\n",
      "Epoch: 21, Train_Loss: 0.2428562194108963, Test_Loss: 0.2629033029079437\n",
      "Epoch: 21, Train_Loss: 0.232283353805542, Test_Loss: 0.2807963788509369\n",
      "Epoch: 21, Train_Loss: 0.3169535994529724, Test_Loss: 0.33683857321739197\n",
      "Epoch: 21, Train_Loss: 0.27320361137390137, Test_Loss: 0.22681905329227448 *\n",
      "Epoch: 21, Train_Loss: 0.23697584867477417, Test_Loss: 0.24598656594753265\n",
      "Epoch: 21, Train_Loss: 0.2230812907218933, Test_Loss: 0.24530431628227234 *\n",
      "Epoch: 21, Train_Loss: 0.2841760516166687, Test_Loss: 0.2427157759666443 *\n",
      "Epoch: 21, Train_Loss: 0.2439306378364563, Test_Loss: 0.2311224788427353 *\n",
      "Epoch: 21, Train_Loss: 0.2321920543909073, Test_Loss: 0.2911939024925232\n",
      "Epoch: 21, Train_Loss: 0.5582643151283264, Test_Loss: 0.2816810607910156 *\n",
      "Epoch: 21, Train_Loss: 0.7703974843025208, Test_Loss: 0.30813080072402954\n",
      "Epoch: 21, Train_Loss: 1.0324808359146118, Test_Loss: 0.3054461181163788 *\n",
      "Epoch: 21, Train_Loss: 0.30099841952323914, Test_Loss: 0.24332444369792938 *\n",
      "Epoch: 21, Train_Loss: 0.4251737594604492, Test_Loss: 0.27045631408691406\n",
      "Epoch: 21, Train_Loss: 1.902980089187622, Test_Loss: 0.2271936535835266 *\n",
      "Epoch: 21, Train_Loss: 0.901699423789978, Test_Loss: 0.23014426231384277\n",
      "Epoch: 21, Train_Loss: 0.23419906198978424, Test_Loss: 0.23077033460140228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train_Loss: 0.23494386672973633, Test_Loss: 0.23983347415924072\n",
      "Epoch: 21, Train_Loss: 0.6812522411346436, Test_Loss: 0.22908857464790344 *\n",
      "Epoch: 21, Train_Loss: 0.6158159971237183, Test_Loss: 0.22517777979373932 *\n",
      "Epoch: 21, Train_Loss: 0.7783161401748657, Test_Loss: 0.23521742224693298\n",
      "Epoch: 21, Train_Loss: 0.2399766445159912, Test_Loss: 0.2335422933101654 *\n",
      "Epoch: 21, Train_Loss: 0.2567835748195648, Test_Loss: 0.23271825909614563 *\n",
      "Epoch: 21, Train_Loss: 0.45845454931259155, Test_Loss: 0.24661487340927124\n",
      "Epoch: 21, Train_Loss: 0.3881608247756958, Test_Loss: 0.2352430671453476 *\n",
      "Epoch: 21, Train_Loss: 0.2829585671424866, Test_Loss: 0.28970277309417725\n",
      "Epoch: 21, Train_Loss: 0.3088001310825348, Test_Loss: 0.2706097364425659 *\n",
      "Epoch: 21, Train_Loss: 0.257482647895813, Test_Loss: 0.4113304615020752\n",
      "Epoch: 21, Train_Loss: 0.24287430942058563, Test_Loss: 0.40506094694137573 *\n",
      "Epoch: 21, Train_Loss: 0.34198522567749023, Test_Loss: 0.26312920451164246 *\n",
      "Epoch: 21, Train_Loss: 0.3375927209854126, Test_Loss: 0.28231027722358704\n",
      "Epoch: 21, Train_Loss: 0.2577627897262573, Test_Loss: 0.2685137391090393 *\n",
      "Epoch: 21, Train_Loss: 0.32097554206848145, Test_Loss: 0.23531383275985718 *\n",
      "Epoch: 21, Train_Loss: 0.34239524602890015, Test_Loss: 0.32016801834106445\n",
      "Epoch: 21, Train_Loss: 0.32834291458129883, Test_Loss: 0.2642231285572052 *\n",
      "Epoch: 21, Train_Loss: 0.344177782535553, Test_Loss: 0.5809131860733032\n",
      "Epoch: 21, Train_Loss: 0.45357745885849, Test_Loss: 0.3191607594490051 *\n",
      "Epoch: 21, Train_Loss: 0.24861609935760498, Test_Loss: 0.29020631313323975 *\n",
      "Epoch: 21, Train_Loss: 0.2494792342185974, Test_Loss: 0.24391797184944153 *\n",
      "Epoch: 21, Train_Loss: 0.2659377157688141, Test_Loss: 0.22573456168174744 *\n",
      "Epoch: 21, Train_Loss: 0.267132967710495, Test_Loss: 0.23626092076301575\n",
      "Epoch: 21, Train_Loss: 0.2242719680070877, Test_Loss: 0.2808718979358673\n",
      "Epoch: 21, Train_Loss: 0.2245788276195526, Test_Loss: 0.23212318122386932 *\n",
      "Epoch: 21, Train_Loss: 0.22357581555843353, Test_Loss: 0.28831708431243896\n",
      "Epoch: 21, Train_Loss: 0.2287154644727707, Test_Loss: 0.37484461069107056\n",
      "Epoch: 21, Train_Loss: 0.2346077412366867, Test_Loss: 0.4224347770214081\n",
      "Epoch: 21, Train_Loss: 0.241636723279953, Test_Loss: 0.5846966505050659\n",
      "Epoch: 21, Train_Loss: 0.23479434847831726, Test_Loss: 0.6652925610542297\n",
      "Epoch: 21, Train_Loss: 0.27473410964012146, Test_Loss: 0.4144590198993683 *\n",
      "Epoch: 21, Train_Loss: 0.3843206465244293, Test_Loss: 0.392598032951355 *\n",
      "Epoch: 21, Train_Loss: 0.3670518398284912, Test_Loss: 0.38996177911758423 *\n",
      "Epoch: 21, Train_Loss: 0.2413969337940216, Test_Loss: 0.390005886554718\n",
      "Model saved at location save_new\\model.ckpt at epoch 21\n",
      "Epoch: 21, Train_Loss: 0.27643242478370667, Test_Loss: 0.37083396315574646 *\n",
      "Epoch: 21, Train_Loss: 0.3210197687149048, Test_Loss: 1.5438783168792725\n",
      "Epoch: 21, Train_Loss: 0.3373456597328186, Test_Loss: 4.404000282287598\n",
      "Epoch: 21, Train_Loss: 0.3713662624359131, Test_Loss: 0.3086731731891632 *\n",
      "Epoch: 21, Train_Loss: 0.4188736081123352, Test_Loss: 0.2878628373146057 *\n",
      "Epoch: 21, Train_Loss: 0.32869282364845276, Test_Loss: 0.2713017463684082 *\n",
      "Epoch: 21, Train_Loss: 0.2801922857761383, Test_Loss: 0.24595651030540466 *\n",
      "Epoch: 21, Train_Loss: 0.35172080993652344, Test_Loss: 0.23318549990653992 *\n",
      "Epoch: 21, Train_Loss: 0.23929128050804138, Test_Loss: 0.30818605422973633\n",
      "Epoch: 21, Train_Loss: 0.2548644542694092, Test_Loss: 0.26556432247161865 *\n",
      "Epoch: 21, Train_Loss: 0.5928449630737305, Test_Loss: 0.23044443130493164 *\n",
      "Epoch: 21, Train_Loss: 0.546334981918335, Test_Loss: 0.2382785826921463\n",
      "Epoch: 21, Train_Loss: 0.32145723700523376, Test_Loss: 0.265238493680954\n",
      "Epoch: 21, Train_Loss: 0.26404303312301636, Test_Loss: 0.4221903681755066\n",
      "Epoch: 21, Train_Loss: 0.2919566035270691, Test_Loss: 0.24906696379184723 *\n",
      "Epoch: 21, Train_Loss: 0.23708416521549225, Test_Loss: 0.25053563714027405\n",
      "Epoch: 21, Train_Loss: 0.5382363796234131, Test_Loss: 0.2794490456581116\n",
      "Epoch: 21, Train_Loss: 0.2573838531970978, Test_Loss: 0.2520434558391571 *\n",
      "Epoch: 21, Train_Loss: 0.2658560276031494, Test_Loss: 0.26311981678009033\n",
      "Epoch: 21, Train_Loss: 0.3656584918498993, Test_Loss: 0.31038931012153625\n",
      "Epoch: 21, Train_Loss: 0.24624672532081604, Test_Loss: 0.3052436113357544 *\n",
      "Epoch: 21, Train_Loss: 0.24205876886844635, Test_Loss: 0.241973876953125 *\n",
      "Epoch: 21, Train_Loss: 0.3010869324207306, Test_Loss: 0.2539830207824707\n",
      "Epoch: 21, Train_Loss: 0.3530529737472534, Test_Loss: 0.22776643931865692 *\n",
      "Epoch: 21, Train_Loss: 0.26555001735687256, Test_Loss: 0.25061410665512085\n",
      "Epoch: 21, Train_Loss: 0.3055117726325989, Test_Loss: 0.27588745951652527\n",
      "Epoch: 21, Train_Loss: 0.24489904940128326, Test_Loss: 0.23043935000896454 *\n",
      "Epoch: 21, Train_Loss: 0.2886073589324951, Test_Loss: 0.22538816928863525 *\n",
      "Epoch: 21, Train_Loss: 0.2643173933029175, Test_Loss: 0.2529810965061188\n",
      "Epoch: 21, Train_Loss: 0.23619931936264038, Test_Loss: 0.2598533034324646\n",
      "Epoch: 21, Train_Loss: 0.22704359889030457, Test_Loss: 0.23542024195194244 *\n",
      "Epoch: 21, Train_Loss: 0.295060396194458, Test_Loss: 0.2812303900718689\n",
      "Epoch: 21, Train_Loss: 0.4927521347999573, Test_Loss: 0.3089999556541443\n",
      "Epoch: 21, Train_Loss: 0.47133955359458923, Test_Loss: 3.7691810131073\n",
      "Epoch: 21, Train_Loss: 0.5503207445144653, Test_Loss: 1.982558012008667 *\n",
      "Epoch: 21, Train_Loss: 0.6538430452346802, Test_Loss: 0.2354666292667389 *\n",
      "Epoch: 21, Train_Loss: 0.42354461550712585, Test_Loss: 0.2327652871608734 *\n",
      "Epoch: 21, Train_Loss: 0.3491968810558319, Test_Loss: 0.25824108719825745\n",
      "Epoch: 21, Train_Loss: 0.28283125162124634, Test_Loss: 0.22799476981163025 *\n",
      "Epoch: 21, Train_Loss: 0.23421356081962585, Test_Loss: 0.23739847540855408\n",
      "Epoch: 21, Train_Loss: 0.23182857036590576, Test_Loss: 0.3021922707557678\n",
      "Epoch: 21, Train_Loss: 0.25421351194381714, Test_Loss: 0.27889394760131836 *\n",
      "Epoch: 21, Train_Loss: 0.4492696523666382, Test_Loss: 0.2277664691209793 *\n",
      "Epoch: 21, Train_Loss: 0.5143906474113464, Test_Loss: 0.24767528474330902\n",
      "Epoch: 21, Train_Loss: 0.550993025302887, Test_Loss: 0.24965985119342804\n",
      "Epoch: 21, Train_Loss: 1.237675666809082, Test_Loss: 0.27996906638145447\n",
      "Epoch: 21, Train_Loss: 0.7093002796173096, Test_Loss: 0.23222321271896362 *\n",
      "Epoch: 21, Train_Loss: 0.43712118268013, Test_Loss: 0.24988052248954773\n",
      "Epoch: 21, Train_Loss: 0.24569502472877502, Test_Loss: 0.28090396523475647\n",
      "Epoch: 21, Train_Loss: 0.23033057153224945, Test_Loss: 0.28861069679260254\n",
      "Epoch: 21, Train_Loss: 0.4426186680793762, Test_Loss: 0.24142178893089294 *\n",
      "Epoch: 21, Train_Loss: 0.7486556768417358, Test_Loss: 0.2808508574962616\n",
      "Epoch: 21, Train_Loss: 0.5362080931663513, Test_Loss: 0.2811284363269806\n",
      "Epoch: 21, Train_Loss: 0.2741059958934784, Test_Loss: 0.24354946613311768 *\n",
      "Epoch: 21, Train_Loss: 0.24379585683345795, Test_Loss: 0.25714612007141113\n",
      "Epoch: 21, Train_Loss: 0.3005967438220978, Test_Loss: 0.26167032122612\n",
      "Epoch: 21, Train_Loss: 0.4984279274940491, Test_Loss: 0.2418588101863861 *\n",
      "Epoch: 21, Train_Loss: 0.37219348549842834, Test_Loss: 0.24771806597709656\n",
      "Epoch: 21, Train_Loss: 0.27369943261146545, Test_Loss: 0.274015337228775\n",
      "Epoch: 21, Train_Loss: 0.3562629520893097, Test_Loss: 0.28077811002731323\n",
      "Epoch: 21, Train_Loss: 0.2449021339416504, Test_Loss: 0.27011778950691223 *\n",
      "Epoch: 21, Train_Loss: 0.22942054271697998, Test_Loss: 0.269649863243103 *\n",
      "Epoch: 21, Train_Loss: 0.2675527036190033, Test_Loss: 0.2636290192604065 *\n",
      "Epoch: 21, Train_Loss: 0.22913065552711487, Test_Loss: 0.22515994310379028 *\n",
      "Epoch: 21, Train_Loss: 0.2870848774909973, Test_Loss: 0.24512411653995514\n",
      "Epoch: 21, Train_Loss: 0.2887764573097229, Test_Loss: 0.3102797269821167\n",
      "Epoch: 21, Train_Loss: 2.6997427940368652, Test_Loss: 0.31220370531082153\n",
      "Epoch: 21, Train_Loss: 13.501847267150879, Test_Loss: 0.30387455224990845 *\n",
      "Epoch: 21, Train_Loss: 0.7347214221954346, Test_Loss: 0.26391738653182983 *\n",
      "Epoch: 21, Train_Loss: 1.0898650884628296, Test_Loss: 0.3086480498313904\n",
      "Epoch: 21, Train_Loss: 0.7117840647697449, Test_Loss: 0.2643483281135559 *\n",
      "Epoch: 21, Train_Loss: 0.28734228014945984, Test_Loss: 0.2611194849014282 *\n",
      "Epoch: 21, Train_Loss: 0.5057051181793213, Test_Loss: 0.41218888759613037\n",
      "Epoch: 21, Train_Loss: 4.973991870880127, Test_Loss: 0.3299420177936554 *\n",
      "Epoch: 21, Train_Loss: 2.111078977584839, Test_Loss: 0.6354896426200867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train_Loss: 0.29915541410446167, Test_Loss: 0.3365137279033661 *\n",
      "Epoch: 21, Train_Loss: 1.730254054069519, Test_Loss: 0.26885470747947693 *\n",
      "Epoch: 21, Train_Loss: 3.5944108963012695, Test_Loss: 0.33140915632247925\n",
      "Epoch: 21, Train_Loss: 0.6003901958465576, Test_Loss: 0.30972784757614136 *\n",
      "Epoch: 21, Train_Loss: 0.2320525050163269, Test_Loss: 0.35792046785354614\n",
      "Epoch: 21, Train_Loss: 0.24814710021018982, Test_Loss: 0.2462497055530548 *\n",
      "Epoch: 21, Train_Loss: 0.2802032232284546, Test_Loss: 0.2766716480255127\n",
      "Epoch: 21, Train_Loss: 0.2654724419116974, Test_Loss: 0.2392510622739792 *\n",
      "Epoch: 21, Train_Loss: 0.22228530049324036, Test_Loss: 0.4139072299003601\n",
      "Epoch: 21, Train_Loss: 0.22113078832626343, Test_Loss: 0.8899639248847961\n",
      "Epoch: 21, Train_Loss: 0.219700887799263, Test_Loss: 0.4322807788848877 *\n",
      "Epoch: 21, Train_Loss: 0.2209976613521576, Test_Loss: 0.32749226689338684 *\n",
      "Epoch: 21, Train_Loss: 0.24057823419570923, Test_Loss: 0.6146715879440308\n",
      "Epoch: 21, Train_Loss: 0.2375112622976303, Test_Loss: 0.7815579771995544\n",
      "Epoch: 21, Train_Loss: 0.28779906034469604, Test_Loss: 0.799797534942627\n",
      "Epoch: 21, Train_Loss: 0.3616587519645691, Test_Loss: 0.7900954484939575 *\n",
      "Epoch: 21, Train_Loss: 0.2515505850315094, Test_Loss: 0.28524813055992126 *\n",
      "Epoch: 21, Train_Loss: 0.2515253722667694, Test_Loss: 4.3392333984375\n",
      "Epoch: 21, Train_Loss: 0.24785266816616058, Test_Loss: 3.5347864627838135 *\n",
      "Epoch: 21, Train_Loss: 0.24177180230617523, Test_Loss: 0.5559853315353394 *\n",
      "Epoch: 21, Train_Loss: 0.22470758855342865, Test_Loss: 0.7713020443916321\n",
      "Epoch: 21, Train_Loss: 0.222458615899086, Test_Loss: 0.8961685299873352\n",
      "Epoch: 21, Train_Loss: 0.22042591869831085, Test_Loss: 0.3280370533466339 *\n",
      "Epoch: 21, Train_Loss: 0.220742866396904, Test_Loss: 0.715309202671051\n",
      "Epoch: 21, Train_Loss: 0.2223612666130066, Test_Loss: 1.176241397857666\n",
      "Model saved at location save_new\\model.ckpt at epoch 21\n",
      "Epoch: 21, Train_Loss: 0.2230554074048996, Test_Loss: 0.885635256767273 *\n",
      "Epoch: 21, Train_Loss: 0.22179196774959564, Test_Loss: 0.775786280632019 *\n",
      "Epoch: 21, Train_Loss: 0.22353890538215637, Test_Loss: 1.1811054944992065\n",
      "Epoch: 21, Train_Loss: 0.24414180219173431, Test_Loss: 1.0472739934921265 *\n",
      "Epoch: 21, Train_Loss: 0.2467050403356552, Test_Loss: 1.9674566984176636\n",
      "Epoch: 21, Train_Loss: 0.3144375681877136, Test_Loss: 1.0766370296478271 *\n",
      "Epoch: 21, Train_Loss: 0.2560034394264221, Test_Loss: 1.5664461851119995\n",
      "Epoch: 21, Train_Loss: 0.39681074023246765, Test_Loss: 1.0286741256713867 *\n",
      "Epoch: 21, Train_Loss: 6.521250247955322, Test_Loss: 0.2716536223888397 *\n",
      "Epoch: 21, Train_Loss: 1.2591958045959473, Test_Loss: 0.37403154373168945\n",
      "Epoch: 21, Train_Loss: 0.24977648258209229, Test_Loss: 0.4063926041126251\n",
      "Epoch: 21, Train_Loss: 0.2772452235221863, Test_Loss: 0.7419269680976868\n",
      "Epoch: 21, Train_Loss: 0.3362300395965576, Test_Loss: 0.3540256917476654 *\n",
      "Epoch: 21, Train_Loss: 0.2843322455883026, Test_Loss: 0.6367273330688477\n",
      "Epoch: 21, Train_Loss: 0.31411656737327576, Test_Loss: 0.35657423734664917 *\n",
      "Epoch: 21, Train_Loss: 0.39223816990852356, Test_Loss: 0.5059096217155457\n",
      "Epoch: 21, Train_Loss: 0.39181506633758545, Test_Loss: 0.5884265899658203\n",
      "Epoch: 21, Train_Loss: 0.33509886264801025, Test_Loss: 0.3085037171840668 *\n",
      "Epoch: 21, Train_Loss: 0.3025236427783966, Test_Loss: 0.2928406298160553 *\n",
      "Epoch: 21, Train_Loss: 0.23048508167266846, Test_Loss: 0.374197781085968\n",
      "Epoch: 21, Train_Loss: 0.24343858659267426, Test_Loss: 0.32446566224098206 *\n",
      "Epoch: 21, Train_Loss: 0.22448256611824036, Test_Loss: 0.2462008148431778 *\n",
      "Epoch: 21, Train_Loss: 0.5092450380325317, Test_Loss: 0.5846123099327087\n",
      "Epoch: 21, Train_Loss: 0.24030974507331848, Test_Loss: 0.3722307085990906 *\n",
      "Epoch: 21, Train_Loss: 0.2617761492729187, Test_Loss: 5.4813971519470215\n",
      "Epoch: 21, Train_Loss: 0.2612914443016052, Test_Loss: 1.1994636058807373 *\n",
      "Epoch: 21, Train_Loss: 0.2549457848072052, Test_Loss: 0.22325049340724945 *\n",
      "Epoch: 21, Train_Loss: 0.24502845108509064, Test_Loss: 0.2397179752588272\n",
      "Epoch: 21, Train_Loss: 0.23297996819019318, Test_Loss: 0.24795055389404297\n",
      "Epoch: 21, Train_Loss: 0.2630961239337921, Test_Loss: 0.23730985820293427 *\n",
      "Epoch: 21, Train_Loss: 0.22860956192016602, Test_Loss: 0.23971165716648102\n",
      "Epoch: 21, Train_Loss: 0.22874672710895538, Test_Loss: 0.34283527731895447\n",
      "Epoch: 21, Train_Loss: 0.2991304397583008, Test_Loss: 0.29940277338027954 *\n",
      "Epoch: 21, Train_Loss: 4.368454933166504, Test_Loss: 0.22133256494998932 *\n",
      "Epoch: 21, Train_Loss: 0.41645050048828125, Test_Loss: 0.25646573305130005\n",
      "Epoch: 21, Train_Loss: 0.21950587630271912, Test_Loss: 0.23356029391288757 *\n",
      "Epoch: 21, Train_Loss: 0.2204027622938156, Test_Loss: 0.23132489621639252 *\n",
      "Epoch: 21, Train_Loss: 0.22203612327575684, Test_Loss: 0.23913118243217468\n",
      "Epoch: 21, Train_Loss: 0.21951721608638763, Test_Loss: 0.2597607374191284\n",
      "Epoch: 21, Train_Loss: 0.23141497373580933, Test_Loss: 0.29845160245895386\n",
      "Epoch: 21, Train_Loss: 0.22152544558048248, Test_Loss: 0.3745564818382263\n",
      "Epoch: 21, Train_Loss: 0.2566247582435608, Test_Loss: 0.308626651763916 *\n",
      "Epoch: 21, Train_Loss: 0.229843407869339, Test_Loss: 0.24398618936538696 *\n",
      "Epoch: 21, Train_Loss: 0.2508181929588318, Test_Loss: 0.2917797267436981\n",
      "Epoch: 21, Train_Loss: 0.21929429471492767, Test_Loss: 0.3193894028663635\n",
      "Epoch: 21, Train_Loss: 0.21925228834152222, Test_Loss: 0.28662601113319397 *\n",
      "Epoch: 21, Train_Loss: 0.22941580414772034, Test_Loss: 0.3160474896430969\n",
      "Epoch: 21, Train_Loss: 0.2331850379705429, Test_Loss: 0.27409762144088745 *\n",
      "Epoch: 21, Train_Loss: 0.22504039108753204, Test_Loss: 0.2737768888473511 *\n",
      "Epoch: 21, Train_Loss: 0.23132896423339844, Test_Loss: 0.3432888090610504\n",
      "Epoch: 21, Train_Loss: 0.23951752483844757, Test_Loss: 0.29958587884902954 *\n",
      "Epoch: 21, Train_Loss: 0.23552271723747253, Test_Loss: 0.2676921486854553 *\n",
      "Epoch: 21, Train_Loss: 0.2199627310037613, Test_Loss: 0.273837149143219\n",
      "Epoch: 21, Train_Loss: 0.22142741084098816, Test_Loss: 0.2578727602958679 *\n",
      "Epoch: 22, Train_Loss: 0.26023656129837036, Test_Loss: 0.22451303899288177 *\n",
      "Epoch: 22, Train_Loss: 0.23503902554512024, Test_Loss: 0.24831144511699677\n",
      "Epoch: 22, Train_Loss: 0.2483561784029007, Test_Loss: 0.3755462169647217\n",
      "Epoch: 22, Train_Loss: 0.248918816447258, Test_Loss: 0.28755807876586914 *\n",
      "Epoch: 22, Train_Loss: 0.30724096298217773, Test_Loss: 0.301585853099823\n",
      "Epoch: 22, Train_Loss: 0.2485867589712143, Test_Loss: 0.2682816684246063 *\n",
      "Epoch: 22, Train_Loss: 0.23955057561397552, Test_Loss: 0.2794075608253479\n",
      "Epoch: 22, Train_Loss: 0.26366302371025085, Test_Loss: 0.24835002422332764 *\n",
      "Epoch: 22, Train_Loss: 0.37770378589630127, Test_Loss: 0.2923719584941864\n",
      "Epoch: 22, Train_Loss: 0.2539719045162201, Test_Loss: 0.5424872040748596\n",
      "Epoch: 22, Train_Loss: 0.28092727065086365, Test_Loss: 0.35204821825027466 *\n",
      "Epoch: 22, Train_Loss: 0.21850034594535828, Test_Loss: 0.48421481251716614\n",
      "Epoch: 22, Train_Loss: 0.22331015765666962, Test_Loss: 0.2796364724636078 *\n",
      "Epoch: 22, Train_Loss: 0.22294825315475464, Test_Loss: 0.25938475131988525 *\n",
      "Epoch: 22, Train_Loss: 0.2225821614265442, Test_Loss: 0.24649326503276825 *\n",
      "Epoch: 22, Train_Loss: 0.22488707304000854, Test_Loss: 0.23435896635055542 *\n",
      "Epoch: 22, Train_Loss: 4.8993754386901855, Test_Loss: 0.24174180626869202\n",
      "Epoch: 22, Train_Loss: 0.6946326494216919, Test_Loss: 0.23662197589874268 *\n",
      "Epoch: 22, Train_Loss: 0.22207224369049072, Test_Loss: 0.24853582680225372\n",
      "Epoch: 22, Train_Loss: 0.23840762674808502, Test_Loss: 0.22590671479701996 *\n",
      "Epoch: 22, Train_Loss: 0.22221212089061737, Test_Loss: 0.33479517698287964\n",
      "Epoch: 22, Train_Loss: 0.21995848417282104, Test_Loss: 0.5563139915466309\n",
      "Epoch: 22, Train_Loss: 0.22020603716373444, Test_Loss: 0.3246157169342041 *\n",
      "Epoch: 22, Train_Loss: 0.2181529402732849, Test_Loss: 0.45893120765686035\n",
      "Epoch: 22, Train_Loss: 0.21862944960594177, Test_Loss: 0.2540431618690491 *\n",
      "Epoch: 22, Train_Loss: 0.219587504863739, Test_Loss: 0.25433972477912903\n",
      "Epoch: 22, Train_Loss: 0.249515101313591, Test_Loss: 0.2552533447742462\n",
      "Epoch: 22, Train_Loss: 0.2607933282852173, Test_Loss: 0.25341418385505676 *\n",
      "Epoch: 22, Train_Loss: 0.2888188660144806, Test_Loss: 0.26082977652549744\n",
      "Epoch: 22, Train_Loss: 0.25298967957496643, Test_Loss: 4.186017990112305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train_Loss: 0.22365130484104156, Test_Loss: 1.6739072799682617 *\n",
      "Epoch: 22, Train_Loss: 0.2978864908218384, Test_Loss: 0.2425447702407837 *\n",
      "Epoch: 22, Train_Loss: 0.44553714990615845, Test_Loss: 0.22895151376724243 *\n",
      "Epoch: 22, Train_Loss: 0.43516820669174194, Test_Loss: 0.22923266887664795\n",
      "Epoch: 22, Train_Loss: 0.4164717197418213, Test_Loss: 0.2360762506723404\n",
      "Epoch: 22, Train_Loss: 0.22645407915115356, Test_Loss: 0.22407114505767822 *\n",
      "Epoch: 22, Train_Loss: 0.2187688797712326, Test_Loss: 0.2270042598247528\n",
      "Epoch: 22, Train_Loss: 0.21841949224472046, Test_Loss: 0.2193482220172882 *\n",
      "Epoch: 22, Train_Loss: 0.22460432350635529, Test_Loss: 0.21961627900600433\n",
      "Epoch: 22, Train_Loss: 0.23154719173908234, Test_Loss: 0.2207174301147461\n",
      "Epoch: 22, Train_Loss: 0.23623481392860413, Test_Loss: 0.22503714263439178\n",
      "Epoch: 22, Train_Loss: 0.2338239699602127, Test_Loss: 0.23928454518318176\n",
      "Epoch: 22, Train_Loss: 0.21786965429782867, Test_Loss: 0.24588444828987122\n",
      "Epoch: 22, Train_Loss: 0.22504198551177979, Test_Loss: 0.232757106423378 *\n",
      "Epoch: 22, Train_Loss: 0.24454310536384583, Test_Loss: 0.21920880675315857 *\n",
      "Epoch: 22, Train_Loss: 0.3715657889842987, Test_Loss: 0.21891948580741882 *\n",
      "Epoch: 22, Train_Loss: 0.3754783868789673, Test_Loss: 0.22093695402145386\n",
      "Epoch: 22, Train_Loss: 0.34920167922973633, Test_Loss: 0.2386750876903534\n",
      "Epoch: 22, Train_Loss: 0.280449777841568, Test_Loss: 0.22644807398319244 *\n",
      "Epoch: 22, Train_Loss: 0.31553277373313904, Test_Loss: 0.22143445909023285 *\n",
      "Epoch: 22, Train_Loss: 0.3107936382293701, Test_Loss: 0.2179681658744812 *\n",
      "Epoch: 22, Train_Loss: 0.23447787761688232, Test_Loss: 0.22676698863506317\n",
      "Epoch: 22, Train_Loss: 0.31982842087745667, Test_Loss: 0.2377476990222931\n",
      "Epoch: 22, Train_Loss: 0.27776774764060974, Test_Loss: 0.24809937179088593\n",
      "Epoch: 22, Train_Loss: 0.436038076877594, Test_Loss: 0.22253206372261047 *\n",
      "Epoch: 22, Train_Loss: 0.22332128882408142, Test_Loss: 0.22060999274253845 *\n",
      "Epoch: 22, Train_Loss: 1.5205342769622803, Test_Loss: 0.22665250301361084\n",
      "Epoch: 22, Train_Loss: 1.8638708591461182, Test_Loss: 0.22732919454574585\n",
      "Epoch: 22, Train_Loss: 0.26508471369743347, Test_Loss: 0.21920627355575562 *\n",
      "Epoch: 22, Train_Loss: 0.26874542236328125, Test_Loss: 0.30199888348579407\n",
      "Epoch: 22, Train_Loss: 0.22692081332206726, Test_Loss: 0.2826325297355652 *\n",
      "Epoch: 22, Train_Loss: 0.23389631509780884, Test_Loss: 5.299114227294922\n",
      "Epoch: 22, Train_Loss: 0.22501444816589355, Test_Loss: 0.2743458151817322 *\n",
      "Epoch: 22, Train_Loss: 0.24681620299816132, Test_Loss: 0.22103191912174225 *\n",
      "Epoch: 22, Train_Loss: 0.3547239303588867, Test_Loss: 0.2521151602268219\n",
      "Epoch: 22, Train_Loss: 0.29067331552505493, Test_Loss: 0.2609420418739319\n",
      "Epoch: 22, Train_Loss: 0.2659256160259247, Test_Loss: 0.272050678730011\n",
      "Epoch: 22, Train_Loss: 0.23519599437713623, Test_Loss: 0.22233827412128448 *\n",
      "Epoch: 22, Train_Loss: 0.23663318157196045, Test_Loss: 0.29534241557121277\n",
      "Epoch: 22, Train_Loss: 0.22597527503967285, Test_Loss: 0.25873854756355286 *\n",
      "Epoch: 22, Train_Loss: 0.23241111636161804, Test_Loss: 0.22454367578029633 *\n",
      "Epoch: 22, Train_Loss: 0.25826337933540344, Test_Loss: 0.23841595649719238\n",
      "Epoch: 22, Train_Loss: 0.25195395946502686, Test_Loss: 0.25105994939804077\n",
      "Epoch: 22, Train_Loss: 0.22911684215068817, Test_Loss: 0.22007715702056885 *\n",
      "Epoch: 22, Train_Loss: 0.21950307488441467, Test_Loss: 0.2591340243816376\n",
      "Epoch: 22, Train_Loss: 0.2359897494316101, Test_Loss: 0.29689821600914\n",
      "Epoch: 22, Train_Loss: 0.23936814069747925, Test_Loss: 0.2557496130466461 *\n",
      "Epoch: 22, Train_Loss: 0.23181980848312378, Test_Loss: 0.2845938205718994\n",
      "Epoch: 22, Train_Loss: 0.22015121579170227, Test_Loss: 0.2530279755592346 *\n",
      "Epoch: 22, Train_Loss: 0.21843203902244568, Test_Loss: 0.2895328402519226\n",
      "Epoch: 22, Train_Loss: 0.2174292802810669, Test_Loss: 0.23829972743988037 *\n",
      "Epoch: 22, Train_Loss: 0.22378121316432953, Test_Loss: 0.24305053055286407\n",
      "Epoch: 22, Train_Loss: 0.21834826469421387, Test_Loss: 0.2504248321056366\n",
      "Epoch: 22, Train_Loss: 0.22616024315357208, Test_Loss: 0.2521716356277466\n",
      "Epoch: 22, Train_Loss: 0.2247355729341507, Test_Loss: 0.24817100167274475 *\n",
      "Epoch: 22, Train_Loss: 0.21932357549667358, Test_Loss: 0.24371927976608276 *\n",
      "Epoch: 22, Train_Loss: 0.21863500773906708, Test_Loss: 0.2298094481229782 *\n",
      "Epoch: 22, Train_Loss: 0.219740629196167, Test_Loss: 0.23988741636276245\n",
      "Epoch: 22, Train_Loss: 0.23003754019737244, Test_Loss: 0.23643870651721954 *\n",
      "Epoch: 22, Train_Loss: 0.2300196886062622, Test_Loss: 0.22465495765209198 *\n",
      "Epoch: 22, Train_Loss: 0.22888724505901337, Test_Loss: 0.23604294657707214\n",
      "Epoch: 22, Train_Loss: 0.24684521555900574, Test_Loss: 0.3034007251262665\n",
      "Epoch: 22, Train_Loss: 0.23171092569828033, Test_Loss: 0.2606101632118225 *\n",
      "Epoch: 22, Train_Loss: 0.23537760972976685, Test_Loss: 0.5112250447273254\n",
      "Epoch: 22, Train_Loss: 0.22822855412960052, Test_Loss: 0.5735752582550049\n",
      "Epoch: 22, Train_Loss: 0.23058286309242249, Test_Loss: 0.3777332305908203 *\n",
      "Epoch: 22, Train_Loss: 0.2571754455566406, Test_Loss: 0.25882601737976074 *\n",
      "Epoch: 22, Train_Loss: 0.22997340559959412, Test_Loss: 0.23708724975585938 *\n",
      "Epoch: 22, Train_Loss: 0.22143371403217316, Test_Loss: 0.22189491987228394 *\n",
      "Epoch: 22, Train_Loss: 0.2219800502061844, Test_Loss: 0.2660062313079834\n",
      "Model saved at location save_new\\model.ckpt at epoch 22\n",
      "Epoch: 22, Train_Loss: 0.22595354914665222, Test_Loss: 0.5236554145812988\n",
      "Epoch: 22, Train_Loss: 0.25045472383499146, Test_Loss: 0.38777148723602295 *\n",
      "Epoch: 22, Train_Loss: 0.28230616450309753, Test_Loss: 0.33379504084587097 *\n",
      "Epoch: 22, Train_Loss: 0.2623227536678314, Test_Loss: 0.3067305088043213 *\n",
      "Epoch: 22, Train_Loss: 0.2169950157403946, Test_Loss: 0.22177977859973907 *\n",
      "Epoch: 22, Train_Loss: 0.26950037479400635, Test_Loss: 0.22226248681545258\n",
      "Epoch: 22, Train_Loss: 0.24153804779052734, Test_Loss: 0.2227099984884262\n",
      "Epoch: 22, Train_Loss: 0.21891075372695923, Test_Loss: 0.2302723228931427\n",
      "Epoch: 22, Train_Loss: 0.23342642188072205, Test_Loss: 0.23498757183551788\n",
      "Epoch: 22, Train_Loss: 0.24775119125843048, Test_Loss: 0.2404155433177948\n",
      "Epoch: 22, Train_Loss: 0.3078297972679138, Test_Loss: 0.22070257365703583 *\n",
      "Epoch: 22, Train_Loss: 0.2712175250053406, Test_Loss: 0.34529879689216614\n",
      "Epoch: 22, Train_Loss: 0.24842217564582825, Test_Loss: 0.5955895781517029\n",
      "Epoch: 22, Train_Loss: 0.23707419633865356, Test_Loss: 0.30983781814575195 *\n",
      "Epoch: 22, Train_Loss: 0.2267867922782898, Test_Loss: 0.40368038415908813\n",
      "Epoch: 22, Train_Loss: 0.2337646335363388, Test_Loss: 0.2427120953798294 *\n",
      "Epoch: 22, Train_Loss: 0.2170240581035614, Test_Loss: 0.24294951558113098\n",
      "Epoch: 22, Train_Loss: 0.2229938507080078, Test_Loss: 0.24293194711208344 *\n",
      "Epoch: 22, Train_Loss: 0.2325277030467987, Test_Loss: 0.2389461100101471 *\n",
      "Epoch: 22, Train_Loss: 0.23819465935230255, Test_Loss: 0.24686112999916077\n",
      "Epoch: 22, Train_Loss: 0.31497496366500854, Test_Loss: 5.513284683227539\n",
      "Epoch: 22, Train_Loss: 0.2167355716228485, Test_Loss: 0.6544021964073181 *\n",
      "Epoch: 22, Train_Loss: 0.2751719653606415, Test_Loss: 0.23746158182621002 *\n",
      "Epoch: 22, Train_Loss: 0.2249266803264618, Test_Loss: 0.22389468550682068 *\n",
      "Epoch: 22, Train_Loss: 0.2487388402223587, Test_Loss: 0.2251434028148651\n",
      "Epoch: 22, Train_Loss: 0.2502870559692383, Test_Loss: 0.22607895731925964\n",
      "Epoch: 22, Train_Loss: 0.5050782561302185, Test_Loss: 0.22331860661506653 *\n",
      "Epoch: 22, Train_Loss: 0.23631837964057922, Test_Loss: 0.22726765275001526\n",
      "Epoch: 22, Train_Loss: 0.24435116350650787, Test_Loss: 0.21837839484214783 *\n",
      "Epoch: 22, Train_Loss: 0.21598681807518005, Test_Loss: 0.21749694645404816 *\n",
      "Epoch: 22, Train_Loss: 0.2159983068704605, Test_Loss: 0.2254534661769867\n",
      "Epoch: 22, Train_Loss: 0.21734970808029175, Test_Loss: 0.2736053466796875\n",
      "Epoch: 22, Train_Loss: 0.2179412692785263, Test_Loss: 0.23116035759449005 *\n",
      "Epoch: 22, Train_Loss: 0.22207970917224884, Test_Loss: 0.23245228826999664\n",
      "Epoch: 22, Train_Loss: 0.22177642583847046, Test_Loss: 0.24053795635700226\n",
      "Epoch: 22, Train_Loss: 0.23178599774837494, Test_Loss: 0.21706126630306244 *\n",
      "Epoch: 22, Train_Loss: 0.22283661365509033, Test_Loss: 0.21690644323825836 *\n",
      "Epoch: 22, Train_Loss: 0.2210017889738083, Test_Loss: 0.21849654614925385\n",
      "Epoch: 22, Train_Loss: 0.22548815608024597, Test_Loss: 0.24354234337806702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train_Loss: 0.21796540915966034, Test_Loss: 0.22035421431064606 *\n",
      "Epoch: 22, Train_Loss: 0.21529127657413483, Test_Loss: 0.2179725170135498 *\n",
      "Epoch: 22, Train_Loss: 0.23146794736385345, Test_Loss: 0.2159191519021988 *\n",
      "Epoch: 22, Train_Loss: 0.23552921414375305, Test_Loss: 0.22137178480625153\n",
      "Epoch: 22, Train_Loss: 0.24703191220760345, Test_Loss: 0.22749562561511993\n",
      "Epoch: 22, Train_Loss: 0.2156597524881363, Test_Loss: 0.23133990168571472\n",
      "Epoch: 22, Train_Loss: 0.2406865358352661, Test_Loss: 0.2167608141899109 *\n",
      "Epoch: 22, Train_Loss: 0.24817070364952087, Test_Loss: 0.2187088578939438\n",
      "Epoch: 22, Train_Loss: 0.24938176572322845, Test_Loss: 0.21833765506744385 *\n",
      "Epoch: 22, Train_Loss: 0.21550916135311127, Test_Loss: 0.21912381052970886\n",
      "Epoch: 22, Train_Loss: 0.24361252784729004, Test_Loss: 0.21805064380168915 *\n",
      "Epoch: 22, Train_Loss: 0.21689370274543762, Test_Loss: 0.3019218444824219\n",
      "Epoch: 22, Train_Loss: 0.2352558672428131, Test_Loss: 0.9571437239646912\n",
      "Epoch: 22, Train_Loss: 0.2181161642074585, Test_Loss: 4.93634033203125\n",
      "Epoch: 22, Train_Loss: 0.23527485132217407, Test_Loss: 0.22543509304523468 *\n",
      "Epoch: 22, Train_Loss: 0.35241571068763733, Test_Loss: 0.21582438051700592 *\n",
      "Epoch: 22, Train_Loss: 3.68910813331604, Test_Loss: 0.2486231029033661\n",
      "Epoch: 22, Train_Loss: 2.1643283367156982, Test_Loss: 0.2467631846666336 *\n",
      "Epoch: 22, Train_Loss: 0.23319394886493683, Test_Loss: 0.25898322463035583\n",
      "Epoch: 22, Train_Loss: 0.21711759269237518, Test_Loss: 0.22840376198291779 *\n",
      "Epoch: 22, Train_Loss: 0.28915369510650635, Test_Loss: 0.34780457615852356\n",
      "Epoch: 22, Train_Loss: 0.30289676785469055, Test_Loss: 0.24532924592494965 *\n",
      "Epoch: 22, Train_Loss: 0.2350907325744629, Test_Loss: 0.21833908557891846 *\n",
      "Epoch: 22, Train_Loss: 0.21505579352378845, Test_Loss: 0.2553270161151886\n",
      "Epoch: 22, Train_Loss: 0.2621210217475891, Test_Loss: 0.22997166216373444 *\n",
      "Epoch: 22, Train_Loss: 0.2445315420627594, Test_Loss: 0.22328731417655945 *\n",
      "Epoch: 22, Train_Loss: 0.2228550910949707, Test_Loss: 0.26228758692741394\n",
      "Epoch: 22, Train_Loss: 0.37419068813323975, Test_Loss: 0.2724049687385559\n",
      "Epoch: 22, Train_Loss: 0.7464698553085327, Test_Loss: 0.28352198004722595\n",
      "Epoch: 22, Train_Loss: 1.032271385192871, Test_Loss: 0.31092000007629395\n",
      "Epoch: 22, Train_Loss: 0.29111212491989136, Test_Loss: 0.2472170889377594 *\n",
      "Epoch: 22, Train_Loss: 0.30101698637008667, Test_Loss: 0.26441678404808044\n",
      "Epoch: 22, Train_Loss: 1.7359164953231812, Test_Loss: 0.22062133252620697 *\n",
      "Epoch: 22, Train_Loss: 1.1630264520645142, Test_Loss: 0.221780925989151\n",
      "Epoch: 22, Train_Loss: 0.22798475623130798, Test_Loss: 0.2249820977449417\n",
      "Epoch: 22, Train_Loss: 0.2194080948829651, Test_Loss: 0.22935685515403748\n",
      "Epoch: 22, Train_Loss: 0.5666205286979675, Test_Loss: 0.22038152813911438 *\n",
      "Epoch: 22, Train_Loss: 0.6766929626464844, Test_Loss: 0.21989570558071136 *\n",
      "Epoch: 22, Train_Loss: 0.8856333494186401, Test_Loss: 0.22724178433418274\n",
      "Epoch: 22, Train_Loss: 0.2263231873512268, Test_Loss: 0.22113841772079468 *\n",
      "Epoch: 22, Train_Loss: 0.24947823584079742, Test_Loss: 0.22130052745342255\n",
      "Epoch: 22, Train_Loss: 0.3409878611564636, Test_Loss: 0.24813103675842285\n",
      "Epoch: 22, Train_Loss: 0.4670463800430298, Test_Loss: 0.22156797349452972 *\n",
      "Epoch: 22, Train_Loss: 0.22957761585712433, Test_Loss: 0.2662619948387146\n",
      "Epoch: 22, Train_Loss: 0.2696291208267212, Test_Loss: 0.24564436078071594 *\n",
      "Epoch: 22, Train_Loss: 0.2567882835865021, Test_Loss: 0.48410356044769287\n",
      "Epoch: 22, Train_Loss: 0.23374685645103455, Test_Loss: 0.3592405617237091 *\n",
      "Epoch: 22, Train_Loss: 0.3137347102165222, Test_Loss: 0.2643563151359558 *\n",
      "Epoch: 22, Train_Loss: 0.31587573885917664, Test_Loss: 0.2882598638534546\n",
      "Epoch: 22, Train_Loss: 0.2524898648262024, Test_Loss: 0.2598072588443756 *\n",
      "Epoch: 22, Train_Loss: 0.2929151654243469, Test_Loss: 0.21905294060707092 *\n",
      "Epoch: 22, Train_Loss: 0.27806827425956726, Test_Loss: 0.2384132444858551\n",
      "Epoch: 22, Train_Loss: 0.2618602514266968, Test_Loss: 0.3100518584251404\n",
      "Epoch: 22, Train_Loss: 0.2899683713912964, Test_Loss: 0.39130640029907227\n",
      "Epoch: 22, Train_Loss: 0.41071152687072754, Test_Loss: 0.28666436672210693 *\n",
      "Epoch: 22, Train_Loss: 0.24568457901477814, Test_Loss: 0.30355700850486755\n",
      "Epoch: 22, Train_Loss: 0.2674367427825928, Test_Loss: 0.2374585121870041 *\n",
      "Epoch: 22, Train_Loss: 0.2806136906147003, Test_Loss: 0.22108054161071777 *\n",
      "Epoch: 22, Train_Loss: 0.25691279768943787, Test_Loss: 0.22747758030891418\n",
      "Epoch: 22, Train_Loss: 0.22299963235855103, Test_Loss: 0.2652572989463806\n",
      "Epoch: 22, Train_Loss: 0.2169129103422165, Test_Loss: 0.22360387444496155 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 22\n",
      "Epoch: 22, Train_Loss: 0.2166149765253067, Test_Loss: 0.24989083409309387\n",
      "Epoch: 22, Train_Loss: 0.2176554948091507, Test_Loss: 0.2441980540752411 *\n",
      "Epoch: 22, Train_Loss: 0.22117863595485687, Test_Loss: 0.48982423543930054\n",
      "Epoch: 22, Train_Loss: 0.23273378610610962, Test_Loss: 0.5591123700141907\n",
      "Epoch: 22, Train_Loss: 0.23047231137752533, Test_Loss: 0.5169823169708252 *\n",
      "Epoch: 22, Train_Loss: 0.2283681184053421, Test_Loss: 0.44089943170547485 *\n",
      "Epoch: 22, Train_Loss: 0.3574649691581726, Test_Loss: 0.3786988854408264 *\n",
      "Epoch: 22, Train_Loss: 0.5118321180343628, Test_Loss: 0.378399133682251 *\n",
      "Epoch: 22, Train_Loss: 0.23310555517673492, Test_Loss: 0.3805372714996338\n",
      "Epoch: 22, Train_Loss: 0.2600387930870056, Test_Loss: 0.34057044982910156 *\n",
      "Epoch: 22, Train_Loss: 0.29460394382476807, Test_Loss: 0.3877602219581604\n",
      "Epoch: 22, Train_Loss: 0.261476069688797, Test_Loss: 5.972476005554199\n",
      "Epoch: 22, Train_Loss: 0.3727946877479553, Test_Loss: 0.3172144293785095 *\n",
      "Epoch: 22, Train_Loss: 0.35770443081855774, Test_Loss: 0.2948586940765381 *\n",
      "Epoch: 22, Train_Loss: 0.4816371202468872, Test_Loss: 0.2737308740615845 *\n",
      "Epoch: 22, Train_Loss: 0.2930915355682373, Test_Loss: 0.23760247230529785 *\n",
      "Epoch: 22, Train_Loss: 0.33706268668174744, Test_Loss: 0.2218935787677765 *\n",
      "Epoch: 22, Train_Loss: 0.232095405459404, Test_Loss: 0.2611788511276245\n",
      "Epoch: 22, Train_Loss: 0.25464582443237305, Test_Loss: 0.26380065083503723\n",
      "Epoch: 22, Train_Loss: 0.4229786992073059, Test_Loss: 0.22204439342021942 *\n",
      "Epoch: 22, Train_Loss: 0.7143627405166626, Test_Loss: 0.2187887579202652 *\n",
      "Epoch: 22, Train_Loss: 0.5242719054222107, Test_Loss: 0.2418954074382782\n",
      "Epoch: 22, Train_Loss: 0.2675003707408905, Test_Loss: 0.3891143202781677\n",
      "Epoch: 22, Train_Loss: 0.26920151710510254, Test_Loss: 0.24197207391262054 *\n",
      "Epoch: 22, Train_Loss: 0.22860212624073029, Test_Loss: 0.22580528259277344 *\n",
      "Epoch: 22, Train_Loss: 0.5344412922859192, Test_Loss: 0.2727273106575012\n",
      "Epoch: 22, Train_Loss: 0.4108971953392029, Test_Loss: 0.23642873764038086 *\n",
      "Epoch: 22, Train_Loss: 0.22120675444602966, Test_Loss: 0.24752327799797058\n",
      "Epoch: 22, Train_Loss: 0.3670588731765747, Test_Loss: 0.3265761435031891\n",
      "Epoch: 22, Train_Loss: 0.23565998673439026, Test_Loss: 0.29264524579048157 *\n",
      "Epoch: 22, Train_Loss: 0.2412947714328766, Test_Loss: 0.23412351310253143 *\n",
      "Epoch: 22, Train_Loss: 0.25613951683044434, Test_Loss: 0.24057592451572418\n",
      "Epoch: 22, Train_Loss: 0.32607126235961914, Test_Loss: 0.22093096375465393 *\n",
      "Epoch: 22, Train_Loss: 0.2780546545982361, Test_Loss: 0.23618793487548828\n",
      "Epoch: 22, Train_Loss: 0.3270418643951416, Test_Loss: 0.2434648871421814\n",
      "Epoch: 22, Train_Loss: 0.2339126467704773, Test_Loss: 0.2518594563007355\n",
      "Epoch: 22, Train_Loss: 0.30555421113967896, Test_Loss: 0.2172151654958725 *\n",
      "Epoch: 22, Train_Loss: 0.24124225974082947, Test_Loss: 0.22917293012142181\n",
      "Epoch: 22, Train_Loss: 0.23395639657974243, Test_Loss: 0.26654356718063354\n",
      "Epoch: 22, Train_Loss: 0.2192845642566681, Test_Loss: 0.22753460705280304 *\n",
      "Epoch: 22, Train_Loss: 0.2745926082134247, Test_Loss: 0.218984454870224 *\n",
      "Epoch: 22, Train_Loss: 0.35715383291244507, Test_Loss: 0.3357996642589569\n",
      "Epoch: 22, Train_Loss: 0.4889149069786072, Test_Loss: 2.0683634281158447\n",
      "Epoch: 22, Train_Loss: 0.43679866194725037, Test_Loss: 3.3563849925994873\n",
      "Epoch: 22, Train_Loss: 0.6952750086784363, Test_Loss: 0.22582869231700897 *\n",
      "Epoch: 22, Train_Loss: 0.5039448738098145, Test_Loss: 0.21603961288928986 *\n",
      "Epoch: 22, Train_Loss: 0.4202730059623718, Test_Loss: 0.26590266823768616\n",
      "Epoch: 22, Train_Loss: 0.28799644112586975, Test_Loss: 0.21693740785121918 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train_Loss: 0.22861169278621674, Test_Loss: 0.23634520173072815\n",
      "Epoch: 22, Train_Loss: 0.22162330150604248, Test_Loss: 0.2789088487625122\n",
      "Epoch: 22, Train_Loss: 0.2269829362630844, Test_Loss: 0.2984134554862976\n",
      "Epoch: 22, Train_Loss: 0.3676798939704895, Test_Loss: 0.2220936268568039 *\n",
      "Epoch: 22, Train_Loss: 0.5125019550323486, Test_Loss: 0.23101234436035156\n",
      "Epoch: 22, Train_Loss: 0.45897388458251953, Test_Loss: 0.24023745954036713\n",
      "Epoch: 22, Train_Loss: 1.2035813331604004, Test_Loss: 0.2950630784034729\n",
      "Epoch: 22, Train_Loss: 1.052069902420044, Test_Loss: 0.23663002252578735 *\n",
      "Epoch: 22, Train_Loss: 0.3600158095359802, Test_Loss: 0.27452361583709717\n",
      "Epoch: 22, Train_Loss: 0.3162055015563965, Test_Loss: 0.24221256375312805 *\n",
      "Epoch: 22, Train_Loss: 0.2200600504875183, Test_Loss: 0.32189828157424927\n",
      "Epoch: 22, Train_Loss: 0.3238620162010193, Test_Loss: 0.23431876301765442 *\n",
      "Epoch: 22, Train_Loss: 0.6346685886383057, Test_Loss: 0.25204160809516907\n",
      "Epoch: 22, Train_Loss: 0.7529633045196533, Test_Loss: 0.319479376077652\n",
      "Epoch: 22, Train_Loss: 0.24954861402511597, Test_Loss: 0.22543412446975708 *\n",
      "Epoch: 22, Train_Loss: 0.2402629256248474, Test_Loss: 0.2575637102127075\n",
      "Epoch: 22, Train_Loss: 0.2928828299045563, Test_Loss: 0.2516690790653229 *\n",
      "Epoch: 22, Train_Loss: 0.48596107959747314, Test_Loss: 0.2367600053548813 *\n",
      "Epoch: 22, Train_Loss: 0.3390128016471863, Test_Loss: 0.24000373482704163\n",
      "Epoch: 22, Train_Loss: 0.34707731008529663, Test_Loss: 0.2416904717683792\n",
      "Epoch: 22, Train_Loss: 0.35370567440986633, Test_Loss: 0.29208797216415405\n",
      "Epoch: 22, Train_Loss: 0.3108839988708496, Test_Loss: 0.25972869992256165 *\n",
      "Epoch: 22, Train_Loss: 0.22284910082817078, Test_Loss: 0.2635786831378937\n",
      "Epoch: 22, Train_Loss: 0.2477918118238449, Test_Loss: 0.2867368459701538\n",
      "Epoch: 22, Train_Loss: 0.2197752594947815, Test_Loss: 0.21769359707832336 *\n",
      "Epoch: 22, Train_Loss: 0.2869701385498047, Test_Loss: 0.2223859578371048\n",
      "Epoch: 22, Train_Loss: 0.2666688859462738, Test_Loss: 0.25432196259498596\n",
      "Epoch: 22, Train_Loss: 0.3188053071498871, Test_Loss: 0.3699232339859009\n",
      "Epoch: 22, Train_Loss: 15.213083267211914, Test_Loss: 0.2905682623386383 *\n",
      "Epoch: 22, Train_Loss: 0.3651406466960907, Test_Loss: 0.23422157764434814 *\n",
      "Epoch: 22, Train_Loss: 1.0090137720108032, Test_Loss: 0.3186371326446533\n",
      "Epoch: 22, Train_Loss: 1.028234601020813, Test_Loss: 0.2932151257991791 *\n",
      "Epoch: 22, Train_Loss: 0.30183809995651245, Test_Loss: 0.2308339923620224 *\n",
      "Epoch: 22, Train_Loss: 0.5305603742599487, Test_Loss: 0.2916101813316345\n",
      "Epoch: 22, Train_Loss: 3.0797791481018066, Test_Loss: 0.30538737773895264\n",
      "Epoch: 22, Train_Loss: 3.771757125854492, Test_Loss: 0.5194941163063049\n",
      "Epoch: 22, Train_Loss: 0.2844107747077942, Test_Loss: 0.35802215337753296 *\n",
      "Epoch: 22, Train_Loss: 0.6432676315307617, Test_Loss: 0.3317609131336212 *\n",
      "Epoch: 22, Train_Loss: 4.30513858795166, Test_Loss: 0.3326362073421478\n",
      "Epoch: 22, Train_Loss: 0.6246020197868347, Test_Loss: 0.27056509256362915 *\n",
      "Epoch: 22, Train_Loss: 0.22376702725887299, Test_Loss: 0.3408287763595581\n",
      "Epoch: 22, Train_Loss: 0.23162487149238586, Test_Loss: 0.2449350655078888 *\n",
      "Epoch: 22, Train_Loss: 0.3069847822189331, Test_Loss: 0.2663370966911316\n",
      "Epoch: 22, Train_Loss: 0.27779465913772583, Test_Loss: 0.22353525459766388 *\n",
      "Epoch: 22, Train_Loss: 0.21320384740829468, Test_Loss: 0.37653154134750366\n",
      "Epoch: 22, Train_Loss: 0.22020447254180908, Test_Loss: 0.6402987241744995\n",
      "Epoch: 22, Train_Loss: 0.2118624597787857, Test_Loss: 0.6589271426200867\n",
      "Epoch: 22, Train_Loss: 0.21318502724170685, Test_Loss: 0.40442484617233276 *\n",
      "Epoch: 22, Train_Loss: 0.23333200812339783, Test_Loss: 0.27133727073669434 *\n",
      "Epoch: 22, Train_Loss: 0.22180436551570892, Test_Loss: 0.21645301580429077 *\n",
      "Epoch: 22, Train_Loss: 0.29374080896377563, Test_Loss: 0.21552932262420654 *\n",
      "Epoch: 22, Train_Loss: 0.31308436393737793, Test_Loss: 0.22618728876113892\n",
      "Model saved at location save_new\\model.ckpt at epoch 22\n",
      "Epoch: 22, Train_Loss: 0.24887125194072723, Test_Loss: 0.24725937843322754\n",
      "Epoch: 22, Train_Loss: 0.23827366530895233, Test_Loss: 1.3077040910720825\n",
      "Epoch: 22, Train_Loss: 0.2439849078655243, Test_Loss: 6.290092468261719\n",
      "Epoch: 22, Train_Loss: 0.22662723064422607, Test_Loss: 0.354506254196167 *\n",
      "Epoch: 22, Train_Loss: 0.23127365112304688, Test_Loss: 0.7391011118888855\n",
      "Epoch: 22, Train_Loss: 0.22047315537929535, Test_Loss: 0.6448681950569153 *\n",
      "Epoch: 22, Train_Loss: 0.2134208381175995, Test_Loss: 0.4043712615966797 *\n",
      "Epoch: 22, Train_Loss: 0.21400873363018036, Test_Loss: 0.3673843443393707 *\n",
      "Epoch: 22, Train_Loss: 0.21762382984161377, Test_Loss: 1.0174448490142822\n",
      "Epoch: 22, Train_Loss: 0.21688856184482574, Test_Loss: 0.8383804559707642 *\n",
      "Epoch: 22, Train_Loss: 0.21423298120498657, Test_Loss: 0.3585766553878784 *\n",
      "Epoch: 22, Train_Loss: 0.21609769761562347, Test_Loss: 0.6195620894432068\n",
      "Epoch: 22, Train_Loss: 0.24159379303455353, Test_Loss: 0.6571384072303772\n",
      "Epoch: 22, Train_Loss: 0.24952907860279083, Test_Loss: 1.7674195766448975\n",
      "Epoch: 22, Train_Loss: 0.2860349416732788, Test_Loss: 0.718085765838623 *\n",
      "Epoch: 22, Train_Loss: 0.27581483125686646, Test_Loss: 0.883773922920227\n",
      "Epoch: 22, Train_Loss: 0.39224427938461304, Test_Loss: 0.6275663375854492 *\n",
      "Epoch: 22, Train_Loss: 3.6822946071624756, Test_Loss: 0.26526010036468506 *\n",
      "Epoch: 22, Train_Loss: 4.620841979980469, Test_Loss: 0.26585471630096436\n",
      "Epoch: 22, Train_Loss: 0.22593346238136292, Test_Loss: 0.2263069897890091 *\n",
      "Epoch: 22, Train_Loss: 0.245001882314682, Test_Loss: 0.7420985698699951\n",
      "Epoch: 22, Train_Loss: 0.2652169466018677, Test_Loss: 0.24968665838241577 *\n",
      "Epoch: 22, Train_Loss: 0.26700639724731445, Test_Loss: 0.3924587666988373\n",
      "Epoch: 22, Train_Loss: 0.2638120949268341, Test_Loss: 0.23663777112960815 *\n",
      "Epoch: 22, Train_Loss: 0.2669456899166107, Test_Loss: 0.4227858781814575\n",
      "Epoch: 22, Train_Loss: 0.3004828989505768, Test_Loss: 0.6268486380577087\n",
      "Epoch: 22, Train_Loss: 0.35270190238952637, Test_Loss: 0.3487420678138733 *\n",
      "Epoch: 22, Train_Loss: 0.29211556911468506, Test_Loss: 0.244642436504364 *\n",
      "Epoch: 22, Train_Loss: 0.23345302045345306, Test_Loss: 0.3616545498371124\n",
      "Epoch: 22, Train_Loss: 0.22591090202331543, Test_Loss: 0.3031427264213562 *\n",
      "Epoch: 22, Train_Loss: 0.22043858468532562, Test_Loss: 0.298401802778244 *\n",
      "Epoch: 22, Train_Loss: 0.3475222587585449, Test_Loss: 0.3214377164840698\n",
      "Epoch: 22, Train_Loss: 0.3006356358528137, Test_Loss: 0.5834952592849731\n",
      "Epoch: 22, Train_Loss: 0.24112094938755035, Test_Loss: 4.029344081878662\n",
      "Epoch: 22, Train_Loss: 0.25279325246810913, Test_Loss: 3.1151487827301025 *\n",
      "Epoch: 22, Train_Loss: 0.22581267356872559, Test_Loss: 0.2351566106081009 *\n",
      "Epoch: 22, Train_Loss: 0.23359915614128113, Test_Loss: 0.21971896290779114 *\n",
      "Epoch: 22, Train_Loss: 0.2531856894493103, Test_Loss: 0.25120028853416443\n",
      "Epoch: 22, Train_Loss: 0.2524425685405731, Test_Loss: 0.3284534513950348\n",
      "Epoch: 22, Train_Loss: 0.21371184289455414, Test_Loss: 0.23611629009246826 *\n",
      "Epoch: 22, Train_Loss: 0.21311397850513458, Test_Loss: 0.3240712881088257\n",
      "Epoch: 22, Train_Loss: 0.2287406027317047, Test_Loss: 0.3315322995185852\n",
      "Epoch: 22, Train_Loss: 2.5531914234161377, Test_Loss: 0.21484336256980896 *\n",
      "Epoch: 22, Train_Loss: 2.5281684398651123, Test_Loss: 0.2358471304178238\n",
      "Epoch: 22, Train_Loss: 0.21389636397361755, Test_Loss: 0.23294177651405334 *\n",
      "Epoch: 22, Train_Loss: 0.21232929825782776, Test_Loss: 0.23742642998695374\n",
      "Epoch: 22, Train_Loss: 0.21583840250968933, Test_Loss: 0.2219887524843216 *\n",
      "Epoch: 22, Train_Loss: 0.2122962325811386, Test_Loss: 0.3558204770088196\n",
      "Epoch: 22, Train_Loss: 0.23155523836612701, Test_Loss: 0.38301581144332886\n",
      "Epoch: 22, Train_Loss: 0.2152574509382248, Test_Loss: 0.37885773181915283 *\n",
      "Epoch: 22, Train_Loss: 0.2335904836654663, Test_Loss: 0.3109799921512604 *\n",
      "Epoch: 22, Train_Loss: 0.22204528748989105, Test_Loss: 0.23407314717769623 *\n",
      "Epoch: 22, Train_Loss: 0.2777022123336792, Test_Loss: 0.28729915618896484\n",
      "Epoch: 22, Train_Loss: 0.21278375387191772, Test_Loss: 0.3399183750152588\n",
      "Epoch: 22, Train_Loss: 0.21275684237480164, Test_Loss: 0.3386094272136688 *\n",
      "Epoch: 22, Train_Loss: 0.221007838845253, Test_Loss: 0.3127731680870056 *\n",
      "Epoch: 22, Train_Loss: 0.22210106253623962, Test_Loss: 0.252557635307312 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train_Loss: 0.21294407546520233, Test_Loss: 0.26328417658805847\n",
      "Epoch: 22, Train_Loss: 0.21731361746788025, Test_Loss: 0.2821038067340851\n",
      "Epoch: 22, Train_Loss: 0.2327127903699875, Test_Loss: 0.3746907711029053\n",
      "Epoch: 22, Train_Loss: 0.2319510579109192, Test_Loss: 0.3235144019126892 *\n",
      "Epoch: 22, Train_Loss: 0.2153744250535965, Test_Loss: 0.2795998454093933 *\n",
      "Epoch: 22, Train_Loss: 0.21639516949653625, Test_Loss: 0.26917555928230286 *\n",
      "Epoch: 22, Train_Loss: 0.25318634510040283, Test_Loss: 0.21525099873542786 *\n",
      "Epoch: 22, Train_Loss: 0.2363227903842926, Test_Loss: 0.24062755703926086\n",
      "Epoch: 22, Train_Loss: 0.2658746540546417, Test_Loss: 0.29719552397727966\n",
      "Epoch: 22, Train_Loss: 0.2533881664276123, Test_Loss: 0.2833438217639923 *\n",
      "Epoch: 22, Train_Loss: 0.294653058052063, Test_Loss: 0.3069113492965698\n",
      "Epoch: 22, Train_Loss: 0.26260095834732056, Test_Loss: 0.26307815313339233 *\n",
      "Epoch: 22, Train_Loss: 0.2265874445438385, Test_Loss: 0.2967457175254822\n",
      "Epoch: 22, Train_Loss: 0.24827931821346283, Test_Loss: 0.2643112242221832 *\n",
      "Epoch: 22, Train_Loss: 0.30916452407836914, Test_Loss: 0.2586964964866638 *\n",
      "Epoch: 22, Train_Loss: 0.2755008041858673, Test_Loss: 0.44325655698776245\n",
      "Epoch: 22, Train_Loss: 0.31529077887535095, Test_Loss: 0.4412229657173157 *\n",
      "Epoch: 22, Train_Loss: 0.21166235208511353, Test_Loss: 0.6003446578979492\n",
      "Epoch: 22, Train_Loss: 0.21483634412288666, Test_Loss: 0.28288960456848145 *\n",
      "Epoch: 22, Train_Loss: 0.21403710544109344, Test_Loss: 0.27406954765319824 *\n",
      "Epoch: 22, Train_Loss: 0.21540117263793945, Test_Loss: 0.2403482347726822 *\n",
      "Epoch: 22, Train_Loss: 0.21421211957931519, Test_Loss: 0.2222592681646347 *\n",
      "Epoch: 22, Train_Loss: 3.5088207721710205, Test_Loss: 0.22651642560958862\n",
      "Epoch: 22, Train_Loss: 1.761362075805664, Test_Loss: 0.22429272532463074 *\n",
      "Epoch: 22, Train_Loss: 0.21217305958271027, Test_Loss: 0.24211522936820984\n",
      "Epoch: 22, Train_Loss: 0.23051808774471283, Test_Loss: 0.21388927102088928 *\n",
      "Epoch: 22, Train_Loss: 0.21735338866710663, Test_Loss: 0.2778567969799042\n",
      "Epoch: 22, Train_Loss: 0.21199990808963776, Test_Loss: 0.34277820587158203\n",
      "Epoch: 22, Train_Loss: 0.2123827487230301, Test_Loss: 0.5544171929359436\n",
      "Epoch: 22, Train_Loss: 0.21173442900180817, Test_Loss: 0.46796414256095886 *\n",
      "Epoch: 22, Train_Loss: 0.21041980385780334, Test_Loss: 0.2468911111354828 *\n",
      "Epoch: 22, Train_Loss: 0.2121252864599228, Test_Loss: 0.24080273509025574 *\n",
      "Epoch: 22, Train_Loss: 0.2272624671459198, Test_Loss: 0.24121226370334625\n",
      "Epoch: 22, Train_Loss: 0.25215110182762146, Test_Loss: 0.24167388677597046\n",
      "Epoch: 22, Train_Loss: 0.26357024908065796, Test_Loss: 0.2455853372812271\n",
      "Epoch: 22, Train_Loss: 0.25587034225463867, Test_Loss: 1.9849170446395874\n",
      "Epoch: 22, Train_Loss: 0.22872892022132874, Test_Loss: 3.794682025909424\n",
      "Epoch: 22, Train_Loss: 0.2355402708053589, Test_Loss: 0.23777416348457336 *\n",
      "Epoch: 22, Train_Loss: 0.4476882219314575, Test_Loss: 0.22398193180561066 *\n",
      "Epoch: 22, Train_Loss: 0.430401086807251, Test_Loss: 0.22726470232009888\n",
      "Epoch: 22, Train_Loss: 0.4295114278793335, Test_Loss: 0.22434312105178833 *\n",
      "Epoch: 22, Train_Loss: 0.2752060890197754, Test_Loss: 0.21604903042316437 *\n",
      "Epoch: 22, Train_Loss: 0.21174679696559906, Test_Loss: 0.22352245450019836\n",
      "Model saved at location save_new\\model.ckpt at epoch 22\n",
      "Epoch: 22, Train_Loss: 0.2100170999765396, Test_Loss: 0.21170684695243835 *\n",
      "Epoch: 22, Train_Loss: 0.21540379524230957, Test_Loss: 0.21142329275608063 *\n",
      "Epoch: 22, Train_Loss: 0.22289881110191345, Test_Loss: 0.21616390347480774\n",
      "Epoch: 22, Train_Loss: 0.22656673192977905, Test_Loss: 0.21287912130355835 *\n",
      "Epoch: 22, Train_Loss: 0.22631824016571045, Test_Loss: 0.24184514582157135\n",
      "Epoch: 22, Train_Loss: 0.2099308967590332, Test_Loss: 0.24098870158195496 *\n",
      "Epoch: 22, Train_Loss: 0.2109951674938202, Test_Loss: 0.23468109965324402 *\n",
      "Epoch: 22, Train_Loss: 0.2308492362499237, Test_Loss: 0.22058060765266418 *\n",
      "Epoch: 22, Train_Loss: 0.31491100788116455, Test_Loss: 0.21251118183135986 *\n",
      "Epoch: 22, Train_Loss: 0.4133108854293823, Test_Loss: 0.21513280272483826\n",
      "Epoch: 22, Train_Loss: 0.34713298082351685, Test_Loss: 0.22439250349998474\n",
      "Epoch: 22, Train_Loss: 0.3060348629951477, Test_Loss: 0.25936052203178406\n",
      "Epoch: 22, Train_Loss: 0.29319584369659424, Test_Loss: 0.21199963986873627 *\n",
      "Epoch: 22, Train_Loss: 0.3100435137748718, Test_Loss: 0.21109801530838013 *\n",
      "Epoch: 22, Train_Loss: 0.21657207608222961, Test_Loss: 0.2114812135696411\n",
      "Epoch: 22, Train_Loss: 0.30639660358428955, Test_Loss: 0.22223946452140808\n",
      "Epoch: 22, Train_Loss: 0.24613164365291595, Test_Loss: 0.24134203791618347\n",
      "Epoch: 22, Train_Loss: 0.44745099544525146, Test_Loss: 0.21330125629901886 *\n",
      "Epoch: 22, Train_Loss: 0.21798644959926605, Test_Loss: 0.21222829818725586 *\n",
      "Epoch: 22, Train_Loss: 0.6571050882339478, Test_Loss: 0.2172277718782425\n",
      "Epoch: 22, Train_Loss: 2.5592122077941895, Test_Loss: 0.21608656644821167 *\n",
      "Epoch: 22, Train_Loss: 0.2536909282207489, Test_Loss: 0.2150251865386963 *\n",
      "Epoch: 22, Train_Loss: 0.26711782813072205, Test_Loss: 0.2725347578525543\n",
      "Epoch: 22, Train_Loss: 0.23221923410892487, Test_Loss: 0.2714214026927948 *\n",
      "Epoch: 22, Train_Loss: 0.21679161489009857, Test_Loss: 4.312476634979248\n",
      "Epoch: 22, Train_Loss: 0.21344590187072754, Test_Loss: 1.626128077507019 *\n",
      "Epoch: 22, Train_Loss: 0.21616068482398987, Test_Loss: 0.21364890038967133 *\n",
      "Epoch: 22, Train_Loss: 0.30853211879730225, Test_Loss: 0.2261677235364914\n",
      "Epoch: 22, Train_Loss: 0.2948474884033203, Test_Loss: 0.28604474663734436\n",
      "Epoch: 22, Train_Loss: 0.2584224343299866, Test_Loss: 0.24859462678432465 *\n",
      "Epoch: 22, Train_Loss: 0.2284407913684845, Test_Loss: 0.22612246870994568 *\n",
      "Epoch: 22, Train_Loss: 0.22133265435695648, Test_Loss: 0.2842542827129364\n",
      "Epoch: 22, Train_Loss: 0.21683287620544434, Test_Loss: 0.26059648394584656 *\n",
      "Epoch: 22, Train_Loss: 0.22516874969005585, Test_Loss: 0.22572457790374756 *\n",
      "Epoch: 22, Train_Loss: 0.23454809188842773, Test_Loss: 0.23092064261436462\n",
      "Epoch: 22, Train_Loss: 0.24969413876533508, Test_Loss: 0.24159874022006989\n",
      "Epoch: 22, Train_Loss: 0.22274547815322876, Test_Loss: 0.25503844022750854\n",
      "Epoch: 22, Train_Loss: 0.21034689247608185, Test_Loss: 0.22244253754615784 *\n",
      "Epoch: 22, Train_Loss: 0.22730711102485657, Test_Loss: 0.3534795641899109\n",
      "Epoch: 22, Train_Loss: 0.22558853030204773, Test_Loss: 0.264316201210022 *\n",
      "Epoch: 22, Train_Loss: 0.22751489281654358, Test_Loss: 0.2609449625015259 *\n",
      "Epoch: 22, Train_Loss: 0.21204285323619843, Test_Loss: 0.23654721677303314 *\n",
      "Epoch: 22, Train_Loss: 0.21026957035064697, Test_Loss: 0.2958209812641144\n",
      "Epoch: 22, Train_Loss: 0.20932723581790924, Test_Loss: 0.2883654832839966 *\n",
      "Epoch: 22, Train_Loss: 0.2143617868423462, Test_Loss: 0.23157811164855957 *\n",
      "Epoch: 22, Train_Loss: 0.21418972313404083, Test_Loss: 0.24687381088733673\n",
      "Epoch: 22, Train_Loss: 0.2141273021697998, Test_Loss: 0.2487032115459442\n",
      "Epoch: 22, Train_Loss: 0.220828577876091, Test_Loss: 0.24648045003414154 *\n",
      "Epoch: 22, Train_Loss: 0.21156692504882812, Test_Loss: 0.2392747402191162 *\n",
      "Epoch: 22, Train_Loss: 0.21120931208133698, Test_Loss: 0.23161737620830536 *\n",
      "Epoch: 22, Train_Loss: 0.21102669835090637, Test_Loss: 0.22202517092227936 *\n",
      "Epoch: 22, Train_Loss: 0.2202460616827011, Test_Loss: 0.22261491417884827\n",
      "Epoch: 22, Train_Loss: 0.2196832001209259, Test_Loss: 0.21812811493873596 *\n",
      "Epoch: 22, Train_Loss: 0.22103774547576904, Test_Loss: 0.21997418999671936\n",
      "Epoch: 23, Train_Loss: 0.2369341105222702, Test_Loss: 0.2693302035331726 *\n",
      "Epoch: 23, Train_Loss: 0.22225423157215118, Test_Loss: 0.31396403908729553\n",
      "Epoch: 23, Train_Loss: 0.23598389327526093, Test_Loss: 0.4241921305656433\n",
      "Epoch: 23, Train_Loss: 0.2221723049879074, Test_Loss: 0.540296733379364\n",
      "Epoch: 23, Train_Loss: 0.21893899142742157, Test_Loss: 0.40365827083587646 *\n",
      "Epoch: 23, Train_Loss: 0.24774527549743652, Test_Loss: 0.26756593585014343 *\n",
      "Epoch: 23, Train_Loss: 0.23097901046276093, Test_Loss: 0.254056841135025 *\n",
      "Epoch: 23, Train_Loss: 0.21293161809444427, Test_Loss: 0.22969681024551392 *\n",
      "Epoch: 23, Train_Loss: 0.21458126604557037, Test_Loss: 0.2553662657737732\n",
      "Epoch: 23, Train_Loss: 0.22232510149478912, Test_Loss: 0.4798254668712616\n",
      "Epoch: 23, Train_Loss: 0.22368669509887695, Test_Loss: 0.35535603761672974 *\n",
      "Epoch: 23, Train_Loss: 0.2662191092967987, Test_Loss: 0.5357463359832764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train_Loss: 0.27319586277008057, Test_Loss: 0.31533560156822205 *\n",
      "Epoch: 23, Train_Loss: 0.21140563488006592, Test_Loss: 0.22279301285743713 *\n",
      "Epoch: 23, Train_Loss: 0.24071434140205383, Test_Loss: 0.21690204739570618 *\n",
      "Epoch: 23, Train_Loss: 0.26199883222579956, Test_Loss: 0.20969952642917633 *\n",
      "Epoch: 23, Train_Loss: 0.2129894196987152, Test_Loss: 0.22821636497974396\n",
      "Epoch: 23, Train_Loss: 0.23255634307861328, Test_Loss: 0.22131624817848206 *\n",
      "Epoch: 23, Train_Loss: 0.22638365626335144, Test_Loss: 0.234684556722641\n",
      "Epoch: 23, Train_Loss: 0.28411534428596497, Test_Loss: 0.21161699295043945 *\n",
      "Epoch: 23, Train_Loss: 0.26923874020576477, Test_Loss: 0.32475054264068604\n",
      "Epoch: 23, Train_Loss: 0.2465764433145523, Test_Loss: 0.49074459075927734\n",
      "Epoch: 23, Train_Loss: 0.2304021418094635, Test_Loss: 0.38894426822662354 *\n",
      "Epoch: 23, Train_Loss: 0.2171962559223175, Test_Loss: 0.4426753520965576\n",
      "Epoch: 23, Train_Loss: 0.23053611814975739, Test_Loss: 0.2357257455587387 *\n",
      "Epoch: 23, Train_Loss: 0.20948854088783264, Test_Loss: 0.23586878180503845\n",
      "Epoch: 23, Train_Loss: 0.21347574889659882, Test_Loss: 0.23535148799419403 *\n",
      "Epoch: 23, Train_Loss: 0.22313442826271057, Test_Loss: 0.23375409841537476 *\n",
      "Epoch: 23, Train_Loss: 0.2316058725118637, Test_Loss: 0.23919135332107544\n",
      "Epoch: 23, Train_Loss: 0.28547704219818115, Test_Loss: 3.6190171241760254\n",
      "Epoch: 23, Train_Loss: 0.23401980102062225, Test_Loss: 2.4870688915252686 *\n",
      "Epoch: 23, Train_Loss: 0.26084840297698975, Test_Loss: 0.24057051539421082 *\n",
      "Epoch: 23, Train_Loss: 0.21802303194999695, Test_Loss: 0.22403894364833832 *\n",
      "Epoch: 23, Train_Loss: 0.239472895860672, Test_Loss: 0.2341841459274292\n",
      "Epoch: 23, Train_Loss: 0.21843935549259186, Test_Loss: 0.2195083349943161 *\n",
      "Epoch: 23, Train_Loss: 0.46141600608825684, Test_Loss: 0.211900532245636 *\n",
      "Epoch: 23, Train_Loss: 0.29446157813072205, Test_Loss: 0.2559230625629425\n",
      "Epoch: 23, Train_Loss: 0.22245295345783234, Test_Loss: 0.21956421434879303 *\n",
      "Epoch: 23, Train_Loss: 0.22701427340507507, Test_Loss: 0.20987850427627563 *\n",
      "Epoch: 23, Train_Loss: 0.2097020000219345, Test_Loss: 0.2528129816055298\n",
      "Epoch: 23, Train_Loss: 0.21079954504966736, Test_Loss: 0.23709601163864136 *\n",
      "Epoch: 23, Train_Loss: 0.21122296154499054, Test_Loss: 0.32942408323287964\n",
      "Epoch: 23, Train_Loss: 0.21129444241523743, Test_Loss: 0.24399270117282867 *\n",
      "Epoch: 23, Train_Loss: 0.21102580428123474, Test_Loss: 0.25387218594551086\n",
      "Epoch: 23, Train_Loss: 0.22689716517925262, Test_Loss: 0.21986065804958344 *\n",
      "Epoch: 23, Train_Loss: 0.2147565484046936, Test_Loss: 0.21313364803791046 *\n",
      "Epoch: 23, Train_Loss: 0.21489059925079346, Test_Loss: 0.21921661496162415\n",
      "Epoch: 23, Train_Loss: 0.21964597702026367, Test_Loss: 0.2800918221473694\n",
      "Epoch: 23, Train_Loss: 0.20933815836906433, Test_Loss: 0.2437608689069748 *\n",
      "Epoch: 23, Train_Loss: 0.20838792622089386, Test_Loss: 0.21079705655574799 *\n",
      "Epoch: 23, Train_Loss: 0.22123320400714874, Test_Loss: 0.21567055583000183\n",
      "Epoch: 23, Train_Loss: 0.2267894595861435, Test_Loss: 0.21758270263671875\n",
      "Epoch: 23, Train_Loss: 0.23405082523822784, Test_Loss: 0.21738475561141968 *\n",
      "Epoch: 23, Train_Loss: 0.21514791250228882, Test_Loss: 0.2391333281993866\n",
      "Epoch: 23, Train_Loss: 0.22374002635478973, Test_Loss: 0.2094939649105072 *\n",
      "Epoch: 23, Train_Loss: 0.23680497705936432, Test_Loss: 0.21037326753139496\n",
      "Epoch: 23, Train_Loss: 0.24159017205238342, Test_Loss: 0.22461090981960297\n",
      "Epoch: 23, Train_Loss: 0.2083619236946106, Test_Loss: 0.21695250272750854 *\n",
      "Epoch: 23, Train_Loss: 0.23040960729122162, Test_Loss: 0.20812800526618958 *\n",
      "Epoch: 23, Train_Loss: 0.21875840425491333, Test_Loss: 0.31129977107048035\n",
      "Epoch: 23, Train_Loss: 0.22155308723449707, Test_Loss: 0.2910204231739044 *\n",
      "Epoch: 23, Train_Loss: 0.21022281050682068, Test_Loss: 5.369607448577881\n",
      "Epoch: 23, Train_Loss: 0.23070180416107178, Test_Loss: 0.5522512793540955 *\n",
      "Epoch: 23, Train_Loss: 0.27042368054389954, Test_Loss: 0.2088843137025833 *\n",
      "Epoch: 23, Train_Loss: 2.5014736652374268, Test_Loss: 0.2335905134677887\n",
      "Epoch: 23, Train_Loss: 3.251662254333496, Test_Loss: 0.255694180727005\n",
      "Epoch: 23, Train_Loss: 0.22691769897937775, Test_Loss: 0.23277124762535095 *\n",
      "Epoch: 23, Train_Loss: 0.21056178212165833, Test_Loss: 0.21470420062541962 *\n",
      "Epoch: 23, Train_Loss: 0.25706350803375244, Test_Loss: 0.33542609214782715\n",
      "Epoch: 23, Train_Loss: 0.32177507877349854, Test_Loss: 0.26598507165908813 *\n",
      "Epoch: 23, Train_Loss: 0.23658302426338196, Test_Loss: 0.21048109233379364 *\n",
      "Epoch: 23, Train_Loss: 0.20979340374469757, Test_Loss: 0.23415367305278778\n",
      "Epoch: 23, Train_Loss: 0.23239773511886597, Test_Loss: 0.22698825597763062 *\n",
      "Epoch: 23, Train_Loss: 0.2634811997413635, Test_Loss: 0.21604932844638824 *\n",
      "Epoch: 23, Train_Loss: 0.22346220910549164, Test_Loss: 0.22143197059631348\n",
      "Epoch: 23, Train_Loss: 0.2265799343585968, Test_Loss: 0.24569225311279297\n",
      "Epoch: 23, Train_Loss: 0.7043759226799011, Test_Loss: 0.25051724910736084\n",
      "Epoch: 23, Train_Loss: 0.7620315551757812, Test_Loss: 0.2747153639793396\n",
      "Epoch: 23, Train_Loss: 0.463920533657074, Test_Loss: 0.24576182663440704 *\n",
      "Epoch: 23, Train_Loss: 0.34804779291152954, Test_Loss: 0.2993617057800293\n",
      "Epoch: 23, Train_Loss: 1.1020419597625732, Test_Loss: 0.24334806203842163 *\n",
      "Epoch: 23, Train_Loss: 1.0139931440353394, Test_Loss: 0.21557340025901794 *\n",
      "Epoch: 23, Train_Loss: 0.23325304687023163, Test_Loss: 0.2170080840587616\n",
      "Epoch: 23, Train_Loss: 0.2151944935321808, Test_Loss: 0.21372106671333313 *\n",
      "Epoch: 23, Train_Loss: 0.3957696855068207, Test_Loss: 0.21873684227466583\n",
      "Epoch: 23, Train_Loss: 0.6059940457344055, Test_Loss: 0.21154077351093292 *\n",
      "Epoch: 23, Train_Loss: 0.6763384938240051, Test_Loss: 0.27369529008865356\n",
      "Epoch: 23, Train_Loss: 0.22808748483657837, Test_Loss: 0.26674529910087585 *\n",
      "Epoch: 23, Train_Loss: 0.26327192783355713, Test_Loss: 0.2532898783683777 *\n",
      "Epoch: 23, Train_Loss: 0.26630255579948425, Test_Loss: 0.2749366760253906\n",
      "Epoch: 23, Train_Loss: 0.3447609543800354, Test_Loss: 0.22837749123573303 *\n",
      "Epoch: 23, Train_Loss: 0.23429249227046967, Test_Loss: 0.22670751810073853 *\n",
      "Epoch: 23, Train_Loss: 0.3033175468444824, Test_Loss: 0.27176740765571594\n",
      "Epoch: 23, Train_Loss: 0.23335859179496765, Test_Loss: 0.4513227939605713\n",
      "Epoch: 23, Train_Loss: 0.2307739555835724, Test_Loss: 0.27486279606819153 *\n",
      "Epoch: 23, Train_Loss: 0.3817073404788971, Test_Loss: 0.285298228263855\n",
      "Epoch: 23, Train_Loss: 0.31050992012023926, Test_Loss: 0.27822762727737427 *\n",
      "Epoch: 23, Train_Loss: 0.2399621307849884, Test_Loss: 0.31785619258880615\n",
      "Epoch: 23, Train_Loss: 0.2507891058921814, Test_Loss: 0.21798549592494965 *\n",
      "Epoch: 23, Train_Loss: 0.29946082830429077, Test_Loss: 0.24144357442855835\n",
      "Model saved at location save_new\\model.ckpt at epoch 23\n",
      "Epoch: 23, Train_Loss: 0.23576946556568146, Test_Loss: 0.449504017829895\n",
      "Epoch: 23, Train_Loss: 0.24123015999794006, Test_Loss: 0.23065854609012604 *\n",
      "Epoch: 23, Train_Loss: 0.32336053252220154, Test_Loss: 0.42809006571769714\n",
      "Epoch: 23, Train_Loss: 0.23647882044315338, Test_Loss: 0.2982821464538574 *\n",
      "Epoch: 23, Train_Loss: 0.29201388359069824, Test_Loss: 0.2377007007598877 *\n",
      "Epoch: 23, Train_Loss: 0.32320278882980347, Test_Loss: 0.22906367480754852 *\n",
      "Epoch: 23, Train_Loss: 0.23062007129192352, Test_Loss: 0.21447429060935974 *\n",
      "Epoch: 23, Train_Loss: 0.21785075962543488, Test_Loss: 0.2585826516151428\n",
      "Epoch: 23, Train_Loss: 0.20680373907089233, Test_Loss: 0.2174932211637497 *\n",
      "Epoch: 23, Train_Loss: 0.20704136788845062, Test_Loss: 0.22967499494552612\n",
      "Epoch: 23, Train_Loss: 0.2084997445344925, Test_Loss: 0.21679042279720306 *\n",
      "Epoch: 23, Train_Loss: 0.21193067729473114, Test_Loss: 0.4558231830596924\n",
      "Epoch: 23, Train_Loss: 0.23833338916301727, Test_Loss: 0.5943503379821777\n",
      "Epoch: 23, Train_Loss: 0.2326499968767166, Test_Loss: 0.3409590721130371 *\n",
      "Epoch: 23, Train_Loss: 0.23325115442276, Test_Loss: 0.5724484920501709\n",
      "Epoch: 23, Train_Loss: 0.3889039158821106, Test_Loss: 0.34785836935043335 *\n",
      "Epoch: 23, Train_Loss: 0.4433702826499939, Test_Loss: 0.3528878688812256\n",
      "Epoch: 23, Train_Loss: 0.214930921792984, Test_Loss: 0.349969744682312 *\n",
      "Epoch: 23, Train_Loss: 0.24739611148834229, Test_Loss: 0.33820754289627075 *\n",
      "Epoch: 23, Train_Loss: 0.27372875809669495, Test_Loss: 0.30802977085113525 *\n",
      "Epoch: 23, Train_Loss: 0.27442967891693115, Test_Loss: 5.269588470458984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train_Loss: 0.3868933618068695, Test_Loss: 1.0741881132125854 *\n",
      "Epoch: 23, Train_Loss: 0.337257981300354, Test_Loss: 0.29202133417129517 *\n",
      "Epoch: 23, Train_Loss: 0.5121991634368896, Test_Loss: 0.26187026500701904 *\n",
      "Epoch: 23, Train_Loss: 0.2644712030887604, Test_Loss: 0.22822053730487823 *\n",
      "Epoch: 23, Train_Loss: 0.33243146538734436, Test_Loss: 0.22065939009189606 *\n",
      "Epoch: 23, Train_Loss: 0.22676284611225128, Test_Loss: 0.23648862540721893\n",
      "Epoch: 23, Train_Loss: 0.22284986078739166, Test_Loss: 0.2540607452392578\n",
      "Epoch: 23, Train_Loss: 0.29760807752609253, Test_Loss: 0.2267857789993286 *\n",
      "Epoch: 23, Train_Loss: 0.7281906008720398, Test_Loss: 0.21014337241649628 *\n",
      "Epoch: 23, Train_Loss: 0.6361225843429565, Test_Loss: 0.21987546980381012\n",
      "Epoch: 23, Train_Loss: 0.22785639762878418, Test_Loss: 0.27501532435417175\n",
      "Epoch: 23, Train_Loss: 0.2768333852291107, Test_Loss: 0.286487877368927\n",
      "Epoch: 23, Train_Loss: 0.2162606120109558, Test_Loss: 0.22944529354572296 *\n",
      "Epoch: 23, Train_Loss: 0.3747606873512268, Test_Loss: 0.23440226912498474\n",
      "Epoch: 23, Train_Loss: 0.5488651990890503, Test_Loss: 0.22005492448806763 *\n",
      "Epoch: 23, Train_Loss: 0.213822141289711, Test_Loss: 0.23659281432628632\n",
      "Epoch: 23, Train_Loss: 0.3525897264480591, Test_Loss: 0.2773538827896118\n",
      "Epoch: 23, Train_Loss: 0.2252621203660965, Test_Loss: 0.28925222158432007\n",
      "Epoch: 23, Train_Loss: 0.24086269736289978, Test_Loss: 0.23201382160186768 *\n",
      "Epoch: 23, Train_Loss: 0.25746482610702515, Test_Loss: 0.2172538787126541 *\n",
      "Epoch: 23, Train_Loss: 0.3057926297187805, Test_Loss: 0.21629080176353455 *\n",
      "Epoch: 23, Train_Loss: 0.3291221261024475, Test_Loss: 0.21075351536273956 *\n",
      "Epoch: 23, Train_Loss: 0.26273801922798157, Test_Loss: 0.22469258308410645\n",
      "Epoch: 23, Train_Loss: 0.21649786829948425, Test_Loss: 0.2568488121032715\n",
      "Epoch: 23, Train_Loss: 0.2951279580593109, Test_Loss: 0.2083420306444168 *\n",
      "Epoch: 23, Train_Loss: 0.24436257779598236, Test_Loss: 0.21428975462913513\n",
      "Epoch: 23, Train_Loss: 0.2247919738292694, Test_Loss: 0.25817057490348816\n",
      "Epoch: 23, Train_Loss: 0.21697759628295898, Test_Loss: 0.22206366062164307 *\n",
      "Epoch: 23, Train_Loss: 0.23154011368751526, Test_Loss: 0.20831288397312164 *\n",
      "Epoch: 23, Train_Loss: 0.27399155497550964, Test_Loss: 0.32866349816322327\n",
      "Epoch: 23, Train_Loss: 0.3758763074874878, Test_Loss: 0.4187212586402893\n",
      "Epoch: 23, Train_Loss: 0.3999955654144287, Test_Loss: 4.684939384460449\n",
      "Epoch: 23, Train_Loss: 0.6369688510894775, Test_Loss: 0.21998348832130432 *\n",
      "Epoch: 23, Train_Loss: 0.5095800757408142, Test_Loss: 0.22797070443630219\n",
      "Epoch: 23, Train_Loss: 0.4075343906879425, Test_Loss: 0.27615246176719666\n",
      "Epoch: 23, Train_Loss: 0.29396528005599976, Test_Loss: 0.21613241732120514 *\n",
      "Epoch: 23, Train_Loss: 0.2404799461364746, Test_Loss: 0.2273353487253189\n",
      "Epoch: 23, Train_Loss: 0.21762137115001678, Test_Loss: 0.22307443618774414 *\n",
      "Epoch: 23, Train_Loss: 0.221140056848526, Test_Loss: 0.2866396903991699\n",
      "Epoch: 23, Train_Loss: 0.32261618971824646, Test_Loss: 0.2392956018447876 *\n",
      "Epoch: 23, Train_Loss: 0.5269545316696167, Test_Loss: 0.2116774171590805 *\n",
      "Epoch: 23, Train_Loss: 0.5898259878158569, Test_Loss: 0.23438605666160583\n",
      "Epoch: 23, Train_Loss: 1.0540074110031128, Test_Loss: 0.30497634410858154\n",
      "Epoch: 23, Train_Loss: 1.190523386001587, Test_Loss: 0.27198198437690735 *\n",
      "Epoch: 23, Train_Loss: 0.3362043499946594, Test_Loss: 0.2715635299682617 *\n",
      "Epoch: 23, Train_Loss: 0.4030423164367676, Test_Loss: 0.24816733598709106 *\n",
      "Epoch: 23, Train_Loss: 0.2102028876543045, Test_Loss: 0.31119537353515625\n",
      "Epoch: 23, Train_Loss: 0.2660266160964966, Test_Loss: 0.23351329565048218 *\n",
      "Epoch: 23, Train_Loss: 0.4827067255973816, Test_Loss: 0.272898405790329\n",
      "Epoch: 23, Train_Loss: 0.7879185676574707, Test_Loss: 0.39464420080184937\n",
      "Epoch: 23, Train_Loss: 0.2442314326763153, Test_Loss: 0.22892720997333527 *\n",
      "Epoch: 23, Train_Loss: 0.2387791872024536, Test_Loss: 0.3345917761325836\n",
      "Epoch: 23, Train_Loss: 0.2488330453634262, Test_Loss: 0.2942426800727844 *\n",
      "Epoch: 23, Train_Loss: 0.40541571378707886, Test_Loss: 0.31210047006607056\n",
      "Epoch: 23, Train_Loss: 0.3917127251625061, Test_Loss: 0.27702033519744873 *\n",
      "Epoch: 23, Train_Loss: 0.4318251311779022, Test_Loss: 0.2796536684036255\n",
      "Epoch: 23, Train_Loss: 0.36754119396209717, Test_Loss: 0.39275479316711426\n",
      "Epoch: 23, Train_Loss: 0.4638078808784485, Test_Loss: 0.29486390948295593 *\n",
      "Epoch: 23, Train_Loss: 0.21451611816883087, Test_Loss: 0.27624741196632385 *\n",
      "Epoch: 23, Train_Loss: 0.22029724717140198, Test_Loss: 0.36300933361053467\n",
      "Epoch: 23, Train_Loss: 0.2081010490655899, Test_Loss: 0.2489956170320511 *\n",
      "Epoch: 23, Train_Loss: 0.2773977518081665, Test_Loss: 0.21188755333423615 *\n",
      "Epoch: 23, Train_Loss: 0.24199271202087402, Test_Loss: 0.28106629848480225\n",
      "Epoch: 23, Train_Loss: 0.2710966467857361, Test_Loss: 0.37497174739837646\n",
      "Epoch: 23, Train_Loss: 14.877108573913574, Test_Loss: 0.2564478814601898 *\n",
      "Epoch: 23, Train_Loss: 0.38060474395751953, Test_Loss: 0.2524740695953369 *\n",
      "Epoch: 23, Train_Loss: 1.4758092164993286, Test_Loss: 0.2728375196456909\n",
      "Epoch: 23, Train_Loss: 1.0912178754806519, Test_Loss: 0.2440813034772873 *\n",
      "Epoch: 23, Train_Loss: 0.24084436893463135, Test_Loss: 0.2102353423833847 *\n",
      "Epoch: 23, Train_Loss: 0.33687475323677063, Test_Loss: 0.2445468008518219\n",
      "Epoch: 23, Train_Loss: 2.1018364429473877, Test_Loss: 0.35983890295028687\n",
      "Epoch: 23, Train_Loss: 4.451857089996338, Test_Loss: 0.33063504099845886 *\n",
      "Epoch: 23, Train_Loss: 0.3052350580692291, Test_Loss: 0.32388317584991455 *\n",
      "Epoch: 23, Train_Loss: 0.36677679419517517, Test_Loss: 0.3255128562450409\n",
      "Epoch: 23, Train_Loss: 4.258602142333984, Test_Loss: 0.25549542903900146 *\n",
      "Epoch: 23, Train_Loss: 0.46253639459609985, Test_Loss: 0.25675323605537415\n",
      "Epoch: 23, Train_Loss: 0.43735840916633606, Test_Loss: 0.2796602249145508\n",
      "Epoch: 23, Train_Loss: 0.21795813739299774, Test_Loss: 0.3244345784187317\n",
      "Epoch: 23, Train_Loss: 0.2524486780166626, Test_Loss: 0.22924575209617615 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 23\n",
      "Epoch: 23, Train_Loss: 0.2920773923397064, Test_Loss: 0.2275158166885376 *\n",
      "Epoch: 23, Train_Loss: 0.2129482477903366, Test_Loss: 0.2902870774269104\n",
      "Epoch: 23, Train_Loss: 0.2236025035381317, Test_Loss: 0.45914894342422485\n",
      "Epoch: 23, Train_Loss: 0.20454691350460052, Test_Loss: 1.0732218027114868\n",
      "Epoch: 23, Train_Loss: 0.20472539961338043, Test_Loss: 0.31966450810432434 *\n",
      "Epoch: 23, Train_Loss: 0.22067487239837646, Test_Loss: 0.5657881498336792\n",
      "Epoch: 23, Train_Loss: 0.2302679866552353, Test_Loss: 0.3165351152420044 *\n",
      "Epoch: 23, Train_Loss: 0.3027958869934082, Test_Loss: 0.3179742693901062\n",
      "Epoch: 23, Train_Loss: 0.3769516944885254, Test_Loss: 0.3249489665031433\n",
      "Epoch: 23, Train_Loss: 0.2847956120967865, Test_Loss: 0.29810482263565063 *\n",
      "Epoch: 23, Train_Loss: 0.22723466157913208, Test_Loss: 0.27418234944343567 *\n",
      "Epoch: 23, Train_Loss: 0.27736470103263855, Test_Loss: 8.818556785583496\n",
      "Epoch: 23, Train_Loss: 0.22374212741851807, Test_Loss: 0.4878075420856476 *\n",
      "Epoch: 23, Train_Loss: 0.2409539520740509, Test_Loss: 0.8085007071495056\n",
      "Epoch: 23, Train_Loss: 0.208067387342453, Test_Loss: 0.6040529608726501 *\n",
      "Epoch: 23, Train_Loss: 0.2051199972629547, Test_Loss: 0.5544790029525757 *\n",
      "Epoch: 23, Train_Loss: 0.20444031059741974, Test_Loss: 0.2671484053134918 *\n",
      "Epoch: 23, Train_Loss: 0.2050982415676117, Test_Loss: 1.1471467018127441\n",
      "Epoch: 23, Train_Loss: 0.20676499605178833, Test_Loss: 1.1537998914718628\n",
      "Epoch: 23, Train_Loss: 0.20488788187503815, Test_Loss: 0.46372106671333313 *\n",
      "Epoch: 23, Train_Loss: 0.20521016418933868, Test_Loss: 0.6827386617660522\n",
      "Epoch: 23, Train_Loss: 0.22309230268001556, Test_Loss: 0.5633708238601685 *\n",
      "Epoch: 23, Train_Loss: 0.25418615341186523, Test_Loss: 1.4660110473632812\n",
      "Epoch: 23, Train_Loss: 0.26673611998558044, Test_Loss: 0.9497883915901184 *\n",
      "Epoch: 23, Train_Loss: 0.3183712661266327, Test_Loss: 0.5676555633544922 *\n",
      "Epoch: 23, Train_Loss: 0.3120586574077606, Test_Loss: 0.629033088684082\n",
      "Epoch: 23, Train_Loss: 1.2207493782043457, Test_Loss: 0.24707573652267456 *\n",
      "Epoch: 23, Train_Loss: 5.210515022277832, Test_Loss: 0.22562754154205322 *\n",
      "Epoch: 23, Train_Loss: 0.22362737357616425, Test_Loss: 0.24603156745433807\n",
      "Epoch: 23, Train_Loss: 0.2553308308124542, Test_Loss: 0.6054819822311401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train_Loss: 0.2456655204296112, Test_Loss: 0.3678700625896454 *\n",
      "Epoch: 23, Train_Loss: 0.2890878915786743, Test_Loss: 0.5304087996482849\n",
      "Epoch: 23, Train_Loss: 0.29836419224739075, Test_Loss: 0.40499401092529297 *\n",
      "Epoch: 23, Train_Loss: 0.3145647644996643, Test_Loss: 0.5110746026039124\n",
      "Epoch: 23, Train_Loss: 0.22879593074321747, Test_Loss: 0.5978658199310303\n",
      "Epoch: 23, Train_Loss: 0.30952581763267517, Test_Loss: 0.6143084168434143\n",
      "Epoch: 23, Train_Loss: 0.24482089281082153, Test_Loss: 0.22505226731300354 *\n",
      "Epoch: 23, Train_Loss: 0.2241503894329071, Test_Loss: 0.3930930495262146\n",
      "Epoch: 23, Train_Loss: 0.2442861795425415, Test_Loss: 0.34428608417510986 *\n",
      "Epoch: 23, Train_Loss: 0.2165706753730774, Test_Loss: 0.345348596572876\n",
      "Epoch: 23, Train_Loss: 0.21690545976161957, Test_Loss: 0.21061597764492035 *\n",
      "Epoch: 23, Train_Loss: 0.32056769728660583, Test_Loss: 0.8439461588859558\n",
      "Epoch: 23, Train_Loss: 0.21573826670646667, Test_Loss: 2.1206741333007812\n",
      "Epoch: 23, Train_Loss: 0.23707015812397003, Test_Loss: 4.947970867156982\n",
      "Epoch: 23, Train_Loss: 0.2095319926738739, Test_Loss: 0.31611141562461853 *\n",
      "Epoch: 23, Train_Loss: 0.23663337528705597, Test_Loss: 0.213221475481987 *\n",
      "Epoch: 23, Train_Loss: 0.2885017991065979, Test_Loss: 0.2778625786304474\n",
      "Epoch: 23, Train_Loss: 0.2551420032978058, Test_Loss: 0.361802339553833\n",
      "Epoch: 23, Train_Loss: 0.2122073918581009, Test_Loss: 0.23637494444847107 *\n",
      "Epoch: 23, Train_Loss: 0.21070468425750732, Test_Loss: 0.2541722059249878\n",
      "Epoch: 23, Train_Loss: 0.2128545194864273, Test_Loss: 0.4508740305900574\n",
      "Epoch: 23, Train_Loss: 1.7411857843399048, Test_Loss: 0.22931864857673645 *\n",
      "Epoch: 23, Train_Loss: 4.3840012550354, Test_Loss: 0.2184218019247055 *\n",
      "Epoch: 23, Train_Loss: 0.20676438510417938, Test_Loss: 0.3551063537597656\n",
      "Epoch: 23, Train_Loss: 0.20567616820335388, Test_Loss: 0.25715458393096924 *\n",
      "Epoch: 23, Train_Loss: 0.20871251821517944, Test_Loss: 0.3336886763572693\n",
      "Epoch: 23, Train_Loss: 0.2085709124803543, Test_Loss: 0.4621848464012146\n",
      "Epoch: 23, Train_Loss: 0.2081902027130127, Test_Loss: 0.37102633714675903 *\n",
      "Epoch: 23, Train_Loss: 0.2084285467863083, Test_Loss: 0.43539637327194214\n",
      "Epoch: 23, Train_Loss: 0.2173023819923401, Test_Loss: 0.8567967414855957\n",
      "Epoch: 23, Train_Loss: 0.21904480457305908, Test_Loss: 0.29351216554641724 *\n",
      "Epoch: 23, Train_Loss: 0.25100672245025635, Test_Loss: 0.2872307300567627 *\n",
      "Epoch: 23, Train_Loss: 0.20723950862884521, Test_Loss: 0.44913068413734436\n",
      "Epoch: 23, Train_Loss: 0.2052142173051834, Test_Loss: 0.6190218925476074\n",
      "Epoch: 23, Train_Loss: 0.2049524039030075, Test_Loss: 0.415556937456131 *\n",
      "Epoch: 23, Train_Loss: 0.22056525945663452, Test_Loss: 0.3563745617866516 *\n",
      "Epoch: 23, Train_Loss: 0.20552489161491394, Test_Loss: 0.37780535221099854\n",
      "Epoch: 23, Train_Loss: 0.20673413574695587, Test_Loss: 0.32041823863983154 *\n",
      "Epoch: 23, Train_Loss: 0.22503668069839478, Test_Loss: 0.6057166457176208\n",
      "Epoch: 23, Train_Loss: 0.238040953874588, Test_Loss: 0.44161027669906616 *\n",
      "Epoch: 23, Train_Loss: 0.20979389548301697, Test_Loss: 0.2403685599565506 *\n",
      "Epoch: 23, Train_Loss: 0.20565809309482574, Test_Loss: 0.2656002640724182\n",
      "Epoch: 23, Train_Loss: 0.23755475878715515, Test_Loss: 0.20998547971248627 *\n",
      "Epoch: 23, Train_Loss: 0.2502313554286957, Test_Loss: 0.24780026078224182\n",
      "Epoch: 23, Train_Loss: 0.2685297131538391, Test_Loss: 0.2752942442893982\n",
      "Epoch: 23, Train_Loss: 0.24421045184135437, Test_Loss: 0.34943583607673645\n",
      "Epoch: 23, Train_Loss: 0.264056533575058, Test_Loss: 0.2798534035682678 *\n",
      "Epoch: 23, Train_Loss: 0.24695266783237457, Test_Loss: 0.23777329921722412 *\n",
      "Epoch: 23, Train_Loss: 0.22831468284130096, Test_Loss: 0.3436771631240845\n",
      "Epoch: 23, Train_Loss: 0.24772441387176514, Test_Loss: 0.3077181577682495 *\n",
      "Epoch: 23, Train_Loss: 0.21165288984775543, Test_Loss: 0.2414567619562149 *\n",
      "Epoch: 23, Train_Loss: 0.3217901587486267, Test_Loss: 0.350238561630249\n",
      "Epoch: 23, Train_Loss: 0.2659749388694763, Test_Loss: 0.5264714956283569\n",
      "Epoch: 23, Train_Loss: 0.20542921125888824, Test_Loss: 0.5945188403129578\n",
      "Epoch: 23, Train_Loss: 0.20759469270706177, Test_Loss: 0.2735499143600464 *\n",
      "Epoch: 23, Train_Loss: 0.20687758922576904, Test_Loss: 0.28515884280204773\n",
      "Epoch: 23, Train_Loss: 0.20792393386363983, Test_Loss: 0.25975343585014343 *\n",
      "Epoch: 23, Train_Loss: 0.2045215517282486, Test_Loss: 0.21752461791038513 *\n",
      "Epoch: 23, Train_Loss: 1.9137072563171387, Test_Loss: 0.2241661548614502\n",
      "Epoch: 23, Train_Loss: 3.5339369773864746, Test_Loss: 0.22255057096481323 *\n",
      "Epoch: 23, Train_Loss: 0.2064560055732727, Test_Loss: 0.2468772679567337\n",
      "Epoch: 23, Train_Loss: 0.22865693271160126, Test_Loss: 0.2129429280757904 *\n",
      "Epoch: 23, Train_Loss: 0.2192322313785553, Test_Loss: 0.22485169768333435\n",
      "Epoch: 23, Train_Loss: 0.20373256504535675, Test_Loss: 0.30441930890083313\n",
      "Epoch: 23, Train_Loss: 0.20575527846813202, Test_Loss: 0.5142565965652466\n",
      "Epoch: 23, Train_Loss: 0.20407497882843018, Test_Loss: 0.4424537718296051 *\n",
      "Epoch: 23, Train_Loss: 0.20350977778434753, Test_Loss: 0.23852963745594025 *\n",
      "Epoch: 23, Train_Loss: 0.20341265201568604, Test_Loss: 0.22893111407756805 *\n",
      "Epoch: 23, Train_Loss: 0.20899595320224762, Test_Loss: 0.22946682572364807\n",
      "Epoch: 23, Train_Loss: 0.2658861577510834, Test_Loss: 0.2311752438545227\n",
      "Model saved at location save_new\\model.ckpt at epoch 23\n",
      "Epoch: 23, Train_Loss: 0.25645148754119873, Test_Loss: 0.22861045598983765 *\n",
      "Epoch: 23, Train_Loss: 0.2681402862071991, Test_Loss: 0.5177068710327148\n",
      "Epoch: 23, Train_Loss: 0.2333861142396927, Test_Loss: 5.838815689086914\n",
      "Epoch: 23, Train_Loss: 0.2051302045583725, Test_Loss: 0.26542866230010986 *\n",
      "Epoch: 23, Train_Loss: 0.39630553126335144, Test_Loss: 0.2611616551876068 *\n",
      "Epoch: 23, Train_Loss: 0.40276050567626953, Test_Loss: 0.22561532258987427 *\n",
      "Epoch: 23, Train_Loss: 0.39230310916900635, Test_Loss: 0.21576166152954102 *\n",
      "Epoch: 23, Train_Loss: 0.3042277693748474, Test_Loss: 0.21234668791294098 *\n",
      "Epoch: 23, Train_Loss: 0.20442013442516327, Test_Loss: 0.21783414483070374\n",
      "Epoch: 23, Train_Loss: 0.20347340404987335, Test_Loss: 0.21136271953582764 *\n",
      "Epoch: 23, Train_Loss: 0.20520314574241638, Test_Loss: 0.2050747126340866 *\n",
      "Epoch: 23, Train_Loss: 0.21202102303504944, Test_Loss: 0.204705148935318 *\n",
      "Epoch: 23, Train_Loss: 0.2199588418006897, Test_Loss: 0.20572391152381897\n",
      "Epoch: 23, Train_Loss: 0.22119173407554626, Test_Loss: 0.26074591279029846\n",
      "Epoch: 23, Train_Loss: 0.2053546905517578, Test_Loss: 0.22129395604133606 *\n",
      "Epoch: 23, Train_Loss: 0.20352229475975037, Test_Loss: 0.2237132489681244\n",
      "Epoch: 23, Train_Loss: 0.22357603907585144, Test_Loss: 0.23937904834747314\n",
      "Epoch: 23, Train_Loss: 0.2636725604534149, Test_Loss: 0.2056075930595398 *\n",
      "Epoch: 23, Train_Loss: 0.3694937825202942, Test_Loss: 0.21748679876327515\n",
      "Epoch: 23, Train_Loss: 0.28599661588668823, Test_Loss: 0.20834708213806152 *\n",
      "Epoch: 23, Train_Loss: 0.30289220809936523, Test_Loss: 0.34334543347358704\n",
      "Epoch: 23, Train_Loss: 0.25873467326164246, Test_Loss: 0.2256542593240738 *\n",
      "Epoch: 23, Train_Loss: 0.2884877324104309, Test_Loss: 0.2122267186641693 *\n",
      "Epoch: 23, Train_Loss: 0.25085142254829407, Test_Loss: 0.20558999478816986 *\n",
      "Epoch: 23, Train_Loss: 0.27833694219589233, Test_Loss: 0.28896698355674744\n",
      "Epoch: 23, Train_Loss: 0.24941802024841309, Test_Loss: 0.36912333965301514\n",
      "Epoch: 23, Train_Loss: 0.43518856167793274, Test_Loss: 0.3203144967556 *\n",
      "Epoch: 23, Train_Loss: 0.21388855576515198, Test_Loss: 0.2141362875699997 *\n",
      "Epoch: 23, Train_Loss: 0.2284867763519287, Test_Loss: 0.26563459634780884\n",
      "Epoch: 23, Train_Loss: 3.0169517993927, Test_Loss: 0.26960110664367676\n",
      "Epoch: 23, Train_Loss: 0.40358591079711914, Test_Loss: 0.24818681180477142 *\n",
      "Epoch: 23, Train_Loss: 0.24365916848182678, Test_Loss: 0.21869692206382751 *\n",
      "Epoch: 23, Train_Loss: 0.2313501387834549, Test_Loss: 0.3771461844444275\n",
      "Epoch: 23, Train_Loss: 0.20823806524276733, Test_Loss: 2.6564176082611084\n",
      "Epoch: 23, Train_Loss: 0.21999627351760864, Test_Loss: 3.0064446926116943\n",
      "Epoch: 23, Train_Loss: 0.2120734602212906, Test_Loss: 0.2093503624200821 *\n",
      "Epoch: 23, Train_Loss: 0.28375595808029175, Test_Loss: 0.20502984523773193 *\n",
      "Epoch: 23, Train_Loss: 0.321408748626709, Test_Loss: 0.24814286828041077\n",
      "Epoch: 23, Train_Loss: 0.26186954975128174, Test_Loss: 0.21026434004306793 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train_Loss: 0.22944363951683044, Test_Loss: 0.2521064281463623\n",
      "Epoch: 23, Train_Loss: 0.20860417187213898, Test_Loss: 0.24588295817375183 *\n",
      "Epoch: 23, Train_Loss: 0.21198266744613647, Test_Loss: 0.288585901260376\n",
      "Epoch: 23, Train_Loss: 0.21229058504104614, Test_Loss: 0.2101706862449646 *\n",
      "Epoch: 23, Train_Loss: 0.2170238345861435, Test_Loss: 0.22290608286857605\n",
      "Epoch: 23, Train_Loss: 0.2574564516544342, Test_Loss: 0.22462520003318787\n",
      "Epoch: 23, Train_Loss: 0.2230282723903656, Test_Loss: 0.25072944164276123\n",
      "Epoch: 23, Train_Loss: 0.2032817155122757, Test_Loss: 0.21408668160438538 *\n",
      "Epoch: 23, Train_Loss: 0.21361476182937622, Test_Loss: 0.2724151313304901\n",
      "Epoch: 23, Train_Loss: 0.22030267119407654, Test_Loss: 0.26383858919143677 *\n",
      "Epoch: 23, Train_Loss: 0.22575828433036804, Test_Loss: 0.2673201560974121\n",
      "Epoch: 23, Train_Loss: 0.20418652892112732, Test_Loss: 0.2405921071767807 *\n",
      "Epoch: 23, Train_Loss: 0.20292823016643524, Test_Loss: 0.24618542194366455\n",
      "Epoch: 23, Train_Loss: 0.20298561453819275, Test_Loss: 0.2858148217201233\n",
      "Epoch: 23, Train_Loss: 0.20573481917381287, Test_Loss: 0.22283294796943665 *\n",
      "Epoch: 23, Train_Loss: 0.2051491141319275, Test_Loss: 0.23478317260742188\n",
      "Epoch: 23, Train_Loss: 0.20352688431739807, Test_Loss: 0.23828093707561493\n",
      "Epoch: 23, Train_Loss: 0.20976240932941437, Test_Loss: 0.24596358835697174\n",
      "Epoch: 23, Train_Loss: 0.2066207379102707, Test_Loss: 0.23353633284568787 *\n",
      "Epoch: 23, Train_Loss: 0.20293858647346497, Test_Loss: 0.22345058619976044 *\n",
      "Epoch: 23, Train_Loss: 0.2028922736644745, Test_Loss: 0.21744298934936523 *\n",
      "Epoch: 23, Train_Loss: 0.20734110474586487, Test_Loss: 0.2206350415945053\n",
      "Epoch: 23, Train_Loss: 0.21112573146820068, Test_Loss: 0.21194958686828613 *\n",
      "Epoch: 23, Train_Loss: 0.21504788100719452, Test_Loss: 0.20659182965755463 *\n",
      "Epoch: 23, Train_Loss: 0.22245287895202637, Test_Loss: 0.24846920371055603\n",
      "Epoch: 23, Train_Loss: 0.22321805357933044, Test_Loss: 0.31070107221603394\n",
      "Epoch: 23, Train_Loss: 0.22930340468883514, Test_Loss: 0.2506933808326721 *\n",
      "Epoch: 23, Train_Loss: 0.21119391918182373, Test_Loss: 0.6634036302566528\n",
      "Epoch: 23, Train_Loss: 0.21253344416618347, Test_Loss: 0.6222805976867676 *\n",
      "Epoch: 23, Train_Loss: 0.24054953455924988, Test_Loss: 0.30483561754226685 *\n",
      "Epoch: 23, Train_Loss: 0.233635812997818, Test_Loss: 0.2325376272201538 *\n",
      "Epoch: 23, Train_Loss: 0.2040947675704956, Test_Loss: 0.2257603406906128 *\n",
      "Epoch: 23, Train_Loss: 0.20594032108783722, Test_Loss: 0.22516727447509766 *\n",
      "Epoch: 23, Train_Loss: 0.206423819065094, Test_Loss: 0.38848209381103516\n",
      "Epoch: 23, Train_Loss: 0.22714167833328247, Test_Loss: 0.5252853035926819\n",
      "Epoch: 23, Train_Loss: 0.25445884466171265, Test_Loss: 0.5709196329116821\n",
      "Epoch: 23, Train_Loss: 0.26384904980659485, Test_Loss: 0.2802227735519409 *\n",
      "Epoch: 23, Train_Loss: 0.21396301686763763, Test_Loss: 0.24530147016048431 *\n",
      "Epoch: 23, Train_Loss: 0.20794542133808136, Test_Loss: 0.20811685919761658 *\n",
      "Epoch: 23, Train_Loss: 0.26193705201148987, Test_Loss: 0.20348095893859863 *\n",
      "Epoch: 23, Train_Loss: 0.2046598196029663, Test_Loss: 0.2106938511133194\n",
      "Epoch: 23, Train_Loss: 0.2138250172138214, Test_Loss: 0.21639104187488556\n",
      "Epoch: 23, Train_Loss: 0.2241520881652832, Test_Loss: 0.24088308215141296\n",
      "Epoch: 23, Train_Loss: 0.22506673634052277, Test_Loss: 0.20340676605701447 *\n",
      "Epoch: 23, Train_Loss: 0.32529395818710327, Test_Loss: 0.24697357416152954\n",
      "Epoch: 23, Train_Loss: 0.25716057419776917, Test_Loss: 0.3326700031757355\n",
      "Epoch: 23, Train_Loss: 0.23498854041099548, Test_Loss: 0.5464316606521606\n",
      "Epoch: 23, Train_Loss: 0.20971959829330444, Test_Loss: 0.4321531057357788 *\n",
      "Epoch: 23, Train_Loss: 0.22715498507022858, Test_Loss: 0.22892485558986664 *\n",
      "Epoch: 23, Train_Loss: 0.20404905080795288, Test_Loss: 0.22139210999011993 *\n",
      "Epoch: 23, Train_Loss: 0.20701761543750763, Test_Loss: 0.22161222994327545\n",
      "Epoch: 23, Train_Loss: 0.2148737758398056, Test_Loss: 0.22209906578063965\n",
      "Epoch: 23, Train_Loss: 0.220178484916687, Test_Loss: 0.2195708304643631 *\n",
      "Epoch: 23, Train_Loss: 0.2534564137458801, Test_Loss: 1.3465412855148315\n",
      "Epoch: 23, Train_Loss: 0.2680474519729614, Test_Loss: 4.721450328826904\n",
      "Epoch: 23, Train_Loss: 0.23475515842437744, Test_Loss: 0.22316783666610718 *\n",
      "Epoch: 23, Train_Loss: 0.2385016232728958, Test_Loss: 0.21169137954711914 *\n",
      "Epoch: 23, Train_Loss: 0.231079563498497, Test_Loss: 0.21159470081329346 *\n",
      "Epoch: 23, Train_Loss: 0.21174529194831848, Test_Loss: 0.21166472136974335\n",
      "Epoch: 23, Train_Loss: 0.3168177008628845, Test_Loss: 0.20673678815364838 *\n",
      "Epoch: 23, Train_Loss: 0.4038865864276886, Test_Loss: 0.20870761573314667\n",
      "Model saved at location save_new\\model.ckpt at epoch 23\n",
      "Epoch: 23, Train_Loss: 0.20257708430290222, Test_Loss: 0.20352472364902496 *\n",
      "Epoch: 23, Train_Loss: 0.23467588424682617, Test_Loss: 0.20276427268981934 *\n",
      "Epoch: 23, Train_Loss: 0.20155377686023712, Test_Loss: 0.20274797081947327 *\n",
      "Epoch: 23, Train_Loss: 0.2017877995967865, Test_Loss: 0.20341037213802338\n",
      "Epoch: 23, Train_Loss: 0.20293715596199036, Test_Loss: 0.21659153699874878\n",
      "Epoch: 23, Train_Loss: 0.20208853483200073, Test_Loss: 0.22550000250339508\n",
      "Epoch: 23, Train_Loss: 0.20818465948104858, Test_Loss: 0.2243959754705429 *\n",
      "Epoch: 23, Train_Loss: 0.22227808833122253, Test_Loss: 0.222259521484375 *\n",
      "Epoch: 23, Train_Loss: 0.20935018360614777, Test_Loss: 0.2030089944601059 *\n",
      "Epoch: 23, Train_Loss: 0.215272918343544, Test_Loss: 0.20403340458869934\n",
      "Epoch: 23, Train_Loss: 0.21538254618644714, Test_Loss: 0.20347507297992706 *\n",
      "Epoch: 23, Train_Loss: 0.20197589695453644, Test_Loss: 0.24995514750480652\n",
      "Epoch: 23, Train_Loss: 0.20257939398288727, Test_Loss: 0.2023676484823227 *\n",
      "Epoch: 23, Train_Loss: 0.20265348255634308, Test_Loss: 0.20361462235450745\n",
      "Epoch: 23, Train_Loss: 0.2205783724784851, Test_Loss: 0.20124685764312744 *\n",
      "Epoch: 23, Train_Loss: 0.22484415769577026, Test_Loss: 0.21113571524620056\n",
      "Epoch: 23, Train_Loss: 0.21782344579696655, Test_Loss: 0.22779299318790436\n",
      "Epoch: 23, Train_Loss: 0.21041300892829895, Test_Loss: 0.20574088394641876 *\n",
      "Epoch: 23, Train_Loss: 0.2257046401500702, Test_Loss: 0.20274777710437775 *\n",
      "Epoch: 23, Train_Loss: 0.2315496951341629, Test_Loss: 0.20666208863258362\n",
      "Epoch: 23, Train_Loss: 0.2193600982427597, Test_Loss: 0.2061062753200531 *\n",
      "Epoch: 23, Train_Loss: 0.214479461312294, Test_Loss: 0.20661616325378418\n",
      "Epoch: 23, Train_Loss: 0.22416962683200836, Test_Loss: 0.23743656277656555\n",
      "Epoch: 23, Train_Loss: 0.20727583765983582, Test_Loss: 0.27310866117477417\n",
      "Epoch: 23, Train_Loss: 0.21177633106708527, Test_Loss: 3.8255531787872314\n",
      "Epoch: 23, Train_Loss: 0.22761790454387665, Test_Loss: 2.319244146347046 *\n",
      "Epoch: 23, Train_Loss: 0.24965935945510864, Test_Loss: 0.2070559859275818 *\n",
      "Epoch: 23, Train_Loss: 2.3319904804229736, Test_Loss: 0.20508095622062683 *\n",
      "Epoch: 23, Train_Loss: 3.3475708961486816, Test_Loss: 0.24328473210334778\n",
      "Epoch: 23, Train_Loss: 0.20735202729701996, Test_Loss: 0.23295041918754578 *\n",
      "Epoch: 23, Train_Loss: 0.2089701145887375, Test_Loss: 0.2234000563621521 *\n",
      "Epoch: 23, Train_Loss: 0.2318495362997055, Test_Loss: 0.27823472023010254\n",
      "Epoch: 23, Train_Loss: 0.32129213213920593, Test_Loss: 0.29478126764297485\n",
      "Epoch: 23, Train_Loss: 0.2251340001821518, Test_Loss: 0.20350530743598938 *\n",
      "Epoch: 23, Train_Loss: 0.20815706253051758, Test_Loss: 0.22814689576625824\n",
      "Epoch: 23, Train_Loss: 0.20592756569385529, Test_Loss: 0.22040294110774994 *\n",
      "Epoch: 23, Train_Loss: 0.2733429968357086, Test_Loss: 0.2193792164325714 *\n",
      "Epoch: 23, Train_Loss: 0.21747547388076782, Test_Loss: 0.2063271701335907 *\n",
      "Epoch: 23, Train_Loss: 0.21386991441249847, Test_Loss: 0.23338715732097626\n",
      "Epoch: 23, Train_Loss: 0.674372673034668, Test_Loss: 0.24854187667369843\n",
      "Epoch: 23, Train_Loss: 0.5448552370071411, Test_Loss: 0.29695677757263184\n",
      "Epoch: 23, Train_Loss: 0.6681090593338013, Test_Loss: 0.24908395111560822 *\n",
      "Epoch: 23, Train_Loss: 0.2893145680427551, Test_Loss: 0.24374902248382568 *\n",
      "Epoch: 23, Train_Loss: 0.7625508308410645, Test_Loss: 0.24828040599822998\n",
      "Epoch: 23, Train_Loss: 1.1252323389053345, Test_Loss: 0.20757511258125305 *\n",
      "Epoch: 23, Train_Loss: 0.46325963735580444, Test_Loss: 0.22307157516479492\n",
      "Epoch: 23, Train_Loss: 0.2028113752603531, Test_Loss: 0.22757937014102936\n",
      "Epoch: 23, Train_Loss: 0.23403896391391754, Test_Loss: 0.2172302007675171 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train_Loss: 0.5643298625946045, Test_Loss: 0.22785890102386475\n",
      "Epoch: 23, Train_Loss: 0.40275102853775024, Test_Loss: 0.26957371830940247\n",
      "Epoch: 23, Train_Loss: 0.33987659215927124, Test_Loss: 0.33130133152008057\n",
      "Epoch: 23, Train_Loss: 0.2161075323820114, Test_Loss: 0.2907711863517761 *\n",
      "Epoch: 23, Train_Loss: 0.25045838952064514, Test_Loss: 0.2860700488090515 *\n",
      "Epoch: 23, Train_Loss: 0.39132118225097656, Test_Loss: 0.2521322965621948 *\n",
      "Epoch: 24, Train_Loss: 0.3044242262840271, Test_Loss: 0.22618985176086426 *\n",
      "Epoch: 24, Train_Loss: 0.2549310028553009, Test_Loss: 0.2906079888343811\n",
      "Epoch: 24, Train_Loss: 0.2409062683582306, Test_Loss: 0.3649858236312866\n",
      "Epoch: 24, Train_Loss: 0.23545002937316895, Test_Loss: 0.2732844948768616 *\n",
      "Epoch: 24, Train_Loss: 0.24991092085838318, Test_Loss: 0.3317071497440338\n",
      "Epoch: 24, Train_Loss: 0.3444070816040039, Test_Loss: 0.25900357961654663 *\n",
      "Epoch: 24, Train_Loss: 0.23567625880241394, Test_Loss: 0.29727470874786377\n",
      "Epoch: 24, Train_Loss: 0.22778618335723877, Test_Loss: 0.2329278439283371 *\n",
      "Epoch: 24, Train_Loss: 0.32089534401893616, Test_Loss: 0.225980743765831 *\n",
      "Epoch: 24, Train_Loss: 0.22983913123607635, Test_Loss: 0.41283443570137024\n",
      "Epoch: 24, Train_Loss: 0.2539452314376831, Test_Loss: 0.24332620203495026 *\n",
      "Epoch: 24, Train_Loss: 0.24393565952777863, Test_Loss: 0.6212257146835327\n",
      "Epoch: 24, Train_Loss: 0.2745019793510437, Test_Loss: 0.2918088138103485 *\n",
      "Epoch: 24, Train_Loss: 0.26994869112968445, Test_Loss: 0.29140445590019226 *\n",
      "Epoch: 24, Train_Loss: 0.2527230978012085, Test_Loss: 0.27719300985336304 *\n",
      "Epoch: 24, Train_Loss: 0.23282253742218018, Test_Loss: 0.20795267820358276 *\n",
      "Epoch: 24, Train_Loss: 0.21272589266300201, Test_Loss: 0.27666473388671875\n",
      "Epoch: 24, Train_Loss: 0.20028987526893616, Test_Loss: 0.23813965916633606 *\n",
      "Epoch: 24, Train_Loss: 0.2012535184621811, Test_Loss: 0.222304105758667 *\n",
      "Epoch: 24, Train_Loss: 0.20262417197227478, Test_Loss: 0.25373202562332153\n",
      "Epoch: 24, Train_Loss: 0.20539531111717224, Test_Loss: 0.42950326204299927\n",
      "Epoch: 24, Train_Loss: 0.22943180799484253, Test_Loss: 0.6291959285736084\n",
      "Epoch: 24, Train_Loss: 0.22729893028736115, Test_Loss: 0.5433357357978821 *\n",
      "Epoch: 24, Train_Loss: 0.21690024435520172, Test_Loss: 0.7015440464019775\n",
      "Epoch: 24, Train_Loss: 0.46502578258514404, Test_Loss: 0.4575924277305603 *\n",
      "Epoch: 24, Train_Loss: 0.3512634038925171, Test_Loss: 0.4401247203350067 *\n",
      "Epoch: 24, Train_Loss: 0.2122339904308319, Test_Loss: 0.42330437898635864 *\n",
      "Epoch: 24, Train_Loss: 0.22877603769302368, Test_Loss: 0.406424880027771 *\n",
      "Epoch: 24, Train_Loss: 0.261263370513916, Test_Loss: 0.33618980646133423 *\n",
      "Epoch: 24, Train_Loss: 0.28400731086730957, Test_Loss: 3.126943349838257\n",
      "Epoch: 24, Train_Loss: 0.4168909192085266, Test_Loss: 2.822246551513672 *\n",
      "Epoch: 24, Train_Loss: 0.2571340799331665, Test_Loss: 0.33360493183135986 *\n",
      "Epoch: 24, Train_Loss: 0.46932509541511536, Test_Loss: 0.24093526601791382 *\n",
      "Epoch: 24, Train_Loss: 0.24961918592453003, Test_Loss: 0.23434393107891083 *\n",
      "Epoch: 24, Train_Loss: 0.3381688594818115, Test_Loss: 0.2152661234140396 *\n",
      "Epoch: 24, Train_Loss: 0.3029797077178955, Test_Loss: 0.20712007582187653 *\n",
      "Epoch: 24, Train_Loss: 0.2168964147567749, Test_Loss: 0.26468273997306824\n",
      "Epoch: 24, Train_Loss: 0.22364486753940582, Test_Loss: 0.22274774312973022 *\n",
      "Epoch: 24, Train_Loss: 0.5508406162261963, Test_Loss: 0.20182250440120697 *\n",
      "Epoch: 24, Train_Loss: 0.7180347442626953, Test_Loss: 0.2089875489473343\n",
      "Epoch: 24, Train_Loss: 0.2125038206577301, Test_Loss: 0.21077269315719604\n",
      "Epoch: 24, Train_Loss: 0.24545451998710632, Test_Loss: 0.2819984555244446\n",
      "Epoch: 24, Train_Loss: 0.23113113641738892, Test_Loss: 0.21927107870578766 *\n",
      "Epoch: 24, Train_Loss: 0.24602068960666656, Test_Loss: 0.21953152120113373\n",
      "Epoch: 24, Train_Loss: 0.5802439451217651, Test_Loss: 0.20965610444545746 *\n",
      "Epoch: 24, Train_Loss: 0.20561623573303223, Test_Loss: 0.2359924167394638\n",
      "Epoch: 24, Train_Loss: 0.30277881026268005, Test_Loss: 0.28821516036987305\n",
      "Epoch: 24, Train_Loss: 0.2554398477077484, Test_Loss: 0.33702030777931213\n",
      "Epoch: 24, Train_Loss: 0.23976533114910126, Test_Loss: 0.2326030433177948 *\n",
      "Epoch: 24, Train_Loss: 0.2719091773033142, Test_Loss: 0.22312548756599426 *\n",
      "Epoch: 24, Train_Loss: 0.3067241907119751, Test_Loss: 0.22066998481750488 *\n",
      "Epoch: 24, Train_Loss: 0.4294004738330841, Test_Loss: 0.2110493928194046 *\n",
      "Epoch: 24, Train_Loss: 0.23578327894210815, Test_Loss: 0.21345144510269165\n",
      "Epoch: 24, Train_Loss: 0.2223690152168274, Test_Loss: 0.23519794642925262\n",
      "Epoch: 24, Train_Loss: 0.24410505592823029, Test_Loss: 0.20615755021572113 *\n",
      "Epoch: 24, Train_Loss: 0.23124834895133972, Test_Loss: 0.21009467542171478\n",
      "Epoch: 24, Train_Loss: 0.23277926445007324, Test_Loss: 0.26496508717536926\n",
      "Epoch: 24, Train_Loss: 0.21476322412490845, Test_Loss: 0.24639689922332764 *\n",
      "Epoch: 24, Train_Loss: 0.2216508388519287, Test_Loss: 0.2078355848789215 *\n",
      "Epoch: 24, Train_Loss: 0.28737592697143555, Test_Loss: 0.2909500002861023\n",
      "Epoch: 24, Train_Loss: 0.4350467920303345, Test_Loss: 0.27974891662597656 *\n",
      "Epoch: 24, Train_Loss: 0.46563297510147095, Test_Loss: 4.493823051452637\n",
      "Epoch: 24, Train_Loss: 0.721261739730835, Test_Loss: 0.9956341981887817 *\n",
      "Epoch: 24, Train_Loss: 0.561966598033905, Test_Loss: 0.2064177691936493 *\n",
      "Epoch: 24, Train_Loss: 0.38283127546310425, Test_Loss: 0.2239174246788025\n",
      "Epoch: 24, Train_Loss: 0.3227824568748474, Test_Loss: 0.23690378665924072\n",
      "Epoch: 24, Train_Loss: 0.24557983875274658, Test_Loss: 0.21461154520511627 *\n",
      "Epoch: 24, Train_Loss: 0.21808373928070068, Test_Loss: 0.2098049521446228 *\n",
      "Epoch: 24, Train_Loss: 0.211713045835495, Test_Loss: 0.2747546136379242\n",
      "Epoch: 24, Train_Loss: 0.2989959120750427, Test_Loss: 0.2441631555557251 *\n",
      "Epoch: 24, Train_Loss: 0.5004245042800903, Test_Loss: 0.20507198572158813 *\n",
      "Epoch: 24, Train_Loss: 0.5829289555549622, Test_Loss: 0.22435401380062103\n",
      "Epoch: 24, Train_Loss: 0.8306450843811035, Test_Loss: 0.22531913220882416\n",
      "Epoch: 24, Train_Loss: 1.1354751586914062, Test_Loss: 0.25005656480789185\n",
      "Epoch: 24, Train_Loss: 0.45757007598876953, Test_Loss: 0.2098316103219986 *\n",
      "Epoch: 24, Train_Loss: 0.4841628968715668, Test_Loss: 0.2167581170797348\n",
      "Epoch: 24, Train_Loss: 0.20133963227272034, Test_Loss: 0.264992892742157\n",
      "Epoch: 24, Train_Loss: 0.21532531082630157, Test_Loss: 0.2508309781551361 *\n",
      "Epoch: 24, Train_Loss: 0.48262980580329895, Test_Loss: 0.23243993520736694 *\n",
      "Epoch: 24, Train_Loss: 0.8352770209312439, Test_Loss: 0.30960017442703247\n",
      "Epoch: 24, Train_Loss: 0.28283658623695374, Test_Loss: 0.26430457830429077 *\n",
      "Epoch: 24, Train_Loss: 0.23977479338645935, Test_Loss: 0.2573901116847992 *\n",
      "Epoch: 24, Train_Loss: 0.21754226088523865, Test_Loss: 0.25340136885643005 *\n",
      "Epoch: 24, Train_Loss: 0.3272704482078552, Test_Loss: 0.2769181728363037\n",
      "Epoch: 24, Train_Loss: 0.5071837902069092, Test_Loss: 0.23340393602848053 *\n",
      "Epoch: 24, Train_Loss: 0.44446754455566406, Test_Loss: 0.2365640550851822\n",
      "Epoch: 24, Train_Loss: 0.3197077512741089, Test_Loss: 0.291878879070282\n",
      "Epoch: 24, Train_Loss: 0.45137664675712585, Test_Loss: 0.2548726797103882 *\n",
      "Epoch: 24, Train_Loss: 0.21036677062511444, Test_Loss: 0.23769976198673248 *\n",
      "Epoch: 24, Train_Loss: 0.21005944907665253, Test_Loss: 0.25087830424308777\n",
      "Epoch: 24, Train_Loss: 0.22373804450035095, Test_Loss: 0.2409885674715042 *\n",
      "Epoch: 24, Train_Loss: 0.251844584941864, Test_Loss: 0.204225093126297 *\n",
      "Epoch: 24, Train_Loss: 0.22203193604946136, Test_Loss: 0.2339496910572052\n",
      "Epoch: 24, Train_Loss: 0.23536022007465363, Test_Loss: 0.35788846015930176\n",
      "Epoch: 24, Train_Loss: 14.558782577514648, Test_Loss: 0.28766289353370667 *\n",
      "Epoch: 24, Train_Loss: 1.1338582038879395, Test_Loss: 0.2909107208251953\n",
      "Epoch: 24, Train_Loss: 1.154132604598999, Test_Loss: 0.2481420785188675 *\n",
      "Epoch: 24, Train_Loss: 0.9477256536483765, Test_Loss: 0.2441580593585968 *\n",
      "Epoch: 24, Train_Loss: 0.24386847019195557, Test_Loss: 0.2174619734287262 *\n",
      "Epoch: 24, Train_Loss: 0.2343117892742157, Test_Loss: 0.25883057713508606\n",
      "Model saved at location save_new\\model.ckpt at epoch 24\n",
      "Epoch: 24, Train_Loss: 1.4543482065200806, Test_Loss: 0.4731445908546448\n",
      "Epoch: 24, Train_Loss: 4.672300338745117, Test_Loss: 0.35196393728256226 *\n",
      "Epoch: 24, Train_Loss: 0.5569088459014893, Test_Loss: 0.5398705005645752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train_Loss: 0.2988795042037964, Test_Loss: 0.2858532965183258 *\n",
      "Epoch: 24, Train_Loss: 4.113797664642334, Test_Loss: 0.24078291654586792 *\n",
      "Epoch: 24, Train_Loss: 0.9359195232391357, Test_Loss: 0.3066406846046448\n",
      "Epoch: 24, Train_Loss: 0.5738435983657837, Test_Loss: 0.2638015151023865 *\n",
      "Epoch: 24, Train_Loss: 0.2043619006872177, Test_Loss: 0.3318665623664856\n",
      "Epoch: 24, Train_Loss: 0.22375193238258362, Test_Loss: 0.22242316603660583 *\n",
      "Epoch: 24, Train_Loss: 0.25363489985466003, Test_Loss: 0.24792930483818054\n",
      "Epoch: 24, Train_Loss: 0.2046547681093216, Test_Loss: 0.3124666213989258\n",
      "Epoch: 24, Train_Loss: 0.21844932436943054, Test_Loss: 0.36981600522994995\n",
      "Epoch: 24, Train_Loss: 0.19827581942081451, Test_Loss: 1.0494831800460815\n",
      "Epoch: 24, Train_Loss: 0.19852010905742645, Test_Loss: 0.3255949020385742 *\n",
      "Epoch: 24, Train_Loss: 0.20292973518371582, Test_Loss: 0.5596801042556763\n",
      "Epoch: 24, Train_Loss: 0.21497014164924622, Test_Loss: 0.41454654932022095 *\n",
      "Epoch: 24, Train_Loss: 0.32296985387802124, Test_Loss: 0.41690593957901\n",
      "Epoch: 24, Train_Loss: 0.2819974720478058, Test_Loss: 0.3465188443660736 *\n",
      "Epoch: 24, Train_Loss: 0.24072879552841187, Test_Loss: 0.32283926010131836 *\n",
      "Epoch: 24, Train_Loss: 0.22601521015167236, Test_Loss: 0.24268978834152222 *\n",
      "Epoch: 24, Train_Loss: 0.24184194207191467, Test_Loss: 7.353977680206299\n",
      "Epoch: 24, Train_Loss: 0.20411793887615204, Test_Loss: 2.1387746334075928 *\n",
      "Epoch: 24, Train_Loss: 0.22977450489997864, Test_Loss: 0.717229962348938 *\n",
      "Epoch: 24, Train_Loss: 0.20464752614498138, Test_Loss: 0.7214742302894592\n",
      "Epoch: 24, Train_Loss: 0.19928790628910065, Test_Loss: 0.6662427186965942 *\n",
      "Epoch: 24, Train_Loss: 0.19888760149478912, Test_Loss: 0.2855784595012665 *\n",
      "Epoch: 24, Train_Loss: 0.19923876225948334, Test_Loss: 0.7109252214431763\n",
      "Epoch: 24, Train_Loss: 0.19911149144172668, Test_Loss: 1.4260175228118896\n",
      "Epoch: 24, Train_Loss: 0.1982661634683609, Test_Loss: 0.8435924649238586 *\n",
      "Epoch: 24, Train_Loss: 0.1980961710214615, Test_Loss: 0.7005515098571777 *\n",
      "Epoch: 24, Train_Loss: 0.2075132578611374, Test_Loss: 0.953204870223999\n",
      "Epoch: 24, Train_Loss: 0.22098907828330994, Test_Loss: 0.7519795894622803 *\n",
      "Epoch: 24, Train_Loss: 0.22405263781547546, Test_Loss: 1.509745478630066\n",
      "Epoch: 24, Train_Loss: 0.30436211824417114, Test_Loss: 0.699764609336853 *\n",
      "Epoch: 24, Train_Loss: 0.2381880283355713, Test_Loss: 1.0755112171173096\n",
      "Epoch: 24, Train_Loss: 0.347201406955719, Test_Loss: 0.5540945529937744 *\n",
      "Epoch: 24, Train_Loss: 6.0692219734191895, Test_Loss: 0.21022436022758484 *\n",
      "Epoch: 24, Train_Loss: 0.2763318419456482, Test_Loss: 0.24788397550582886\n",
      "Epoch: 24, Train_Loss: 0.21981969475746155, Test_Loss: 0.40846994519233704\n",
      "Epoch: 24, Train_Loss: 0.23351693153381348, Test_Loss: 0.40099647641181946 *\n",
      "Epoch: 24, Train_Loss: 0.29591524600982666, Test_Loss: 0.3442588448524475 *\n",
      "Epoch: 24, Train_Loss: 0.24885588884353638, Test_Loss: 0.5799108147621155\n",
      "Epoch: 24, Train_Loss: 0.3321470618247986, Test_Loss: 0.3839680552482605 *\n",
      "Epoch: 24, Train_Loss: 0.3391084671020508, Test_Loss: 0.6034082174301147\n",
      "Epoch: 24, Train_Loss: 0.36999666690826416, Test_Loss: 0.5903934836387634 *\n",
      "Epoch: 24, Train_Loss: 0.2882566750049591, Test_Loss: 0.23610971868038177 *\n",
      "Epoch: 24, Train_Loss: 0.23339897394180298, Test_Loss: 0.3325299024581909\n",
      "Epoch: 24, Train_Loss: 0.21308991312980652, Test_Loss: 0.47520682215690613\n",
      "Epoch: 24, Train_Loss: 0.22974669933319092, Test_Loss: 0.3567487299442291 *\n",
      "Epoch: 24, Train_Loss: 0.20272599160671234, Test_Loss: 0.21023866534233093 *\n",
      "Epoch: 24, Train_Loss: 0.3652855455875397, Test_Loss: 0.7179477214813232\n",
      "Epoch: 24, Train_Loss: 0.20658370852470398, Test_Loss: 0.3546501398086548 *\n",
      "Epoch: 24, Train_Loss: 0.21786461770534515, Test_Loss: 5.73344087600708\n",
      "Epoch: 24, Train_Loss: 0.22005166113376617, Test_Loss: 0.39574721455574036 *\n",
      "Epoch: 24, Train_Loss: 0.22079497575759888, Test_Loss: 0.2003321349620819 *\n",
      "Epoch: 24, Train_Loss: 0.24410942196846008, Test_Loss: 0.2508718967437744\n",
      "Epoch: 24, Train_Loss: 0.2419518530368805, Test_Loss: 0.3755483627319336\n",
      "Epoch: 24, Train_Loss: 0.20555834472179413, Test_Loss: 0.2576929032802582 *\n",
      "Epoch: 24, Train_Loss: 0.2013900876045227, Test_Loss: 0.2136804461479187 *\n",
      "Epoch: 24, Train_Loss: 0.20548491179943085, Test_Loss: 0.2962613105773926\n",
      "Epoch: 24, Train_Loss: 0.4297277331352234, Test_Loss: 0.2479526698589325 *\n",
      "Epoch: 24, Train_Loss: 4.361639022827148, Test_Loss: 0.20022819936275482 *\n",
      "Epoch: 24, Train_Loss: 0.2058582454919815, Test_Loss: 0.2246188074350357\n",
      "Epoch: 24, Train_Loss: 0.2000228762626648, Test_Loss: 0.21423675119876862 *\n",
      "Epoch: 24, Train_Loss: 0.2112025022506714, Test_Loss: 0.20949308574199677 *\n",
      "Epoch: 24, Train_Loss: 0.20530395209789276, Test_Loss: 0.3149322271347046\n",
      "Epoch: 24, Train_Loss: 0.1988818198442459, Test_Loss: 0.5884816646575928\n",
      "Epoch: 24, Train_Loss: 0.2059798240661621, Test_Loss: 0.3872600197792053 *\n",
      "Epoch: 24, Train_Loss: 0.20640923082828522, Test_Loss: 0.34402674436569214 *\n",
      "Epoch: 24, Train_Loss: 0.20940351486206055, Test_Loss: 0.24513718485832214 *\n",
      "Epoch: 24, Train_Loss: 0.31295356154441833, Test_Loss: 0.23769663274288177 *\n",
      "Epoch: 24, Train_Loss: 0.2457410842180252, Test_Loss: 0.4514733552932739\n",
      "Epoch: 24, Train_Loss: 0.20017018914222717, Test_Loss: 0.7469643354415894\n",
      "Epoch: 24, Train_Loss: 0.20026639103889465, Test_Loss: 0.5731715559959412 *\n",
      "Epoch: 24, Train_Loss: 0.21880368888378143, Test_Loss: 0.598091185092926\n",
      "Epoch: 24, Train_Loss: 0.20930181443691254, Test_Loss: 0.4772717356681824 *\n",
      "Epoch: 24, Train_Loss: 0.20758505165576935, Test_Loss: 0.42288386821746826 *\n",
      "Epoch: 24, Train_Loss: 0.21280770003795624, Test_Loss: 0.7169138789176941\n",
      "Epoch: 24, Train_Loss: 0.2155199497938156, Test_Loss: 0.43209123611450195 *\n",
      "Epoch: 24, Train_Loss: 0.21412579715251923, Test_Loss: 0.2748083174228668 *\n",
      "Epoch: 24, Train_Loss: 0.19901731610298157, Test_Loss: 0.3146659731864929\n",
      "Epoch: 24, Train_Loss: 0.2049330472946167, Test_Loss: 0.22946828603744507 *\n",
      "Epoch: 24, Train_Loss: 0.252086877822876, Test_Loss: 0.20398646593093872 *\n",
      "Epoch: 24, Train_Loss: 0.2974637746810913, Test_Loss: 0.2782469391822815\n",
      "Epoch: 24, Train_Loss: 0.2807406783103943, Test_Loss: 0.35382822155952454\n",
      "Epoch: 24, Train_Loss: 0.26172494888305664, Test_Loss: 0.2378353774547577 *\n",
      "Epoch: 24, Train_Loss: 0.24784591794013977, Test_Loss: 0.261762410402298\n",
      "Epoch: 24, Train_Loss: 0.22040241956710815, Test_Loss: 0.2514846622943878 *\n",
      "Epoch: 24, Train_Loss: 0.2238965630531311, Test_Loss: 0.24111050367355347 *\n",
      "Epoch: 24, Train_Loss: 0.20946983993053436, Test_Loss: 0.20928852260112762 *\n",
      "Epoch: 24, Train_Loss: 0.3635101318359375, Test_Loss: 0.25048914551734924\n",
      "Epoch: 24, Train_Loss: 0.4192632734775543, Test_Loss: 0.4516477584838867\n",
      "Epoch: 24, Train_Loss: 0.21139734983444214, Test_Loss: 0.330718457698822 *\n",
      "Epoch: 24, Train_Loss: 0.20612893998622894, Test_Loss: 0.33860349655151367\n",
      "Epoch: 24, Train_Loss: 0.21125444769859314, Test_Loss: 0.25818154215812683 *\n",
      "Epoch: 24, Train_Loss: 0.20749427378177643, Test_Loss: 0.26440295577049255\n",
      "Epoch: 24, Train_Loss: 0.20265990495681763, Test_Loss: 0.20882762968540192 *\n",
      "Epoch: 24, Train_Loss: 0.4358874559402466, Test_Loss: 0.20532526075839996 *\n",
      "Epoch: 24, Train_Loss: 4.287441253662109, Test_Loss: 0.21845743060112\n",
      "Epoch: 24, Train_Loss: 0.228105366230011, Test_Loss: 0.21312932670116425 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 24\n",
      "Epoch: 24, Train_Loss: 0.2076481282711029, Test_Loss: 0.22179821133613586\n",
      "Epoch: 24, Train_Loss: 0.20715059340000153, Test_Loss: 0.2025209218263626 *\n",
      "Epoch: 24, Train_Loss: 0.19783666729927063, Test_Loss: 0.33060353994369507\n",
      "Epoch: 24, Train_Loss: 0.2007758617401123, Test_Loss: 0.559436559677124\n",
      "Epoch: 24, Train_Loss: 0.20042109489440918, Test_Loss: 0.29552966356277466 *\n",
      "Epoch: 24, Train_Loss: 0.19766953587532043, Test_Loss: 0.4089716672897339\n",
      "Epoch: 24, Train_Loss: 0.1998983472585678, Test_Loss: 0.2348550409078598 *\n",
      "Epoch: 24, Train_Loss: 0.20127983391284943, Test_Loss: 0.23578515648841858\n",
      "Epoch: 24, Train_Loss: 0.22408156096935272, Test_Loss: 0.23655931651592255\n",
      "Epoch: 24, Train_Loss: 0.2325681746006012, Test_Loss: 0.23336850106716156 *\n",
      "Epoch: 24, Train_Loss: 0.24651633203029633, Test_Loss: 0.2506563067436218\n",
      "Epoch: 24, Train_Loss: 0.22268879413604736, Test_Loss: 5.40061616897583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train_Loss: 0.20221751928329468, Test_Loss: 0.6897097826004028 *\n",
      "Epoch: 24, Train_Loss: 0.3526519238948822, Test_Loss: 0.24586381018161774 *\n",
      "Epoch: 24, Train_Loss: 0.40329059958457947, Test_Loss: 0.23080605268478394 *\n",
      "Epoch: 24, Train_Loss: 0.4339703917503357, Test_Loss: 0.22324387729167938 *\n",
      "Epoch: 24, Train_Loss: 0.37349265813827515, Test_Loss: 0.2155676633119583 *\n",
      "Epoch: 24, Train_Loss: 0.19900843501091003, Test_Loss: 0.2158372551202774\n",
      "Epoch: 24, Train_Loss: 0.1981278955936432, Test_Loss: 0.20473118126392365 *\n",
      "Epoch: 24, Train_Loss: 0.1985442042350769, Test_Loss: 0.1995634138584137 *\n",
      "Epoch: 24, Train_Loss: 0.2070598304271698, Test_Loss: 0.20186558365821838\n",
      "Epoch: 24, Train_Loss: 0.20692498981952667, Test_Loss: 0.2203548401594162\n",
      "Epoch: 24, Train_Loss: 0.2143784910440445, Test_Loss: 0.2709435224533081\n",
      "Epoch: 24, Train_Loss: 0.20435670018196106, Test_Loss: 0.22701644897460938 *\n",
      "Epoch: 24, Train_Loss: 0.19688911736011505, Test_Loss: 0.22797614336013794\n",
      "Epoch: 24, Train_Loss: 0.21322329342365265, Test_Loss: 0.2365533709526062\n",
      "Epoch: 24, Train_Loss: 0.22917363047599792, Test_Loss: 0.20164784789085388 *\n",
      "Epoch: 24, Train_Loss: 0.37727299332618713, Test_Loss: 0.209539532661438\n",
      "Epoch: 24, Train_Loss: 0.2843996584415436, Test_Loss: 0.2195732146501541\n",
      "Epoch: 24, Train_Loss: 0.24916160106658936, Test_Loss: 0.2483256310224533\n",
      "Epoch: 24, Train_Loss: 0.23003356158733368, Test_Loss: 0.2163025289773941 *\n",
      "Epoch: 24, Train_Loss: 0.2955784499645233, Test_Loss: 0.1991993933916092 *\n",
      "Epoch: 24, Train_Loss: 0.2750243544578552, Test_Loss: 0.198633074760437 *\n",
      "Epoch: 24, Train_Loss: 0.26332777738571167, Test_Loss: 0.20828545093536377\n",
      "Epoch: 24, Train_Loss: 0.24394533038139343, Test_Loss: 0.21192552149295807\n",
      "Epoch: 24, Train_Loss: 0.3386237323284149, Test_Loss: 0.22065469622612\n",
      "Epoch: 24, Train_Loss: 0.21435336768627167, Test_Loss: 0.19937564432621002 *\n",
      "Epoch: 24, Train_Loss: 0.2027406096458435, Test_Loss: 0.20097875595092773\n",
      "Epoch: 24, Train_Loss: 2.6071550846099854, Test_Loss: 0.24349868297576904\n",
      "Epoch: 24, Train_Loss: 0.775391161441803, Test_Loss: 0.20947152376174927 *\n",
      "Epoch: 24, Train_Loss: 0.24785031378269196, Test_Loss: 0.19872073829174042 *\n",
      "Epoch: 24, Train_Loss: 0.2404194474220276, Test_Loss: 0.31963270902633667\n",
      "Epoch: 24, Train_Loss: 0.20208902657032013, Test_Loss: 0.912040650844574\n",
      "Epoch: 24, Train_Loss: 0.22197897732257843, Test_Loss: 4.6118855476379395\n",
      "Epoch: 24, Train_Loss: 0.20130190253257751, Test_Loss: 0.2133699357509613 *\n",
      "Epoch: 24, Train_Loss: 0.23200614750385284, Test_Loss: 0.20431527495384216 *\n",
      "Epoch: 24, Train_Loss: 0.28987058997154236, Test_Loss: 0.25607600808143616\n",
      "Epoch: 24, Train_Loss: 0.2454158365726471, Test_Loss: 0.3167150020599365\n",
      "Epoch: 24, Train_Loss: 0.2193346917629242, Test_Loss: 0.2526923418045044 *\n",
      "Epoch: 24, Train_Loss: 0.20337463915348053, Test_Loss: 0.20273645222187042 *\n",
      "Epoch: 24, Train_Loss: 0.21097595989704132, Test_Loss: 0.26327618956565857\n",
      "Epoch: 24, Train_Loss: 0.20106108486652374, Test_Loss: 0.22473512589931488 *\n",
      "Epoch: 24, Train_Loss: 0.20787662267684937, Test_Loss: 0.21743349730968475 *\n",
      "Epoch: 24, Train_Loss: 0.2409781515598297, Test_Loss: 0.22313924133777618\n",
      "Epoch: 24, Train_Loss: 0.21762922406196594, Test_Loss: 0.27745258808135986\n",
      "Epoch: 24, Train_Loss: 0.19832955300807953, Test_Loss: 0.22872895002365112 *\n",
      "Epoch: 24, Train_Loss: 0.20246516168117523, Test_Loss: 0.30521827936172485\n",
      "Epoch: 24, Train_Loss: 0.21274816989898682, Test_Loss: 0.42982566356658936\n",
      "Epoch: 24, Train_Loss: 0.21789777278900146, Test_Loss: 0.23457226157188416 *\n",
      "Epoch: 24, Train_Loss: 0.20228232443332672, Test_Loss: 0.2028285712003708 *\n",
      "Epoch: 24, Train_Loss: 0.1965867131948471, Test_Loss: 0.24261987209320068\n",
      "Epoch: 24, Train_Loss: 0.1973867416381836, Test_Loss: 0.4176756739616394\n",
      "Epoch: 24, Train_Loss: 0.19662094116210938, Test_Loss: 0.22140854597091675 *\n",
      "Epoch: 24, Train_Loss: 0.21050933003425598, Test_Loss: 0.2524762451648712\n",
      "Epoch: 24, Train_Loss: 0.19900524616241455, Test_Loss: 0.2535940408706665\n",
      "Epoch: 24, Train_Loss: 0.20139676332473755, Test_Loss: 0.26072627305984497\n",
      "Epoch: 24, Train_Loss: 0.20218022167682648, Test_Loss: 0.23571068048477173 *\n",
      "Epoch: 24, Train_Loss: 0.1964898407459259, Test_Loss: 0.2231130599975586 *\n",
      "Epoch: 24, Train_Loss: 0.19706450402736664, Test_Loss: 0.21648027002811432 *\n",
      "Epoch: 24, Train_Loss: 0.1996229588985443, Test_Loss: 0.20713086426258087 *\n",
      "Epoch: 24, Train_Loss: 0.2107701301574707, Test_Loss: 0.2262381762266159\n",
      "Epoch: 24, Train_Loss: 0.2064952850341797, Test_Loss: 0.20530901849269867 *\n",
      "Epoch: 24, Train_Loss: 0.20791707932949066, Test_Loss: 0.22615331411361694\n",
      "Epoch: 24, Train_Loss: 0.22234398126602173, Test_Loss: 0.2983764111995697\n",
      "Epoch: 24, Train_Loss: 0.23185646533966064, Test_Loss: 0.25305628776550293 *\n",
      "Epoch: 24, Train_Loss: 0.2179763913154602, Test_Loss: 0.5877707004547119\n",
      "Epoch: 24, Train_Loss: 0.20826934278011322, Test_Loss: 0.5237199664115906 *\n",
      "Epoch: 24, Train_Loss: 0.2347872257232666, Test_Loss: 0.3149595856666565 *\n",
      "Epoch: 24, Train_Loss: 0.23333632946014404, Test_Loss: 0.2360907793045044 *\n",
      "Epoch: 24, Train_Loss: 0.2024838924407959, Test_Loss: 0.21981503069400787 *\n",
      "Epoch: 24, Train_Loss: 0.20209738612174988, Test_Loss: 0.2029990255832672 *\n",
      "Epoch: 24, Train_Loss: 0.20365601778030396, Test_Loss: 0.2486943006515503\n",
      "Epoch: 24, Train_Loss: 0.20998108386993408, Test_Loss: 0.37347710132598877\n",
      "Epoch: 24, Train_Loss: 0.25016844272613525, Test_Loss: 0.40907031297683716\n",
      "Epoch: 24, Train_Loss: 0.2576081454753876, Test_Loss: 0.2572331130504608 *\n",
      "Epoch: 24, Train_Loss: 0.215654194355011, Test_Loss: 0.29327839612960815\n",
      "Epoch: 24, Train_Loss: 0.19762054085731506, Test_Loss: 0.19972242414951324 *\n",
      "Epoch: 24, Train_Loss: 0.2547236979007721, Test_Loss: 0.19920527935028076 *\n",
      "Epoch: 24, Train_Loss: 0.20909914374351501, Test_Loss: 0.20434482395648956\n",
      "Epoch: 24, Train_Loss: 0.20068690180778503, Test_Loss: 0.20851871371269226\n",
      "Epoch: 24, Train_Loss: 0.20914433896541595, Test_Loss: 0.21916159987449646\n",
      "Epoch: 24, Train_Loss: 0.21825869381427765, Test_Loss: 0.2120109349489212 *\n",
      "Epoch: 24, Train_Loss: 0.2941552400588989, Test_Loss: 0.2031743824481964 *\n",
      "Epoch: 24, Train_Loss: 0.24056746065616608, Test_Loss: 0.32150885462760925\n",
      "Epoch: 24, Train_Loss: 0.2234071046113968, Test_Loss: 0.579623818397522\n",
      "Epoch: 24, Train_Loss: 0.20436474680900574, Test_Loss: 0.36996006965637207 *\n",
      "Epoch: 24, Train_Loss: 0.20923416316509247, Test_Loss: 0.29030847549438477 *\n",
      "Epoch: 24, Train_Loss: 0.20845849812030792, Test_Loss: 0.22054003179073334 *\n",
      "Epoch: 24, Train_Loss: 0.19845443964004517, Test_Loss: 0.22043859958648682 *\n",
      "Epoch: 24, Train_Loss: 0.2060677409172058, Test_Loss: 0.22091615200042725\n",
      "Model saved at location save_new\\model.ckpt at epoch 24\n",
      "Epoch: 24, Train_Loss: 0.21610701084136963, Test_Loss: 0.21587438881397247 *\n",
      "Epoch: 24, Train_Loss: 0.22345110774040222, Test_Loss: 0.32040148973464966\n",
      "Epoch: 24, Train_Loss: 0.29074230790138245, Test_Loss: 5.928504943847656\n",
      "Epoch: 24, Train_Loss: 0.20081520080566406, Test_Loss: 0.319589227437973 *\n",
      "Epoch: 24, Train_Loss: 0.24756482243537903, Test_Loss: 0.22211024165153503 *\n",
      "Epoch: 24, Train_Loss: 0.2129836529493332, Test_Loss: 0.21644362807273865 *\n",
      "Epoch: 24, Train_Loss: 0.21810553967952728, Test_Loss: 0.21054790914058685 *\n",
      "Epoch: 24, Train_Loss: 0.2888222932815552, Test_Loss: 0.20286066830158234 *\n",
      "Epoch: 24, Train_Loss: 0.3971341550350189, Test_Loss: 0.21321037411689758\n",
      "Epoch: 24, Train_Loss: 0.2026558518409729, Test_Loss: 0.2004254013299942 *\n",
      "Epoch: 24, Train_Loss: 0.2246105819940567, Test_Loss: 0.19939754903316498 *\n",
      "Epoch: 24, Train_Loss: 0.19588826596736908, Test_Loss: 0.19939462840557098 *\n",
      "Epoch: 24, Train_Loss: 0.19527959823608398, Test_Loss: 0.21401260793209076\n",
      "Epoch: 24, Train_Loss: 0.19721874594688416, Test_Loss: 0.2714897394180298\n",
      "Epoch: 24, Train_Loss: 0.19630829989910126, Test_Loss: 0.21494987607002258 *\n",
      "Epoch: 24, Train_Loss: 0.20054543018341064, Test_Loss: 0.21800479292869568\n",
      "Epoch: 24, Train_Loss: 0.2056061029434204, Test_Loss: 0.23958808183670044\n",
      "Epoch: 24, Train_Loss: 0.2050851285457611, Test_Loss: 0.19839444756507874 *\n",
      "Epoch: 24, Train_Loss: 0.2025199979543686, Test_Loss: 0.2077968418598175\n",
      "Epoch: 24, Train_Loss: 0.2035185843706131, Test_Loss: 0.20106390118598938 *\n",
      "Epoch: 24, Train_Loss: 0.20225435495376587, Test_Loss: 0.2365999072790146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train_Loss: 0.19640520215034485, Test_Loss: 0.20396290719509125 *\n",
      "Epoch: 24, Train_Loss: 0.1950756013393402, Test_Loss: 0.199870303273201 *\n",
      "Epoch: 24, Train_Loss: 0.21021151542663574, Test_Loss: 0.1955208033323288 *\n",
      "Epoch: 24, Train_Loss: 0.2113478183746338, Test_Loss: 0.20185686647891998\n",
      "Epoch: 24, Train_Loss: 0.21671007573604584, Test_Loss: 0.20407763123512268\n",
      "Epoch: 24, Train_Loss: 0.19794560968875885, Test_Loss: 0.20580488443374634\n",
      "Epoch: 24, Train_Loss: 0.2157241851091385, Test_Loss: 0.1957566738128662 *\n",
      "Epoch: 24, Train_Loss: 0.22015613317489624, Test_Loss: 0.19807015359401703\n",
      "Epoch: 24, Train_Loss: 0.2255784571170807, Test_Loss: 0.2192213237285614\n",
      "Epoch: 24, Train_Loss: 0.2019968181848526, Test_Loss: 0.19898906350135803 *\n",
      "Epoch: 24, Train_Loss: 0.22121721506118774, Test_Loss: 0.19805240631103516 *\n",
      "Epoch: 24, Train_Loss: 0.19557201862335205, Test_Loss: 0.3014066815376282\n",
      "Epoch: 24, Train_Loss: 0.21465986967086792, Test_Loss: 2.2232913970947266\n",
      "Epoch: 24, Train_Loss: 0.20960794389247894, Test_Loss: 3.9547064304351807\n",
      "Epoch: 24, Train_Loss: 0.22043175995349884, Test_Loss: 0.21118301153182983 *\n",
      "Epoch: 24, Train_Loss: 1.7859452962875366, Test_Loss: 0.19501350820064545 *\n",
      "Epoch: 24, Train_Loss: 3.9279823303222656, Test_Loss: 0.23964673280715942\n",
      "Epoch: 24, Train_Loss: 0.2891415059566498, Test_Loss: 0.2180384248495102 *\n",
      "Epoch: 24, Train_Loss: 0.21334850788116455, Test_Loss: 0.22748392820358276\n",
      "Epoch: 24, Train_Loss: 0.20522072911262512, Test_Loss: 0.23801647126674652\n",
      "Epoch: 24, Train_Loss: 0.2859528064727783, Test_Loss: 0.3035735487937927\n",
      "Epoch: 24, Train_Loss: 0.23845195770263672, Test_Loss: 0.20387375354766846 *\n",
      "Epoch: 24, Train_Loss: 0.20963314175605774, Test_Loss: 0.20865853130817413\n",
      "Epoch: 24, Train_Loss: 0.19471265375614166, Test_Loss: 0.2212125062942505\n",
      "Epoch: 24, Train_Loss: 0.2627803087234497, Test_Loss: 0.22388850152492523\n",
      "Epoch: 24, Train_Loss: 0.2173178493976593, Test_Loss: 0.20425811409950256 *\n",
      "Epoch: 24, Train_Loss: 0.2041861116886139, Test_Loss: 0.2583935558795929\n",
      "Epoch: 24, Train_Loss: 0.5629737973213196, Test_Loss: 0.23721131682395935 *\n",
      "Epoch: 24, Train_Loss: 0.52064448595047, Test_Loss: 0.27320653200149536\n",
      "Epoch: 24, Train_Loss: 0.9114798307418823, Test_Loss: 0.22607259452342987 *\n",
      "Epoch: 24, Train_Loss: 0.28890788555145264, Test_Loss: 0.21790756285190582 *\n",
      "Epoch: 24, Train_Loss: 0.42475923895835876, Test_Loss: 0.2885376513004303\n",
      "Epoch: 24, Train_Loss: 1.5101739168167114, Test_Loss: 0.1993989497423172 *\n",
      "Epoch: 24, Train_Loss: 0.7137148976325989, Test_Loss: 0.20481780171394348\n",
      "Epoch: 24, Train_Loss: 0.19865864515304565, Test_Loss: 0.20079195499420166 *\n",
      "Epoch: 24, Train_Loss: 0.2068808674812317, Test_Loss: 0.20438270270824432\n",
      "Epoch: 24, Train_Loss: 0.668988823890686, Test_Loss: 0.20052510499954224 *\n",
      "Epoch: 24, Train_Loss: 0.48757103085517883, Test_Loss: 0.1965334713459015 *\n",
      "Epoch: 24, Train_Loss: 0.7242461442947388, Test_Loss: 0.2411297857761383\n",
      "Epoch: 24, Train_Loss: 0.22204817831516266, Test_Loss: 0.2224254459142685 *\n",
      "Epoch: 24, Train_Loss: 0.2419046014547348, Test_Loss: 0.2105492800474167 *\n",
      "Epoch: 24, Train_Loss: 0.4199231266975403, Test_Loss: 0.2281033992767334\n",
      "Epoch: 24, Train_Loss: 0.3669481873512268, Test_Loss: 0.2092823088169098 *\n",
      "Epoch: 24, Train_Loss: 0.23379945755004883, Test_Loss: 0.2596386969089508\n",
      "Epoch: 24, Train_Loss: 0.26402759552001953, Test_Loss: 0.23509761691093445 *\n",
      "Epoch: 24, Train_Loss: 0.2318270206451416, Test_Loss: 0.4617738127708435\n",
      "Epoch: 24, Train_Loss: 0.22554951906204224, Test_Loss: 0.39143428206443787 *\n",
      "Epoch: 24, Train_Loss: 0.3365338444709778, Test_Loss: 0.2503862977027893 *\n",
      "Epoch: 24, Train_Loss: 0.25923001766204834, Test_Loss: 0.2456463724374771 *\n",
      "Epoch: 24, Train_Loss: 0.22463050484657288, Test_Loss: 0.23254308104515076 *\n",
      "Epoch: 24, Train_Loss: 0.292704313993454, Test_Loss: 0.20307917892932892 *\n",
      "Epoch: 24, Train_Loss: 0.24419555068016052, Test_Loss: 0.25445008277893066\n",
      "Epoch: 24, Train_Loss: 0.2602526545524597, Test_Loss: 0.24450445175170898 *\n",
      "Epoch: 24, Train_Loss: 0.2662491798400879, Test_Loss: 0.4625749886035919\n",
      "Epoch: 24, Train_Loss: 0.3904285430908203, Test_Loss: 0.27982446551322937 *\n",
      "Epoch: 24, Train_Loss: 0.2330438643693924, Test_Loss: 0.26042208075523376 *\n",
      "Epoch: 24, Train_Loss: 0.22937746345996857, Test_Loss: 0.20599348843097687 *\n",
      "Epoch: 24, Train_Loss: 0.2346782237291336, Test_Loss: 0.19782578945159912 *\n",
      "Epoch: 24, Train_Loss: 0.2175453156232834, Test_Loss: 0.20725160837173462\n",
      "Epoch: 24, Train_Loss: 0.1953403353691101, Test_Loss: 0.21495793759822845\n",
      "Epoch: 24, Train_Loss: 0.19480496644973755, Test_Loss: 0.21080102026462555 *\n",
      "Epoch: 24, Train_Loss: 0.19441594183444977, Test_Loss: 0.21048837900161743 *\n",
      "Epoch: 24, Train_Loss: 0.20011673867702484, Test_Loss: 0.2383272647857666\n",
      "Epoch: 24, Train_Loss: 0.2091381549835205, Test_Loss: 0.347663551568985\n",
      "Epoch: 24, Train_Loss: 0.2213798314332962, Test_Loss: 0.4785168766975403\n",
      "Epoch: 24, Train_Loss: 0.2091285139322281, Test_Loss: 0.5188589692115784\n",
      "Epoch: 24, Train_Loss: 0.26366081833839417, Test_Loss: 0.2915226221084595 *\n",
      "Epoch: 24, Train_Loss: 0.3236250877380371, Test_Loss: 0.28179556131362915 *\n",
      "Epoch: 24, Train_Loss: 0.3026775121688843, Test_Loss: 0.2815800607204437 *\n",
      "Epoch: 24, Train_Loss: 0.2135862112045288, Test_Loss: 0.28306668996810913\n",
      "Epoch: 24, Train_Loss: 0.2371816486120224, Test_Loss: 0.26799389719963074 *\n",
      "Epoch: 24, Train_Loss: 0.2875029444694519, Test_Loss: 0.745308518409729\n",
      "Epoch: 24, Train_Loss: 0.34118637442588806, Test_Loss: 5.522392749786377\n",
      "Epoch: 24, Train_Loss: 0.2697804868221283, Test_Loss: 0.2516837418079376 *\n",
      "Epoch: 24, Train_Loss: 0.2903875410556793, Test_Loss: 0.25372543931007385\n",
      "Epoch: 24, Train_Loss: 0.25928717851638794, Test_Loss: 0.25441357493400574\n",
      "Epoch: 24, Train_Loss: 0.25118309259414673, Test_Loss: 0.21404680609703064 *\n",
      "Epoch: 24, Train_Loss: 0.33312541246414185, Test_Loss: 0.2003074735403061 *\n",
      "Epoch: 24, Train_Loss: 0.21272271871566772, Test_Loss: 0.28392496705055237\n",
      "Model saved at location save_new\\model.ckpt at epoch 24\n",
      "Epoch: 24, Train_Loss: 0.22279952466487885, Test_Loss: 0.2501577138900757 *\n",
      "Epoch: 24, Train_Loss: 0.5978829264640808, Test_Loss: 0.20350579917430878 *\n",
      "Epoch: 24, Train_Loss: 0.5060461759567261, Test_Loss: 0.20187407732009888 *\n",
      "Epoch: 24, Train_Loss: 0.26919281482696533, Test_Loss: 0.2336837649345398\n",
      "Epoch: 24, Train_Loss: 0.23992028832435608, Test_Loss: 0.410114586353302\n",
      "Epoch: 24, Train_Loss: 0.26072752475738525, Test_Loss: 0.2307550609111786 *\n",
      "Epoch: 24, Train_Loss: 0.2067980319261551, Test_Loss: 0.22246596217155457 *\n",
      "Epoch: 24, Train_Loss: 0.48266687989234924, Test_Loss: 0.25262701511383057\n",
      "Epoch: 24, Train_Loss: 0.22124847769737244, Test_Loss: 0.23042406141757965 *\n",
      "Epoch: 24, Train_Loss: 0.24890808761119843, Test_Loss: 0.245279461145401\n",
      "Epoch: 24, Train_Loss: 0.32594582438468933, Test_Loss: 0.28231510519981384\n",
      "Epoch: 24, Train_Loss: 0.22438178956508636, Test_Loss: 0.2787788510322571 *\n",
      "Epoch: 24, Train_Loss: 0.21783925592899323, Test_Loss: 0.22302234172821045 *\n",
      "Epoch: 24, Train_Loss: 0.27042609453201294, Test_Loss: 0.22542306780815125\n",
      "Epoch: 24, Train_Loss: 0.35154035687446594, Test_Loss: 0.19876034557819366 *\n",
      "Epoch: 24, Train_Loss: 0.2426779717206955, Test_Loss: 0.2212764322757721\n",
      "Epoch: 24, Train_Loss: 0.25990432500839233, Test_Loss: 0.24867185950279236\n",
      "Epoch: 24, Train_Loss: 0.22897452116012573, Test_Loss: 0.21575425565242767 *\n",
      "Epoch: 24, Train_Loss: 0.2430511862039566, Test_Loss: 0.19544920325279236 *\n",
      "Epoch: 24, Train_Loss: 0.2355361431837082, Test_Loss: 0.2234414666891098\n",
      "Epoch: 24, Train_Loss: 0.21368595957756042, Test_Loss: 0.23834767937660217\n",
      "Epoch: 24, Train_Loss: 0.20361381769180298, Test_Loss: 0.21220062673091888 *\n",
      "Epoch: 24, Train_Loss: 0.2502075433731079, Test_Loss: 0.21565666794776917\n",
      "Epoch: 24, Train_Loss: 0.465787410736084, Test_Loss: 0.34264808893203735\n",
      "Epoch: 24, Train_Loss: 0.45539209246635437, Test_Loss: 3.166808843612671\n",
      "Epoch: 24, Train_Loss: 0.5467821359634399, Test_Loss: 2.5271525382995605 *\n",
      "Epoch: 24, Train_Loss: 0.5542867183685303, Test_Loss: 0.2086467146873474 *\n",
      "Epoch: 24, Train_Loss: 0.4083763062953949, Test_Loss: 0.20051658153533936 *\n",
      "Epoch: 24, Train_Loss: 0.3016839027404785, Test_Loss: 0.22843435406684875\n",
      "Epoch: 24, Train_Loss: 0.25794342160224915, Test_Loss: 0.2146986871957779 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train_Loss: 0.20441529154777527, Test_Loss: 0.21308407187461853 *\n",
      "Epoch: 24, Train_Loss: 0.2024172991514206, Test_Loss: 0.27152642607688904\n",
      "Epoch: 24, Train_Loss: 0.22642838954925537, Test_Loss: 0.24601778388023376 *\n",
      "Epoch: 24, Train_Loss: 0.43191200494766235, Test_Loss: 0.20138968527317047 *\n",
      "Epoch: 24, Train_Loss: 0.5063920617103577, Test_Loss: 0.22546428442001343\n",
      "Epoch: 24, Train_Loss: 0.5588364005088806, Test_Loss: 0.22058607637882233 *\n",
      "Epoch: 24, Train_Loss: 0.9516744017601013, Test_Loss: 0.28348177671432495\n",
      "Epoch: 24, Train_Loss: 0.625694751739502, Test_Loss: 0.21211440861225128 *\n",
      "Epoch: 24, Train_Loss: 0.37265002727508545, Test_Loss: 0.27773162722587585\n",
      "Epoch: 24, Train_Loss: 0.20892228186130524, Test_Loss: 0.30470553040504456\n",
      "Epoch: 24, Train_Loss: 0.20209236443042755, Test_Loss: 0.28906065225601196 *\n",
      "Epoch: 24, Train_Loss: 0.41253483295440674, Test_Loss: 0.20437437295913696 *\n",
      "Epoch: 24, Train_Loss: 0.5335015654563904, Test_Loss: 0.2436087429523468\n",
      "Epoch: 24, Train_Loss: 0.4723617434501648, Test_Loss: 0.28760001063346863\n",
      "Epoch: 24, Train_Loss: 0.24326783418655396, Test_Loss: 0.26300495862960815 *\n",
      "Epoch: 24, Train_Loss: 0.212857186794281, Test_Loss: 0.3341121971607208\n",
      "Epoch: 24, Train_Loss: 0.27498820424079895, Test_Loss: 0.34918683767318726\n",
      "Epoch: 24, Train_Loss: 0.4733406901359558, Test_Loss: 0.27206653356552124 *\n",
      "Epoch: 24, Train_Loss: 0.3559848666191101, Test_Loss: 0.2858079671859741\n",
      "Epoch: 24, Train_Loss: 0.24006515741348267, Test_Loss: 0.2978413701057434\n",
      "Epoch: 24, Train_Loss: 0.35721123218536377, Test_Loss: 0.37880754470825195\n",
      "Epoch: 24, Train_Loss: 0.2236979603767395, Test_Loss: 0.2898359000682831 *\n",
      "Epoch: 24, Train_Loss: 0.19668877124786377, Test_Loss: 0.2715272009372711 *\n",
      "Epoch: 24, Train_Loss: 0.22126267850399017, Test_Loss: 0.27523863315582275\n",
      "Epoch: 25, Train_Loss: 0.2131882905960083, Test_Loss: 0.2108515053987503 *\n",
      "Epoch: 25, Train_Loss: 0.2526145279407501, Test_Loss: 0.2259484827518463\n",
      "Epoch: 25, Train_Loss: 0.26736387610435486, Test_Loss: 0.2623060643672943\n",
      "Epoch: 25, Train_Loss: 4.898343086242676, Test_Loss: 0.2755395770072937\n",
      "Epoch: 25, Train_Loss: 10.90563678741455, Test_Loss: 0.29436948895454407\n",
      "Epoch: 25, Train_Loss: 0.8462644815444946, Test_Loss: 0.2367924153804779 *\n",
      "Epoch: 25, Train_Loss: 1.0212328433990479, Test_Loss: 0.2859586477279663\n",
      "Epoch: 25, Train_Loss: 0.6109492778778076, Test_Loss: 0.24886192381381989 *\n",
      "Epoch: 25, Train_Loss: 0.25211209058761597, Test_Loss: 0.2196851372718811 *\n",
      "Epoch: 25, Train_Loss: 0.508084237575531, Test_Loss: 0.34506410360336304\n",
      "Epoch: 25, Train_Loss: 3.8101391792297363, Test_Loss: 0.2968500256538391 *\n",
      "Epoch: 25, Train_Loss: 1.6575579643249512, Test_Loss: 0.5570806264877319\n",
      "Epoch: 25, Train_Loss: 0.3124610483646393, Test_Loss: 0.34132587909698486 *\n",
      "Epoch: 25, Train_Loss: 2.026118278503418, Test_Loss: 0.2632293403148651 *\n",
      "Epoch: 25, Train_Loss: 2.867832660675049, Test_Loss: 0.31344664096832275\n",
      "Epoch: 25, Train_Loss: 0.7408765554428101, Test_Loss: 0.2909969687461853 *\n",
      "Epoch: 25, Train_Loss: 0.20692890882492065, Test_Loss: 0.35271379351615906\n",
      "Epoch: 25, Train_Loss: 0.2573891282081604, Test_Loss: 0.221114844083786 *\n",
      "Epoch: 25, Train_Loss: 0.2729026675224304, Test_Loss: 0.2742968797683716\n",
      "Epoch: 25, Train_Loss: 0.23899371922016144, Test_Loss: 0.20695385336875916 *\n",
      "Epoch: 25, Train_Loss: 0.1970251351594925, Test_Loss: 0.3397018015384674\n",
      "Epoch: 25, Train_Loss: 0.1954132616519928, Test_Loss: 0.8702059984207153\n",
      "Epoch: 25, Train_Loss: 0.19376227259635925, Test_Loss: 0.4491634964942932 *\n",
      "Epoch: 25, Train_Loss: 0.19731725752353668, Test_Loss: 0.35107558965682983 *\n",
      "Epoch: 25, Train_Loss: 0.24421201646327972, Test_Loss: 1.1630526781082153\n",
      "Epoch: 25, Train_Loss: 0.2615448832511902, Test_Loss: 1.0878503322601318 *\n",
      "Epoch: 25, Train_Loss: 0.30552923679351807, Test_Loss: 0.8466922044754028 *\n",
      "Epoch: 25, Train_Loss: 0.3638845682144165, Test_Loss: 0.5907882452011108 *\n",
      "Epoch: 25, Train_Loss: 0.25881946086883545, Test_Loss: 0.2949357032775879 *\n",
      "Epoch: 25, Train_Loss: 0.22660119831562042, Test_Loss: 3.3344242572784424\n",
      "Epoch: 25, Train_Loss: 0.21515221893787384, Test_Loss: 4.729086875915527\n",
      "Epoch: 25, Train_Loss: 0.21758964657783508, Test_Loss: 0.357824444770813 *\n",
      "Epoch: 25, Train_Loss: 0.19678983092308044, Test_Loss: 0.3734390139579773\n",
      "Epoch: 25, Train_Loss: 0.1960047334432602, Test_Loss: 0.3726656436920166 *\n",
      "Epoch: 25, Train_Loss: 0.19304990768432617, Test_Loss: 0.2103130966424942 *\n",
      "Epoch: 25, Train_Loss: 0.1931428462266922, Test_Loss: 0.24004681408405304\n",
      "Epoch: 25, Train_Loss: 0.1935962438583374, Test_Loss: 0.5948203802108765\n",
      "Epoch: 25, Train_Loss: 0.1940700113773346, Test_Loss: 0.3682295083999634 *\n",
      "Epoch: 25, Train_Loss: 0.19330008327960968, Test_Loss: 0.23810754716396332 *\n",
      "Epoch: 25, Train_Loss: 0.19747981429100037, Test_Loss: 0.35526108741760254\n",
      "Epoch: 25, Train_Loss: 0.20996332168579102, Test_Loss: 0.2855905294418335 *\n",
      "Epoch: 25, Train_Loss: 0.20718394219875336, Test_Loss: 0.7486017346382141\n",
      "Epoch: 25, Train_Loss: 0.2915761470794678, Test_Loss: 0.27814340591430664 *\n",
      "Epoch: 25, Train_Loss: 0.2035348266363144, Test_Loss: 0.30029648542404175\n",
      "Epoch: 25, Train_Loss: 0.2575092911720276, Test_Loss: 0.2553975582122803 *\n",
      "Epoch: 25, Train_Loss: 6.399796962738037, Test_Loss: 0.19810986518859863 *\n",
      "Epoch: 25, Train_Loss: 1.1285936832427979, Test_Loss: 0.2110910266637802\n",
      "Epoch: 25, Train_Loss: 0.20292067527770996, Test_Loss: 0.2267298549413681\n",
      "Epoch: 25, Train_Loss: 0.21207039058208466, Test_Loss: 0.5041424036026001\n",
      "Epoch: 25, Train_Loss: 0.25931668281555176, Test_Loss: 0.22326235473155975 *\n",
      "Epoch: 25, Train_Loss: 0.20522484183311462, Test_Loss: 0.5464468598365784\n",
      "Epoch: 25, Train_Loss: 0.21687892079353333, Test_Loss: 0.233531653881073 *\n",
      "Epoch: 25, Train_Loss: 0.2494126707315445, Test_Loss: 0.3830570578575134\n",
      "Epoch: 25, Train_Loss: 0.3261887729167938, Test_Loss: 0.5216902494430542\n",
      "Epoch: 25, Train_Loss: 0.3599216639995575, Test_Loss: 0.2659365236759186 *\n",
      "Epoch: 25, Train_Loss: 0.31413960456848145, Test_Loss: 0.23675937950611115 *\n",
      "Epoch: 25, Train_Loss: 0.20089295506477356, Test_Loss: 0.42006146907806396\n",
      "Epoch: 25, Train_Loss: 0.2364705204963684, Test_Loss: 0.37440574169158936 *\n",
      "Epoch: 25, Train_Loss: 0.2125314176082611, Test_Loss: 0.2524224519729614 *\n",
      "Epoch: 25, Train_Loss: 0.3724045157432556, Test_Loss: 0.46441584825515747\n",
      "Epoch: 25, Train_Loss: 0.20202454924583435, Test_Loss: 0.48299020528793335\n",
      "Epoch: 25, Train_Loss: 0.22865654528141022, Test_Loss: 5.411661148071289\n",
      "Epoch: 25, Train_Loss: 0.23698878288269043, Test_Loss: 2.296109199523926 *\n",
      "Epoch: 25, Train_Loss: 0.2234601229429245, Test_Loss: 0.21079254150390625 *\n",
      "Epoch: 25, Train_Loss: 0.23190470039844513, Test_Loss: 0.27629703283309937\n",
      "Epoch: 25, Train_Loss: 0.2398940473794937, Test_Loss: 0.3611767888069153\n",
      "Epoch: 25, Train_Loss: 0.2265344113111496, Test_Loss: 0.4781128764152527\n",
      "Epoch: 25, Train_Loss: 0.1970052570104599, Test_Loss: 0.26054689288139343 *\n",
      "Epoch: 25, Train_Loss: 0.19634577631950378, Test_Loss: 0.3451980650424957\n",
      "Epoch: 25, Train_Loss: 0.3031860589981079, Test_Loss: 0.29298126697540283 *\n",
      "Epoch: 25, Train_Loss: 3.5938212871551514, Test_Loss: 0.1969834417104721 *\n",
      "Epoch: 25, Train_Loss: 0.2872447371482849, Test_Loss: 0.2373233139514923\n",
      "Epoch: 25, Train_Loss: 0.19890834391117096, Test_Loss: 0.22022907435894012 *\n",
      "Epoch: 25, Train_Loss: 0.27043604850769043, Test_Loss: 0.22973021864891052\n",
      "Epoch: 25, Train_Loss: 0.20254403352737427, Test_Loss: 0.2533568739891052\n",
      "Epoch: 25, Train_Loss: 0.1937486231327057, Test_Loss: 0.4991346001625061\n",
      "Epoch: 25, Train_Loss: 0.21657735109329224, Test_Loss: 0.5256025195121765\n",
      "Epoch: 25, Train_Loss: 0.19842225313186646, Test_Loss: 0.4759793281555176 *\n",
      "Epoch: 25, Train_Loss: 0.21369217336177826, Test_Loss: 0.3370615839958191 *\n",
      "Epoch: 25, Train_Loss: 0.2699950933456421, Test_Loss: 0.20743390917778015 *\n",
      "Epoch: 25, Train_Loss: 0.24774262309074402, Test_Loss: 0.40208420157432556\n",
      "Epoch: 25, Train_Loss: 0.19519267976284027, Test_Loss: 0.7043302655220032\n",
      "Epoch: 25, Train_Loss: 0.20248304307460785, Test_Loss: 0.6588670015335083 *\n",
      "Epoch: 25, Train_Loss: 0.20779640972614288, Test_Loss: 0.6630106568336487\n",
      "Epoch: 25, Train_Loss: 0.20880119502544403, Test_Loss: 0.5275007486343384 *\n",
      "Epoch: 25, Train_Loss: 0.19826960563659668, Test_Loss: 0.5190631151199341 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train_Loss: 0.21194405853748322, Test_Loss: 0.6385254263877869\n",
      "Epoch: 25, Train_Loss: 0.214908629655838, Test_Loss: 0.5671738386154175 *\n",
      "Epoch: 25, Train_Loss: 0.21074514091014862, Test_Loss: 0.38922789692878723 *\n",
      "Epoch: 25, Train_Loss: 0.1951250582933426, Test_Loss: 0.31910470128059387 *\n",
      "Epoch: 25, Train_Loss: 0.19382750988006592, Test_Loss: 0.2728285491466522 *\n",
      "Epoch: 25, Train_Loss: 0.2419329732656479, Test_Loss: 0.21308033168315887 *\n",
      "Epoch: 25, Train_Loss: 0.2710249423980713, Test_Loss: 0.23707741498947144\n",
      "Epoch: 25, Train_Loss: 0.25952017307281494, Test_Loss: 0.3326869010925293\n",
      "Epoch: 25, Train_Loss: 0.2630763649940491, Test_Loss: 0.24422553181648254 *\n",
      "Epoch: 25, Train_Loss: 0.23407196998596191, Test_Loss: 0.31268277764320374\n",
      "Epoch: 25, Train_Loss: 0.2173072248697281, Test_Loss: 0.252200186252594 *\n",
      "Epoch: 25, Train_Loss: 0.21199092268943787, Test_Loss: 0.22643525898456573 *\n",
      "Epoch: 25, Train_Loss: 0.2340271770954132, Test_Loss: 0.21288366615772247 *\n",
      "Epoch: 25, Train_Loss: 0.2946535348892212, Test_Loss: 0.23442460596561432\n",
      "Model saved at location save_new\\model.ckpt at epoch 25\n",
      "Epoch: 25, Train_Loss: 0.23122715950012207, Test_Loss: 0.4454352557659149\n",
      "Epoch: 25, Train_Loss: 0.24477876722812653, Test_Loss: 0.25341159105300903 *\n",
      "Epoch: 25, Train_Loss: 0.19340157508850098, Test_Loss: 0.5760992169380188\n",
      "Epoch: 25, Train_Loss: 0.20178279280662537, Test_Loss: 0.25545334815979004 *\n",
      "Epoch: 25, Train_Loss: 0.19963470101356506, Test_Loss: 0.22516970336437225 *\n",
      "Epoch: 25, Train_Loss: 0.199171781539917, Test_Loss: 0.2324492186307907\n",
      "Epoch: 25, Train_Loss: 0.20497111976146698, Test_Loss: 0.2101745307445526 *\n",
      "Epoch: 25, Train_Loss: 4.651567459106445, Test_Loss: 0.22149509191513062\n",
      "Epoch: 25, Train_Loss: 0.5442070960998535, Test_Loss: 0.21484921872615814 *\n",
      "Epoch: 25, Train_Loss: 0.19496257603168488, Test_Loss: 0.22897844016551971\n",
      "Epoch: 25, Train_Loss: 0.2070896327495575, Test_Loss: 0.1953825205564499 *\n",
      "Epoch: 25, Train_Loss: 0.1952381730079651, Test_Loss: 0.2832377851009369\n",
      "Epoch: 25, Train_Loss: 0.19354523718357086, Test_Loss: 0.459043949842453\n",
      "Epoch: 25, Train_Loss: 0.19397830963134766, Test_Loss: 0.342162162065506 *\n",
      "Epoch: 25, Train_Loss: 0.1919347494840622, Test_Loss: 0.4412795305252075\n",
      "Epoch: 25, Train_Loss: 0.19246795773506165, Test_Loss: 0.2274826169013977 *\n",
      "Epoch: 25, Train_Loss: 0.19265532493591309, Test_Loss: 0.2262580543756485 *\n",
      "Epoch: 25, Train_Loss: 0.2243412584066391, Test_Loss: 0.22611159086227417 *\n",
      "Epoch: 25, Train_Loss: 0.23495201766490936, Test_Loss: 0.22550512850284576 *\n",
      "Epoch: 25, Train_Loss: 0.2661680281162262, Test_Loss: 0.2257390320301056\n",
      "Epoch: 25, Train_Loss: 0.23483827710151672, Test_Loss: 3.5314128398895264\n",
      "Epoch: 25, Train_Loss: 0.19550907611846924, Test_Loss: 2.5532491207122803 *\n",
      "Epoch: 25, Train_Loss: 0.27187126874923706, Test_Loss: 0.20940253138542175 *\n",
      "Epoch: 25, Train_Loss: 0.35400527715682983, Test_Loss: 0.20700307190418243 *\n",
      "Epoch: 25, Train_Loss: 0.36031824350357056, Test_Loss: 0.19931578636169434 *\n",
      "Epoch: 25, Train_Loss: 0.37401148676872253, Test_Loss: 0.205205038189888\n",
      "Epoch: 25, Train_Loss: 0.19761934876441956, Test_Loss: 0.1949760615825653 *\n",
      "Epoch: 25, Train_Loss: 0.19197113811969757, Test_Loss: 0.21677175164222717\n",
      "Epoch: 25, Train_Loss: 0.19315224885940552, Test_Loss: 0.19813261926174164 *\n",
      "Epoch: 25, Train_Loss: 0.19542546570301056, Test_Loss: 0.19343194365501404 *\n",
      "Epoch: 25, Train_Loss: 0.1975962072610855, Test_Loss: 0.20089401304721832\n",
      "Epoch: 25, Train_Loss: 0.2071417272090912, Test_Loss: 0.19971181452274323 *\n",
      "Epoch: 25, Train_Loss: 0.20446360111236572, Test_Loss: 0.27708327770233154\n",
      "Epoch: 25, Train_Loss: 0.1918177306652069, Test_Loss: 0.20981192588806152 *\n",
      "Epoch: 25, Train_Loss: 0.19694721698760986, Test_Loss: 0.20700109004974365 *\n",
      "Epoch: 25, Train_Loss: 0.21214871108531952, Test_Loss: 0.19455304741859436 *\n",
      "Epoch: 25, Train_Loss: 0.3244989812374115, Test_Loss: 0.19543901085853577\n",
      "Epoch: 25, Train_Loss: 0.3023306429386139, Test_Loss: 0.19753751158714294\n",
      "Epoch: 25, Train_Loss: 0.29063087701797485, Test_Loss: 0.21460650861263275\n",
      "Epoch: 25, Train_Loss: 0.24643754959106445, Test_Loss: 0.2055167853832245 *\n",
      "Epoch: 25, Train_Loss: 0.31247490644454956, Test_Loss: 0.19555215537548065 *\n",
      "Epoch: 25, Train_Loss: 0.28346604108810425, Test_Loss: 0.19287239015102386 *\n",
      "Epoch: 25, Train_Loss: 0.22122688591480255, Test_Loss: 0.19688557088375092\n",
      "Epoch: 25, Train_Loss: 0.26482656598091125, Test_Loss: 0.20836108922958374\n",
      "Epoch: 25, Train_Loss: 0.2250872105360031, Test_Loss: 0.22867999970912933\n",
      "Epoch: 25, Train_Loss: 0.4032849073410034, Test_Loss: 0.19883060455322266 *\n",
      "Epoch: 25, Train_Loss: 0.19864916801452637, Test_Loss: 0.19843776524066925 *\n",
      "Epoch: 25, Train_Loss: 1.6067512035369873, Test_Loss: 0.20554016530513763\n",
      "Epoch: 25, Train_Loss: 1.6192982196807861, Test_Loss: 0.20559121668338776\n",
      "Epoch: 25, Train_Loss: 0.2396422028541565, Test_Loss: 0.19297176599502563 *\n",
      "Epoch: 25, Train_Loss: 0.22238866984844208, Test_Loss: 0.31183913350105286\n",
      "Epoch: 25, Train_Loss: 0.1966637372970581, Test_Loss: 0.23522061109542847 *\n",
      "Epoch: 25, Train_Loss: 0.21033358573913574, Test_Loss: 4.941762447357178\n",
      "Epoch: 25, Train_Loss: 0.20368275046348572, Test_Loss: 0.5826571583747864 *\n",
      "Epoch: 25, Train_Loss: 0.22347816824913025, Test_Loss: 0.19404906034469604 *\n",
      "Epoch: 25, Train_Loss: 0.3154997229576111, Test_Loss: 0.21150408685207367\n",
      "Epoch: 25, Train_Loss: 0.25222188234329224, Test_Loss: 0.20106185972690582 *\n",
      "Epoch: 25, Train_Loss: 0.22319293022155762, Test_Loss: 0.20687219500541687\n",
      "Epoch: 25, Train_Loss: 0.2055153250694275, Test_Loss: 0.19931849837303162 *\n",
      "Epoch: 25, Train_Loss: 0.2083897888660431, Test_Loss: 0.27536627650260925\n",
      "Epoch: 25, Train_Loss: 0.19651220738887787, Test_Loss: 0.23796965181827545 *\n",
      "Epoch: 25, Train_Loss: 0.20565767586231232, Test_Loss: 0.19825391471385956 *\n",
      "Epoch: 25, Train_Loss: 0.23114189505577087, Test_Loss: 0.2118379920721054\n",
      "Epoch: 25, Train_Loss: 0.21284079551696777, Test_Loss: 0.21595513820648193\n",
      "Epoch: 25, Train_Loss: 0.20016013085842133, Test_Loss: 0.20450864732265472 *\n",
      "Epoch: 25, Train_Loss: 0.1938629448413849, Test_Loss: 0.2047833353281021\n",
      "Epoch: 25, Train_Loss: 0.2054942548274994, Test_Loss: 0.22052273154258728\n",
      "Epoch: 25, Train_Loss: 0.2096693217754364, Test_Loss: 0.24203644692897797\n",
      "Epoch: 25, Train_Loss: 0.2057541310787201, Test_Loss: 0.2523375451564789\n",
      "Epoch: 25, Train_Loss: 0.19250330328941345, Test_Loss: 0.2240605652332306 *\n",
      "Epoch: 25, Train_Loss: 0.19217315316200256, Test_Loss: 0.28793299198150635\n",
      "Epoch: 25, Train_Loss: 0.19194211065769196, Test_Loss: 0.22626245021820068 *\n",
      "Epoch: 25, Train_Loss: 0.2022661417722702, Test_Loss: 0.1986873745918274 *\n",
      "Epoch: 25, Train_Loss: 0.19282245635986328, Test_Loss: 0.1990664303302765\n",
      "Epoch: 25, Train_Loss: 0.2027917355298996, Test_Loss: 0.19833216071128845 *\n",
      "Epoch: 25, Train_Loss: 0.2020111382007599, Test_Loss: 0.20024922490119934\n",
      "Epoch: 25, Train_Loss: 0.1930239051580429, Test_Loss: 0.1940934807062149 *\n",
      "Epoch: 25, Train_Loss: 0.19205120205879211, Test_Loss: 0.20726804435253143\n",
      "Epoch: 25, Train_Loss: 0.19735567271709442, Test_Loss: 0.19868558645248413 *\n",
      "Epoch: 25, Train_Loss: 0.20626278221607208, Test_Loss: 0.19781726598739624 *\n",
      "Epoch: 25, Train_Loss: 0.20366643369197845, Test_Loss: 0.20378075540065765\n",
      "Epoch: 25, Train_Loss: 0.20483696460723877, Test_Loss: 0.2023240476846695 *\n",
      "Epoch: 25, Train_Loss: 0.22078679502010345, Test_Loss: 0.22973038256168365\n",
      "Epoch: 25, Train_Loss: 0.20990300178527832, Test_Loss: 0.2529435157775879\n",
      "Epoch: 25, Train_Loss: 0.22830358147621155, Test_Loss: 0.4536644518375397\n",
      "Epoch: 25, Train_Loss: 0.21096599102020264, Test_Loss: 0.46357929706573486\n",
      "Epoch: 25, Train_Loss: 0.216281458735466, Test_Loss: 0.3749505877494812 *\n",
      "Epoch: 25, Train_Loss: 0.2423677295446396, Test_Loss: 0.2567012310028076 *\n",
      "Epoch: 25, Train_Loss: 0.20381511747837067, Test_Loss: 0.20634210109710693 *\n",
      "Epoch: 25, Train_Loss: 0.1947174221277237, Test_Loss: 0.19849297404289246 *\n",
      "Epoch: 25, Train_Loss: 0.1964692771434784, Test_Loss: 0.22452184557914734\n",
      "Epoch: 25, Train_Loss: 0.20149733126163483, Test_Loss: 0.444766640663147\n",
      "Epoch: 25, Train_Loss: 0.21904543042182922, Test_Loss: 0.2564227879047394 *\n",
      "Epoch: 25, Train_Loss: 0.25855666399002075, Test_Loss: 0.40182673931121826\n",
      "Epoch: 25, Train_Loss: 0.2264852076768875, Test_Loss: 0.28135696053504944 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train_Loss: 0.19169455766677856, Test_Loss: 0.19750650227069855 *\n",
      "Epoch: 25, Train_Loss: 0.2391015738248825, Test_Loss: 0.20103882253170013\n",
      "Epoch: 25, Train_Loss: 0.21224182844161987, Test_Loss: 0.1948835551738739 *\n",
      "Epoch: 25, Train_Loss: 0.19333288073539734, Test_Loss: 0.2137438803911209\n",
      "Epoch: 25, Train_Loss: 0.2082938551902771, Test_Loss: 0.2046135663986206 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 25\n",
      "Epoch: 25, Train_Loss: 0.22636981308460236, Test_Loss: 0.21733969449996948\n",
      "Epoch: 25, Train_Loss: 0.2775440216064453, Test_Loss: 0.19477884471416473 *\n",
      "Epoch: 25, Train_Loss: 0.23285359144210815, Test_Loss: 0.31354761123657227\n",
      "Epoch: 25, Train_Loss: 0.21802887320518494, Test_Loss: 0.536708652973175\n",
      "Epoch: 25, Train_Loss: 0.20739687979221344, Test_Loss: 0.2584664225578308 *\n",
      "Epoch: 25, Train_Loss: 0.20127128064632416, Test_Loss: 0.4210072159767151\n",
      "Epoch: 25, Train_Loss: 0.20545507967472076, Test_Loss: 0.22223863005638123 *\n",
      "Epoch: 25, Train_Loss: 0.19212590157985687, Test_Loss: 0.2223878800868988\n",
      "Epoch: 25, Train_Loss: 0.19811207056045532, Test_Loss: 0.2221274971961975 *\n",
      "Epoch: 25, Train_Loss: 0.2034936398267746, Test_Loss: 0.218350350856781 *\n",
      "Epoch: 25, Train_Loss: 0.20842653512954712, Test_Loss: 0.21897152066230774\n",
      "Epoch: 25, Train_Loss: 0.2891645133495331, Test_Loss: 5.045947551727295\n",
      "Epoch: 25, Train_Loss: 0.1914956271648407, Test_Loss: 1.2869774103164673 *\n",
      "Epoch: 25, Train_Loss: 0.24592448770999908, Test_Loss: 0.20535396039485931 *\n",
      "Epoch: 25, Train_Loss: 0.20226140320301056, Test_Loss: 0.20499515533447266 *\n",
      "Epoch: 25, Train_Loss: 0.21740205585956573, Test_Loss: 0.19933412969112396 *\n",
      "Epoch: 25, Train_Loss: 0.2358931005001068, Test_Loss: 0.1985781043767929 *\n",
      "Epoch: 25, Train_Loss: 0.4654543995857239, Test_Loss: 0.20828311145305634\n",
      "Epoch: 25, Train_Loss: 0.19995304942131042, Test_Loss: 0.2171204537153244\n",
      "Epoch: 25, Train_Loss: 0.2133220136165619, Test_Loss: 0.2020154744386673 *\n",
      "Epoch: 25, Train_Loss: 0.19092078506946564, Test_Loss: 0.19285252690315247 *\n",
      "Epoch: 25, Train_Loss: 0.1906411498785019, Test_Loss: 0.21802549064159393\n",
      "Epoch: 25, Train_Loss: 0.19267183542251587, Test_Loss: 0.25407373905181885\n",
      "Epoch: 25, Train_Loss: 0.19259099662303925, Test_Loss: 0.29659655690193176\n",
      "Epoch: 25, Train_Loss: 0.19561071693897247, Test_Loss: 0.21065986156463623 *\n",
      "Epoch: 25, Train_Loss: 0.19555996358394623, Test_Loss: 0.2120581418275833\n",
      "Epoch: 25, Train_Loss: 0.2049637734889984, Test_Loss: 0.19334504008293152 *\n",
      "Epoch: 25, Train_Loss: 0.19562610983848572, Test_Loss: 0.19362318515777588\n",
      "Epoch: 25, Train_Loss: 0.19613361358642578, Test_Loss: 0.1955219954252243\n",
      "Epoch: 25, Train_Loss: 0.20187361538410187, Test_Loss: 0.21908371150493622\n",
      "Epoch: 25, Train_Loss: 0.19240128993988037, Test_Loss: 0.19872578978538513 *\n",
      "Epoch: 25, Train_Loss: 0.1899186223745346, Test_Loss: 0.1910817176103592 *\n",
      "Epoch: 25, Train_Loss: 0.20707204937934875, Test_Loss: 0.19123217463493347\n",
      "Epoch: 25, Train_Loss: 0.1987898349761963, Test_Loss: 0.19216524064540863\n",
      "Epoch: 25, Train_Loss: 0.21219301223754883, Test_Loss: 0.19697950780391693\n",
      "Epoch: 25, Train_Loss: 0.1902821809053421, Test_Loss: 0.20905041694641113\n",
      "Epoch: 25, Train_Loss: 0.2087569385766983, Test_Loss: 0.19109497964382172 *\n",
      "Epoch: 25, Train_Loss: 0.2148749679327011, Test_Loss: 0.19351504743099213\n",
      "Epoch: 25, Train_Loss: 0.22554056346416473, Test_Loss: 0.20047429203987122\n",
      "Epoch: 25, Train_Loss: 0.19145768880844116, Test_Loss: 0.1951676309108734 *\n",
      "Epoch: 25, Train_Loss: 0.2096426784992218, Test_Loss: 0.19077564775943756 *\n",
      "Epoch: 25, Train_Loss: 0.1910930573940277, Test_Loss: 0.29773080348968506\n",
      "Epoch: 25, Train_Loss: 0.2091771960258484, Test_Loss: 0.3806804418563843\n",
      "Epoch: 25, Train_Loss: 0.19424490630626678, Test_Loss: 5.545801162719727\n",
      "Epoch: 25, Train_Loss: 0.20670536160469055, Test_Loss: 0.22444763779640198 *\n",
      "Epoch: 25, Train_Loss: 0.4208994507789612, Test_Loss: 0.19063042104244232 *\n",
      "Epoch: 25, Train_Loss: 4.016740322113037, Test_Loss: 0.20966562628746033\n",
      "Epoch: 25, Train_Loss: 1.7922464609146118, Test_Loss: 0.20142927765846252 *\n",
      "Epoch: 25, Train_Loss: 0.20533695816993713, Test_Loss: 0.20557965338230133\n",
      "Epoch: 25, Train_Loss: 0.19241797924041748, Test_Loss: 0.20153993368148804 *\n",
      "Epoch: 25, Train_Loss: 0.2561435401439667, Test_Loss: 0.32552945613861084\n",
      "Epoch: 25, Train_Loss: 0.2673628032207489, Test_Loss: 0.23316024243831635 *\n",
      "Epoch: 25, Train_Loss: 0.21189185976982117, Test_Loss: 0.1910582333803177 *\n",
      "Epoch: 25, Train_Loss: 0.1901106834411621, Test_Loss: 0.22697219252586365\n",
      "Epoch: 25, Train_Loss: 0.24142731726169586, Test_Loss: 0.20192447304725647 *\n",
      "Epoch: 25, Train_Loss: 0.22383157908916473, Test_Loss: 0.19739899039268494 *\n",
      "Epoch: 25, Train_Loss: 0.1972428262233734, Test_Loss: 0.21998535096645355\n",
      "Epoch: 25, Train_Loss: 0.3685082793235779, Test_Loss: 0.21656765043735504 *\n",
      "Epoch: 25, Train_Loss: 0.47859251499176025, Test_Loss: 0.2630620002746582\n",
      "Epoch: 25, Train_Loss: 0.8254532814025879, Test_Loss: 0.2758941650390625\n",
      "Epoch: 25, Train_Loss: 0.26754412055015564, Test_Loss: 0.2323218286037445 *\n",
      "Epoch: 25, Train_Loss: 0.2955249547958374, Test_Loss: 0.24291783571243286\n",
      "Epoch: 25, Train_Loss: 1.5297930240631104, Test_Loss: 0.19476418197155 *\n",
      "Epoch: 25, Train_Loss: 0.8029114603996277, Test_Loss: 0.20231656730175018\n",
      "Epoch: 25, Train_Loss: 0.19577299058437347, Test_Loss: 0.194830521941185 *\n",
      "Epoch: 25, Train_Loss: 0.1959219127893448, Test_Loss: 0.19864363968372345\n",
      "Epoch: 25, Train_Loss: 0.611151397228241, Test_Loss: 0.1957760900259018 *\n",
      "Epoch: 25, Train_Loss: 0.45508575439453125, Test_Loss: 0.19603662192821503\n",
      "Epoch: 25, Train_Loss: 0.8387669324874878, Test_Loss: 0.25041982531547546\n",
      "Epoch: 25, Train_Loss: 0.20340725779533386, Test_Loss: 0.2118363231420517 *\n",
      "Epoch: 25, Train_Loss: 0.22370439767837524, Test_Loss: 0.20209361612796783 *\n",
      "Epoch: 25, Train_Loss: 0.33209264278411865, Test_Loss: 0.2366933524608612\n",
      "Epoch: 25, Train_Loss: 0.42585062980651855, Test_Loss: 0.19810421764850616 *\n",
      "Epoch: 25, Train_Loss: 0.20447638630867004, Test_Loss: 0.20699989795684814\n",
      "Epoch: 25, Train_Loss: 0.23421676456928253, Test_Loss: 0.23548313975334167\n",
      "Epoch: 25, Train_Loss: 0.22905872762203217, Test_Loss: 0.46313121914863586\n",
      "Epoch: 25, Train_Loss: 0.2362799346446991, Test_Loss: 0.34064310789108276 *\n",
      "Epoch: 25, Train_Loss: 0.3365704417228699, Test_Loss: 0.30084872245788574 *\n",
      "Epoch: 25, Train_Loss: 0.2630177140235901, Test_Loss: 0.24575698375701904 *\n",
      "Epoch: 25, Train_Loss: 0.24083586037158966, Test_Loss: 0.21133700013160706 *\n",
      "Epoch: 25, Train_Loss: 0.2527270019054413, Test_Loss: 0.19315685331821442 *\n",
      "Epoch: 25, Train_Loss: 0.2636074125766754, Test_Loss: 0.2092394232749939\n",
      "Epoch: 25, Train_Loss: 0.25479525327682495, Test_Loss: 0.35155799984931946\n",
      "Epoch: 25, Train_Loss: 0.28691843152046204, Test_Loss: 0.2619624435901642 *\n",
      "Epoch: 25, Train_Loss: 0.38668471574783325, Test_Loss: 0.2876307964324951\n",
      "Epoch: 25, Train_Loss: 0.22141975164413452, Test_Loss: 0.26191821694374084 *\n",
      "Epoch: 25, Train_Loss: 0.22339284420013428, Test_Loss: 0.21387258172035217 *\n",
      "Epoch: 25, Train_Loss: 0.24046653509140015, Test_Loss: 0.2057831883430481 *\n",
      "Epoch: 25, Train_Loss: 0.22559988498687744, Test_Loss: 0.20764026045799255\n",
      "Epoch: 25, Train_Loss: 0.1950257122516632, Test_Loss: 0.23359698057174683\n",
      "Epoch: 25, Train_Loss: 0.19009524583816528, Test_Loss: 0.19883179664611816 *\n",
      "Epoch: 25, Train_Loss: 0.18956232070922852, Test_Loss: 0.21133920550346375\n",
      "Epoch: 25, Train_Loss: 0.19520825147628784, Test_Loss: 0.1999337375164032 *\n",
      "Epoch: 25, Train_Loss: 0.19435621798038483, Test_Loss: 0.33888760209083557\n",
      "Epoch: 25, Train_Loss: 0.21359632909297943, Test_Loss: 0.5722015500068665\n",
      "Epoch: 25, Train_Loss: 0.2042965292930603, Test_Loss: 0.30815863609313965 *\n",
      "Epoch: 25, Train_Loss: 0.20963628590106964, Test_Loss: 0.4747375547885895\n",
      "Epoch: 25, Train_Loss: 0.3207147717475891, Test_Loss: 0.2822580337524414 *\n",
      "Epoch: 25, Train_Loss: 0.4668799638748169, Test_Loss: 0.28286483883857727\n",
      "Epoch: 25, Train_Loss: 0.21257899701595306, Test_Loss: 0.2817177176475525 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 25\n",
      "Epoch: 25, Train_Loss: 0.22705750167369843, Test_Loss: 0.27379530668258667 *\n",
      "Epoch: 25, Train_Loss: 0.26132261753082275, Test_Loss: 0.29452693462371826\n",
      "Epoch: 25, Train_Loss: 0.24787363409996033, Test_Loss: 6.03012228012085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train_Loss: 0.2958502769470215, Test_Loss: 0.4442204236984253 *\n",
      "Epoch: 25, Train_Loss: 0.27977922558784485, Test_Loss: 0.24800847470760345 *\n",
      "Epoch: 25, Train_Loss: 0.42228594422340393, Test_Loss: 0.2343260794878006 *\n",
      "Epoch: 25, Train_Loss: 0.2459641546010971, Test_Loss: 0.21260367333889008 *\n",
      "Epoch: 25, Train_Loss: 0.33820512890815735, Test_Loss: 0.20015370845794678 *\n",
      "Epoch: 25, Train_Loss: 0.20908018946647644, Test_Loss: 0.2607825994491577\n",
      "Epoch: 25, Train_Loss: 0.22634142637252808, Test_Loss: 0.2665959596633911\n",
      "Epoch: 25, Train_Loss: 0.45073235034942627, Test_Loss: 0.23316964507102966 *\n",
      "Epoch: 25, Train_Loss: 0.6679558157920837, Test_Loss: 0.20154337584972382 *\n",
      "Epoch: 25, Train_Loss: 0.47478264570236206, Test_Loss: 0.25122344493865967\n",
      "Epoch: 25, Train_Loss: 0.23383337259292603, Test_Loss: 0.4060380458831787\n",
      "Epoch: 25, Train_Loss: 0.23452812433242798, Test_Loss: 0.2831228971481323 *\n",
      "Epoch: 25, Train_Loss: 0.19604536890983582, Test_Loss: 0.21068638563156128 *\n",
      "Epoch: 25, Train_Loss: 0.48223501443862915, Test_Loss: 0.2316499650478363\n",
      "Epoch: 25, Train_Loss: 0.3352661728858948, Test_Loss: 0.19975432753562927 *\n",
      "Epoch: 25, Train_Loss: 0.20040537416934967, Test_Loss: 0.23281075060367584\n",
      "Epoch: 25, Train_Loss: 0.35614657402038574, Test_Loss: 0.2633281350135803\n",
      "Epoch: 25, Train_Loss: 0.2136467546224594, Test_Loss: 0.2869492173194885\n",
      "Epoch: 25, Train_Loss: 0.20478203892707825, Test_Loss: 0.21335303783416748 *\n",
      "Epoch: 25, Train_Loss: 0.22640031576156616, Test_Loss: 0.21837374567985535\n",
      "Epoch: 25, Train_Loss: 0.3044887185096741, Test_Loss: 0.1982559710741043 *\n",
      "Epoch: 25, Train_Loss: 0.2670767903327942, Test_Loss: 0.20862159132957458\n",
      "Epoch: 25, Train_Loss: 0.2942838668823242, Test_Loss: 0.21381472051143646\n",
      "Epoch: 25, Train_Loss: 0.2082396000623703, Test_Loss: 0.23020786046981812\n",
      "Epoch: 25, Train_Loss: 0.27951639890670776, Test_Loss: 0.19151361286640167 *\n",
      "Epoch: 25, Train_Loss: 0.22191676497459412, Test_Loss: 0.20170123875141144\n",
      "Epoch: 25, Train_Loss: 0.210603728890419, Test_Loss: 0.29352059960365295\n",
      "Epoch: 25, Train_Loss: 0.19637015461921692, Test_Loss: 0.20778252184391022 *\n",
      "Epoch: 25, Train_Loss: 0.2455248385667801, Test_Loss: 0.1917484998703003 *\n",
      "Epoch: 25, Train_Loss: 0.3591807186603546, Test_Loss: 0.3261191248893738\n",
      "Epoch: 25, Train_Loss: 0.45206207036972046, Test_Loss: 1.3077545166015625\n",
      "Epoch: 25, Train_Loss: 0.42277419567108154, Test_Loss: 4.074798107147217\n",
      "Epoch: 25, Train_Loss: 0.6324008107185364, Test_Loss: 0.2076789140701294 *\n",
      "Epoch: 25, Train_Loss: 0.4620881676673889, Test_Loss: 0.19378139078617096 *\n",
      "Epoch: 25, Train_Loss: 0.3748847246170044, Test_Loss: 0.22848665714263916\n",
      "Epoch: 25, Train_Loss: 0.26744920015335083, Test_Loss: 0.2054734081029892 *\n",
      "Epoch: 25, Train_Loss: 0.2042173445224762, Test_Loss: 0.20066651701927185 *\n",
      "Epoch: 25, Train_Loss: 0.19751040637493134, Test_Loss: 0.24446530640125275\n",
      "Epoch: 25, Train_Loss: 0.20420098304748535, Test_Loss: 0.26901090145111084\n",
      "Epoch: 25, Train_Loss: 0.35226893424987793, Test_Loss: 0.20675839483737946 *\n",
      "Epoch: 25, Train_Loss: 0.4555795192718506, Test_Loss: 0.20049342513084412 *\n",
      "Epoch: 25, Train_Loss: 0.431796669960022, Test_Loss: 0.21802589297294617\n",
      "Epoch: 25, Train_Loss: 0.9823437929153442, Test_Loss: 0.2560829222202301\n",
      "Epoch: 25, Train_Loss: 1.0619930028915405, Test_Loss: 0.2078550159931183 *\n",
      "Epoch: 25, Train_Loss: 0.322194367647171, Test_Loss: 0.24784795939922333\n",
      "Epoch: 25, Train_Loss: 0.26493164896965027, Test_Loss: 0.24072590470314026 *\n",
      "Epoch: 25, Train_Loss: 0.19493591785430908, Test_Loss: 0.309261679649353\n",
      "Epoch: 25, Train_Loss: 0.30988699197769165, Test_Loss: 0.20186571776866913 *\n",
      "Epoch: 25, Train_Loss: 0.532947301864624, Test_Loss: 0.22054632008075714\n",
      "Epoch: 25, Train_Loss: 0.7120181322097778, Test_Loss: 0.3328494429588318\n",
      "Epoch: 25, Train_Loss: 0.22350139915943146, Test_Loss: 0.21945010125637054 *\n",
      "Epoch: 25, Train_Loss: 0.21331343054771423, Test_Loss: 0.35203126072883606\n",
      "Epoch: 25, Train_Loss: 0.2649158239364624, Test_Loss: 0.28356605768203735 *\n",
      "Epoch: 25, Train_Loss: 0.4637792110443115, Test_Loss: 0.2788470685482025 *\n",
      "Epoch: 25, Train_Loss: 0.32192403078079224, Test_Loss: 0.2732447385787964 *\n",
      "Epoch: 25, Train_Loss: 0.29687952995300293, Test_Loss: 0.24720706045627594 *\n",
      "Epoch: 25, Train_Loss: 0.31980836391448975, Test_Loss: 0.3465297222137451\n",
      "Epoch: 25, Train_Loss: 0.26188552379608154, Test_Loss: 0.2703702449798584 *\n",
      "Epoch: 25, Train_Loss: 0.1954745352268219, Test_Loss: 0.22757595777511597 *\n",
      "Epoch: 25, Train_Loss: 0.21505126357078552, Test_Loss: 0.2769712209701538\n",
      "Epoch: 25, Train_Loss: 0.1932104378938675, Test_Loss: 0.1987348049879074 *\n",
      "Epoch: 25, Train_Loss: 0.25337204337120056, Test_Loss: 0.1923496127128601 *\n",
      "Epoch: 25, Train_Loss: 0.25256767868995667, Test_Loss: 0.21309854090213776\n",
      "Epoch: 25, Train_Loss: 0.34840577840805054, Test_Loss: 0.3645787239074707\n",
      "Epoch: 25, Train_Loss: 14.90617561340332, Test_Loss: 0.27436092495918274 *\n",
      "Epoch: 25, Train_Loss: 0.3327873647212982, Test_Loss: 0.22450746595859528 *\n",
      "Epoch: 25, Train_Loss: 0.9210865497589111, Test_Loss: 0.24014902114868164\n",
      "Epoch: 25, Train_Loss: 0.9535930156707764, Test_Loss: 0.2276824712753296 *\n",
      "Epoch: 25, Train_Loss: 0.26069676876068115, Test_Loss: 0.19330456852912903 *\n",
      "Epoch: 25, Train_Loss: 0.5243799686431885, Test_Loss: 0.2257859706878662\n",
      "Epoch: 25, Train_Loss: 2.8043408393859863, Test_Loss: 0.27785542607307434\n",
      "Epoch: 25, Train_Loss: 3.5199248790740967, Test_Loss: 0.41204142570495605\n",
      "Epoch: 25, Train_Loss: 0.2735331356525421, Test_Loss: 0.27799585461616516 *\n",
      "Epoch: 25, Train_Loss: 0.6008318662643433, Test_Loss: 0.29706043004989624\n",
      "Epoch: 25, Train_Loss: 4.51259183883667, Test_Loss: 0.23390185832977295 *\n",
      "Epoch: 25, Train_Loss: 0.43496042490005493, Test_Loss: 0.2123519331216812 *\n",
      "Epoch: 25, Train_Loss: 0.20350857079029083, Test_Loss: 0.25921112298965454\n",
      "Epoch: 25, Train_Loss: 0.19353057444095612, Test_Loss: 0.23655451834201813 *\n",
      "Epoch: 25, Train_Loss: 0.25750619173049927, Test_Loss: 0.21661587059497833 *\n",
      "Epoch: 25, Train_Loss: 0.2225460410118103, Test_Loss: 0.19999973475933075 *\n",
      "Epoch: 25, Train_Loss: 0.19171026349067688, Test_Loss: 0.24137696623802185\n",
      "Epoch: 25, Train_Loss: 0.18925583362579346, Test_Loss: 0.32131510972976685\n",
      "Epoch: 25, Train_Loss: 0.1874285489320755, Test_Loss: 0.5721760988235474\n",
      "Epoch: 25, Train_Loss: 0.18875496089458466, Test_Loss: 0.3819177746772766 *\n",
      "Epoch: 25, Train_Loss: 0.21694667637348175, Test_Loss: 0.2728617191314697 *\n",
      "Epoch: 25, Train_Loss: 0.19233253598213196, Test_Loss: 0.2295207530260086 *\n",
      "Epoch: 25, Train_Loss: 0.248296320438385, Test_Loss: 0.24241003394126892\n",
      "Epoch: 25, Train_Loss: 0.33191952109336853, Test_Loss: 0.23666244745254517 *\n",
      "Epoch: 25, Train_Loss: 0.28575146198272705, Test_Loss: 0.2845720946788788\n",
      "Epoch: 25, Train_Loss: 0.21287932991981506, Test_Loss: 0.6480291485786438\n",
      "Epoch: 25, Train_Loss: 0.20020703971385956, Test_Loss: 8.954785346984863\n",
      "Epoch: 25, Train_Loss: 0.2001485675573349, Test_Loss: 0.3653275966644287 *\n",
      "Epoch: 25, Train_Loss: 0.21311475336551666, Test_Loss: 0.48850661516189575\n",
      "Epoch: 25, Train_Loss: 0.21227611601352692, Test_Loss: 0.46520254015922546 *\n",
      "Epoch: 25, Train_Loss: 0.19458997249603271, Test_Loss: 0.3509349226951599 *\n",
      "Epoch: 25, Train_Loss: 0.1935887485742569, Test_Loss: 0.2531997859477997 *\n",
      "Epoch: 25, Train_Loss: 0.1954612135887146, Test_Loss: 0.6124011874198914\n",
      "Model saved at location save_new\\model.ckpt at epoch 25\n",
      "Epoch: 25, Train_Loss: 0.1945287585258484, Test_Loss: 0.5164787769317627 *\n",
      "Epoch: 25, Train_Loss: 0.19230525195598602, Test_Loss: 0.21310433745384216 *\n",
      "Epoch: 25, Train_Loss: 0.19779296219348907, Test_Loss: 0.4157032370567322\n",
      "Epoch: 25, Train_Loss: 0.21774980425834656, Test_Loss: 0.3111318349838257 *\n",
      "Epoch: 25, Train_Loss: 0.22176873683929443, Test_Loss: 1.0021142959594727\n",
      "Epoch: 25, Train_Loss: 0.2597818076610565, Test_Loss: 0.4475458860397339 *\n",
      "Epoch: 25, Train_Loss: 0.21778486669063568, Test_Loss: 0.34837037324905396 *\n",
      "Epoch: 25, Train_Loss: 0.3124844431877136, Test_Loss: 0.30271217226982117 *\n",
      "Epoch: 25, Train_Loss: 3.6863856315612793, Test_Loss: 0.2074037790298462 *\n",
      "Epoch: 25, Train_Loss: 3.5416512489318848, Test_Loss: 0.21019893884658813\n",
      "Epoch: 25, Train_Loss: 0.21148744225502014, Test_Loss: 0.20684821903705597 *\n",
      "Epoch: 25, Train_Loss: 0.2315065860748291, Test_Loss: 0.5557304620742798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train_Loss: 0.2412731647491455, Test_Loss: 0.20550653338432312 *\n",
      "Epoch: 25, Train_Loss: 0.23514492809772491, Test_Loss: 0.6052236557006836\n",
      "Epoch: 25, Train_Loss: 0.24206367135047913, Test_Loss: 0.25402623414993286 *\n",
      "Epoch: 25, Train_Loss: 0.22588478028774261, Test_Loss: 0.2861751914024353\n",
      "Epoch: 25, Train_Loss: 0.22067642211914062, Test_Loss: 0.397926926612854\n",
      "Epoch: 25, Train_Loss: 0.31852829456329346, Test_Loss: 0.2910170257091522 *\n",
      "Epoch: 25, Train_Loss: 0.30354294180870056, Test_Loss: 0.19648486375808716 *\n",
      "Epoch: 25, Train_Loss: 0.20731958746910095, Test_Loss: 0.27538424730300903\n",
      "Epoch: 25, Train_Loss: 0.20498186349868774, Test_Loss: 0.26294654607772827 *\n",
      "Epoch: 25, Train_Loss: 0.19917865097522736, Test_Loss: 0.2641928195953369\n",
      "Epoch: 25, Train_Loss: 0.20903216302394867, Test_Loss: 0.21490389108657837 *\n",
      "Epoch: 25, Train_Loss: 0.19092367589473724, Test_Loss: 0.5202051997184753\n",
      "Epoch: 25, Train_Loss: 0.2075776755809784, Test_Loss: 3.098536252975464\n",
      "Epoch: 25, Train_Loss: 0.22475385665893555, Test_Loss: 3.678537130355835\n",
      "Epoch: 25, Train_Loss: 0.20328733325004578, Test_Loss: 0.21804508566856384 *\n",
      "Epoch: 25, Train_Loss: 0.21282105147838593, Test_Loss: 0.20707523822784424 *\n",
      "Epoch: 25, Train_Loss: 0.24458235502243042, Test_Loss: 0.25073808431625366\n",
      "Epoch: 25, Train_Loss: 0.21645283699035645, Test_Loss: 0.4625459909439087\n",
      "Epoch: 25, Train_Loss: 0.18860512971878052, Test_Loss: 0.24841512739658356 *\n",
      "Epoch: 25, Train_Loss: 0.1876019686460495, Test_Loss: 0.3166539967060089\n",
      "Epoch: 25, Train_Loss: 0.21469998359680176, Test_Loss: 0.2994723320007324 *\n",
      "Epoch: 25, Train_Loss: 2.666891098022461, Test_Loss: 0.19461214542388916 *\n",
      "Epoch: 25, Train_Loss: 2.124889850616455, Test_Loss: 0.20806176960468292\n",
      "Epoch: 25, Train_Loss: 0.19830116629600525, Test_Loss: 0.22494348883628845\n",
      "Epoch: 25, Train_Loss: 0.301561176776886, Test_Loss: 0.23493407666683197\n",
      "Epoch: 25, Train_Loss: 0.20271004736423492, Test_Loss: 0.2020399123430252 *\n",
      "Epoch: 25, Train_Loss: 0.1888570934534073, Test_Loss: 0.4397210478782654\n",
      "Epoch: 25, Train_Loss: 0.22134624421596527, Test_Loss: 0.5060988664627075\n",
      "Epoch: 25, Train_Loss: 0.19388149678707123, Test_Loss: 0.4129944443702698 *\n",
      "Epoch: 25, Train_Loss: 0.21357019245624542, Test_Loss: 0.23424077033996582 *\n",
      "Epoch: 25, Train_Loss: 0.22514382004737854, Test_Loss: 0.20856912434101105 *\n",
      "Epoch: 25, Train_Loss: 0.24987272918224335, Test_Loss: 0.356184184551239\n",
      "Epoch: 25, Train_Loss: 0.18992047011852264, Test_Loss: 0.49199825525283813\n",
      "Epoch: 25, Train_Loss: 0.19599182903766632, Test_Loss: 0.6933467388153076\n",
      "Epoch: 25, Train_Loss: 0.2029154896736145, Test_Loss: 0.6141548156738281 *\n",
      "Epoch: 25, Train_Loss: 0.19676566123962402, Test_Loss: 0.43727996945381165 *\n",
      "Epoch: 25, Train_Loss: 0.18847344815731049, Test_Loss: 0.4963397681713104\n",
      "Epoch: 25, Train_Loss: 0.19699929654598236, Test_Loss: 0.4611116051673889 *\n",
      "Epoch: 25, Train_Loss: 0.2022007405757904, Test_Loss: 0.5866518616676331\n",
      "Epoch: 25, Train_Loss: 0.2035929262638092, Test_Loss: 0.4603564143180847 *\n",
      "Epoch: 25, Train_Loss: 0.18816204369068146, Test_Loss: 0.2754545211791992 *\n",
      "Epoch: 25, Train_Loss: 0.18860352039337158, Test_Loss: 0.26844531297683716 *\n",
      "Epoch: 26, Train_Loss: 0.2450842261314392, Test_Loss: 0.19440621137619019 *\n",
      "Epoch: 26, Train_Loss: 0.23275956511497498, Test_Loss: 0.20297686755657196\n",
      "Epoch: 26, Train_Loss: 0.24892303347587585, Test_Loss: 0.2341236174106598\n",
      "Epoch: 26, Train_Loss: 0.21906721591949463, Test_Loss: 0.28446924686431885\n",
      "Epoch: 26, Train_Loss: 0.2898450493812561, Test_Loss: 0.28863224387168884\n",
      "Epoch: 26, Train_Loss: 0.22419816255569458, Test_Loss: 0.23391464352607727 *\n",
      "Epoch: 26, Train_Loss: 0.19961203634738922, Test_Loss: 0.24068212509155273\n",
      "Epoch: 26, Train_Loss: 0.2331351339817047, Test_Loss: 0.2301386445760727 *\n",
      "Epoch: 26, Train_Loss: 0.2496776282787323, Test_Loss: 0.2095150202512741 *\n",
      "Epoch: 26, Train_Loss: 0.2399856448173523, Test_Loss: 0.34312838315963745\n",
      "Epoch: 26, Train_Loss: 0.21783950924873352, Test_Loss: 0.3337383270263672 *\n",
      "Epoch: 26, Train_Loss: 0.18811562657356262, Test_Loss: 0.5347938537597656\n",
      "Epoch: 26, Train_Loss: 0.1924259215593338, Test_Loss: 0.25444549322128296 *\n",
      "Epoch: 26, Train_Loss: 0.1916411817073822, Test_Loss: 0.23952171206474304 *\n",
      "Epoch: 26, Train_Loss: 0.19268597662448883, Test_Loss: 0.2213853895664215 *\n",
      "Epoch: 26, Train_Loss: 0.18966439366340637, Test_Loss: 0.19511665403842926 *\n",
      "Epoch: 26, Train_Loss: 3.777425765991211, Test_Loss: 0.21348732709884644\n",
      "Epoch: 26, Train_Loss: 1.3107666969299316, Test_Loss: 0.196416974067688 *\n",
      "Epoch: 26, Train_Loss: 0.1876436173915863, Test_Loss: 0.2144664227962494\n",
      "Epoch: 26, Train_Loss: 0.19650277495384216, Test_Loss: 0.19200344383716583 *\n",
      "Epoch: 26, Train_Loss: 0.19078437983989716, Test_Loss: 0.2352386713027954\n",
      "Epoch: 26, Train_Loss: 0.18935871124267578, Test_Loss: 0.4029051661491394\n",
      "Epoch: 26, Train_Loss: 0.18942807614803314, Test_Loss: 0.4032666087150574\n",
      "Epoch: 26, Train_Loss: 0.18860340118408203, Test_Loss: 0.47728151082992554\n",
      "Epoch: 26, Train_Loss: 0.1869802325963974, Test_Loss: 0.24826061725616455 *\n",
      "Epoch: 26, Train_Loss: 0.1892799586057663, Test_Loss: 0.23681659996509552 *\n",
      "Epoch: 26, Train_Loss: 0.2047143578529358, Test_Loss: 0.2366820126771927 *\n",
      "Epoch: 26, Train_Loss: 0.22155296802520752, Test_Loss: 0.238190695643425\n",
      "Epoch: 26, Train_Loss: 0.24079865217208862, Test_Loss: 0.24479037523269653\n",
      "Epoch: 26, Train_Loss: 0.2221139818429947, Test_Loss: 1.28493070602417\n",
      "Epoch: 26, Train_Loss: 0.20176614820957184, Test_Loss: 5.067321300506592\n",
      "Epoch: 26, Train_Loss: 0.21403302252292633, Test_Loss: 0.22733190655708313 *\n",
      "Epoch: 26, Train_Loss: 0.3249460458755493, Test_Loss: 0.22873392701148987\n",
      "Epoch: 26, Train_Loss: 0.21424934267997742, Test_Loss: 0.2283507138490677 *\n",
      "Epoch: 26, Train_Loss: 0.2345317304134369, Test_Loss: 0.1978100836277008 *\n",
      "Epoch: 26, Train_Loss: 0.23707064986228943, Test_Loss: 0.1915348917245865 *\n",
      "Epoch: 26, Train_Loss: 0.19204145669937134, Test_Loss: 0.25029250979423523\n",
      "Epoch: 26, Train_Loss: 0.19087907671928406, Test_Loss: 0.26101943850517273\n",
      "Epoch: 26, Train_Loss: 0.1939125806093216, Test_Loss: 0.19143864512443542 *\n",
      "Epoch: 26, Train_Loss: 0.18772004544734955, Test_Loss: 0.19566646218299866\n",
      "Epoch: 26, Train_Loss: 0.20229408144950867, Test_Loss: 0.22803732752799988\n",
      "Epoch: 26, Train_Loss: 0.20706301927566528, Test_Loss: 0.46899178624153137\n",
      "Epoch: 26, Train_Loss: 0.1866738200187683, Test_Loss: 0.21764127910137177 *\n",
      "Epoch: 26, Train_Loss: 0.1896563321352005, Test_Loss: 0.21243591606616974 *\n",
      "Epoch: 26, Train_Loss: 0.21962685883045197, Test_Loss: 0.21920445561408997\n",
      "Epoch: 26, Train_Loss: 0.28263765573501587, Test_Loss: 0.21118277311325073 *\n",
      "Epoch: 26, Train_Loss: 0.2518429756164551, Test_Loss: 0.2369556576013565\n",
      "Epoch: 26, Train_Loss: 0.2144223302602768, Test_Loss: 0.19636361300945282 *\n",
      "Epoch: 26, Train_Loss: 0.3208414316177368, Test_Loss: 0.5137338638305664\n",
      "Epoch: 26, Train_Loss: 0.26950812339782715, Test_Loss: 0.21689026057720184 *\n",
      "Epoch: 26, Train_Loss: 0.2654234766960144, Test_Loss: 0.22482150793075562\n",
      "Epoch: 26, Train_Loss: 0.18928666412830353, Test_Loss: 0.19089294970035553 *\n",
      "Epoch: 26, Train_Loss: 0.24486076831817627, Test_Loss: 0.3100759983062744\n",
      "Epoch: 26, Train_Loss: 0.20250001549720764, Test_Loss: 0.48734480142593384\n",
      "Epoch: 26, Train_Loss: 0.43848323822021484, Test_Loss: 0.2864483594894409 *\n",
      "Epoch: 26, Train_Loss: 0.1924387812614441, Test_Loss: 0.24009670317173004 *\n",
      "Epoch: 26, Train_Loss: 0.6869866847991943, Test_Loss: 0.2614726722240448\n",
      "Epoch: 26, Train_Loss: 2.4342434406280518, Test_Loss: 0.27807173132896423\n",
      "Epoch: 26, Train_Loss: 0.24032826721668243, Test_Loss: 0.22491776943206787 *\n",
      "Epoch: 26, Train_Loss: 0.2032792568206787, Test_Loss: 0.26035597920417786\n",
      "Epoch: 26, Train_Loss: 0.21553125977516174, Test_Loss: 0.3881399631500244\n",
      "Epoch: 26, Train_Loss: 0.20292042195796967, Test_Loss: 3.488579273223877\n",
      "Epoch: 26, Train_Loss: 0.187428280711174, Test_Loss: 2.123509645462036 *\n",
      "Epoch: 26, Train_Loss: 0.18923713266849518, Test_Loss: 0.2011106312274933 *\n",
      "Epoch: 26, Train_Loss: 0.26713165640830994, Test_Loss: 0.19421954452991486 *\n",
      "Epoch: 26, Train_Loss: 0.2589380145072937, Test_Loss: 0.21442538499832153\n",
      "Epoch: 26, Train_Loss: 0.2287759780883789, Test_Loss: 0.20077544450759888 *\n",
      "Epoch: 26, Train_Loss: 0.2065775990486145, Test_Loss: 0.2056017816066742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train_Loss: 0.20101848244667053, Test_Loss: 0.2520679533481598\n",
      "Epoch: 26, Train_Loss: 0.19233959913253784, Test_Loss: 0.22280630469322205 *\n",
      "Epoch: 26, Train_Loss: 0.20222431421279907, Test_Loss: 0.2009902149438858 *\n",
      "Epoch: 26, Train_Loss: 0.2161441296339035, Test_Loss: 0.20881083607673645\n",
      "Epoch: 26, Train_Loss: 0.2141280472278595, Test_Loss: 0.2090032994747162\n",
      "Epoch: 26, Train_Loss: 0.19667625427246094, Test_Loss: 0.25059181451797485\n",
      "Epoch: 26, Train_Loss: 0.1867201328277588, Test_Loss: 0.19708110392093658 *\n",
      "Epoch: 26, Train_Loss: 0.20044291019439697, Test_Loss: 0.3057790994644165\n",
      "Epoch: 26, Train_Loss: 0.19806498289108276, Test_Loss: 0.25057291984558105 *\n",
      "Epoch: 26, Train_Loss: 0.2035398632287979, Test_Loss: 0.2338811159133911 *\n",
      "Epoch: 26, Train_Loss: 0.18710169196128845, Test_Loss: 0.21499446034431458 *\n",
      "Epoch: 26, Train_Loss: 0.18599635362625122, Test_Loss: 0.23669999837875366\n",
      "Epoch: 26, Train_Loss: 0.18586556613445282, Test_Loss: 0.3406215310096741\n",
      "Epoch: 26, Train_Loss: 0.19016097486019135, Test_Loss: 0.19916003942489624 *\n",
      "Epoch: 26, Train_Loss: 0.18868020176887512, Test_Loss: 0.21458469331264496\n",
      "Epoch: 26, Train_Loss: 0.19283334910869598, Test_Loss: 0.20553149282932281 *\n",
      "Epoch: 26, Train_Loss: 0.19705389440059662, Test_Loss: 0.21486257016658783\n",
      "Epoch: 26, Train_Loss: 0.18754440546035767, Test_Loss: 0.19927947223186493 *\n",
      "Epoch: 26, Train_Loss: 0.18782363831996918, Test_Loss: 0.19583828747272491 *\n",
      "Epoch: 26, Train_Loss: 0.18653608858585358, Test_Loss: 0.20540384948253632\n",
      "Epoch: 26, Train_Loss: 0.198157399892807, Test_Loss: 0.21276985108852386\n",
      "Epoch: 26, Train_Loss: 0.19950352609157562, Test_Loss: 0.19317343831062317 *\n",
      "Epoch: 26, Train_Loss: 0.19756923615932465, Test_Loss: 0.1969458907842636\n",
      "Epoch: 26, Train_Loss: 0.21410682797431946, Test_Loss: 0.22321803867816925\n",
      "Epoch: 26, Train_Loss: 0.20397840440273285, Test_Loss: 0.28771668672561646\n",
      "Epoch: 26, Train_Loss: 0.20912374556064606, Test_Loss: 0.3186984360218048\n",
      "Epoch: 26, Train_Loss: 0.1981072574853897, Test_Loss: 0.539587140083313\n",
      "Epoch: 26, Train_Loss: 0.1939360350370407, Test_Loss: 0.5081496238708496 *\n",
      "Epoch: 26, Train_Loss: 0.23194506764411926, Test_Loss: 0.28050222992897034 *\n",
      "Epoch: 26, Train_Loss: 0.20343290269374847, Test_Loss: 0.20450949668884277 *\n",
      "Epoch: 26, Train_Loss: 0.1888575255870819, Test_Loss: 0.20839060842990875\n",
      "Epoch: 26, Train_Loss: 0.1886611431837082, Test_Loss: 0.21952766180038452\n",
      "Model saved at location save_new\\model.ckpt at epoch 26\n",
      "Epoch: 26, Train_Loss: 0.19625915586948395, Test_Loss: 0.42484211921691895\n",
      "Epoch: 26, Train_Loss: 0.19910146296024323, Test_Loss: 0.3500750660896301 *\n",
      "Epoch: 26, Train_Loss: 0.24593214690685272, Test_Loss: 0.5351771116256714\n",
      "Epoch: 26, Train_Loss: 0.24588555097579956, Test_Loss: 0.2893105745315552 *\n",
      "Epoch: 26, Train_Loss: 0.1864544302225113, Test_Loss: 0.22550290822982788 *\n",
      "Epoch: 26, Train_Loss: 0.21926304697990417, Test_Loss: 0.19697146117687225 *\n",
      "Epoch: 26, Train_Loss: 0.23313763737678528, Test_Loss: 0.18553809821605682 *\n",
      "Epoch: 26, Train_Loss: 0.19010098278522491, Test_Loss: 0.20549824833869934\n",
      "Epoch: 26, Train_Loss: 0.23304429650306702, Test_Loss: 0.19323231279850006 *\n",
      "Epoch: 26, Train_Loss: 0.21145305037498474, Test_Loss: 0.20520703494548798\n",
      "Epoch: 26, Train_Loss: 0.2586696743965149, Test_Loss: 0.18892231583595276 *\n",
      "Epoch: 26, Train_Loss: 0.22943022847175598, Test_Loss: 0.279653400182724\n",
      "Epoch: 26, Train_Loss: 0.21357086300849915, Test_Loss: 0.3490973711013794\n",
      "Epoch: 26, Train_Loss: 0.20030920207500458, Test_Loss: 0.4512445330619812\n",
      "Epoch: 26, Train_Loss: 0.19673407077789307, Test_Loss: 0.43735289573669434 *\n",
      "Epoch: 26, Train_Loss: 0.20261871814727783, Test_Loss: 0.2200309932231903 *\n",
      "Epoch: 26, Train_Loss: 0.18605192005634308, Test_Loss: 0.21622459590435028 *\n",
      "Epoch: 26, Train_Loss: 0.18855947256088257, Test_Loss: 0.21611745655536652 *\n",
      "Epoch: 26, Train_Loss: 0.20129026472568512, Test_Loss: 0.2151755690574646 *\n",
      "Epoch: 26, Train_Loss: 0.20948448777198792, Test_Loss: 0.22045324742794037\n",
      "Epoch: 26, Train_Loss: 0.27786338329315186, Test_Loss: 2.639133930206299\n",
      "Epoch: 26, Train_Loss: 0.20163780450820923, Test_Loss: 3.249401569366455\n",
      "Epoch: 26, Train_Loss: 0.24007174372673035, Test_Loss: 0.20603308081626892 *\n",
      "Epoch: 26, Train_Loss: 0.19014443457126617, Test_Loss: 0.2017137110233307 *\n",
      "Epoch: 26, Train_Loss: 0.22358863055706024, Test_Loss: 0.20395590364933014\n",
      "Epoch: 26, Train_Loss: 0.19708432257175446, Test_Loss: 0.19926577806472778 *\n",
      "Epoch: 26, Train_Loss: 0.4710943102836609, Test_Loss: 0.18857553601264954 *\n",
      "Epoch: 26, Train_Loss: 0.24752238392829895, Test_Loss: 0.2038101702928543\n",
      "Epoch: 26, Train_Loss: 0.20114876329898834, Test_Loss: 0.19091804325580597 *\n",
      "Epoch: 26, Train_Loss: 0.1997961550951004, Test_Loss: 0.1862063854932785 *\n",
      "Epoch: 26, Train_Loss: 0.18687471747398376, Test_Loss: 0.19253456592559814\n",
      "Epoch: 26, Train_Loss: 0.19183829426765442, Test_Loss: 0.19581854343414307\n",
      "Epoch: 26, Train_Loss: 0.19089895486831665, Test_Loss: 0.2551799714565277\n",
      "Epoch: 26, Train_Loss: 0.18999040126800537, Test_Loss: 0.22089122235774994 *\n",
      "Epoch: 26, Train_Loss: 0.1863645613193512, Test_Loss: 0.2253376543521881\n",
      "Epoch: 26, Train_Loss: 0.20156168937683105, Test_Loss: 0.19299758970737457 *\n",
      "Epoch: 26, Train_Loss: 0.19154590368270874, Test_Loss: 0.18915238976478577 *\n",
      "Epoch: 26, Train_Loss: 0.1917130947113037, Test_Loss: 0.1902909278869629\n",
      "Epoch: 26, Train_Loss: 0.19655713438987732, Test_Loss: 0.21663673222064972\n",
      "Epoch: 26, Train_Loss: 0.18639162182807922, Test_Loss: 0.2263171523809433\n",
      "Epoch: 26, Train_Loss: 0.1845039278268814, Test_Loss: 0.19068562984466553 *\n",
      "Epoch: 26, Train_Loss: 0.20371940732002258, Test_Loss: 0.19106850028038025\n",
      "Epoch: 26, Train_Loss: 0.19523832201957703, Test_Loss: 0.18565654754638672 *\n",
      "Epoch: 26, Train_Loss: 0.20885075628757477, Test_Loss: 0.1879107654094696\n",
      "Epoch: 26, Train_Loss: 0.19002719223499298, Test_Loss: 0.19148458540439606\n",
      "Epoch: 26, Train_Loss: 0.19791853427886963, Test_Loss: 0.1854824274778366 *\n",
      "Epoch: 26, Train_Loss: 0.19961757957935333, Test_Loss: 0.18638277053833008\n",
      "Epoch: 26, Train_Loss: 0.21473431587219238, Test_Loss: 0.1950778067111969\n",
      "Epoch: 26, Train_Loss: 0.18575353920459747, Test_Loss: 0.18833479285240173 *\n",
      "Epoch: 26, Train_Loss: 0.202594593167305, Test_Loss: 0.1851721853017807 *\n",
      "Epoch: 26, Train_Loss: 0.19463294744491577, Test_Loss: 0.24789145588874817\n",
      "Epoch: 26, Train_Loss: 0.19652295112609863, Test_Loss: 0.24970664083957672\n",
      "Epoch: 26, Train_Loss: 0.18551655113697052, Test_Loss: 4.807852268218994\n",
      "Epoch: 26, Train_Loss: 0.20299206674098969, Test_Loss: 1.2259178161621094 *\n",
      "Epoch: 26, Train_Loss: 0.252817839384079, Test_Loss: 0.1861308366060257 *\n",
      "Epoch: 26, Train_Loss: 2.614279270172119, Test_Loss: 0.20057760179042816\n",
      "Epoch: 26, Train_Loss: 3.102813482284546, Test_Loss: 0.24617928266525269\n",
      "Epoch: 26, Train_Loss: 0.19590894877910614, Test_Loss: 0.2234693467617035 *\n",
      "Epoch: 26, Train_Loss: 0.18612605333328247, Test_Loss: 0.19380316138267517 *\n",
      "Epoch: 26, Train_Loss: 0.23419524729251862, Test_Loss: 0.29071134328842163\n",
      "Epoch: 26, Train_Loss: 0.29082190990448, Test_Loss: 0.22896534204483032 *\n",
      "Epoch: 26, Train_Loss: 0.21456944942474365, Test_Loss: 0.19149468839168549 *\n",
      "Epoch: 26, Train_Loss: 0.18884332478046417, Test_Loss: 0.20883990824222565\n",
      "Epoch: 26, Train_Loss: 0.21511025726795197, Test_Loss: 0.2011326402425766 *\n",
      "Epoch: 26, Train_Loss: 0.2508246898651123, Test_Loss: 0.20534156262874603\n",
      "Epoch: 26, Train_Loss: 0.19636328518390656, Test_Loss: 0.18993158638477325 *\n",
      "Epoch: 26, Train_Loss: 0.21909314393997192, Test_Loss: 0.2980796694755554\n",
      "Epoch: 26, Train_Loss: 0.5518689155578613, Test_Loss: 0.22806771099567413 *\n",
      "Epoch: 26, Train_Loss: 0.5035794973373413, Test_Loss: 0.2258768379688263 *\n",
      "Epoch: 26, Train_Loss: 0.34451740980148315, Test_Loss: 0.21751004457473755 *\n",
      "Epoch: 26, Train_Loss: 0.2964656352996826, Test_Loss: 0.2737599015235901\n",
      "Epoch: 26, Train_Loss: 1.0704678297042847, Test_Loss: 0.2908356785774231\n",
      "Epoch: 26, Train_Loss: 0.7462460398674011, Test_Loss: 0.23382070660591125 *\n",
      "Epoch: 26, Train_Loss: 0.19668330252170563, Test_Loss: 0.22312971949577332 *\n",
      "Epoch: 26, Train_Loss: 0.19035537540912628, Test_Loss: 0.24900081753730774\n",
      "Epoch: 26, Train_Loss: 0.43452006578445435, Test_Loss: 0.21836094558238983 *\n",
      "Epoch: 26, Train_Loss: 0.4658612012863159, Test_Loss: 0.22989927232265472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train_Loss: 0.5857990384101868, Test_Loss: 0.3007369339466095\n",
      "Epoch: 26, Train_Loss: 0.19127263128757477, Test_Loss: 0.2835500240325928 *\n",
      "Epoch: 26, Train_Loss: 0.20868535339832306, Test_Loss: 0.26707690954208374 *\n",
      "Epoch: 26, Train_Loss: 0.25888508558273315, Test_Loss: 0.23189863562583923 *\n",
      "Epoch: 26, Train_Loss: 0.3318168520927429, Test_Loss: 0.20952461659908295 *\n",
      "Epoch: 26, Train_Loss: 0.1946791708469391, Test_Loss: 0.20572558045387268 *\n",
      "Epoch: 26, Train_Loss: 0.2466440200805664, Test_Loss: 0.26435932517051697\n",
      "Epoch: 26, Train_Loss: 0.20871210098266602, Test_Loss: 0.41791343688964844\n",
      "Epoch: 26, Train_Loss: 0.23486050963401794, Test_Loss: 0.28369367122650146 *\n",
      "Epoch: 26, Train_Loss: 0.305991530418396, Test_Loss: 0.3323388993740082\n",
      "Epoch: 26, Train_Loss: 0.3104785978794098, Test_Loss: 0.2462875247001648 *\n",
      "Epoch: 26, Train_Loss: 0.22329634428024292, Test_Loss: 0.2255040407180786 *\n",
      "Epoch: 26, Train_Loss: 0.23469120264053345, Test_Loss: 0.1983060985803604 *\n",
      "Epoch: 26, Train_Loss: 0.29408177733421326, Test_Loss: 0.2174530029296875\n",
      "Epoch: 26, Train_Loss: 0.2255866825580597, Test_Loss: 0.46314185857772827\n",
      "Epoch: 26, Train_Loss: 0.25737911462783813, Test_Loss: 0.20228081941604614 *\n",
      "Epoch: 26, Train_Loss: 0.3134884834289551, Test_Loss: 0.527259886264801\n",
      "Epoch: 26, Train_Loss: 0.21141909062862396, Test_Loss: 0.26313164830207825 *\n",
      "Epoch: 26, Train_Loss: 0.2664123773574829, Test_Loss: 0.200773686170578 *\n",
      "Epoch: 26, Train_Loss: 0.2622150778770447, Test_Loss: 0.2172451913356781\n",
      "Epoch: 26, Train_Loss: 0.21525171399116516, Test_Loss: 0.189705491065979 *\n",
      "Epoch: 26, Train_Loss: 0.1955725997686386, Test_Loss: 0.25340592861175537\n",
      "Epoch: 26, Train_Loss: 0.18457232415676117, Test_Loss: 0.19360780715942383 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 26\n",
      "Epoch: 26, Train_Loss: 0.18346653878688812, Test_Loss: 0.20831027626991272\n",
      "Epoch: 26, Train_Loss: 0.18552742898464203, Test_Loss: 0.21377918124198914\n",
      "Epoch: 26, Train_Loss: 0.18938949704170227, Test_Loss: 0.42777103185653687\n",
      "Epoch: 26, Train_Loss: 0.21131905913352966, Test_Loss: 0.6600532531738281\n",
      "Epoch: 26, Train_Loss: 0.2002161145210266, Test_Loss: 0.4186607599258423 *\n",
      "Epoch: 26, Train_Loss: 0.1923511028289795, Test_Loss: 0.7043833136558533\n",
      "Epoch: 26, Train_Loss: 0.4325369894504547, Test_Loss: 0.42357563972473145 *\n",
      "Epoch: 26, Train_Loss: 0.4482554793357849, Test_Loss: 0.41642242670059204 *\n",
      "Epoch: 26, Train_Loss: 0.19778333604335785, Test_Loss: 0.4049767255783081 *\n",
      "Epoch: 26, Train_Loss: 0.23668083548545837, Test_Loss: 0.3836761713027954 *\n",
      "Epoch: 26, Train_Loss: 0.22292959690093994, Test_Loss: 0.3439711332321167 *\n",
      "Epoch: 26, Train_Loss: 0.22262342274188995, Test_Loss: 4.082871437072754\n",
      "Epoch: 26, Train_Loss: 0.3883340358734131, Test_Loss: 1.6461135149002075 *\n",
      "Epoch: 26, Train_Loss: 0.28544583916664124, Test_Loss: 0.24305644631385803 *\n",
      "Epoch: 26, Train_Loss: 0.5340675711631775, Test_Loss: 0.23251621425151825 *\n",
      "Epoch: 26, Train_Loss: 0.24196384847164154, Test_Loss: 0.21344026923179626 *\n",
      "Epoch: 26, Train_Loss: 0.3119184374809265, Test_Loss: 0.1928625851869583 *\n",
      "Epoch: 26, Train_Loss: 0.20151640474796295, Test_Loss: 0.1966753602027893\n",
      "Epoch: 26, Train_Loss: 0.20121635496616364, Test_Loss: 0.3250603675842285\n",
      "Epoch: 26, Train_Loss: 0.314572274684906, Test_Loss: 0.23876604437828064 *\n",
      "Epoch: 26, Train_Loss: 0.6659415364265442, Test_Loss: 0.1885470300912857 *\n",
      "Epoch: 26, Train_Loss: 0.6341362595558167, Test_Loss: 0.24747751653194427\n",
      "Epoch: 26, Train_Loss: 0.19933193922042847, Test_Loss: 0.24529390037059784 *\n",
      "Epoch: 26, Train_Loss: 0.2385220229625702, Test_Loss: 0.40043574571609497\n",
      "Epoch: 26, Train_Loss: 0.1886707991361618, Test_Loss: 0.20265668630599976 *\n",
      "Epoch: 26, Train_Loss: 0.35825616121292114, Test_Loss: 0.20641252398490906\n",
      "Epoch: 26, Train_Loss: 0.5108429193496704, Test_Loss: 0.19423429667949677 *\n",
      "Epoch: 26, Train_Loss: 0.1927526295185089, Test_Loss: 0.2032575011253357\n",
      "Epoch: 26, Train_Loss: 0.33856678009033203, Test_Loss: 0.23268234729766846\n",
      "Epoch: 26, Train_Loss: 0.20309537649154663, Test_Loss: 0.2703022062778473\n",
      "Epoch: 26, Train_Loss: 0.20959094166755676, Test_Loss: 0.2141231894493103 *\n",
      "Epoch: 26, Train_Loss: 0.2181461602449417, Test_Loss: 0.1972721815109253 *\n",
      "Epoch: 26, Train_Loss: 0.2686325013637543, Test_Loss: 0.20044898986816406\n",
      "Epoch: 26, Train_Loss: 0.28816118836402893, Test_Loss: 0.1919005662202835 *\n",
      "Epoch: 26, Train_Loss: 0.24751073122024536, Test_Loss: 0.19824089109897614\n",
      "Epoch: 26, Train_Loss: 0.19527336955070496, Test_Loss: 0.22955194115638733\n",
      "Epoch: 26, Train_Loss: 0.28906548023223877, Test_Loss: 0.18389342725276947 *\n",
      "Epoch: 26, Train_Loss: 0.2089129388332367, Test_Loss: 0.19053655862808228\n",
      "Epoch: 26, Train_Loss: 0.2055787742137909, Test_Loss: 0.2462666630744934\n",
      "Epoch: 26, Train_Loss: 0.20138396322727203, Test_Loss: 0.20854593813419342 *\n",
      "Epoch: 26, Train_Loss: 0.20718394219875336, Test_Loss: 0.1864934116601944 *\n",
      "Epoch: 26, Train_Loss: 0.2572473883628845, Test_Loss: 0.31652724742889404\n",
      "Epoch: 26, Train_Loss: 0.3657039403915405, Test_Loss: 0.27555006742477417 *\n",
      "Epoch: 26, Train_Loss: 0.3614536225795746, Test_Loss: 5.152273654937744\n",
      "Epoch: 26, Train_Loss: 0.6176831722259521, Test_Loss: 0.3107909858226776 *\n",
      "Epoch: 26, Train_Loss: 0.5047562122344971, Test_Loss: 0.19855135679244995 *\n",
      "Epoch: 26, Train_Loss: 0.3825545310974121, Test_Loss: 0.2399211972951889\n",
      "Epoch: 26, Train_Loss: 0.25707945227622986, Test_Loss: 0.26655441522598267\n",
      "Epoch: 26, Train_Loss: 0.2064281553030014, Test_Loss: 0.19717353582382202 *\n",
      "Epoch: 26, Train_Loss: 0.19331412017345428, Test_Loss: 0.20152312517166138\n",
      "Epoch: 26, Train_Loss: 0.1963183879852295, Test_Loss: 0.25005072355270386\n",
      "Epoch: 26, Train_Loss: 0.30403536558151245, Test_Loss: 0.2236616015434265 *\n",
      "Epoch: 26, Train_Loss: 0.502590537071228, Test_Loss: 0.19839999079704285 *\n",
      "Epoch: 26, Train_Loss: 0.5449824929237366, Test_Loss: 0.21430736780166626\n",
      "Epoch: 26, Train_Loss: 1.0486419200897217, Test_Loss: 0.25996261835098267\n",
      "Epoch: 26, Train_Loss: 1.1410003900527954, Test_Loss: 0.29065224528312683\n",
      "Epoch: 26, Train_Loss: 0.3120649456977844, Test_Loss: 0.2211451381444931 *\n",
      "Epoch: 26, Train_Loss: 0.3673703372478485, Test_Loss: 0.22129514813423157\n",
      "Epoch: 26, Train_Loss: 0.18854159116744995, Test_Loss: 0.27095967531204224\n",
      "Epoch: 26, Train_Loss: 0.25030720233917236, Test_Loss: 0.20537398755550385 *\n",
      "Epoch: 26, Train_Loss: 0.500454306602478, Test_Loss: 0.25095757842063904\n",
      "Epoch: 26, Train_Loss: 0.7675519585609436, Test_Loss: 0.420154333114624\n",
      "Epoch: 26, Train_Loss: 0.21628835797309875, Test_Loss: 0.24317248165607452 *\n",
      "Epoch: 26, Train_Loss: 0.21482302248477936, Test_Loss: 0.30147284269332886\n",
      "Epoch: 26, Train_Loss: 0.22862295806407928, Test_Loss: 0.26943108439445496 *\n",
      "Epoch: 26, Train_Loss: 0.41322141885757446, Test_Loss: 0.3046489953994751\n",
      "Epoch: 26, Train_Loss: 0.34013858437538147, Test_Loss: 0.24589623510837555 *\n",
      "Epoch: 26, Train_Loss: 0.3933088183403015, Test_Loss: 0.23824335634708405 *\n",
      "Epoch: 26, Train_Loss: 0.33765125274658203, Test_Loss: 0.3306284248828888\n",
      "Epoch: 26, Train_Loss: 0.40226292610168457, Test_Loss: 0.25979670882225037 *\n",
      "Epoch: 26, Train_Loss: 0.19024892151355743, Test_Loss: 0.22455160319805145 *\n",
      "Epoch: 26, Train_Loss: 0.20600971579551697, Test_Loss: 0.25005054473876953\n",
      "Epoch: 26, Train_Loss: 0.18878822028636932, Test_Loss: 0.21049194037914276 *\n",
      "Epoch: 26, Train_Loss: 0.21907299757003784, Test_Loss: 0.1890890747308731 *\n",
      "Epoch: 26, Train_Loss: 0.21803146600723267, Test_Loss: 0.2241179198026657\n",
      "Epoch: 26, Train_Loss: 0.26606547832489014, Test_Loss: 0.36009371280670166\n",
      "Epoch: 26, Train_Loss: 14.987488746643066, Test_Loss: 0.284191757440567 *\n",
      "Epoch: 26, Train_Loss: 0.314942330121994, Test_Loss: 0.26772674918174744 *\n",
      "Epoch: 26, Train_Loss: 1.2125965356826782, Test_Loss: 0.22562135756015778 *\n",
      "Epoch: 26, Train_Loss: 1.0742439031600952, Test_Loss: 0.21692819893360138 *\n",
      "Epoch: 26, Train_Loss: 0.23151974380016327, Test_Loss: 0.18794359266757965 *\n",
      "Epoch: 26, Train_Loss: 0.2753545045852661, Test_Loss: 0.2335491180419922\n",
      "Epoch: 26, Train_Loss: 2.1597940921783447, Test_Loss: 0.4077247977256775\n",
      "Epoch: 26, Train_Loss: 4.410788059234619, Test_Loss: 0.2613994777202606 *\n",
      "Epoch: 26, Train_Loss: 0.25658556818962097, Test_Loss: 0.3612748384475708\n",
      "Epoch: 26, Train_Loss: 0.37568768858909607, Test_Loss: 0.25033843517303467 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train_Loss: 4.536288261413574, Test_Loss: 0.21097791194915771 *\n",
      "Epoch: 26, Train_Loss: 0.2910999357700348, Test_Loss: 0.2404264509677887\n",
      "Epoch: 26, Train_Loss: 0.2935618460178375, Test_Loss: 0.21880663931369781 *\n",
      "Epoch: 26, Train_Loss: 0.19294017553329468, Test_Loss: 0.25920915603637695\n",
      "Epoch: 26, Train_Loss: 0.2166411280632019, Test_Loss: 0.1946217566728592 *\n",
      "Epoch: 26, Train_Loss: 0.24503855407238007, Test_Loss: 0.20446111261844635\n",
      "Epoch: 26, Train_Loss: 0.18888242542743683, Test_Loss: 0.19417309761047363 *\n",
      "Epoch: 26, Train_Loss: 0.19808906316757202, Test_Loss: 0.29527145624160767\n",
      "Epoch: 26, Train_Loss: 0.18208125233650208, Test_Loss: 0.7596120834350586\n",
      "Epoch: 26, Train_Loss: 0.1819673627614975, Test_Loss: 0.24987183511257172 *\n",
      "Epoch: 26, Train_Loss: 0.1876872330904007, Test_Loss: 0.5144882798194885\n",
      "Epoch: 26, Train_Loss: 0.19491930305957794, Test_Loss: 0.292019784450531 *\n",
      "Epoch: 26, Train_Loss: 0.2677902281284332, Test_Loss: 0.29572761058807373\n",
      "Epoch: 26, Train_Loss: 0.2616218328475952, Test_Loss: 0.2982901930809021\n",
      "Model saved at location save_new\\model.ckpt at epoch 26\n",
      "Epoch: 26, Train_Loss: 0.22644349932670593, Test_Loss: 0.28071361780166626 *\n",
      "Epoch: 26, Train_Loss: 0.2066996693611145, Test_Loss: 0.22517964243888855 *\n",
      "Epoch: 26, Train_Loss: 0.20898669958114624, Test_Loss: 8.78152084350586\n",
      "Epoch: 26, Train_Loss: 0.20420163869857788, Test_Loss: 0.9215068817138672 *\n",
      "Epoch: 26, Train_Loss: 0.23041780292987823, Test_Loss: 0.5537652969360352 *\n",
      "Epoch: 26, Train_Loss: 0.18638209998607635, Test_Loss: 0.5077900290489197 *\n",
      "Epoch: 26, Train_Loss: 0.18236780166625977, Test_Loss: 0.5605171918869019\n",
      "Epoch: 26, Train_Loss: 0.18187133967876434, Test_Loss: 0.2871534526348114 *\n",
      "Epoch: 26, Train_Loss: 0.18208034336566925, Test_Loss: 0.9380762577056885\n",
      "Epoch: 26, Train_Loss: 0.18405109643936157, Test_Loss: 1.2152737379074097\n",
      "Epoch: 26, Train_Loss: 0.18198491632938385, Test_Loss: 0.8576264977455139 *\n",
      "Epoch: 26, Train_Loss: 0.18215718865394592, Test_Loss: 1.0007466077804565\n",
      "Epoch: 26, Train_Loss: 0.19475853443145752, Test_Loss: 0.786198616027832 *\n",
      "Epoch: 26, Train_Loss: 0.31073468923568726, Test_Loss: 1.1572380065917969\n",
      "Epoch: 26, Train_Loss: 0.27485090494155884, Test_Loss: 1.371875286102295\n",
      "Epoch: 26, Train_Loss: 0.2254469096660614, Test_Loss: 0.3807458281517029 *\n",
      "Epoch: 26, Train_Loss: 0.2751407027244568, Test_Loss: 0.3719354271888733 *\n",
      "Epoch: 26, Train_Loss: 1.265709400177002, Test_Loss: 0.2924095690250397 *\n",
      "Epoch: 26, Train_Loss: 4.092034816741943, Test_Loss: 0.20279942452907562 *\n",
      "Epoch: 26, Train_Loss: 0.22107411921024323, Test_Loss: 0.21752601861953735\n",
      "Epoch: 26, Train_Loss: 0.24221912026405334, Test_Loss: 0.46830523014068604\n",
      "Epoch: 26, Train_Loss: 0.2224123775959015, Test_Loss: 0.38002654910087585 *\n",
      "Epoch: 26, Train_Loss: 0.2845838665962219, Test_Loss: 0.540116012096405\n",
      "Epoch: 26, Train_Loss: 0.273619145154953, Test_Loss: 0.5676696300506592\n",
      "Epoch: 26, Train_Loss: 0.27418339252471924, Test_Loss: 0.3049209713935852 *\n",
      "Epoch: 26, Train_Loss: 0.19540122151374817, Test_Loss: 0.3437480926513672\n",
      "Epoch: 26, Train_Loss: 0.29101306200027466, Test_Loss: 0.4633135199546814\n",
      "Epoch: 26, Train_Loss: 0.2572949528694153, Test_Loss: 0.1913156509399414 *\n",
      "Epoch: 26, Train_Loss: 0.2053254097700119, Test_Loss: 0.2693917751312256\n",
      "Epoch: 26, Train_Loss: 0.19631291925907135, Test_Loss: 0.2851131558418274\n",
      "Epoch: 26, Train_Loss: 0.19987985491752625, Test_Loss: 0.2878917157649994\n",
      "Epoch: 26, Train_Loss: 0.195451021194458, Test_Loss: 0.1852795034646988 *\n",
      "Epoch: 26, Train_Loss: 0.21352237462997437, Test_Loss: 0.658594012260437\n",
      "Epoch: 26, Train_Loss: 0.19428808987140656, Test_Loss: 0.9108792543411255\n",
      "Epoch: 26, Train_Loss: 0.22633233666419983, Test_Loss: 5.792613506317139\n",
      "Epoch: 26, Train_Loss: 0.187161386013031, Test_Loss: 0.2757033407688141 *\n",
      "Epoch: 26, Train_Loss: 0.20670491456985474, Test_Loss: 0.19325067102909088 *\n",
      "Epoch: 26, Train_Loss: 0.2555316686630249, Test_Loss: 0.35753530263900757\n",
      "Epoch: 26, Train_Loss: 0.22813987731933594, Test_Loss: 0.4646782875061035\n",
      "Epoch: 26, Train_Loss: 0.19829890131950378, Test_Loss: 0.2759310305118561 *\n",
      "Epoch: 26, Train_Loss: 0.18267780542373657, Test_Loss: 0.21764777600765228 *\n",
      "Epoch: 26, Train_Loss: 0.18635451793670654, Test_Loss: 0.2931511700153351\n",
      "Epoch: 26, Train_Loss: 1.5487902164459229, Test_Loss: 0.21357838809490204 *\n",
      "Epoch: 26, Train_Loss: 3.8164303302764893, Test_Loss: 0.18615931272506714 *\n",
      "Epoch: 26, Train_Loss: 0.18539603054523468, Test_Loss: 0.25046366453170776\n",
      "Epoch: 26, Train_Loss: 0.18404699862003326, Test_Loss: 0.21804960072040558 *\n",
      "Epoch: 26, Train_Loss: 0.18424583971500397, Test_Loss: 0.20463663339614868 *\n",
      "Epoch: 26, Train_Loss: 0.18303239345550537, Test_Loss: 0.392773300409317\n",
      "Epoch: 26, Train_Loss: 0.1846538484096527, Test_Loss: 0.49344366788864136\n",
      "Epoch: 26, Train_Loss: 0.18220485746860504, Test_Loss: 0.38493484258651733 *\n",
      "Epoch: 26, Train_Loss: 0.19785921275615692, Test_Loss: 0.2821800708770752 *\n",
      "Epoch: 26, Train_Loss: 0.2078523188829422, Test_Loss: 0.29418861865997314\n",
      "Epoch: 26, Train_Loss: 0.21211649477481842, Test_Loss: 0.25185951590538025 *\n",
      "Epoch: 26, Train_Loss: 0.1832214891910553, Test_Loss: 0.43461984395980835\n",
      "Epoch: 26, Train_Loss: 0.18212033808231354, Test_Loss: 0.8397383093833923\n",
      "Epoch: 26, Train_Loss: 0.18453530967235565, Test_Loss: 0.6017255187034607 *\n",
      "Epoch: 26, Train_Loss: 0.19810384511947632, Test_Loss: 0.569429874420166 *\n",
      "Epoch: 26, Train_Loss: 0.18297770619392395, Test_Loss: 0.5529704689979553 *\n",
      "Epoch: 26, Train_Loss: 0.18501414358615875, Test_Loss: 0.4791643023490906 *\n",
      "Epoch: 26, Train_Loss: 0.19653825461864471, Test_Loss: 0.7601436972618103\n",
      "Epoch: 26, Train_Loss: 0.2081741988658905, Test_Loss: 0.5000039935112 *\n",
      "Epoch: 26, Train_Loss: 0.1857331097126007, Test_Loss: 0.21387979388237 *\n",
      "Epoch: 26, Train_Loss: 0.18313078582286835, Test_Loss: 0.26483461260795593\n",
      "Epoch: 26, Train_Loss: 0.20890484750270844, Test_Loss: 0.18815365433692932 *\n",
      "Epoch: 26, Train_Loss: 0.22523576021194458, Test_Loss: 0.20170101523399353\n",
      "Epoch: 26, Train_Loss: 0.24851375818252563, Test_Loss: 0.2538648843765259\n",
      "Epoch: 26, Train_Loss: 0.21039018034934998, Test_Loss: 0.36886730790138245\n",
      "Epoch: 26, Train_Loss: 0.25102493166923523, Test_Loss: 0.2384658008813858 *\n",
      "Epoch: 26, Train_Loss: 0.23157517611980438, Test_Loss: 0.23752377927303314 *\n",
      "Epoch: 26, Train_Loss: 0.20540831983089447, Test_Loss: 0.265846848487854\n",
      "Epoch: 26, Train_Loss: 0.2310744822025299, Test_Loss: 0.21225452423095703 *\n",
      "Epoch: 26, Train_Loss: 0.20796221494674683, Test_Loss: 0.20038391649723053 *\n",
      "Epoch: 26, Train_Loss: 0.2989831566810608, Test_Loss: 0.3076237440109253\n",
      "Epoch: 26, Train_Loss: 0.1970604807138443, Test_Loss: 0.5573214292526245\n",
      "Epoch: 26, Train_Loss: 0.183170348405838, Test_Loss: 0.538280725479126 *\n",
      "Epoch: 26, Train_Loss: 0.18235789239406586, Test_Loss: 0.2539161145687103 *\n",
      "Epoch: 26, Train_Loss: 0.18291854858398438, Test_Loss: 0.24831444025039673 *\n",
      "Epoch: 26, Train_Loss: 0.18394963443279266, Test_Loss: 0.20454223453998566 *\n",
      "Epoch: 26, Train_Loss: 0.1820223331451416, Test_Loss: 0.1934695690870285 *\n",
      "Epoch: 26, Train_Loss: 2.03349232673645, Test_Loss: 0.19482344388961792\n",
      "Epoch: 26, Train_Loss: 2.9768354892730713, Test_Loss: 0.1954132616519928\n",
      "Epoch: 26, Train_Loss: 0.18576590716838837, Test_Loss: 0.20089071989059448\n",
      "Epoch: 26, Train_Loss: 0.18917636573314667, Test_Loss: 0.19820575416088104 *\n",
      "Epoch: 26, Train_Loss: 0.1885516345500946, Test_Loss: 0.1878233402967453 *\n",
      "Epoch: 26, Train_Loss: 0.18245279788970947, Test_Loss: 0.29025399684906006\n",
      "Epoch: 26, Train_Loss: 0.18198567628860474, Test_Loss: 0.5880628824234009\n",
      "Epoch: 26, Train_Loss: 0.18563705682754517, Test_Loss: 0.3448847830295563 *\n",
      "Epoch: 26, Train_Loss: 0.182395339012146, Test_Loss: 0.33900564908981323 *\n",
      "Epoch: 26, Train_Loss: 0.18376849591732025, Test_Loss: 0.2503401041030884 *\n",
      "Epoch: 26, Train_Loss: 0.18905065953731537, Test_Loss: 0.2530446946620941\n",
      "Epoch: 26, Train_Loss: 0.2167799472808838, Test_Loss: 0.25784173607826233\n",
      "Epoch: 26, Train_Loss: 0.2262377142906189, Test_Loss: 0.24751828610897064 *\n",
      "Epoch: 26, Train_Loss: 0.23479901254177094, Test_Loss: 0.2903501093387604\n",
      "Epoch: 26, Train_Loss: 0.21053940057754517, Test_Loss: 6.994639873504639\n",
      "Epoch: 26, Train_Loss: 0.18317559361457825, Test_Loss: 0.3082585334777832 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train_Loss: 0.3427842855453491, Test_Loss: 0.2157718688249588 *\n",
      "Epoch: 26, Train_Loss: 0.22074900567531586, Test_Loss: 0.22106170654296875\n",
      "Epoch: 26, Train_Loss: 0.2845715284347534, Test_Loss: 0.21100518107414246 *\n",
      "Epoch: 26, Train_Loss: 0.28821977972984314, Test_Loss: 0.19355089962482452 *\n",
      "Epoch: 26, Train_Loss: 0.1848258078098297, Test_Loss: 0.25054579973220825\n",
      "Model saved at location save_new\\model.ckpt at epoch 26\n",
      "Epoch: 26, Train_Loss: 0.1813565045595169, Test_Loss: 0.27919700741767883\n",
      "Epoch: 26, Train_Loss: 0.18398873507976532, Test_Loss: 0.230793297290802 *\n",
      "Epoch: 26, Train_Loss: 0.1875600814819336, Test_Loss: 0.19397810101509094 *\n",
      "Epoch: 26, Train_Loss: 0.1988249570131302, Test_Loss: 0.2418529987335205\n",
      "Epoch: 26, Train_Loss: 0.2039080262184143, Test_Loss: 0.6858238577842712\n",
      "Epoch: 26, Train_Loss: 0.18327274918556213, Test_Loss: 0.31674933433532715 *\n",
      "Epoch: 26, Train_Loss: 0.18255609273910522, Test_Loss: 0.21579891443252563 *\n",
      "Epoch: 26, Train_Loss: 0.20255506038665771, Test_Loss: 0.22629165649414062\n",
      "Epoch: 26, Train_Loss: 0.23957081139087677, Test_Loss: 0.1923854649066925 *\n",
      "Epoch: 26, Train_Loss: 0.25904035568237305, Test_Loss: 0.23943577706813812\n",
      "Epoch: 26, Train_Loss: 0.22803492844104767, Test_Loss: 0.20018170773983002 *\n",
      "Epoch: 26, Train_Loss: 0.2971024513244629, Test_Loss: 0.5496363043785095\n",
      "Epoch: 26, Train_Loss: 0.23596183955669403, Test_Loss: 0.24279645085334778 *\n",
      "Epoch: 26, Train_Loss: 0.25356078147888184, Test_Loss: 0.2635459303855896\n",
      "Epoch: 26, Train_Loss: 0.21968097984790802, Test_Loss: 0.18873444199562073 *\n",
      "Epoch: 26, Train_Loss: 0.23831963539123535, Test_Loss: 0.4385349452495575\n",
      "Epoch: 26, Train_Loss: 0.20126576721668243, Test_Loss: 0.44892215728759766\n",
      "Epoch: 26, Train_Loss: 0.40342384576797485, Test_Loss: 0.6367355585098267\n",
      "Epoch: 26, Train_Loss: 0.19881689548492432, Test_Loss: 0.23787574470043182 *\n",
      "Epoch: 26, Train_Loss: 0.24393320083618164, Test_Loss: 0.3509543538093567\n",
      "Epoch: 26, Train_Loss: 2.6251304149627686, Test_Loss: 0.30704811215400696 *\n",
      "Epoch: 26, Train_Loss: 0.33952581882476807, Test_Loss: 0.33001554012298584\n",
      "Epoch: 26, Train_Loss: 0.2025776207447052, Test_Loss: 0.18545503914356232 *\n",
      "Epoch: 26, Train_Loss: 0.2139934003353119, Test_Loss: 0.5497311949729919\n",
      "Epoch: 26, Train_Loss: 0.19902291893959045, Test_Loss: 2.0336058139801025\n",
      "Epoch: 26, Train_Loss: 0.1924489289522171, Test_Loss: 3.411928415298462\n",
      "Epoch: 26, Train_Loss: 0.18509073555469513, Test_Loss: 0.2095932960510254 *\n",
      "Epoch: 26, Train_Loss: 0.23739475011825562, Test_Loss: 0.1932906061410904 *\n",
      "Epoch: 26, Train_Loss: 0.2735768258571625, Test_Loss: 0.21886664628982544\n",
      "Epoch: 26, Train_Loss: 0.23762454092502594, Test_Loss: 0.2064822018146515 *\n",
      "Epoch: 26, Train_Loss: 0.21395811438560486, Test_Loss: 0.2118048518896103\n",
      "Epoch: 26, Train_Loss: 0.19401705265045166, Test_Loss: 0.21464550495147705\n",
      "Epoch: 26, Train_Loss: 0.19203807413578033, Test_Loss: 0.21608611941337585\n",
      "Epoch: 26, Train_Loss: 0.19814172387123108, Test_Loss: 0.19402527809143066 *\n",
      "Epoch: 26, Train_Loss: 0.19149482250213623, Test_Loss: 0.1950480043888092\n",
      "Epoch: 26, Train_Loss: 0.21819448471069336, Test_Loss: 0.2116318941116333\n",
      "Epoch: 26, Train_Loss: 0.192690908908844, Test_Loss: 0.2148137092590332\n",
      "Epoch: 26, Train_Loss: 0.18065400421619415, Test_Loss: 0.19522272050380707 *\n",
      "Epoch: 26, Train_Loss: 0.1924341917037964, Test_Loss: 0.25297895073890686\n",
      "Epoch: 26, Train_Loss: 0.19291023910045624, Test_Loss: 0.4263109564781189\n",
      "Epoch: 26, Train_Loss: 0.2000419795513153, Test_Loss: 0.20618611574172974 *\n",
      "Epoch: 26, Train_Loss: 0.1816914975643158, Test_Loss: 0.20575173199176788 *\n",
      "Epoch: 26, Train_Loss: 0.18135100603103638, Test_Loss: 0.20815855264663696\n",
      "Epoch: 26, Train_Loss: 0.18187613785266876, Test_Loss: 0.33134227991104126\n",
      "Epoch: 26, Train_Loss: 0.18581540882587433, Test_Loss: 0.21055102348327637 *\n",
      "Epoch: 26, Train_Loss: 0.18405874073505402, Test_Loss: 0.231220081448555\n",
      "Epoch: 26, Train_Loss: 0.1847413331270218, Test_Loss: 0.20243655145168304 *\n",
      "Epoch: 26, Train_Loss: 0.1939140409231186, Test_Loss: 0.21358215808868408\n",
      "Epoch: 26, Train_Loss: 0.18178367614746094, Test_Loss: 0.20468983054161072 *\n",
      "Epoch: 26, Train_Loss: 0.1808880865573883, Test_Loss: 0.19558995962142944 *\n",
      "Epoch: 26, Train_Loss: 0.18251588940620422, Test_Loss: 0.21181568503379822\n",
      "Epoch: 26, Train_Loss: 0.18610787391662598, Test_Loss: 0.23685045540332794\n",
      "Epoch: 26, Train_Loss: 0.18950499594211578, Test_Loss: 0.22358785569667816 *\n",
      "Epoch: 26, Train_Loss: 0.19921068847179413, Test_Loss: 0.19754169881343842 *\n",
      "Epoch: 27, Train_Loss: 0.20310606062412262, Test_Loss: 0.2156389206647873 *\n",
      "Epoch: 27, Train_Loss: 0.2234196811914444, Test_Loss: 0.2632698118686676\n",
      "Epoch: 27, Train_Loss: 0.22535184025764465, Test_Loss: 0.23510336875915527 *\n",
      "Epoch: 27, Train_Loss: 0.18464791774749756, Test_Loss: 0.7346528172492981\n",
      "Epoch: 27, Train_Loss: 0.1891682744026184, Test_Loss: 0.6088753938674927 *\n",
      "Epoch: 27, Train_Loss: 0.2146914303302765, Test_Loss: 0.30043840408325195 *\n",
      "Epoch: 27, Train_Loss: 0.20872601866722107, Test_Loss: 0.21115243434906006 *\n",
      "Epoch: 27, Train_Loss: 0.18212890625, Test_Loss: 0.20375409722328186 *\n",
      "Epoch: 27, Train_Loss: 0.1818356215953827, Test_Loss: 0.19486942887306213 *\n",
      "Epoch: 27, Train_Loss: 0.1837061494588852, Test_Loss: 0.30427706241607666\n",
      "Epoch: 27, Train_Loss: 0.20705008506774902, Test_Loss: 0.2871309220790863 *\n",
      "Epoch: 27, Train_Loss: 0.23694588243961334, Test_Loss: 0.4606128931045532\n",
      "Epoch: 27, Train_Loss: 0.24032914638519287, Test_Loss: 0.24432697892189026 *\n",
      "Epoch: 27, Train_Loss: 0.19038672745227814, Test_Loss: 0.28364303708076477\n",
      "Epoch: 27, Train_Loss: 0.18880003690719604, Test_Loss: 0.2224106639623642 *\n",
      "Epoch: 27, Train_Loss: 0.2282341718673706, Test_Loss: 0.1826111078262329 *\n",
      "Epoch: 27, Train_Loss: 0.18818975985050201, Test_Loss: 0.1925426870584488\n",
      "Epoch: 27, Train_Loss: 0.18933522701263428, Test_Loss: 0.19398367404937744\n",
      "Epoch: 27, Train_Loss: 0.23169492185115814, Test_Loss: 0.19935324788093567\n",
      "Epoch: 27, Train_Loss: 0.21213732659816742, Test_Loss: 0.18624833226203918 *\n",
      "Epoch: 27, Train_Loss: 0.24702775478363037, Test_Loss: 0.2070796638727188\n",
      "Epoch: 27, Train_Loss: 0.20272289216518402, Test_Loss: 0.33736294507980347\n",
      "Epoch: 27, Train_Loss: 0.19825786352157593, Test_Loss: 0.5421984791755676\n",
      "Epoch: 27, Train_Loss: 0.18959572911262512, Test_Loss: 0.43401771783828735 *\n",
      "Epoch: 27, Train_Loss: 0.20418775081634521, Test_Loss: 0.23004375398159027 *\n",
      "Epoch: 27, Train_Loss: 0.18229323625564575, Test_Loss: 0.22028911113739014 *\n",
      "Epoch: 27, Train_Loss: 0.18315041065216064, Test_Loss: 0.22053276002407074\n",
      "Epoch: 27, Train_Loss: 0.19454966485500336, Test_Loss: 0.22113358974456787\n",
      "Epoch: 27, Train_Loss: 0.19763334095478058, Test_Loss: 0.21205860376358032 *\n",
      "Epoch: 27, Train_Loss: 0.24569348990917206, Test_Loss: 0.6844024062156677\n",
      "Epoch: 27, Train_Loss: 0.2468280792236328, Test_Loss: 5.3169379234313965\n",
      "Epoch: 27, Train_Loss: 0.21061380207538605, Test_Loss: 0.2230670154094696 *\n",
      "Epoch: 27, Train_Loss: 0.20721159875392914, Test_Loss: 0.20573008060455322 *\n",
      "Epoch: 27, Train_Loss: 0.2099255919456482, Test_Loss: 0.21112778782844543\n",
      "Epoch: 27, Train_Loss: 0.1867554932832718, Test_Loss: 0.1946909874677658 *\n",
      "Epoch: 27, Train_Loss: 0.31168806552886963, Test_Loss: 0.18727625906467438 *\n",
      "Epoch: 27, Train_Loss: 0.34846168756484985, Test_Loss: 0.19728147983551025\n",
      "Epoch: 27, Train_Loss: 0.1801815927028656, Test_Loss: 0.18260779976844788 *\n",
      "Epoch: 27, Train_Loss: 0.21191295981407166, Test_Loss: 0.1834360510110855\n",
      "Epoch: 27, Train_Loss: 0.18090008199214935, Test_Loss: 0.1830589920282364 *\n",
      "Epoch: 27, Train_Loss: 0.18348775804042816, Test_Loss: 0.18761274218559265\n",
      "Epoch: 27, Train_Loss: 0.1897445172071457, Test_Loss: 0.2762339115142822\n",
      "Epoch: 27, Train_Loss: 0.18407303094863892, Test_Loss: 0.24147556722164154 *\n",
      "Epoch: 27, Train_Loss: 0.18051408231258392, Test_Loss: 0.23830467462539673 *\n",
      "Epoch: 27, Train_Loss: 0.19863596558570862, Test_Loss: 0.22389164566993713 *\n",
      "Epoch: 27, Train_Loss: 0.1864350140094757, Test_Loss: 0.19845616817474365 *\n",
      "Epoch: 27, Train_Loss: 0.19370722770690918, Test_Loss: 0.18763591349124908 *\n",
      "Epoch: 27, Train_Loss: 0.19265694916248322, Test_Loss: 0.1906479150056839\n",
      "Epoch: 27, Train_Loss: 0.1803150326013565, Test_Loss: 0.24200153350830078\n",
      "Epoch: 27, Train_Loss: 0.18098071217536926, Test_Loss: 0.21060273051261902 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train_Loss: 0.19323311746120453, Test_Loss: 0.18693163990974426 *\n",
      "Epoch: 27, Train_Loss: 0.18772338330745697, Test_Loss: 0.18156945705413818 *\n",
      "Epoch: 27, Train_Loss: 0.19330079853534698, Test_Loss: 0.18771792948246002\n",
      "Epoch: 27, Train_Loss: 0.193976029753685, Test_Loss: 0.1866511106491089 *\n",
      "Epoch: 27, Train_Loss: 0.18908771872520447, Test_Loss: 0.18295496702194214 *\n",
      "Epoch: 27, Train_Loss: 0.19326552748680115, Test_Loss: 0.1827116459608078 *\n",
      "Epoch: 27, Train_Loss: 0.20549336075782776, Test_Loss: 0.18936951458454132\n",
      "Epoch: 27, Train_Loss: 0.19901132583618164, Test_Loss: 0.19338496029376984\n",
      "Epoch: 27, Train_Loss: 0.1903211921453476, Test_Loss: 0.18204888701438904 *\n",
      "Epoch: 27, Train_Loss: 0.20090408623218536, Test_Loss: 0.18509607017040253\n",
      "Epoch: 27, Train_Loss: 0.18505366146564484, Test_Loss: 0.31318920850753784\n",
      "Epoch: 27, Train_Loss: 0.1884303092956543, Test_Loss: 3.1852006912231445\n",
      "Epoch: 27, Train_Loss: 0.2022731751203537, Test_Loss: 2.8539834022521973 *\n",
      "Epoch: 27, Train_Loss: 0.22363856434822083, Test_Loss: 0.18592995405197144 *\n",
      "Epoch: 27, Train_Loss: 2.360826015472412, Test_Loss: 0.18310484290122986 *\n",
      "Epoch: 27, Train_Loss: 2.9029595851898193, Test_Loss: 0.21458929777145386\n",
      "Epoch: 27, Train_Loss: 0.1866980791091919, Test_Loss: 0.24618545174598694\n",
      "Epoch: 27, Train_Loss: 0.18610775470733643, Test_Loss: 0.20283633470535278 *\n",
      "Epoch: 27, Train_Loss: 0.21026144921779633, Test_Loss: 0.2739977538585663\n",
      "Epoch: 27, Train_Loss: 0.2958992123603821, Test_Loss: 0.2259419858455658 *\n",
      "Epoch: 27, Train_Loss: 0.20502464473247528, Test_Loss: 0.18698592483997345 *\n",
      "Epoch: 27, Train_Loss: 0.1873411387205124, Test_Loss: 0.19921469688415527\n",
      "Epoch: 27, Train_Loss: 0.18660281598567963, Test_Loss: 0.20179137587547302\n",
      "Epoch: 27, Train_Loss: 0.27134472131729126, Test_Loss: 0.21412350237369537\n",
      "Epoch: 27, Train_Loss: 0.19771873950958252, Test_Loss: 0.18851327896118164 *\n",
      "Epoch: 27, Train_Loss: 0.1922900676727295, Test_Loss: 0.2677138149738312\n",
      "Epoch: 27, Train_Loss: 0.5841467380523682, Test_Loss: 0.2519338130950928 *\n",
      "Epoch: 27, Train_Loss: 0.3615270256996155, Test_Loss: 0.22632618248462677 *\n",
      "Epoch: 27, Train_Loss: 0.5825585126876831, Test_Loss: 0.20198772847652435 *\n",
      "Epoch: 27, Train_Loss: 0.2617681622505188, Test_Loss: 0.21171598136425018\n",
      "Epoch: 27, Train_Loss: 0.6973749399185181, Test_Loss: 0.34394747018814087\n",
      "Epoch: 27, Train_Loss: 0.7696542739868164, Test_Loss: 0.3014262914657593 *\n",
      "Epoch: 27, Train_Loss: 0.4284815192222595, Test_Loss: 0.3461763858795166\n",
      "Epoch: 27, Train_Loss: 0.18656697869300842, Test_Loss: 0.35465216636657715\n",
      "Epoch: 27, Train_Loss: 0.22616514563560486, Test_Loss: 0.28315943479537964 *\n",
      "Epoch: 27, Train_Loss: 0.42900514602661133, Test_Loss: 0.3305339217185974\n",
      "Epoch: 27, Train_Loss: 0.32162725925445557, Test_Loss: 0.318325012922287 *\n",
      "Epoch: 27, Train_Loss: 0.25111478567123413, Test_Loss: 0.507254958152771\n",
      "Epoch: 27, Train_Loss: 0.1905982494354248, Test_Loss: 0.40220823884010315 *\n",
      "Epoch: 27, Train_Loss: 0.19744452834129333, Test_Loss: 0.2518708407878876 *\n",
      "Epoch: 27, Train_Loss: 0.310798704624176, Test_Loss: 0.20592539012432098 *\n",
      "Epoch: 27, Train_Loss: 0.2830173969268799, Test_Loss: 0.2029082179069519 *\n",
      "Epoch: 27, Train_Loss: 0.2602996528148651, Test_Loss: 0.3156373202800751\n",
      "Epoch: 27, Train_Loss: 0.20704641938209534, Test_Loss: 0.26671984791755676 *\n",
      "Epoch: 27, Train_Loss: 0.23077517747879028, Test_Loss: 0.29416489601135254\n",
      "Epoch: 27, Train_Loss: 0.2986918091773987, Test_Loss: 0.3554558753967285\n",
      "Epoch: 27, Train_Loss: 0.3014259934425354, Test_Loss: 0.24362295866012573 *\n",
      "Epoch: 27, Train_Loss: 0.21742351353168488, Test_Loss: 0.2726800739765167\n",
      "Epoch: 27, Train_Loss: 0.22955085337162018, Test_Loss: 0.21743187308311462 *\n",
      "Epoch: 27, Train_Loss: 0.2701917290687561, Test_Loss: 0.19685538113117218 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 27\n",
      "Epoch: 27, Train_Loss: 0.2151370495557785, Test_Loss: 0.3461092710494995\n",
      "Epoch: 27, Train_Loss: 0.21607032418251038, Test_Loss: 0.2102694809436798 *\n",
      "Epoch: 27, Train_Loss: 0.2385995239019394, Test_Loss: 0.5487897992134094\n",
      "Epoch: 27, Train_Loss: 0.24490351974964142, Test_Loss: 0.27112963795661926 *\n",
      "Epoch: 27, Train_Loss: 0.24607056379318237, Test_Loss: 0.2276461124420166 *\n",
      "Epoch: 27, Train_Loss: 0.2522760033607483, Test_Loss: 0.21103942394256592 *\n",
      "Epoch: 27, Train_Loss: 0.20759427547454834, Test_Loss: 0.18043632805347443 *\n",
      "Epoch: 27, Train_Loss: 0.1894671469926834, Test_Loss: 0.20397232472896576\n",
      "Epoch: 27, Train_Loss: 0.17971482872962952, Test_Loss: 0.19776739180088043 *\n",
      "Epoch: 27, Train_Loss: 0.17902590334415436, Test_Loss: 0.19320082664489746 *\n",
      "Epoch: 27, Train_Loss: 0.17921538650989532, Test_Loss: 0.20420588552951813\n",
      "Epoch: 27, Train_Loss: 0.1848575323820114, Test_Loss: 0.28977614641189575\n",
      "Epoch: 27, Train_Loss: 0.20256242156028748, Test_Loss: 0.49509233236312866\n",
      "Epoch: 27, Train_Loss: 0.1978406012058258, Test_Loss: 0.4855974018573761 *\n",
      "Epoch: 27, Train_Loss: 0.1918206661939621, Test_Loss: 0.5883271098136902\n",
      "Epoch: 27, Train_Loss: 0.43123114109039307, Test_Loss: 0.36214643716812134 *\n",
      "Epoch: 27, Train_Loss: 0.31364506483078003, Test_Loss: 0.3415393829345703 *\n",
      "Epoch: 27, Train_Loss: 0.1879217028617859, Test_Loss: 0.3374248147010803 *\n",
      "Epoch: 27, Train_Loss: 0.21261213719844818, Test_Loss: 0.33497148752212524 *\n",
      "Epoch: 27, Train_Loss: 0.23098669946193695, Test_Loss: 0.28872910141944885 *\n",
      "Epoch: 27, Train_Loss: 0.2356417179107666, Test_Loss: 2.0364346504211426\n",
      "Epoch: 27, Train_Loss: 0.4113420844078064, Test_Loss: 3.8288421630859375\n",
      "Epoch: 27, Train_Loss: 0.2356751263141632, Test_Loss: 0.2195143848657608 *\n",
      "Epoch: 27, Train_Loss: 0.44069674611091614, Test_Loss: 0.21658386290073395 *\n",
      "Epoch: 27, Train_Loss: 0.2312239557504654, Test_Loss: 0.2134254276752472 *\n",
      "Epoch: 27, Train_Loss: 0.29598021507263184, Test_Loss: 0.19031104445457458 *\n",
      "Epoch: 27, Train_Loss: 0.2585555911064148, Test_Loss: 0.1831413060426712 *\n",
      "Epoch: 27, Train_Loss: 0.19485312700271606, Test_Loss: 0.27799496054649353\n",
      "Epoch: 27, Train_Loss: 0.20014575123786926, Test_Loss: 0.22664405405521393 *\n",
      "Epoch: 27, Train_Loss: 0.5378025770187378, Test_Loss: 0.19046659767627716 *\n",
      "Epoch: 27, Train_Loss: 0.6552610397338867, Test_Loss: 0.2064916342496872\n",
      "Epoch: 27, Train_Loss: 0.18718062341213226, Test_Loss: 0.2161937803030014\n",
      "Epoch: 27, Train_Loss: 0.23893789947032928, Test_Loss: 0.3631386160850525\n",
      "Epoch: 27, Train_Loss: 0.1940729022026062, Test_Loss: 0.1976706087589264 *\n",
      "Epoch: 27, Train_Loss: 0.22823932766914368, Test_Loss: 0.1997644007205963\n",
      "Epoch: 27, Train_Loss: 0.5072551369667053, Test_Loss: 0.19771292805671692 *\n",
      "Epoch: 27, Train_Loss: 0.18564997613430023, Test_Loss: 0.20315244793891907\n",
      "Epoch: 27, Train_Loss: 0.2911519408226013, Test_Loss: 0.24068103730678558\n",
      "Epoch: 27, Train_Loss: 0.2316020131111145, Test_Loss: 0.28317999839782715\n",
      "Epoch: 27, Train_Loss: 0.2165769785642624, Test_Loss: 0.25017961859703064 *\n",
      "Epoch: 27, Train_Loss: 0.22040504217147827, Test_Loss: 0.20846515893936157 *\n",
      "Epoch: 27, Train_Loss: 0.282023549079895, Test_Loss: 0.20378392934799194 *\n",
      "Epoch: 27, Train_Loss: 0.3725641071796417, Test_Loss: 0.1866336613893509 *\n",
      "Epoch: 27, Train_Loss: 0.21612146496772766, Test_Loss: 0.19946283102035522\n",
      "Epoch: 27, Train_Loss: 0.20034198462963104, Test_Loss: 0.2228618562221527\n",
      "Epoch: 27, Train_Loss: 0.23317457735538483, Test_Loss: 0.18272942304611206 *\n",
      "Epoch: 27, Train_Loss: 0.2197299301624298, Test_Loss: 0.18549713492393494\n",
      "Epoch: 27, Train_Loss: 0.2067052721977234, Test_Loss: 0.22992467880249023\n",
      "Epoch: 27, Train_Loss: 0.19538384675979614, Test_Loss: 0.23940590023994446\n",
      "Epoch: 27, Train_Loss: 0.19480037689208984, Test_Loss: 0.19233328104019165 *\n",
      "Epoch: 27, Train_Loss: 0.25480738282203674, Test_Loss: 0.23204123973846436\n",
      "Epoch: 27, Train_Loss: 0.3976792097091675, Test_Loss: 0.3037674129009247\n",
      "Epoch: 27, Train_Loss: 0.4504912495613098, Test_Loss: 3.84861421585083\n",
      "Epoch: 27, Train_Loss: 0.6603248119354248, Test_Loss: 1.7512952089309692 *\n",
      "Epoch: 27, Train_Loss: 0.4914712905883789, Test_Loss: 0.1957259178161621 *\n",
      "Epoch: 27, Train_Loss: 0.36776003241539, Test_Loss: 0.19110359251499176 *\n",
      "Epoch: 27, Train_Loss: 0.2753601670265198, Test_Loss: 0.20305918157100677\n",
      "Epoch: 27, Train_Loss: 0.2185669243335724, Test_Loss: 0.19048868119716644 *\n",
      "Epoch: 27, Train_Loss: 0.1966230571269989, Test_Loss: 0.19171887636184692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train_Loss: 0.19015419483184814, Test_Loss: 0.24236378073692322\n",
      "Epoch: 27, Train_Loss: 0.27745887637138367, Test_Loss: 0.21787919104099274 *\n",
      "Epoch: 27, Train_Loss: 0.47257763147354126, Test_Loss: 0.18747127056121826 *\n",
      "Epoch: 27, Train_Loss: 0.5411748290061951, Test_Loss: 0.2067803591489792\n",
      "Epoch: 27, Train_Loss: 0.8268048763275146, Test_Loss: 0.20460456609725952 *\n",
      "Epoch: 27, Train_Loss: 1.0435577630996704, Test_Loss: 0.24350188672542572\n",
      "Epoch: 27, Train_Loss: 0.4120616912841797, Test_Loss: 0.18752244114875793 *\n",
      "Epoch: 27, Train_Loss: 0.4384547472000122, Test_Loss: 0.2352527678012848\n",
      "Epoch: 27, Train_Loss: 0.17887422442436218, Test_Loss: 0.2930419445037842\n",
      "Epoch: 27, Train_Loss: 0.2001633197069168, Test_Loss: 0.2324574589729309 *\n",
      "Epoch: 27, Train_Loss: 0.4091643989086151, Test_Loss: 0.20648567378520966 *\n",
      "Epoch: 27, Train_Loss: 0.6817415952682495, Test_Loss: 0.255845308303833\n",
      "Epoch: 27, Train_Loss: 0.25045663118362427, Test_Loss: 0.3367037773132324\n",
      "Epoch: 27, Train_Loss: 0.21285858750343323, Test_Loss: 0.32567358016967773 *\n",
      "Epoch: 27, Train_Loss: 0.1996522843837738, Test_Loss: 0.32247382402420044 *\n",
      "Epoch: 27, Train_Loss: 0.2983272671699524, Test_Loss: 0.34730446338653564\n",
      "Epoch: 27, Train_Loss: 0.4961993098258972, Test_Loss: 0.2721173167228699 *\n",
      "Epoch: 27, Train_Loss: 0.4346810579299927, Test_Loss: 0.2947940528392792\n",
      "Epoch: 27, Train_Loss: 0.3129900097846985, Test_Loss: 0.3808373808860779\n",
      "Epoch: 27, Train_Loss: 0.44391340017318726, Test_Loss: 0.3454294204711914 *\n",
      "Epoch: 27, Train_Loss: 0.18815919756889343, Test_Loss: 0.28193923830986023 *\n",
      "Epoch: 27, Train_Loss: 0.19239476323127747, Test_Loss: 0.22603337466716766 *\n",
      "Epoch: 27, Train_Loss: 0.20017990469932556, Test_Loss: 0.22386392951011658 *\n",
      "Epoch: 27, Train_Loss: 0.22353291511535645, Test_Loss: 0.18138466775417328 *\n",
      "Epoch: 27, Train_Loss: 0.20983192324638367, Test_Loss: 0.21951109170913696\n",
      "Epoch: 27, Train_Loss: 0.24883174896240234, Test_Loss: 0.29963815212249756\n",
      "Epoch: 27, Train_Loss: 14.718989372253418, Test_Loss: 0.2598941922187805 *\n",
      "Epoch: 27, Train_Loss: 0.6527777314186096, Test_Loss: 0.2983384132385254\n",
      "Epoch: 27, Train_Loss: 1.1920170783996582, Test_Loss: 0.22842121124267578 *\n",
      "Epoch: 27, Train_Loss: 0.8809011578559875, Test_Loss: 0.22189846634864807 *\n",
      "Epoch: 27, Train_Loss: 0.22673943638801575, Test_Loss: 0.20127925276756287 *\n",
      "Epoch: 27, Train_Loss: 0.21848304569721222, Test_Loss: 0.2134036123752594\n",
      "Epoch: 27, Train_Loss: 1.6048896312713623, Test_Loss: 0.40154188871383667\n",
      "Epoch: 27, Train_Loss: 3.8832788467407227, Test_Loss: 0.29945164918899536 *\n",
      "Epoch: 27, Train_Loss: 0.43422165513038635, Test_Loss: 0.5493714809417725\n",
      "Epoch: 27, Train_Loss: 0.2990598678588867, Test_Loss: 0.24272266030311584 *\n",
      "Epoch: 27, Train_Loss: 4.341586589813232, Test_Loss: 0.20456287264823914 *\n",
      "Epoch: 27, Train_Loss: 0.6530609726905823, Test_Loss: 0.26627689599990845\n",
      "Epoch: 27, Train_Loss: 0.36468392610549927, Test_Loss: 0.20657356083393097 *\n",
      "Epoch: 27, Train_Loss: 0.18646863102912903, Test_Loss: 0.2696422040462494\n",
      "Epoch: 27, Train_Loss: 0.19358406960964203, Test_Loss: 0.1913956254720688 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 27\n",
      "Epoch: 27, Train_Loss: 0.22387662529945374, Test_Loss: 0.20620408654212952\n",
      "Epoch: 27, Train_Loss: 0.18298238515853882, Test_Loss: 0.18508610129356384 *\n",
      "Epoch: 27, Train_Loss: 0.19044338166713715, Test_Loss: 0.2952818274497986\n",
      "Epoch: 27, Train_Loss: 0.17733687162399292, Test_Loss: 0.8119649291038513\n",
      "Epoch: 27, Train_Loss: 0.17715153098106384, Test_Loss: 0.3828917145729065 *\n",
      "Epoch: 27, Train_Loss: 0.17795750498771667, Test_Loss: 0.5170871615409851\n",
      "Epoch: 27, Train_Loss: 0.1856149584054947, Test_Loss: 0.22373075783252716 *\n",
      "Epoch: 27, Train_Loss: 0.27387458086013794, Test_Loss: 0.17921562492847443 *\n",
      "Epoch: 27, Train_Loss: 0.22440576553344727, Test_Loss: 0.1798403561115265\n",
      "Epoch: 27, Train_Loss: 0.20559678971767426, Test_Loss: 0.19198210537433624\n",
      "Epoch: 27, Train_Loss: 0.1965365707874298, Test_Loss: 0.21608060598373413\n",
      "Epoch: 27, Train_Loss: 0.19221265614032745, Test_Loss: 5.719255447387695\n",
      "Epoch: 27, Train_Loss: 0.18307480216026306, Test_Loss: 2.8906962871551514 *\n",
      "Epoch: 27, Train_Loss: 0.2566526532173157, Test_Loss: 0.44305336475372314 *\n",
      "Epoch: 27, Train_Loss: 0.19056038558483124, Test_Loss: 0.5025852918624878\n",
      "Epoch: 27, Train_Loss: 0.1787392944097519, Test_Loss: 0.582079291343689\n",
      "Epoch: 27, Train_Loss: 0.1780705451965332, Test_Loss: 0.2702804207801819 *\n",
      "Epoch: 27, Train_Loss: 0.17834728956222534, Test_Loss: 0.4809662699699402\n",
      "Epoch: 27, Train_Loss: 0.17809826135635376, Test_Loss: 1.3351906538009644\n",
      "Epoch: 27, Train_Loss: 0.17719435691833496, Test_Loss: 0.777334988117218 *\n",
      "Epoch: 27, Train_Loss: 0.1770721673965454, Test_Loss: 0.5604629516601562 *\n",
      "Epoch: 27, Train_Loss: 0.18248359858989716, Test_Loss: 0.8363931179046631\n",
      "Epoch: 27, Train_Loss: 0.2223203182220459, Test_Loss: 0.5421609282493591 *\n",
      "Epoch: 27, Train_Loss: 0.27098333835601807, Test_Loss: 1.7084994316101074\n",
      "Epoch: 27, Train_Loss: 0.22994017601013184, Test_Loss: 0.4598921537399292 *\n",
      "Epoch: 27, Train_Loss: 0.20436933636665344, Test_Loss: 0.5035129189491272\n",
      "Epoch: 27, Train_Loss: 0.4489467740058899, Test_Loss: 0.3434194326400757 *\n",
      "Epoch: 27, Train_Loss: 4.663027286529541, Test_Loss: 0.18348443508148193 *\n",
      "Epoch: 27, Train_Loss: 0.275257408618927, Test_Loss: 0.19328555464744568\n",
      "Epoch: 27, Train_Loss: 0.20879551768302917, Test_Loss: 0.32381927967071533\n",
      "Epoch: 27, Train_Loss: 0.20988361537456512, Test_Loss: 0.4474582076072693\n",
      "Epoch: 27, Train_Loss: 0.2668323218822479, Test_Loss: 0.41626834869384766 *\n",
      "Epoch: 27, Train_Loss: 0.21078786253929138, Test_Loss: 0.8369177579879761\n",
      "Epoch: 27, Train_Loss: 0.27642685174942017, Test_Loss: 0.20818489789962769 *\n",
      "Epoch: 27, Train_Loss: 0.2017730176448822, Test_Loss: 0.4710073173046112\n",
      "Epoch: 27, Train_Loss: 0.2911691665649414, Test_Loss: 0.4164949059486389 *\n",
      "Epoch: 27, Train_Loss: 0.26279428601264954, Test_Loss: 0.20101960003376007 *\n",
      "Epoch: 27, Train_Loss: 0.20736010372638702, Test_Loss: 0.28875666856765747\n",
      "Epoch: 27, Train_Loss: 0.1875084489583969, Test_Loss: 0.3318750858306885\n",
      "Epoch: 27, Train_Loss: 0.2040587216615677, Test_Loss: 0.3231227397918701 *\n",
      "Epoch: 27, Train_Loss: 0.18268920481204987, Test_Loss: 0.1848599910736084 *\n",
      "Epoch: 27, Train_Loss: 0.1971750557422638, Test_Loss: 0.5665258169174194\n",
      "Epoch: 27, Train_Loss: 0.18440812826156616, Test_Loss: 0.3637726902961731 *\n",
      "Epoch: 27, Train_Loss: 0.21827897429466248, Test_Loss: 6.500896453857422\n",
      "Epoch: 27, Train_Loss: 0.1976020634174347, Test_Loss: 0.8990943431854248 *\n",
      "Epoch: 27, Train_Loss: 0.20167586207389832, Test_Loss: 0.1817745715379715 *\n",
      "Epoch: 27, Train_Loss: 0.2567138373851776, Test_Loss: 0.24014273285865784\n",
      "Epoch: 27, Train_Loss: 0.23420265316963196, Test_Loss: 0.27158135175704956\n",
      "Epoch: 27, Train_Loss: 0.18984556198120117, Test_Loss: 0.29049214720726013\n",
      "Epoch: 27, Train_Loss: 0.17773932218551636, Test_Loss: 0.19558121263980865 *\n",
      "Epoch: 27, Train_Loss: 0.17822255194187164, Test_Loss: 0.2885419726371765\n",
      "Epoch: 27, Train_Loss: 0.5713933706283569, Test_Loss: 0.22406116127967834 *\n",
      "Epoch: 27, Train_Loss: 4.090423107147217, Test_Loss: 0.18049001693725586 *\n",
      "Epoch: 27, Train_Loss: 0.18226628005504608, Test_Loss: 0.22247804701328278\n",
      "Epoch: 27, Train_Loss: 0.23362930119037628, Test_Loss: 0.19700096547603607 *\n",
      "Epoch: 27, Train_Loss: 0.1900472193956375, Test_Loss: 0.20705801248550415\n",
      "Epoch: 27, Train_Loss: 0.1805976778268814, Test_Loss: 0.31296151876449585\n",
      "Epoch: 27, Train_Loss: 0.1781800091266632, Test_Loss: 0.7718302011489868\n",
      "Epoch: 27, Train_Loss: 0.21052509546279907, Test_Loss: 0.4417012333869934 *\n",
      "Epoch: 27, Train_Loss: 0.18825113773345947, Test_Loss: 0.35112738609313965 *\n",
      "Epoch: 27, Train_Loss: 0.18851704895496368, Test_Loss: 0.26277291774749756 *\n",
      "Epoch: 27, Train_Loss: 0.35572952032089233, Test_Loss: 0.21509045362472534 *\n",
      "Epoch: 27, Train_Loss: 0.21531939506530762, Test_Loss: 0.48110079765319824\n",
      "Epoch: 27, Train_Loss: 0.1784048080444336, Test_Loss: 1.1870723962783813\n",
      "Epoch: 27, Train_Loss: 0.19552376866340637, Test_Loss: 0.8997413516044617 *\n",
      "Epoch: 27, Train_Loss: 0.20139865577220917, Test_Loss: 1.0771360397338867\n",
      "Epoch: 27, Train_Loss: 0.18395203351974487, Test_Loss: 0.7189783453941345 *\n",
      "Epoch: 27, Train_Loss: 0.18199384212493896, Test_Loss: 0.6768397092819214 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train_Loss: 0.1888805627822876, Test_Loss: 0.9677097797393799\n",
      "Epoch: 27, Train_Loss: 0.19445624947547913, Test_Loss: 0.7069506645202637 *\n",
      "Epoch: 27, Train_Loss: 0.18853604793548584, Test_Loss: 0.3837582468986511 *\n",
      "Epoch: 27, Train_Loss: 0.17786459624767303, Test_Loss: 0.31418710947036743 *\n",
      "Epoch: 27, Train_Loss: 0.18157652020454407, Test_Loss: 0.2278004139661789 *\n",
      "Epoch: 27, Train_Loss: 0.23573686182498932, Test_Loss: 0.19118325412273407 *\n",
      "Epoch: 27, Train_Loss: 0.30690157413482666, Test_Loss: 0.2571147382259369\n",
      "Epoch: 27, Train_Loss: 0.2014530450105667, Test_Loss: 0.45043694972991943\n",
      "Epoch: 27, Train_Loss: 0.3066481351852417, Test_Loss: 0.2520425617694855 *\n",
      "Epoch: 27, Train_Loss: 0.21139656007289886, Test_Loss: 0.2378184199333191 *\n",
      "Epoch: 27, Train_Loss: 0.1949993073940277, Test_Loss: 0.24460723996162415\n",
      "Epoch: 27, Train_Loss: 0.20573534071445465, Test_Loss: 0.28044021129608154\n",
      "Epoch: 27, Train_Loss: 0.18749088048934937, Test_Loss: 0.20001481473445892 *\n",
      "Epoch: 27, Train_Loss: 0.3271307647228241, Test_Loss: 0.2409564107656479\n",
      "Epoch: 27, Train_Loss: 0.2649117410182953, Test_Loss: 0.46232038736343384\n",
      "Epoch: 27, Train_Loss: 0.18494372069835663, Test_Loss: 0.33600884675979614 *\n",
      "Epoch: 27, Train_Loss: 0.18199636042118073, Test_Loss: 0.41976794600486755\n",
      "Epoch: 27, Train_Loss: 0.18551957607269287, Test_Loss: 0.23639944195747375 *\n",
      "Epoch: 27, Train_Loss: 0.18466119468212128, Test_Loss: 0.24641409516334534\n",
      "Epoch: 27, Train_Loss: 0.18221931159496307, Test_Loss: 0.22242794930934906 *\n",
      "Epoch: 27, Train_Loss: 0.5582636594772339, Test_Loss: 0.1897452175617218 *\n",
      "Epoch: 27, Train_Loss: 3.885458469390869, Test_Loss: 0.21210046112537384\n",
      "Epoch: 27, Train_Loss: 0.19505912065505981, Test_Loss: 0.19148562848567963 *\n",
      "Epoch: 27, Train_Loss: 0.18727174401283264, Test_Loss: 0.20316651463508606\n",
      "Epoch: 27, Train_Loss: 0.18533307313919067, Test_Loss: 0.1848987489938736 *\n",
      "Epoch: 27, Train_Loss: 0.1775512397289276, Test_Loss: 0.30092766880989075\n",
      "Epoch: 27, Train_Loss: 0.1812148094177246, Test_Loss: 0.6030113697052002\n",
      "Epoch: 27, Train_Loss: 0.18167325854301453, Test_Loss: 0.2624412477016449 *\n",
      "Epoch: 27, Train_Loss: 0.17912428081035614, Test_Loss: 0.46790409088134766\n",
      "Epoch: 27, Train_Loss: 0.18386109173297882, Test_Loss: 0.24341005086898804 *\n",
      "Epoch: 27, Train_Loss: 0.18094933032989502, Test_Loss: 0.24600467085838318\n",
      "Epoch: 27, Train_Loss: 0.20345313847064972, Test_Loss: 0.2480301856994629\n",
      "Model saved at location save_new\\model.ckpt at epoch 27\n",
      "Epoch: 27, Train_Loss: 0.21132604777812958, Test_Loss: 0.24351269006729126 *\n",
      "Epoch: 27, Train_Loss: 0.21838034689426422, Test_Loss: 0.2402079701423645 *\n",
      "Epoch: 27, Train_Loss: 0.2040717899799347, Test_Loss: 5.154170036315918\n",
      "Epoch: 27, Train_Loss: 0.18049685657024384, Test_Loss: 1.2302488088607788 *\n",
      "Epoch: 27, Train_Loss: 0.3265807032585144, Test_Loss: 0.2232336401939392 *\n",
      "Epoch: 27, Train_Loss: 0.2802732586860657, Test_Loss: 0.2188492715358734 *\n",
      "Epoch: 27, Train_Loss: 0.33424580097198486, Test_Loss: 0.20589029788970947 *\n",
      "Epoch: 27, Train_Loss: 0.31058287620544434, Test_Loss: 0.19927605986595154 *\n",
      "Epoch: 27, Train_Loss: 0.17939351499080658, Test_Loss: 0.20485694706439972\n",
      "Epoch: 27, Train_Loss: 0.17696458101272583, Test_Loss: 0.26017963886260986\n",
      "Epoch: 27, Train_Loss: 0.17922332882881165, Test_Loss: 0.21023207902908325 *\n",
      "Epoch: 27, Train_Loss: 0.1814553141593933, Test_Loss: 0.18287138640880585 *\n",
      "Epoch: 27, Train_Loss: 0.18538498878479004, Test_Loss: 0.23218496143817902\n",
      "Epoch: 27, Train_Loss: 0.19928202033042908, Test_Loss: 0.2509949803352356\n",
      "Epoch: 27, Train_Loss: 0.18372254073619843, Test_Loss: 0.38029804825782776\n",
      "Epoch: 27, Train_Loss: 0.17701268196105957, Test_Loss: 0.20405606925487518 *\n",
      "Epoch: 27, Train_Loss: 0.19191497564315796, Test_Loss: 0.21169808506965637\n",
      "Epoch: 27, Train_Loss: 0.21439781785011292, Test_Loss: 0.18477904796600342 *\n",
      "Epoch: 27, Train_Loss: 0.2964807152748108, Test_Loss: 0.19450867176055908\n",
      "Epoch: 27, Train_Loss: 0.2279074788093567, Test_Loss: 0.21635282039642334\n",
      "Epoch: 27, Train_Loss: 0.24297627806663513, Test_Loss: 0.2405078113079071\n",
      "Epoch: 27, Train_Loss: 0.21245786547660828, Test_Loss: 0.19482599198818207 *\n",
      "Epoch: 27, Train_Loss: 0.27801594138145447, Test_Loss: 0.18131135404109955 *\n",
      "Epoch: 27, Train_Loss: 0.24712803959846497, Test_Loss: 0.18635858595371246\n",
      "Epoch: 27, Train_Loss: 0.2401701807975769, Test_Loss: 0.19098584353923798\n",
      "Epoch: 27, Train_Loss: 0.21112841367721558, Test_Loss: 0.2210800051689148\n",
      "Epoch: 27, Train_Loss: 0.32629871368408203, Test_Loss: 0.31714335083961487\n",
      "Epoch: 27, Train_Loss: 0.1890275478363037, Test_Loss: 0.1816544085741043 *\n",
      "Epoch: 27, Train_Loss: 0.19521886110305786, Test_Loss: 0.19005580246448517\n",
      "Epoch: 27, Train_Loss: 2.2783398628234863, Test_Loss: 0.250935822725296\n",
      "Epoch: 27, Train_Loss: 0.5707108378410339, Test_Loss: 0.2278987169265747 *\n",
      "Epoch: 27, Train_Loss: 0.19776180386543274, Test_Loss: 0.1792147159576416 *\n",
      "Epoch: 27, Train_Loss: 0.19975237548351288, Test_Loss: 0.43197357654571533\n",
      "Epoch: 27, Train_Loss: 0.19559726119041443, Test_Loss: 0.32127416133880615 *\n",
      "Epoch: 27, Train_Loss: 0.20879624783992767, Test_Loss: 4.175299167633057\n",
      "Epoch: 27, Train_Loss: 0.19628901779651642, Test_Loss: 0.21454370021820068 *\n",
      "Epoch: 27, Train_Loss: 0.2287113070487976, Test_Loss: 0.21244917809963226 *\n",
      "Epoch: 27, Train_Loss: 0.283349871635437, Test_Loss: 0.2435188591480255\n",
      "Epoch: 27, Train_Loss: 0.22189539670944214, Test_Loss: 0.22429725527763367 *\n",
      "Epoch: 27, Train_Loss: 0.2008482962846756, Test_Loss: 0.2235325127840042 *\n",
      "Epoch: 27, Train_Loss: 0.18092800676822662, Test_Loss: 0.18494167923927307 *\n",
      "Epoch: 27, Train_Loss: 0.18924346566200256, Test_Loss: 0.22871708869934082\n",
      "Epoch: 27, Train_Loss: 0.1807725578546524, Test_Loss: 0.21245864033699036 *\n",
      "Epoch: 27, Train_Loss: 0.18931680917739868, Test_Loss: 0.19977113604545593 *\n",
      "Epoch: 27, Train_Loss: 0.22487184405326843, Test_Loss: 0.21942514181137085\n",
      "Epoch: 27, Train_Loss: 0.19454741477966309, Test_Loss: 0.300403892993927\n",
      "Epoch: 27, Train_Loss: 0.1779506951570511, Test_Loss: 0.26367419958114624 *\n",
      "Epoch: 27, Train_Loss: 0.18301889300346375, Test_Loss: 0.2572658360004425 *\n",
      "Epoch: 27, Train_Loss: 0.18810983002185822, Test_Loss: 0.2565789222717285 *\n",
      "Epoch: 27, Train_Loss: 0.19477960467338562, Test_Loss: 0.2295982837677002 *\n",
      "Epoch: 27, Train_Loss: 0.1815512776374817, Test_Loss: 0.19272297620773315 *\n",
      "Epoch: 27, Train_Loss: 0.17695972323417664, Test_Loss: 0.2760079801082611\n",
      "Epoch: 27, Train_Loss: 0.17747445404529572, Test_Loss: 0.43721747398376465\n",
      "Epoch: 27, Train_Loss: 0.1778598427772522, Test_Loss: 0.22367170453071594 *\n",
      "Epoch: 27, Train_Loss: 0.1964937299489975, Test_Loss: 0.2861151099205017\n",
      "Epoch: 27, Train_Loss: 0.178957998752594, Test_Loss: 0.2558867037296295 *\n",
      "Epoch: 27, Train_Loss: 0.1849156618118286, Test_Loss: 0.26885759830474854\n",
      "Epoch: 27, Train_Loss: 0.19416089355945587, Test_Loss: 0.2101341038942337 *\n",
      "Epoch: 27, Train_Loss: 0.17999239265918732, Test_Loss: 0.20440427958965302 *\n",
      "Epoch: 27, Train_Loss: 0.1774078905582428, Test_Loss: 0.2777163088321686\n",
      "Epoch: 27, Train_Loss: 0.19073334336280823, Test_Loss: 0.19774872064590454 *\n",
      "Epoch: 27, Train_Loss: 0.19700080156326294, Test_Loss: 0.19828195869922638\n",
      "Epoch: 27, Train_Loss: 0.18651987612247467, Test_Loss: 0.20535191893577576\n",
      "Epoch: 27, Train_Loss: 0.1858515590429306, Test_Loss: 0.18727989494800568 *\n",
      "Epoch: 27, Train_Loss: 0.19929252564907074, Test_Loss: 0.22720405459403992\n",
      "Epoch: 27, Train_Loss: 0.212120920419693, Test_Loss: 0.2249085158109665 *\n",
      "Epoch: 27, Train_Loss: 0.20811831951141357, Test_Loss: 0.40844056010246277\n",
      "Epoch: 27, Train_Loss: 0.2011817991733551, Test_Loss: 0.3350645899772644 *\n",
      "Epoch: 27, Train_Loss: 0.2322327345609665, Test_Loss: 0.2789428234100342 *\n",
      "Epoch: 27, Train_Loss: 0.2088654637336731, Test_Loss: 0.23805251717567444 *\n",
      "Epoch: 27, Train_Loss: 0.18077245354652405, Test_Loss: 0.20229455828666687 *\n",
      "Epoch: 27, Train_Loss: 0.1811346560716629, Test_Loss: 0.18055810034275055 *\n",
      "Epoch: 27, Train_Loss: 0.18284715712070465, Test_Loss: 0.23036736249923706\n",
      "Epoch: 27, Train_Loss: 0.19184671342372894, Test_Loss: 0.4067692756652832\n",
      "Epoch: 27, Train_Loss: 0.2345651388168335, Test_Loss: 0.32376694679260254 *\n",
      "Epoch: 27, Train_Loss: 0.227898970246315, Test_Loss: 0.2860216200351715 *\n",
      "Epoch: 27, Train_Loss: 0.18920360505580902, Test_Loss: 0.24547845125198364 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train_Loss: 0.17655903100967407, Test_Loss: 0.18658094108104706 *\n",
      "Epoch: 27, Train_Loss: 0.22158131003379822, Test_Loss: 0.18474800884723663 *\n",
      "Epoch: 27, Train_Loss: 0.19158367812633514, Test_Loss: 0.18485362827777863\n",
      "Epoch: 27, Train_Loss: 0.1823877990245819, Test_Loss: 0.19265301525592804\n",
      "Epoch: 27, Train_Loss: 0.18916389346122742, Test_Loss: 0.1944153904914856\n",
      "Epoch: 27, Train_Loss: 0.19523316621780396, Test_Loss: 0.19883209466934204\n",
      "Epoch: 27, Train_Loss: 0.27606964111328125, Test_Loss: 0.18050608038902283 *\n",
      "Epoch: 27, Train_Loss: 0.2213437557220459, Test_Loss: 0.2704787850379944\n",
      "Epoch: 27, Train_Loss: 0.2088562250137329, Test_Loss: 0.5315170288085938\n",
      "Epoch: 27, Train_Loss: 0.1889851987361908, Test_Loss: 0.2570582628250122 *\n",
      "Epoch: 27, Train_Loss: 0.192490354180336, Test_Loss: 0.385555624961853\n",
      "Epoch: 27, Train_Loss: 0.18994012475013733, Test_Loss: 0.2172558456659317 *\n",
      "Epoch: 27, Train_Loss: 0.17997972667217255, Test_Loss: 0.21853427588939667\n",
      "Epoch: 27, Train_Loss: 0.19151771068572998, Test_Loss: 0.21877577900886536\n",
      "Epoch: 27, Train_Loss: 0.18109096586704254, Test_Loss: 0.21189972758293152 *\n",
      "Epoch: 27, Train_Loss: 0.1908137947320938, Test_Loss: 0.2206551432609558\n",
      "Epoch: 27, Train_Loss: 0.26275670528411865, Test_Loss: 6.63441276550293\n",
      "Epoch: 27, Train_Loss: 0.18355169892311096, Test_Loss: 0.4754970073699951 *\n",
      "Epoch: 27, Train_Loss: 0.2279369980096817, Test_Loss: 0.19325457513332367 *\n",
      "Epoch: 27, Train_Loss: 0.19295473396778107, Test_Loss: 0.19500887393951416\n",
      "Epoch: 27, Train_Loss: 0.18638043105602264, Test_Loss: 0.19228056073188782 *\n",
      "Epoch: 27, Train_Loss: 0.27404215931892395, Test_Loss: 0.18638743460178375 *\n",
      "Epoch: 27, Train_Loss: 0.34077632427215576, Test_Loss: 0.21764753758907318\n",
      "Model saved at location save_new\\model.ckpt at epoch 27\n",
      "Epoch: 27, Train_Loss: 0.17941401898860931, Test_Loss: 0.24101266264915466\n",
      "Epoch: 27, Train_Loss: 0.20480284094810486, Test_Loss: 0.21291622519493103 *\n",
      "Epoch: 27, Train_Loss: 0.17567670345306396, Test_Loss: 0.1806144118309021 *\n",
      "Epoch: 27, Train_Loss: 0.1759602278470993, Test_Loss: 0.2622303366661072\n",
      "Epoch: 27, Train_Loss: 0.18059980869293213, Test_Loss: 0.43143951892852783\n",
      "Epoch: 27, Train_Loss: 0.17864364385604858, Test_Loss: 0.27355316281318665 *\n",
      "Epoch: 27, Train_Loss: 0.1782536655664444, Test_Loss: 0.19555604457855225 *\n",
      "Epoch: 27, Train_Loss: 0.1894892305135727, Test_Loss: 0.2154999077320099\n",
      "Epoch: 27, Train_Loss: 0.1836574524641037, Test_Loss: 0.18238167464733124 *\n",
      "Epoch: 27, Train_Loss: 0.1822272092103958, Test_Loss: 0.18924659490585327\n",
      "Epoch: 27, Train_Loss: 0.1845518797636032, Test_Loss: 0.18061907589435577 *\n",
      "Epoch: 27, Train_Loss: 0.18134458363056183, Test_Loss: 0.21291795372962952\n",
      "Epoch: 27, Train_Loss: 0.17678570747375488, Test_Loss: 0.18678776919841766 *\n",
      "Epoch: 27, Train_Loss: 0.1761838048696518, Test_Loss: 0.1913493573665619\n",
      "Epoch: 27, Train_Loss: 0.188272163271904, Test_Loss: 0.18027323484420776 *\n",
      "Epoch: 27, Train_Loss: 0.1843932569026947, Test_Loss: 0.18196187913417816\n",
      "Epoch: 27, Train_Loss: 0.19535745680332184, Test_Loss: 0.18719489872455597\n",
      "Epoch: 27, Train_Loss: 0.18038292229175568, Test_Loss: 0.2051996886730194\n",
      "Epoch: 27, Train_Loss: 0.18892502784729004, Test_Loss: 0.1763133853673935 *\n",
      "Epoch: 27, Train_Loss: 0.19266855716705322, Test_Loss: 0.19623208045959473\n",
      "Epoch: 27, Train_Loss: 0.20124581456184387, Test_Loss: 0.1941717118024826 *\n",
      "Epoch: 27, Train_Loss: 0.18361921608448029, Test_Loss: 0.18692366778850555 *\n",
      "Epoch: 27, Train_Loss: 0.2000531554222107, Test_Loss: 0.17727546393871307 *\n",
      "Epoch: 27, Train_Loss: 0.17619295418262482, Test_Loss: 0.32254427671432495\n",
      "Epoch: 27, Train_Loss: 0.19072900712490082, Test_Loss: 1.2693073749542236\n",
      "Epoch: 27, Train_Loss: 0.19315731525421143, Test_Loss: 4.601790428161621\n",
      "Epoch: 27, Train_Loss: 0.19950588047504425, Test_Loss: 0.19425362348556519 *\n",
      "Epoch: 27, Train_Loss: 1.9952354431152344, Test_Loss: 0.1765325665473938 *\n",
      "Epoch: 27, Train_Loss: 3.5675113201141357, Test_Loss: 0.1943974494934082\n",
      "Epoch: 27, Train_Loss: 0.21560829877853394, Test_Loss: 0.18473531305789948 *\n",
      "Epoch: 27, Train_Loss: 0.19071060419082642, Test_Loss: 0.19388265907764435\n",
      "Epoch: 27, Train_Loss: 0.18783307075500488, Test_Loss: 0.2070879340171814\n",
      "Epoch: 27, Train_Loss: 0.26837998628616333, Test_Loss: 0.29303401708602905\n",
      "Epoch: 27, Train_Loss: 0.21401628851890564, Test_Loss: 0.19636598229408264 *\n",
      "Epoch: 27, Train_Loss: 0.18979021906852722, Test_Loss: 0.18207316100597382 *\n",
      "Epoch: 27, Train_Loss: 0.17516490817070007, Test_Loss: 0.2047041952610016\n",
      "Epoch: 27, Train_Loss: 0.24995017051696777, Test_Loss: 0.19693882763385773 *\n",
      "Epoch: 27, Train_Loss: 0.19678014516830444, Test_Loss: 0.18244528770446777 *\n",
      "Epoch: 27, Train_Loss: 0.1858251988887787, Test_Loss: 0.23186065256595612\n",
      "Epoch: 27, Train_Loss: 0.5700933933258057, Test_Loss: 0.21409767866134644 *\n",
      "Epoch: 27, Train_Loss: 0.40106338262557983, Test_Loss: 0.25675904750823975\n",
      "Epoch: 27, Train_Loss: 0.7680487632751465, Test_Loss: 0.21221132576465607 *\n",
      "Epoch: 27, Train_Loss: 0.24574577808380127, Test_Loss: 0.2044934183359146 *\n",
      "Epoch: 27, Train_Loss: 0.4474518299102783, Test_Loss: 0.2385946810245514\n",
      "Epoch: 27, Train_Loss: 1.030518651008606, Test_Loss: 0.18821494281291962 *\n",
      "Epoch: 27, Train_Loss: 0.6646230220794678, Test_Loss: 0.26581114530563354\n",
      "Epoch: 27, Train_Loss: 0.17727330327033997, Test_Loss: 0.2089424431324005 *\n",
      "Epoch: 27, Train_Loss: 0.19217023253440857, Test_Loss: 0.21389050781726837\n",
      "Epoch: 27, Train_Loss: 0.6238056421279907, Test_Loss: 0.20511776208877563 *\n",
      "Epoch: 27, Train_Loss: 0.41811010241508484, Test_Loss: 0.19676156342029572 *\n",
      "Epoch: 27, Train_Loss: 0.6218774914741516, Test_Loss: 0.3083796501159668\n",
      "Epoch: 27, Train_Loss: 0.19109337031841278, Test_Loss: 0.2345278263092041 *\n",
      "Epoch: 27, Train_Loss: 0.18877927958965302, Test_Loss: 0.1944018304347992 *\n",
      "Epoch: 27, Train_Loss: 0.41443803906440735, Test_Loss: 0.22428037226200104\n",
      "Epoch: 28, Train_Loss: 0.35666340589523315, Test_Loss: 0.18312546610832214 *\n",
      "Epoch: 28, Train_Loss: 0.24002128839492798, Test_Loss: 0.2127453237771988\n",
      "Epoch: 28, Train_Loss: 0.24922794103622437, Test_Loss: 0.2212023288011551\n",
      "Epoch: 28, Train_Loss: 0.20943276584148407, Test_Loss: 0.4090059995651245\n",
      "Epoch: 28, Train_Loss: 0.20782043039798737, Test_Loss: 0.31592994928359985 *\n",
      "Epoch: 28, Train_Loss: 0.3096623122692108, Test_Loss: 0.2405458688735962 *\n",
      "Epoch: 28, Train_Loss: 0.24799573421478271, Test_Loss: 0.26484233140945435\n",
      "Epoch: 28, Train_Loss: 0.22815874218940735, Test_Loss: 0.2084144651889801 *\n",
      "Epoch: 28, Train_Loss: 0.2712614834308624, Test_Loss: 0.17881470918655396 *\n",
      "Epoch: 28, Train_Loss: 0.2330392450094223, Test_Loss: 0.21059773862361908\n",
      "Epoch: 28, Train_Loss: 0.27519136667251587, Test_Loss: 0.2579858601093292\n",
      "Epoch: 28, Train_Loss: 0.25388839840888977, Test_Loss: 0.3689546585083008\n",
      "Epoch: 28, Train_Loss: 0.36355310678482056, Test_Loss: 0.2401515245437622 *\n",
      "Epoch: 28, Train_Loss: 0.2089184820652008, Test_Loss: 0.241509348154068\n",
      "Epoch: 28, Train_Loss: 0.19693472981452942, Test_Loss: 0.19229663908481598 *\n",
      "Epoch: 28, Train_Loss: 0.21129441261291504, Test_Loss: 0.18075206875801086 *\n",
      "Epoch: 28, Train_Loss: 0.2023538500070572, Test_Loss: 0.18783973157405853\n",
      "Epoch: 28, Train_Loss: 0.17799530923366547, Test_Loss: 0.20681865513324738\n",
      "Epoch: 28, Train_Loss: 0.1751670390367508, Test_Loss: 0.18278640508651733 *\n",
      "Epoch: 28, Train_Loss: 0.17502251267433167, Test_Loss: 0.20423626899719238\n",
      "Epoch: 28, Train_Loss: 0.18194639682769775, Test_Loss: 0.2036292850971222 *\n",
      "Epoch: 28, Train_Loss: 0.18611671030521393, Test_Loss: 0.3611809015274048\n",
      "Epoch: 28, Train_Loss: 0.1889786571264267, Test_Loss: 0.5137147307395935\n",
      "Epoch: 28, Train_Loss: 0.18216699361801147, Test_Loss: 0.5163770914077759\n",
      "Epoch: 28, Train_Loss: 0.260966956615448, Test_Loss: 0.3566455543041229 *\n",
      "Epoch: 28, Train_Loss: 0.29943621158599854, Test_Loss: 0.3116185963153839 *\n",
      "Epoch: 28, Train_Loss: 0.2564551830291748, Test_Loss: 0.31066763401031494 *\n",
      "Epoch: 28, Train_Loss: 0.19663655757904053, Test_Loss: 0.31281664967536926\n",
      "Epoch: 28, Train_Loss: 0.2180713266134262, Test_Loss: 0.293893426656723 *\n",
      "Epoch: 28, Train_Loss: 0.2547409236431122, Test_Loss: 0.4212353229522705\n",
      "Epoch: 28, Train_Loss: 0.33228516578674316, Test_Loss: 6.2428297996521\n",
      "Epoch: 28, Train_Loss: 0.25826185941696167, Test_Loss: 0.24986563622951508 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train_Loss: 0.27370813488960266, Test_Loss: 0.22098605334758759 *\n",
      "Epoch: 28, Train_Loss: 0.22706183791160583, Test_Loss: 0.23468957841396332\n",
      "Epoch: 28, Train_Loss: 0.2526092529296875, Test_Loss: 0.20396828651428223 *\n",
      "Epoch: 28, Train_Loss: 0.3375924229621887, Test_Loss: 0.1834934651851654 *\n",
      "Epoch: 28, Train_Loss: 0.1914919763803482, Test_Loss: 0.24238261580467224\n",
      "Epoch: 28, Train_Loss: 0.2057012915611267, Test_Loss: 0.24775001406669617\n",
      "Epoch: 28, Train_Loss: 0.5700148940086365, Test_Loss: 0.18834343552589417 *\n",
      "Epoch: 28, Train_Loss: 0.4632118344306946, Test_Loss: 0.191350057721138\n",
      "Epoch: 28, Train_Loss: 0.2299124151468277, Test_Loss: 0.21572363376617432\n",
      "Epoch: 28, Train_Loss: 0.2118982970714569, Test_Loss: 0.405271977186203\n",
      "Epoch: 28, Train_Loss: 0.22288978099822998, Test_Loss: 0.2179916799068451 *\n",
      "Epoch: 28, Train_Loss: 0.18705636262893677, Test_Loss: 0.2074921578168869 *\n",
      "Epoch: 28, Train_Loss: 0.4411489963531494, Test_Loss: 0.2228514552116394\n",
      "Epoch: 28, Train_Loss: 0.19675050675868988, Test_Loss: 0.20398002862930298 *\n",
      "Epoch: 28, Train_Loss: 0.23774763941764832, Test_Loss: 0.2109372913837433\n",
      "Epoch: 28, Train_Loss: 0.29461362957954407, Test_Loss: 0.24976472556591034\n",
      "Epoch: 28, Train_Loss: 0.2074417620897293, Test_Loss: 0.23741105198860168 *\n",
      "Epoch: 28, Train_Loss: 0.20059409737586975, Test_Loss: 0.20672376453876495 *\n",
      "Epoch: 28, Train_Loss: 0.2511167526245117, Test_Loss: 0.21589061617851257\n",
      "Epoch: 28, Train_Loss: 0.3404430150985718, Test_Loss: 0.18133953213691711 *\n",
      "Epoch: 28, Train_Loss: 0.21779261529445648, Test_Loss: 0.20802538096904755\n",
      "Epoch: 28, Train_Loss: 0.22626881301403046, Test_Loss: 0.21955840289592743\n",
      "Epoch: 28, Train_Loss: 0.21475060284137726, Test_Loss: 0.21118520200252533 *\n",
      "Epoch: 28, Train_Loss: 0.21381451189517975, Test_Loss: 0.17593035101890564 *\n",
      "Epoch: 28, Train_Loss: 0.2164875715970993, Test_Loss: 0.1942691206932068\n",
      "Epoch: 28, Train_Loss: 0.19735701382160187, Test_Loss: 0.2229907065629959\n",
      "Epoch: 28, Train_Loss: 0.18308892846107483, Test_Loss: 0.19053354859352112 *\n",
      "Epoch: 28, Train_Loss: 0.22486402094364166, Test_Loss: 0.1816290318965912 *\n",
      "Epoch: 28, Train_Loss: 0.4359888434410095, Test_Loss: 0.3285212516784668\n",
      "Epoch: 28, Train_Loss: 0.4470812678337097, Test_Loss: 2.4191129207611084\n",
      "Epoch: 28, Train_Loss: 0.5351906418800354, Test_Loss: 3.1533472537994385\n",
      "Epoch: 28, Train_Loss: 0.5044676661491394, Test_Loss: 0.19250082969665527 *\n",
      "Epoch: 28, Train_Loss: 0.3893044888973236, Test_Loss: 0.17967794835567474 *\n",
      "Epoch: 28, Train_Loss: 0.2727080285549164, Test_Loss: 0.21072179079055786\n",
      "Epoch: 28, Train_Loss: 0.23857071995735168, Test_Loss: 0.1934681534767151 *\n",
      "Epoch: 28, Train_Loss: 0.18431459367275238, Test_Loss: 0.19172917306423187 *\n",
      "Epoch: 28, Train_Loss: 0.18282799422740936, Test_Loss: 0.24165022373199463\n",
      "Epoch: 28, Train_Loss: 0.2054973989725113, Test_Loss: 0.2552352547645569\n",
      "Epoch: 28, Train_Loss: 0.42395082116127014, Test_Loss: 0.18342159688472748 *\n",
      "Epoch: 28, Train_Loss: 0.49068427085876465, Test_Loss: 0.19972558319568634\n",
      "Epoch: 28, Train_Loss: 0.5784509181976318, Test_Loss: 0.20076115429401398\n",
      "Epoch: 28, Train_Loss: 0.8983652591705322, Test_Loss: 0.25050368905067444\n",
      "Epoch: 28, Train_Loss: 0.5173536539077759, Test_Loss: 0.18883566558361053 *\n",
      "Epoch: 28, Train_Loss: 0.38497114181518555, Test_Loss: 0.2726728916168213\n",
      "Epoch: 28, Train_Loss: 0.1840016096830368, Test_Loss: 0.2852398753166199\n",
      "Epoch: 28, Train_Loss: 0.1831827312707901, Test_Loss: 0.2826507091522217 *\n",
      "Epoch: 28, Train_Loss: 0.3852272629737854, Test_Loss: 0.18589986860752106 *\n",
      "Epoch: 28, Train_Loss: 0.5133488774299622, Test_Loss: 0.21360266208648682\n",
      "Epoch: 28, Train_Loss: 0.39628374576568604, Test_Loss: 0.26880860328674316\n",
      "Epoch: 28, Train_Loss: 0.21910077333450317, Test_Loss: 0.23497629165649414 *\n",
      "Epoch: 28, Train_Loss: 0.1930224597454071, Test_Loss: 0.3182169198989868\n",
      "Epoch: 28, Train_Loss: 0.2534382939338684, Test_Loss: 0.30480068922042847 *\n",
      "Epoch: 28, Train_Loss: 0.4429885149002075, Test_Loss: 0.23484812676906586 *\n",
      "Epoch: 28, Train_Loss: 0.3359742760658264, Test_Loss: 0.25847285985946655\n",
      "Epoch: 28, Train_Loss: 0.23808486759662628, Test_Loss: 0.2571645975112915 *\n",
      "Epoch: 28, Train_Loss: 0.3594527840614319, Test_Loss: 0.3511224389076233\n",
      "Epoch: 28, Train_Loss: 0.20193219184875488, Test_Loss: 0.2830467224121094 *\n",
      "Epoch: 28, Train_Loss: 0.17761290073394775, Test_Loss: 0.23440289497375488 *\n",
      "Epoch: 28, Train_Loss: 0.19994238018989563, Test_Loss: 0.25926733016967773\n",
      "Epoch: 28, Train_Loss: 0.18977956473827362, Test_Loss: 0.19037939608097076 *\n",
      "Epoch: 28, Train_Loss: 0.21671313047409058, Test_Loss: 0.19522599875926971\n",
      "Epoch: 28, Train_Loss: 0.2638598680496216, Test_Loss: 0.2290542721748352\n",
      "Epoch: 28, Train_Loss: 6.798567771911621, Test_Loss: 0.2619619369506836\n",
      "Epoch: 28, Train_Loss: 8.972240447998047, Test_Loss: 0.25754326581954956 *\n",
      "Epoch: 28, Train_Loss: 0.8820074796676636, Test_Loss: 0.20409706234931946 *\n",
      "Epoch: 28, Train_Loss: 0.954160749912262, Test_Loss: 0.2692124843597412\n",
      "Epoch: 28, Train_Loss: 0.5091332197189331, Test_Loss: 0.2260986864566803 *\n",
      "Epoch: 28, Train_Loss: 0.2289634644985199, Test_Loss: 0.19034530222415924 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 28\n",
      "Epoch: 28, Train_Loss: 0.6447195410728455, Test_Loss: 0.29444998502731323\n",
      "Epoch: 28, Train_Loss: 3.571115016937256, Test_Loss: 0.2952733039855957\n",
      "Epoch: 28, Train_Loss: 1.5184000730514526, Test_Loss: 0.48608070611953735\n",
      "Epoch: 28, Train_Loss: 0.25666362047195435, Test_Loss: 0.2486017942428589 *\n",
      "Epoch: 28, Train_Loss: 2.398569345474243, Test_Loss: 0.22456508874893188 *\n",
      "Epoch: 28, Train_Loss: 2.7815918922424316, Test_Loss: 0.2774471938610077\n",
      "Epoch: 28, Train_Loss: 0.4534481167793274, Test_Loss: 0.2297777533531189 *\n",
      "Epoch: 28, Train_Loss: 0.20674464106559753, Test_Loss: 0.30516523122787476\n",
      "Epoch: 28, Train_Loss: 0.22506166994571686, Test_Loss: 0.20610076189041138 *\n",
      "Epoch: 28, Train_Loss: 0.24984616041183472, Test_Loss: 0.22403544187545776\n",
      "Epoch: 28, Train_Loss: 0.19775715470314026, Test_Loss: 0.18178780376911163 *\n",
      "Epoch: 28, Train_Loss: 0.18429900705814362, Test_Loss: 0.25598686933517456\n",
      "Epoch: 28, Train_Loss: 0.1792517751455307, Test_Loss: 0.5316945910453796\n",
      "Epoch: 28, Train_Loss: 0.17517967522144318, Test_Loss: 0.3929203748703003 *\n",
      "Epoch: 28, Train_Loss: 0.17831970751285553, Test_Loss: 0.43168893456459045\n",
      "Epoch: 28, Train_Loss: 0.1941780000925064, Test_Loss: 0.2746649980545044 *\n",
      "Epoch: 28, Train_Loss: 0.196110337972641, Test_Loss: 0.17577427625656128 *\n",
      "Epoch: 28, Train_Loss: 0.220660001039505, Test_Loss: 0.1754641830921173 *\n",
      "Epoch: 28, Train_Loss: 0.2975344657897949, Test_Loss: 0.17692051827907562\n",
      "Epoch: 28, Train_Loss: 0.2196289747953415, Test_Loss: 0.22104740142822266\n",
      "Epoch: 28, Train_Loss: 0.1937832236289978, Test_Loss: 1.889630913734436\n",
      "Epoch: 28, Train_Loss: 0.1812281310558319, Test_Loss: 6.98279333114624\n",
      "Epoch: 28, Train_Loss: 0.19528014957904816, Test_Loss: 0.34018218517303467 *\n",
      "Epoch: 28, Train_Loss: 0.18048986792564392, Test_Loss: 0.7457638382911682\n",
      "Epoch: 28, Train_Loss: 0.17976634204387665, Test_Loss: 0.5882679224014282 *\n",
      "Epoch: 28, Train_Loss: 0.17421044409275055, Test_Loss: 0.3281301259994507 *\n",
      "Epoch: 28, Train_Loss: 0.17439129948616028, Test_Loss: 0.2903591990470886 *\n",
      "Epoch: 28, Train_Loss: 0.18268610537052155, Test_Loss: 0.942602276802063\n",
      "Epoch: 28, Train_Loss: 0.17560727894306183, Test_Loss: 0.8200470209121704 *\n",
      "Epoch: 28, Train_Loss: 0.1743735373020172, Test_Loss: 0.39415353536605835 *\n",
      "Epoch: 28, Train_Loss: 0.1789925992488861, Test_Loss: 1.169449806213379\n",
      "Epoch: 28, Train_Loss: 0.22044800221920013, Test_Loss: 0.5279229879379272 *\n",
      "Epoch: 28, Train_Loss: 0.2975579500198364, Test_Loss: 1.587741494178772\n",
      "Epoch: 28, Train_Loss: 0.29754623770713806, Test_Loss: 0.5988104939460754 *\n",
      "Epoch: 28, Train_Loss: 0.1851755976676941, Test_Loss: 0.6810023188591003\n",
      "Epoch: 28, Train_Loss: 0.3187832832336426, Test_Loss: 0.5164965391159058 *\n",
      "Epoch: 28, Train_Loss: 5.108912944793701, Test_Loss: 0.23682841658592224 *\n",
      "Epoch: 28, Train_Loss: 0.6659252643585205, Test_Loss: 0.22345224022865295 *\n",
      "Epoch: 28, Train_Loss: 0.1873963326215744, Test_Loss: 0.18438482284545898 *\n",
      "Epoch: 28, Train_Loss: 0.19951532781124115, Test_Loss: 0.4698180556297302\n",
      "Epoch: 28, Train_Loss: 0.23601150512695312, Test_Loss: 0.1989060789346695 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train_Loss: 0.18738622963428497, Test_Loss: 0.838814914226532\n",
      "Epoch: 28, Train_Loss: 0.20668554306030273, Test_Loss: 0.1832367181777954 *\n",
      "Epoch: 28, Train_Loss: 0.25841253995895386, Test_Loss: 0.2826862633228302\n",
      "Epoch: 28, Train_Loss: 0.2742269039154053, Test_Loss: 0.3469793200492859\n",
      "Epoch: 28, Train_Loss: 0.2814618945121765, Test_Loss: 0.21139152348041534 *\n",
      "Epoch: 28, Train_Loss: 0.24404770135879517, Test_Loss: 0.18359965085983276 *\n",
      "Epoch: 28, Train_Loss: 0.182769313454628, Test_Loss: 0.25782284140586853\n",
      "Epoch: 28, Train_Loss: 0.208195760846138, Test_Loss: 0.2428228110074997 *\n",
      "Epoch: 28, Train_Loss: 0.1853737086057663, Test_Loss: 0.19439400732517242 *\n",
      "Epoch: 28, Train_Loss: 0.24263811111450195, Test_Loss: 0.2309669554233551\n",
      "Epoch: 28, Train_Loss: 0.1797269880771637, Test_Loss: 0.45945900678634644\n",
      "Epoch: 28, Train_Loss: 0.21215881407260895, Test_Loss: 3.936889171600342\n",
      "Epoch: 28, Train_Loss: 0.21968284249305725, Test_Loss: 2.6521730422973633 *\n",
      "Epoch: 28, Train_Loss: 0.20042921602725983, Test_Loss: 0.20094048976898193 *\n",
      "Epoch: 28, Train_Loss: 0.22246716916561127, Test_Loss: 0.22683817148208618\n",
      "Epoch: 28, Train_Loss: 0.20262356102466583, Test_Loss: 0.20696203410625458 *\n",
      "Epoch: 28, Train_Loss: 0.19924674928188324, Test_Loss: 0.28495264053344727\n",
      "Epoch: 28, Train_Loss: 0.17468740046024323, Test_Loss: 0.199920192360878 *\n",
      "Epoch: 28, Train_Loss: 0.17379973828792572, Test_Loss: 0.2976250946521759\n",
      "Epoch: 28, Train_Loss: 0.29042866826057434, Test_Loss: 0.323499470949173\n",
      "Epoch: 28, Train_Loss: 3.40669584274292, Test_Loss: 0.1770297735929489 *\n",
      "Epoch: 28, Train_Loss: 0.22238703072071075, Test_Loss: 0.20189440250396729\n",
      "Epoch: 28, Train_Loss: 0.18524900078773499, Test_Loss: 0.22030629217624664\n",
      "Epoch: 28, Train_Loss: 0.20395012199878693, Test_Loss: 0.2038693130016327 *\n",
      "Epoch: 28, Train_Loss: 0.17709845304489136, Test_Loss: 0.18937146663665771 *\n",
      "Epoch: 28, Train_Loss: 0.17390134930610657, Test_Loss: 0.25691044330596924\n",
      "Epoch: 28, Train_Loss: 0.18274269998073578, Test_Loss: 0.4018441438674927\n",
      "Epoch: 28, Train_Loss: 0.1777486950159073, Test_Loss: 0.4269942343235016\n",
      "Epoch: 28, Train_Loss: 0.19676953554153442, Test_Loss: 0.24566659331321716 *\n",
      "Epoch: 28, Train_Loss: 0.21246805787086487, Test_Loss: 0.19659091532230377 *\n",
      "Epoch: 28, Train_Loss: 0.1921428143978119, Test_Loss: 0.2635507583618164\n",
      "Epoch: 28, Train_Loss: 0.17513367533683777, Test_Loss: 0.5223984122276306\n",
      "Epoch: 28, Train_Loss: 0.18388620018959045, Test_Loss: 0.411332905292511 *\n",
      "Epoch: 28, Train_Loss: 0.18983742594718933, Test_Loss: 0.3863157629966736 *\n",
      "Epoch: 28, Train_Loss: 0.18826255202293396, Test_Loss: 0.3069492280483246 *\n",
      "Epoch: 28, Train_Loss: 0.1785631775856018, Test_Loss: 0.30889391899108887\n",
      "Epoch: 28, Train_Loss: 0.18907450139522552, Test_Loss: 0.32911843061447144\n",
      "Epoch: 28, Train_Loss: 0.18512985110282898, Test_Loss: 0.3978121876716614\n",
      "Epoch: 28, Train_Loss: 0.18727119266986847, Test_Loss: 0.352941632270813 *\n",
      "Epoch: 28, Train_Loss: 0.18050841987133026, Test_Loss: 0.2544725835323334 *\n",
      "Epoch: 28, Train_Loss: 0.17442986369132996, Test_Loss: 0.22401617467403412 *\n",
      "Epoch: 28, Train_Loss: 0.26768624782562256, Test_Loss: 0.17772932350635529 *\n",
      "Epoch: 28, Train_Loss: 0.35348036885261536, Test_Loss: 0.19703415036201477\n",
      "Epoch: 28, Train_Loss: 0.27458542585372925, Test_Loss: 0.2599712014198303\n",
      "Epoch: 28, Train_Loss: 0.22529441118240356, Test_Loss: 0.2691563069820404\n",
      "Epoch: 28, Train_Loss: 0.19616149365901947, Test_Loss: 0.30276694893836975\n",
      "Epoch: 28, Train_Loss: 0.19506777822971344, Test_Loss: 0.22589176893234253 *\n",
      "Epoch: 28, Train_Loss: 0.19049489498138428, Test_Loss: 0.2344815731048584\n",
      "Epoch: 28, Train_Loss: 0.21060793101787567, Test_Loss: 0.20287804305553436 *\n",
      "Epoch: 28, Train_Loss: 0.2521558701992035, Test_Loss: 0.21828308701515198\n",
      "Epoch: 28, Train_Loss: 0.20987749099731445, Test_Loss: 0.4391653537750244\n",
      "Epoch: 28, Train_Loss: 0.1823580414056778, Test_Loss: 0.2963285744190216 *\n",
      "Epoch: 28, Train_Loss: 0.17432153224945068, Test_Loss: 0.5604348182678223\n",
      "Epoch: 28, Train_Loss: 0.18005572259426117, Test_Loss: 0.23484063148498535 *\n",
      "Epoch: 28, Train_Loss: 0.18007628619670868, Test_Loss: 0.19973964989185333 *\n",
      "Epoch: 28, Train_Loss: 0.17846347391605377, Test_Loss: 0.21780602633953094\n",
      "Epoch: 28, Train_Loss: 0.19088293612003326, Test_Loss: 0.1825224906206131 *\n",
      "Epoch: 28, Train_Loss: 4.160702228546143, Test_Loss: 0.20014557242393494\n",
      "Epoch: 28, Train_Loss: 0.424996942281723, Test_Loss: 0.18668024241924286 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 28\n",
      "Epoch: 28, Train_Loss: 0.17587794363498688, Test_Loss: 0.20301997661590576\n",
      "Epoch: 28, Train_Loss: 0.18241667747497559, Test_Loss: 0.17775841057300568 *\n",
      "Epoch: 28, Train_Loss: 0.176017627120018, Test_Loss: 0.25595590472221375\n",
      "Epoch: 28, Train_Loss: 0.17555992305278778, Test_Loss: 0.41364169120788574\n",
      "Epoch: 28, Train_Loss: 0.17750512063503265, Test_Loss: 0.3893488347530365 *\n",
      "Epoch: 28, Train_Loss: 0.17450319230556488, Test_Loss: 0.4535382390022278\n",
      "Epoch: 28, Train_Loss: 0.17478667199611664, Test_Loss: 0.2421058863401413 *\n",
      "Epoch: 28, Train_Loss: 0.17399188876152039, Test_Loss: 0.23502007126808167 *\n",
      "Epoch: 28, Train_Loss: 0.2011968195438385, Test_Loss: 0.2362174093723297\n",
      "Epoch: 28, Train_Loss: 0.21951384842395782, Test_Loss: 0.23827823996543884\n",
      "Epoch: 28, Train_Loss: 0.23530997335910797, Test_Loss: 0.2409505844116211\n",
      "Epoch: 28, Train_Loss: 0.20696964859962463, Test_Loss: 2.585662364959717\n",
      "Epoch: 28, Train_Loss: 0.17612680792808533, Test_Loss: 3.4313371181488037\n",
      "Epoch: 28, Train_Loss: 0.242127925157547, Test_Loss: 0.2065296769142151 *\n",
      "Epoch: 28, Train_Loss: 0.21187077462673187, Test_Loss: 0.21015745401382446\n",
      "Epoch: 28, Train_Loss: 0.1968054175376892, Test_Loss: 0.189569890499115 *\n",
      "Epoch: 28, Train_Loss: 0.26796430349349976, Test_Loss: 0.18737736344337463 *\n",
      "Epoch: 28, Train_Loss: 0.18601979315280914, Test_Loss: 0.17697298526763916 *\n",
      "Epoch: 28, Train_Loss: 0.17330296337604523, Test_Loss: 0.2543586492538452\n",
      "Epoch: 28, Train_Loss: 0.17800173163414001, Test_Loss: 0.22595854103565216 *\n",
      "Epoch: 28, Train_Loss: 0.17527131736278534, Test_Loss: 0.18187350034713745 *\n",
      "Epoch: 28, Train_Loss: 0.1794084906578064, Test_Loss: 0.20763838291168213\n",
      "Epoch: 28, Train_Loss: 0.19729328155517578, Test_Loss: 0.21709173917770386\n",
      "Epoch: 28, Train_Loss: 0.18858429789543152, Test_Loss: 0.4880106449127197\n",
      "Epoch: 28, Train_Loss: 0.17532870173454285, Test_Loss: 0.207161545753479 *\n",
      "Epoch: 28, Train_Loss: 0.1793183833360672, Test_Loss: 0.22196906805038452\n",
      "Epoch: 28, Train_Loss: 0.19823765754699707, Test_Loss: 0.1901979148387909 *\n",
      "Epoch: 28, Train_Loss: 0.2543538212776184, Test_Loss: 0.19467085599899292\n",
      "Epoch: 28, Train_Loss: 0.2143886238336563, Test_Loss: 0.20129725337028503\n",
      "Epoch: 28, Train_Loss: 0.2205457091331482, Test_Loss: 0.22923609614372253\n",
      "Epoch: 28, Train_Loss: 0.23230311274528503, Test_Loss: 0.3731365203857422\n",
      "Epoch: 28, Train_Loss: 0.2678314745426178, Test_Loss: 0.18385164439678192 *\n",
      "Epoch: 28, Train_Loss: 0.24179378151893616, Test_Loss: 0.19282230734825134\n",
      "Epoch: 28, Train_Loss: 0.21705572307109833, Test_Loss: 0.17959946393966675 *\n",
      "Epoch: 28, Train_Loss: 0.21070435643196106, Test_Loss: 0.27214109897613525\n",
      "Epoch: 28, Train_Loss: 0.21383067965507507, Test_Loss: 0.4684063792228699\n",
      "Epoch: 28, Train_Loss: 0.37435591220855713, Test_Loss: 0.2309100329875946 *\n",
      "Epoch: 28, Train_Loss: 0.18841305375099182, Test_Loss: 0.2301158607006073 *\n",
      "Epoch: 28, Train_Loss: 1.3574618101119995, Test_Loss: 0.23008617758750916 *\n",
      "Epoch: 28, Train_Loss: 1.1460659503936768, Test_Loss: 0.2774888277053833\n",
      "Epoch: 28, Train_Loss: 0.21598732471466064, Test_Loss: 0.2033662050962448 *\n",
      "Epoch: 28, Train_Loss: 0.1826246976852417, Test_Loss: 0.36787134408950806\n",
      "Epoch: 28, Train_Loss: 0.22423051297664642, Test_Loss: 0.3443996012210846 *\n",
      "Epoch: 28, Train_Loss: 0.21319793164730072, Test_Loss: 4.212507724761963\n",
      "Epoch: 28, Train_Loss: 0.17630845308303833, Test_Loss: 1.2745835781097412 *\n",
      "Epoch: 28, Train_Loss: 0.1876668632030487, Test_Loss: 0.18339137732982635 *\n",
      "Epoch: 28, Train_Loss: 0.27385589480400085, Test_Loss: 0.2510077953338623\n",
      "Epoch: 28, Train_Loss: 0.22350426018238068, Test_Loss: 0.2405860722064972 *\n",
      "Epoch: 28, Train_Loss: 0.20673230290412903, Test_Loss: 0.21472030878067017 *\n",
      "Epoch: 28, Train_Loss: 0.18562668561935425, Test_Loss: 0.19106213748455048 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train_Loss: 0.19154492020606995, Test_Loss: 0.2760927081108093\n",
      "Epoch: 28, Train_Loss: 0.1812894195318222, Test_Loss: 0.20963191986083984 *\n",
      "Epoch: 28, Train_Loss: 0.18852658569812775, Test_Loss: 0.18441785871982574 *\n",
      "Epoch: 28, Train_Loss: 0.20648883283138275, Test_Loss: 0.20502515137195587\n",
      "Epoch: 28, Train_Loss: 0.18052151799201965, Test_Loss: 0.1954273283481598 *\n",
      "Epoch: 28, Train_Loss: 0.17987120151519775, Test_Loss: 0.2452070415019989\n",
      "Epoch: 28, Train_Loss: 0.17510220408439636, Test_Loss: 0.20976002514362335 *\n",
      "Epoch: 28, Train_Loss: 0.18291357159614563, Test_Loss: 0.21610113978385925\n",
      "Epoch: 28, Train_Loss: 0.18571104109287262, Test_Loss: 0.2453153133392334\n",
      "Epoch: 28, Train_Loss: 0.18726032972335815, Test_Loss: 0.2000301480293274 *\n",
      "Epoch: 28, Train_Loss: 0.17307579517364502, Test_Loss: 0.20612074434757233\n",
      "Epoch: 28, Train_Loss: 0.17300160229206085, Test_Loss: 0.2769233286380768\n",
      "Epoch: 28, Train_Loss: 0.17337839305400848, Test_Loss: 0.31557610630989075\n",
      "Epoch: 28, Train_Loss: 0.1834796965122223, Test_Loss: 0.3258982002735138\n",
      "Epoch: 28, Train_Loss: 0.17810875177383423, Test_Loss: 0.26561239361763 *\n",
      "Epoch: 28, Train_Loss: 0.18416236340999603, Test_Loss: 0.25967469811439514 *\n",
      "Epoch: 28, Train_Loss: 0.18685108423233032, Test_Loss: 0.2308986485004425 *\n",
      "Epoch: 28, Train_Loss: 0.17450109124183655, Test_Loss: 0.23882535099983215\n",
      "Epoch: 28, Train_Loss: 0.17357173562049866, Test_Loss: 0.2903350591659546\n",
      "Epoch: 28, Train_Loss: 0.18300271034240723, Test_Loss: 0.2594548165798187 *\n",
      "Epoch: 28, Train_Loss: 0.18698380887508392, Test_Loss: 0.2194928228855133 *\n",
      "Epoch: 28, Train_Loss: 0.18350188434123993, Test_Loss: 0.1866500973701477 *\n",
      "Epoch: 28, Train_Loss: 0.18307755887508392, Test_Loss: 0.1857692450284958 *\n",
      "Epoch: 28, Train_Loss: 0.2003224790096283, Test_Loss: 0.2044425755739212\n",
      "Epoch: 28, Train_Loss: 0.19766780734062195, Test_Loss: 0.27915331721305847\n",
      "Epoch: 28, Train_Loss: 0.1978958249092102, Test_Loss: 0.43056297302246094\n",
      "Epoch: 28, Train_Loss: 0.19508442282676697, Test_Loss: 0.38029730319976807 *\n",
      "Epoch: 28, Train_Loss: 0.19863061606884003, Test_Loss: 0.38647907972335815\n",
      "Epoch: 28, Train_Loss: 0.22692137956619263, Test_Loss: 0.23406925797462463 *\n",
      "Epoch: 28, Train_Loss: 0.18133112788200378, Test_Loss: 0.19263015687465668 *\n",
      "Epoch: 28, Train_Loss: 0.17622530460357666, Test_Loss: 0.18931925296783447 *\n",
      "Epoch: 28, Train_Loss: 0.17526820302009583, Test_Loss: 0.21399199962615967\n",
      "Epoch: 28, Train_Loss: 0.1829146444797516, Test_Loss: 0.4826836585998535\n",
      "Epoch: 28, Train_Loss: 0.21120207011699677, Test_Loss: 0.19306333363056183 *\n",
      "Epoch: 28, Train_Loss: 0.22899171710014343, Test_Loss: 0.511907696723938\n",
      "Epoch: 28, Train_Loss: 0.1933891475200653, Test_Loss: 0.2671208083629608 *\n",
      "Epoch: 28, Train_Loss: 0.17312613129615784, Test_Loss: 0.2114710807800293 *\n",
      "Epoch: 28, Train_Loss: 0.20913943648338318, Test_Loss: 0.19119249284267426 *\n",
      "Epoch: 28, Train_Loss: 0.19638806581497192, Test_Loss: 0.1749744564294815 *\n",
      "Epoch: 28, Train_Loss: 0.1761692315340042, Test_Loss: 0.2066696584224701\n",
      "Epoch: 28, Train_Loss: 0.19350896775722504, Test_Loss: 0.18256987631320953 *\n",
      "Epoch: 28, Train_Loss: 0.2126665860414505, Test_Loss: 0.1961633265018463\n",
      "Epoch: 28, Train_Loss: 0.2580021023750305, Test_Loss: 0.1755903959274292 *\n",
      "Epoch: 28, Train_Loss: 0.20578688383102417, Test_Loss: 0.29967236518859863\n",
      "Epoch: 28, Train_Loss: 0.19720011949539185, Test_Loss: 0.5014771819114685\n",
      "Epoch: 28, Train_Loss: 0.1860697716474533, Test_Loss: 0.2507178485393524 *\n",
      "Epoch: 28, Train_Loss: 0.18276068568229675, Test_Loss: 0.4321224093437195\n",
      "Epoch: 28, Train_Loss: 0.18531492352485657, Test_Loss: 0.21480028331279755 *\n",
      "Epoch: 28, Train_Loss: 0.17439378798007965, Test_Loss: 0.21393302083015442 *\n",
      "Epoch: 28, Train_Loss: 0.18192973732948303, Test_Loss: 0.21323776245117188 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 28\n",
      "Epoch: 28, Train_Loss: 0.1839022934436798, Test_Loss: 0.2092742919921875 *\n",
      "Epoch: 28, Train_Loss: 0.1871607005596161, Test_Loss: 0.20066320896148682 *\n",
      "Epoch: 28, Train_Loss: 0.26744547486305237, Test_Loss: 4.323733806610107\n",
      "Epoch: 28, Train_Loss: 0.17396672070026398, Test_Loss: 1.9789921045303345 *\n",
      "Epoch: 28, Train_Loss: 0.228019118309021, Test_Loss: 0.20331382751464844 *\n",
      "Epoch: 28, Train_Loss: 0.187022864818573, Test_Loss: 0.20532333850860596\n",
      "Epoch: 28, Train_Loss: 0.18647345900535583, Test_Loss: 0.18546539545059204 *\n",
      "Epoch: 28, Train_Loss: 0.23048686981201172, Test_Loss: 0.19244365394115448\n",
      "Epoch: 28, Train_Loss: 0.4280858337879181, Test_Loss: 0.17960777878761292 *\n",
      "Epoch: 28, Train_Loss: 0.1773429960012436, Test_Loss: 0.2071908563375473\n",
      "Epoch: 28, Train_Loss: 0.19421829283237457, Test_Loss: 0.18634887039661407 *\n",
      "Epoch: 28, Train_Loss: 0.17260023951530457, Test_Loss: 0.1769852191209793 *\n",
      "Epoch: 28, Train_Loss: 0.173211470246315, Test_Loss: 0.19172458350658417\n",
      "Epoch: 28, Train_Loss: 0.18089382350444794, Test_Loss: 0.2096986472606659\n",
      "Epoch: 28, Train_Loss: 0.17902682721614838, Test_Loss: 0.2943437993526459\n",
      "Epoch: 28, Train_Loss: 0.17700284719467163, Test_Loss: 0.21149665117263794 *\n",
      "Epoch: 28, Train_Loss: 0.1780928522348404, Test_Loss: 0.23176805675029755\n",
      "Epoch: 28, Train_Loss: 0.1857757419347763, Test_Loss: 0.18675240874290466 *\n",
      "Epoch: 28, Train_Loss: 0.18730977177619934, Test_Loss: 0.1912325918674469\n",
      "Epoch: 28, Train_Loss: 0.17958371341228485, Test_Loss: 0.18440940976142883 *\n",
      "Epoch: 28, Train_Loss: 0.18381570279598236, Test_Loss: 0.22278793156147003\n",
      "Epoch: 28, Train_Loss: 0.17385011911392212, Test_Loss: 0.19847071170806885 *\n",
      "Epoch: 28, Train_Loss: 0.17178083956241608, Test_Loss: 0.18699440360069275 *\n",
      "Epoch: 28, Train_Loss: 0.19543567299842834, Test_Loss: 0.17747214436531067 *\n",
      "Epoch: 28, Train_Loss: 0.17610818147659302, Test_Loss: 0.17427657544612885 *\n",
      "Epoch: 28, Train_Loss: 0.19604527950286865, Test_Loss: 0.1780572533607483\n",
      "Epoch: 28, Train_Loss: 0.1736183911561966, Test_Loss: 0.19114814698696136\n",
      "Epoch: 28, Train_Loss: 0.18099668622016907, Test_Loss: 0.17249828577041626 *\n",
      "Epoch: 28, Train_Loss: 0.18858809769153595, Test_Loss: 0.17458190023899078\n",
      "Epoch: 28, Train_Loss: 0.20044143497943878, Test_Loss: 0.207100048661232\n",
      "Epoch: 28, Train_Loss: 0.1746739149093628, Test_Loss: 0.18870118260383606 *\n",
      "Epoch: 28, Train_Loss: 0.19094838201999664, Test_Loss: 0.1744338423013687 *\n",
      "Epoch: 28, Train_Loss: 0.17261902987957, Test_Loss: 0.29201048612594604\n",
      "Epoch: 28, Train_Loss: 0.18598122894763947, Test_Loss: 0.24670997262001038 *\n",
      "Epoch: 28, Train_Loss: 0.17767323553562164, Test_Loss: 5.491941452026367\n",
      "Epoch: 28, Train_Loss: 0.18830949068069458, Test_Loss: 0.3871293067932129 *\n",
      "Epoch: 28, Train_Loss: 0.5694078207015991, Test_Loss: 0.17290468513965607 *\n",
      "Epoch: 28, Train_Loss: 4.240102291107178, Test_Loss: 0.2010456621646881\n",
      "Epoch: 28, Train_Loss: 1.4146357774734497, Test_Loss: 0.17775192856788635 *\n",
      "Epoch: 28, Train_Loss: 0.1908145397901535, Test_Loss: 0.18471941351890564\n",
      "Epoch: 28, Train_Loss: 0.17602868378162384, Test_Loss: 0.18402348458766937 *\n",
      "Epoch: 28, Train_Loss: 0.25531333684921265, Test_Loss: 0.3163539171218872\n",
      "Epoch: 28, Train_Loss: 0.25989893078804016, Test_Loss: 0.21975253522396088 *\n",
      "Epoch: 28, Train_Loss: 0.19875189661979675, Test_Loss: 0.17374366521835327 *\n",
      "Epoch: 28, Train_Loss: 0.17262442409992218, Test_Loss: 0.20960672199726105\n",
      "Epoch: 28, Train_Loss: 0.22871418297290802, Test_Loss: 0.180073544383049 *\n",
      "Epoch: 28, Train_Loss: 0.2057252824306488, Test_Loss: 0.18231801688671112\n",
      "Epoch: 28, Train_Loss: 0.17945431172847748, Test_Loss: 0.1940518021583557\n",
      "Epoch: 28, Train_Loss: 0.3748989701271057, Test_Loss: 0.1895669400691986 *\n",
      "Epoch: 28, Train_Loss: 0.36531662940979004, Test_Loss: 0.21318958699703217\n",
      "Epoch: 28, Train_Loss: 0.6617536544799805, Test_Loss: 0.23711369931697845\n",
      "Epoch: 28, Train_Loss: 0.2346593141555786, Test_Loss: 0.23392102122306824 *\n",
      "Epoch: 28, Train_Loss: 0.281463623046875, Test_Loss: 0.23045367002487183 *\n",
      "Epoch: 28, Train_Loss: 1.3351128101348877, Test_Loss: 0.1890271157026291 *\n",
      "Epoch: 28, Train_Loss: 0.7609624862670898, Test_Loss: 0.27158811688423157\n",
      "Epoch: 28, Train_Loss: 0.17566299438476562, Test_Loss: 0.2444571554660797 *\n",
      "Epoch: 28, Train_Loss: 0.18039682507514954, Test_Loss: 0.24667856097221375\n",
      "Epoch: 28, Train_Loss: 0.6219707727432251, Test_Loss: 0.22070281207561493 *\n",
      "Epoch: 28, Train_Loss: 0.34777188301086426, Test_Loss: 0.2362561821937561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train_Loss: 0.8110311031341553, Test_Loss: 0.3278766870498657\n",
      "Epoch: 28, Train_Loss: 0.18017981946468353, Test_Loss: 0.27514126896858215 *\n",
      "Epoch: 28, Train_Loss: 0.18697211146354675, Test_Loss: 0.19591642916202545 *\n",
      "Epoch: 28, Train_Loss: 0.3480895161628723, Test_Loss: 0.2069312334060669\n",
      "Epoch: 28, Train_Loss: 0.4632108211517334, Test_Loss: 0.18374508619308472 *\n",
      "Epoch: 28, Train_Loss: 0.18809881806373596, Test_Loss: 0.18846242129802704\n",
      "Epoch: 28, Train_Loss: 0.21937212347984314, Test_Loss: 0.2331305295228958\n",
      "Epoch: 28, Train_Loss: 0.21016298234462738, Test_Loss: 0.429042249917984\n",
      "Epoch: 28, Train_Loss: 0.229206383228302, Test_Loss: 0.32635241746902466 *\n",
      "Epoch: 28, Train_Loss: 0.3496544361114502, Test_Loss: 0.3097462058067322 *\n",
      "Epoch: 28, Train_Loss: 0.21766714751720428, Test_Loss: 0.22034704685211182 *\n",
      "Epoch: 28, Train_Loss: 0.27347272634506226, Test_Loss: 0.20611411333084106 *\n",
      "Epoch: 28, Train_Loss: 0.24675840139389038, Test_Loss: 0.17699413001537323 *\n",
      "Epoch: 28, Train_Loss: 0.22554722428321838, Test_Loss: 0.21129164099693298\n",
      "Epoch: 28, Train_Loss: 0.24635346233844757, Test_Loss: 0.44791579246520996\n",
      "Epoch: 28, Train_Loss: 0.28623950481414795, Test_Loss: 0.21788504719734192 *\n",
      "Epoch: 28, Train_Loss: 0.36052048206329346, Test_Loss: 0.35729509592056274\n",
      "Epoch: 28, Train_Loss: 0.21008051931858063, Test_Loss: 0.24780994653701782 *\n",
      "Epoch: 28, Train_Loss: 0.23736310005187988, Test_Loss: 0.19897697865962982 *\n",
      "Epoch: 28, Train_Loss: 0.23697122931480408, Test_Loss: 0.21514727175235748\n",
      "Epoch: 28, Train_Loss: 0.19214099645614624, Test_Loss: 0.18947802484035492 *\n",
      "Epoch: 28, Train_Loss: 0.1739640235900879, Test_Loss: 0.21662546694278717\n",
      "Epoch: 28, Train_Loss: 0.17260144650936127, Test_Loss: 0.1776549518108368 *\n",
      "Epoch: 28, Train_Loss: 0.17225590348243713, Test_Loss: 0.19131983816623688\n",
      "Epoch: 28, Train_Loss: 0.1752421110868454, Test_Loss: 0.17650660872459412 *\n",
      "Epoch: 28, Train_Loss: 0.1807563751935959, Test_Loss: 0.3104395568370819\n",
      "Epoch: 28, Train_Loss: 0.19607515633106232, Test_Loss: 0.6011701822280884\n",
      "Epoch: 28, Train_Loss: 0.19339224696159363, Test_Loss: 0.22828778624534607 *\n",
      "Epoch: 28, Train_Loss: 0.19692544639110565, Test_Loss: 0.46884095668792725\n",
      "Epoch: 28, Train_Loss: 0.2977668046951294, Test_Loss: 0.23610714077949524 *\n",
      "Epoch: 28, Train_Loss: 0.44140195846557617, Test_Loss: 0.2376418113708496\n",
      "Epoch: 28, Train_Loss: 0.19264861941337585, Test_Loss: 0.236892968416214 *\n",
      "Epoch: 28, Train_Loss: 0.22434961795806885, Test_Loss: 0.2307041734457016 *\n",
      "Epoch: 28, Train_Loss: 0.25670531392097473, Test_Loss: 0.2165723741054535 *\n",
      "Epoch: 28, Train_Loss: 0.25684821605682373, Test_Loss: 5.825603485107422\n",
      "Epoch: 28, Train_Loss: 0.21640561521053314, Test_Loss: 0.8973639011383057 *\n",
      "Epoch: 28, Train_Loss: 0.2147611528635025, Test_Loss: 0.20141322910785675 *\n",
      "Epoch: 28, Train_Loss: 0.350987046957016, Test_Loss: 0.2159595787525177\n",
      "Epoch: 28, Train_Loss: 0.2350950837135315, Test_Loss: 0.19167619943618774 *\n",
      "Epoch: 28, Train_Loss: 0.29776912927627563, Test_Loss: 0.18166963756084442 *\n",
      "Epoch: 28, Train_Loss: 0.19104990363121033, Test_Loss: 0.2109180986881256\n",
      "Model saved at location save_new\\model.ckpt at epoch 28\n",
      "Epoch: 28, Train_Loss: 0.20979711413383484, Test_Loss: 0.24988722801208496\n",
      "Epoch: 28, Train_Loss: 0.44585752487182617, Test_Loss: 0.21239742636680603 *\n",
      "Epoch: 28, Train_Loss: 0.7124804258346558, Test_Loss: 0.17642298340797424 *\n",
      "Epoch: 28, Train_Loss: 0.39337772130966187, Test_Loss: 0.2185407429933548\n",
      "Epoch: 28, Train_Loss: 0.21801771223545074, Test_Loss: 0.3434678912162781\n",
      "Epoch: 28, Train_Loss: 0.20826217532157898, Test_Loss: 0.2872103452682495 *\n",
      "Epoch: 28, Train_Loss: 0.17871248722076416, Test_Loss: 0.19637730717658997 *\n",
      "Epoch: 28, Train_Loss: 0.46185004711151123, Test_Loss: 0.2009871006011963\n",
      "Epoch: 28, Train_Loss: 0.28487497568130493, Test_Loss: 0.18331915140151978 *\n",
      "Epoch: 28, Train_Loss: 0.18277837336063385, Test_Loss: 0.20016345381736755\n",
      "Epoch: 28, Train_Loss: 0.3255517780780792, Test_Loss: 0.2454327642917633\n",
      "Epoch: 28, Train_Loss: 0.19524751603603363, Test_Loss: 0.2662547528743744\n",
      "Epoch: 28, Train_Loss: 0.1896311640739441, Test_Loss: 0.20892682671546936 *\n",
      "Epoch: 28, Train_Loss: 0.22114111483097076, Test_Loss: 0.20922353863716125\n",
      "Epoch: 28, Train_Loss: 0.2855899930000305, Test_Loss: 0.18833714723587036 *\n",
      "Epoch: 28, Train_Loss: 0.2430303692817688, Test_Loss: 0.19304409623146057\n",
      "Epoch: 28, Train_Loss: 0.27323126792907715, Test_Loss: 0.20593012869358063\n",
      "Epoch: 28, Train_Loss: 0.19748114049434662, Test_Loss: 0.2481479048728943\n",
      "Epoch: 28, Train_Loss: 0.24528345465660095, Test_Loss: 0.17368970811367035 *\n",
      "Epoch: 28, Train_Loss: 0.21014341711997986, Test_Loss: 0.18670427799224854\n",
      "Epoch: 28, Train_Loss: 0.19447821378707886, Test_Loss: 0.2415435016155243\n",
      "Epoch: 28, Train_Loss: 0.1822805255651474, Test_Loss: 0.19250816106796265 *\n",
      "Epoch: 28, Train_Loss: 0.2151399850845337, Test_Loss: 0.1756923496723175 *\n",
      "Epoch: 28, Train_Loss: 0.37073469161987305, Test_Loss: 0.3390539884567261\n",
      "Epoch: 28, Train_Loss: 0.39931023120880127, Test_Loss: 0.5738116502761841\n",
      "Epoch: 28, Train_Loss: 0.41791361570358276, Test_Loss: 4.498559951782227\n",
      "Epoch: 28, Train_Loss: 0.5327699780464172, Test_Loss: 0.20296087861061096 *\n",
      "Epoch: 28, Train_Loss: 0.4411815404891968, Test_Loss: 0.1844903528690338 *\n",
      "Epoch: 28, Train_Loss: 0.3021281063556671, Test_Loss: 0.20448678731918335\n",
      "Epoch: 28, Train_Loss: 0.2575203776359558, Test_Loss: 0.19909405708312988 *\n",
      "Epoch: 28, Train_Loss: 0.18615858256816864, Test_Loss: 0.1861046403646469 *\n",
      "Epoch: 28, Train_Loss: 0.17897599935531616, Test_Loss: 0.20699332654476166\n",
      "Epoch: 28, Train_Loss: 0.1867932826280594, Test_Loss: 0.24513816833496094\n",
      "Epoch: 28, Train_Loss: 0.3415299654006958, Test_Loss: 0.2005479782819748 *\n",
      "Epoch: 28, Train_Loss: 0.3980633020401001, Test_Loss: 0.1845727115869522 *\n",
      "Epoch: 28, Train_Loss: 0.4238157570362091, Test_Loss: 0.1965995877981186\n",
      "Epoch: 28, Train_Loss: 0.7848496437072754, Test_Loss: 0.24314230680465698\n",
      "Epoch: 28, Train_Loss: 1.0110137462615967, Test_Loss: 0.21064502000808716 *\n",
      "Epoch: 28, Train_Loss: 0.3563118577003479, Test_Loss: 0.2554958760738373\n",
      "Epoch: 28, Train_Loss: 0.2280934453010559, Test_Loss: 0.2833569049835205\n",
      "Epoch: 28, Train_Loss: 0.17870892584323883, Test_Loss: 0.2933562994003296\n",
      "Epoch: 28, Train_Loss: 0.29791927337646484, Test_Loss: 0.1853802502155304 *\n",
      "Epoch: 28, Train_Loss: 0.4227820932865143, Test_Loss: 0.21688531339168549\n",
      "Epoch: 28, Train_Loss: 0.6742175221443176, Test_Loss: 0.3349456787109375\n",
      "Epoch: 28, Train_Loss: 0.20554713904857635, Test_Loss: 0.21422842144966125 *\n",
      "Epoch: 28, Train_Loss: 0.19374758005142212, Test_Loss: 0.47276920080184937\n",
      "Epoch: 28, Train_Loss: 0.24377045035362244, Test_Loss: 0.36551254987716675 *\n",
      "Epoch: 28, Train_Loss: 0.43090707063674927, Test_Loss: 0.36917150020599365\n",
      "Epoch: 28, Train_Loss: 0.2997019290924072, Test_Loss: 0.3179289400577545 *\n",
      "Epoch: 28, Train_Loss: 0.248639315366745, Test_Loss: 0.3193686008453369\n",
      "Epoch: 28, Train_Loss: 0.2887263596057892, Test_Loss: 0.4333084225654602\n",
      "Epoch: 28, Train_Loss: 0.22515800595283508, Test_Loss: 0.300164133310318 *\n",
      "Epoch: 28, Train_Loss: 0.1772514134645462, Test_Loss: 0.20646187663078308 *\n",
      "Epoch: 28, Train_Loss: 0.18984311819076538, Test_Loss: 0.2532297670841217\n",
      "Epoch: 29, Train_Loss: 0.17321741580963135, Test_Loss: 0.1886090338230133 *\n",
      "Epoch: 29, Train_Loss: 0.23837347328662872, Test_Loss: 0.17474286258220673 *\n",
      "Epoch: 29, Train_Loss: 0.2546277642250061, Test_Loss: 0.2143431007862091\n",
      "Epoch: 29, Train_Loss: 0.4106612205505371, Test_Loss: 0.315876305103302\n",
      "Epoch: 29, Train_Loss: 14.45425033569336, Test_Loss: 0.23890432715415955 *\n",
      "Epoch: 29, Train_Loss: 0.39148762822151184, Test_Loss: 0.22468967735767365 *\n",
      "Epoch: 29, Train_Loss: 0.8034355044364929, Test_Loss: 0.23243406414985657\n",
      "Epoch: 29, Train_Loss: 0.8543914556503296, Test_Loss: 0.19991768896579742 *\n",
      "Epoch: 29, Train_Loss: 0.25771990418434143, Test_Loss: 0.17379425466060638 *\n",
      "Epoch: 29, Train_Loss: 0.5053843259811401, Test_Loss: 0.1993493139743805\n",
      "Epoch: 29, Train_Loss: 2.493281602859497, Test_Loss: 0.30476808547973633\n",
      "Epoch: 29, Train_Loss: 3.2290589809417725, Test_Loss: 0.32282447814941406\n",
      "Epoch: 29, Train_Loss: 0.3003176152706146, Test_Loss: 0.26130932569503784 *\n",
      "Epoch: 29, Train_Loss: 0.7008647918701172, Test_Loss: 0.2104494422674179 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train_Loss: 4.409598350524902, Test_Loss: 0.21055877208709717\n",
      "Epoch: 29, Train_Loss: 0.425650417804718, Test_Loss: 0.22411568462848663\n",
      "Epoch: 29, Train_Loss: 0.1826346069574356, Test_Loss: 0.2369196116924286\n",
      "Epoch: 29, Train_Loss: 0.18330639600753784, Test_Loss: 0.24024330079555511\n",
      "Epoch: 29, Train_Loss: 0.2791041135787964, Test_Loss: 0.18988598883152008 *\n",
      "Epoch: 29, Train_Loss: 0.2062036097049713, Test_Loss: 0.18823491036891937 *\n",
      "Epoch: 29, Train_Loss: 0.17375995218753815, Test_Loss: 0.18676429986953735 *\n",
      "Epoch: 29, Train_Loss: 0.1715710610151291, Test_Loss: 0.26169195771217346\n",
      "Epoch: 29, Train_Loss: 0.16970406472682953, Test_Loss: 0.5604151487350464\n",
      "Epoch: 29, Train_Loss: 0.1753019541501999, Test_Loss: 0.24237778782844543 *\n",
      "Epoch: 29, Train_Loss: 0.2251822054386139, Test_Loss: 0.321108877658844\n",
      "Epoch: 29, Train_Loss: 0.21228009462356567, Test_Loss: 0.19336482882499695 *\n",
      "Epoch: 29, Train_Loss: 0.23656734824180603, Test_Loss: 0.1827113777399063 *\n",
      "Epoch: 29, Train_Loss: 0.3362034559249878, Test_Loss: 0.1744670867919922 *\n",
      "Epoch: 29, Train_Loss: 0.3172874450683594, Test_Loss: 0.18903014063835144\n",
      "Epoch: 29, Train_Loss: 0.18005946278572083, Test_Loss: 0.2527954578399658\n",
      "Epoch: 29, Train_Loss: 0.17694920301437378, Test_Loss: 8.693352699279785\n",
      "Epoch: 29, Train_Loss: 0.18082818388938904, Test_Loss: 0.4249732494354248 *\n",
      "Epoch: 29, Train_Loss: 0.18365125358104706, Test_Loss: 0.21040946245193481 *\n",
      "Epoch: 29, Train_Loss: 0.18170566856861115, Test_Loss: 0.2200871706008911\n",
      "Epoch: 29, Train_Loss: 0.17493478953838348, Test_Loss: 0.213287353515625 *\n",
      "Epoch: 29, Train_Loss: 0.1744365692138672, Test_Loss: 0.1894553005695343 *\n",
      "Epoch: 29, Train_Loss: 0.1738900989294052, Test_Loss: 0.38417425751686096\n",
      "Epoch: 29, Train_Loss: 0.1765824556350708, Test_Loss: 0.28852418065071106 *\n",
      "Epoch: 29, Train_Loss: 0.1751815676689148, Test_Loss: 0.1974967122077942 *\n",
      "Epoch: 29, Train_Loss: 0.17806026339530945, Test_Loss: 0.21989499032497406\n",
      "Epoch: 29, Train_Loss: 0.1904224455356598, Test_Loss: 0.2281101942062378\n",
      "Epoch: 29, Train_Loss: 0.2062530219554901, Test_Loss: 0.5656352043151855\n",
      "Epoch: 29, Train_Loss: 0.23787850141525269, Test_Loss: 0.326458603143692 *\n",
      "Epoch: 29, Train_Loss: 0.18733811378479004, Test_Loss: 0.23769637942314148 *\n",
      "Epoch: 29, Train_Loss: 0.27208346128463745, Test_Loss: 0.24477045238018036\n",
      "Epoch: 29, Train_Loss: 3.9850995540618896, Test_Loss: 0.17810890078544617 *\n",
      "Epoch: 29, Train_Loss: 2.947585105895996, Test_Loss: 0.18834374845027924\n",
      "Epoch: 29, Train_Loss: 0.20211473107337952, Test_Loss: 0.1875355988740921 *\n",
      "Epoch: 29, Train_Loss: 0.2060328722000122, Test_Loss: 0.3737300634384155\n",
      "Epoch: 29, Train_Loss: 0.20840249955654144, Test_Loss: 0.2078738659620285 *\n",
      "Epoch: 29, Train_Loss: 0.20530091226100922, Test_Loss: 0.4045071005821228\n",
      "Epoch: 29, Train_Loss: 0.2339726984500885, Test_Loss: 0.2322542667388916 *\n",
      "Epoch: 29, Train_Loss: 0.2093418538570404, Test_Loss: 0.2581784427165985\n",
      "Epoch: 29, Train_Loss: 0.21220996975898743, Test_Loss: 0.28236719965934753\n",
      "Epoch: 29, Train_Loss: 0.28275442123413086, Test_Loss: 0.30017951130867004\n",
      "Epoch: 29, Train_Loss: 0.2759072780609131, Test_Loss: 0.18064802885055542 *\n",
      "Epoch: 29, Train_Loss: 0.18868444859981537, Test_Loss: 0.26986414194107056\n",
      "Epoch: 29, Train_Loss: 0.1878788322210312, Test_Loss: 0.2922173738479614\n",
      "Epoch: 29, Train_Loss: 0.18427518010139465, Test_Loss: 0.2956315279006958\n",
      "Epoch: 29, Train_Loss: 0.202254056930542, Test_Loss: 0.19796428084373474 *\n",
      "Epoch: 29, Train_Loss: 0.17772488296031952, Test_Loss: 0.4570133090019226\n",
      "Epoch: 29, Train_Loss: 0.19219200313091278, Test_Loss: 2.0526325702667236\n",
      "Epoch: 29, Train_Loss: 0.2120443880558014, Test_Loss: 4.296409606933594\n",
      "Epoch: 29, Train_Loss: 0.18834178149700165, Test_Loss: 0.19106899201869965 *\n",
      "Epoch: 29, Train_Loss: 0.20220598578453064, Test_Loss: 0.17458748817443848 *\n",
      "Epoch: 29, Train_Loss: 0.20744451880455017, Test_Loss: 0.2284076064825058\n",
      "Epoch: 29, Train_Loss: 0.20025724172592163, Test_Loss: 0.4025157690048218\n",
      "Epoch: 29, Train_Loss: 0.17576293647289276, Test_Loss: 0.22059497237205505 *\n",
      "Epoch: 29, Train_Loss: 0.1695035994052887, Test_Loss: 0.23243999481201172\n",
      "Epoch: 29, Train_Loss: 0.21627473831176758, Test_Loss: 0.23751190304756165\n",
      "Epoch: 29, Train_Loss: 2.5560290813446045, Test_Loss: 0.1828078180551529 *\n",
      "Epoch: 29, Train_Loss: 1.700747013092041, Test_Loss: 0.1846579909324646\n",
      "Epoch: 29, Train_Loss: 0.21518608927726746, Test_Loss: 0.19475296139717102\n",
      "Epoch: 29, Train_Loss: 0.2900764048099518, Test_Loss: 0.21269282698631287\n",
      "Epoch: 29, Train_Loss: 0.1788787990808487, Test_Loss: 0.18163852393627167 *\n",
      "Epoch: 29, Train_Loss: 0.17057356238365173, Test_Loss: 0.3601837754249573\n",
      "Epoch: 29, Train_Loss: 0.17992857098579407, Test_Loss: 0.5077775716781616\n",
      "Epoch: 29, Train_Loss: 0.17225490510463715, Test_Loss: 0.3204836845397949 *\n",
      "Epoch: 29, Train_Loss: 0.19402971863746643, Test_Loss: 0.19176577031612396 *\n",
      "Epoch: 29, Train_Loss: 0.18768088519573212, Test_Loss: 0.1988125443458557\n",
      "Epoch: 29, Train_Loss: 0.20588499307632446, Test_Loss: 0.3010719418525696\n",
      "Epoch: 29, Train_Loss: 0.17259955406188965, Test_Loss: 0.3156276345252991\n",
      "Epoch: 29, Train_Loss: 0.18710647523403168, Test_Loss: 0.7102023363113403\n",
      "Epoch: 29, Train_Loss: 0.18455922603607178, Test_Loss: 0.4610815942287445 *\n",
      "Epoch: 29, Train_Loss: 0.17874208092689514, Test_Loss: 0.35755354166030884 *\n",
      "Epoch: 29, Train_Loss: 0.17169994115829468, Test_Loss: 0.38611918687820435\n",
      "Epoch: 29, Train_Loss: 0.18165160715579987, Test_Loss: 0.3741232752799988 *\n",
      "Epoch: 29, Train_Loss: 0.18118993937969208, Test_Loss: 0.498751699924469\n",
      "Epoch: 29, Train_Loss: 0.17978674173355103, Test_Loss: 0.34296271204948425 *\n",
      "Epoch: 29, Train_Loss: 0.17427998781204224, Test_Loss: 0.20521315932273865 *\n",
      "Epoch: 29, Train_Loss: 0.1703767627477646, Test_Loss: 0.2444903403520584\n",
      "Epoch: 29, Train_Loss: 0.23245693743228912, Test_Loss: 0.1724647581577301 *\n",
      "Epoch: 29, Train_Loss: 0.19672861695289612, Test_Loss: 0.20554372668266296\n",
      "Epoch: 29, Train_Loss: 0.21824650466442108, Test_Loss: 0.24757924675941467\n",
      "Epoch: 29, Train_Loss: 0.19801706075668335, Test_Loss: 0.3182397484779358\n",
      "Epoch: 29, Train_Loss: 0.25037866830825806, Test_Loss: 0.2902328372001648 *\n",
      "Epoch: 29, Train_Loss: 0.21722497045993805, Test_Loss: 0.24058791995048523 *\n",
      "Epoch: 29, Train_Loss: 0.1814226508140564, Test_Loss: 0.21360808610916138 *\n",
      "Epoch: 29, Train_Loss: 0.21449124813079834, Test_Loss: 0.1985967457294464 *\n",
      "Epoch: 29, Train_Loss: 0.26917678117752075, Test_Loss: 0.1860049068927765 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 29\n",
      "Epoch: 29, Train_Loss: 0.22035735845565796, Test_Loss: 0.276613712310791\n",
      "Epoch: 29, Train_Loss: 0.18199826776981354, Test_Loss: 0.3993639349937439\n",
      "Epoch: 29, Train_Loss: 0.17009393870830536, Test_Loss: 0.5073865056037903\n",
      "Epoch: 29, Train_Loss: 0.1705658882856369, Test_Loss: 0.24518278241157532 *\n",
      "Epoch: 29, Train_Loss: 0.1712757796049118, Test_Loss: 0.23459050059318542 *\n",
      "Epoch: 29, Train_Loss: 0.17142972350120544, Test_Loss: 0.18210652470588684 *\n",
      "Epoch: 29, Train_Loss: 0.17136646807193756, Test_Loss: 0.17178381979465485 *\n",
      "Epoch: 29, Train_Loss: 3.9239165782928467, Test_Loss: 0.18470586836338043\n",
      "Epoch: 29, Train_Loss: 1.1003657579421997, Test_Loss: 0.17779657244682312 *\n",
      "Epoch: 29, Train_Loss: 0.1761304885149002, Test_Loss: 0.19212403893470764\n",
      "Epoch: 29, Train_Loss: 0.17267926037311554, Test_Loss: 0.17761746048927307 *\n",
      "Epoch: 29, Train_Loss: 0.17247213423252106, Test_Loss: 0.19277270138263702\n",
      "Epoch: 29, Train_Loss: 0.17431190609931946, Test_Loss: 0.31265801191329956\n",
      "Epoch: 29, Train_Loss: 0.17365029454231262, Test_Loss: 0.49249976873397827\n",
      "Epoch: 29, Train_Loss: 0.17774449288845062, Test_Loss: 0.47107434272766113 *\n",
      "Epoch: 29, Train_Loss: 0.17301565408706665, Test_Loss: 0.2511967420578003 *\n",
      "Epoch: 29, Train_Loss: 0.17619970440864563, Test_Loss: 0.2404615432024002 *\n",
      "Epoch: 29, Train_Loss: 0.18665456771850586, Test_Loss: 0.2407197654247284\n",
      "Epoch: 29, Train_Loss: 0.19551445543766022, Test_Loss: 0.2413167655467987\n",
      "Epoch: 29, Train_Loss: 0.21620947122573853, Test_Loss: 0.24074651300907135 *\n",
      "Epoch: 29, Train_Loss: 0.20928692817687988, Test_Loss: 0.5829924941062927\n",
      "Epoch: 29, Train_Loss: 0.18144270777702332, Test_Loss: 4.955654144287109\n",
      "Epoch: 29, Train_Loss: 0.2079617828130722, Test_Loss: 0.21422840654850006 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train_Loss: 0.3368261158466339, Test_Loss: 0.20380184054374695 *\n",
      "Epoch: 29, Train_Loss: 0.271156907081604, Test_Loss: 0.21422018110752106\n",
      "Epoch: 29, Train_Loss: 0.39694759249687195, Test_Loss: 0.19436359405517578 *\n",
      "Epoch: 29, Train_Loss: 0.21900278329849243, Test_Loss: 0.17958052456378937 *\n",
      "Epoch: 29, Train_Loss: 0.1722177267074585, Test_Loss: 0.18446236848831177\n",
      "Epoch: 29, Train_Loss: 0.1695479303598404, Test_Loss: 0.17063702642917633 *\n",
      "Epoch: 29, Train_Loss: 0.17118506133556366, Test_Loss: 0.1745178997516632\n",
      "Epoch: 29, Train_Loss: 0.172306627035141, Test_Loss: 0.17161674797534943 *\n",
      "Epoch: 29, Train_Loss: 0.17611384391784668, Test_Loss: 0.1774168610572815\n",
      "Epoch: 29, Train_Loss: 0.18321292102336884, Test_Loss: 0.27846118807792664\n",
      "Epoch: 29, Train_Loss: 0.16895025968551636, Test_Loss: 0.19810661673545837 *\n",
      "Epoch: 29, Train_Loss: 0.17105571925640106, Test_Loss: 0.20183683931827545\n",
      "Epoch: 29, Train_Loss: 0.17580574750900269, Test_Loss: 0.20587223768234253\n",
      "Epoch: 29, Train_Loss: 0.23094573616981506, Test_Loss: 0.19364556670188904 *\n",
      "Epoch: 29, Train_Loss: 0.24778422713279724, Test_Loss: 0.18170437216758728 *\n",
      "Epoch: 29, Train_Loss: 0.262259840965271, Test_Loss: 0.19190704822540283\n",
      "Epoch: 29, Train_Loss: 0.2635286748409271, Test_Loss: 0.2350957989692688\n",
      "Epoch: 29, Train_Loss: 0.28137701749801636, Test_Loss: 0.18937447667121887 *\n",
      "Epoch: 29, Train_Loss: 0.26601964235305786, Test_Loss: 0.1789279729127884 *\n",
      "Epoch: 29, Train_Loss: 0.17419622838497162, Test_Loss: 0.17063094675540924 *\n",
      "Epoch: 29, Train_Loss: 0.23269160091876984, Test_Loss: 0.17450352013111115\n",
      "Epoch: 29, Train_Loss: 0.17404749989509583, Test_Loss: 0.19733084738254547\n",
      "Epoch: 29, Train_Loss: 0.49035149812698364, Test_Loss: 0.1825840175151825 *\n",
      "Epoch: 29, Train_Loss: 0.1966107040643692, Test_Loss: 0.17167967557907104 *\n",
      "Epoch: 29, Train_Loss: 0.8011378049850464, Test_Loss: 0.1784343421459198\n",
      "Epoch: 29, Train_Loss: 1.9998267889022827, Test_Loss: 0.18496954441070557\n",
      "Epoch: 29, Train_Loss: 0.22052884101867676, Test_Loss: 0.18840332329273224\n",
      "Epoch: 29, Train_Loss: 0.2084774672985077, Test_Loss: 0.1806936115026474 *\n",
      "Epoch: 29, Train_Loss: 0.17862795293331146, Test_Loss: 0.35907402634620667\n",
      "Epoch: 29, Train_Loss: 0.18636339902877808, Test_Loss: 2.85408353805542\n",
      "Epoch: 29, Train_Loss: 0.17160098254680634, Test_Loss: 2.5069580078125 *\n",
      "Epoch: 29, Train_Loss: 0.1726059764623642, Test_Loss: 0.1817571222782135 *\n",
      "Epoch: 29, Train_Loss: 0.2444819062948227, Test_Loss: 0.18766151368618011\n",
      "Epoch: 29, Train_Loss: 0.23306375741958618, Test_Loss: 0.23019939661026\n",
      "Epoch: 29, Train_Loss: 0.20326727628707886, Test_Loss: 0.2704416811466217\n",
      "Epoch: 29, Train_Loss: 0.1876564770936966, Test_Loss: 0.19323322176933289 *\n",
      "Epoch: 29, Train_Loss: 0.18050989508628845, Test_Loss: 0.23726966977119446\n",
      "Epoch: 29, Train_Loss: 0.17408299446105957, Test_Loss: 0.20070777833461761 *\n",
      "Epoch: 29, Train_Loss: 0.18504764139652252, Test_Loss: 0.18616200983524323 *\n",
      "Epoch: 29, Train_Loss: 0.20506168901920319, Test_Loss: 0.20697815716266632\n",
      "Epoch: 29, Train_Loss: 0.2019365429878235, Test_Loss: 0.2041817307472229 *\n",
      "Epoch: 29, Train_Loss: 0.18082548677921295, Test_Loss: 0.30409058928489685\n",
      "Epoch: 29, Train_Loss: 0.17421291768550873, Test_Loss: 0.202697291970253 *\n",
      "Epoch: 29, Train_Loss: 0.18045483529567719, Test_Loss: 0.35979700088500977\n",
      "Epoch: 29, Train_Loss: 0.17829956114292145, Test_Loss: 0.30243897438049316 *\n",
      "Epoch: 29, Train_Loss: 0.18295229971408844, Test_Loss: 0.20988407731056213 *\n",
      "Epoch: 29, Train_Loss: 0.1728833019733429, Test_Loss: 0.17701469361782074 *\n",
      "Epoch: 29, Train_Loss: 0.16964997351169586, Test_Loss: 0.24629032611846924\n",
      "Epoch: 29, Train_Loss: 0.17055164277553558, Test_Loss: 0.3793398141860962\n",
      "Epoch: 29, Train_Loss: 0.17808279395103455, Test_Loss: 0.3270743489265442 *\n",
      "Epoch: 29, Train_Loss: 0.18097907304763794, Test_Loss: 0.5054196715354919\n",
      "Epoch: 29, Train_Loss: 0.18053853511810303, Test_Loss: 0.5162237286567688\n",
      "Epoch: 29, Train_Loss: 0.19967500865459442, Test_Loss: 0.34648019075393677 *\n",
      "Epoch: 29, Train_Loss: 0.17477180063724518, Test_Loss: 0.39118891954421997\n",
      "Epoch: 29, Train_Loss: 0.1763143241405487, Test_Loss: 0.3893188238143921 *\n",
      "Epoch: 29, Train_Loss: 0.17416220903396606, Test_Loss: 0.4113892614841461\n",
      "Epoch: 29, Train_Loss: 0.20034579932689667, Test_Loss: 0.2632116377353668 *\n",
      "Epoch: 29, Train_Loss: 0.18982936441898346, Test_Loss: 0.20019163191318512 *\n",
      "Epoch: 29, Train_Loss: 0.17753036320209503, Test_Loss: 0.21358820796012878\n",
      "Epoch: 29, Train_Loss: 0.1920505166053772, Test_Loss: 0.17667923867702484 *\n",
      "Epoch: 29, Train_Loss: 0.17864644527435303, Test_Loss: 0.24670597910881042\n",
      "Epoch: 29, Train_Loss: 0.20243214070796967, Test_Loss: 0.2222801148891449 *\n",
      "Epoch: 29, Train_Loss: 0.19481055438518524, Test_Loss: 0.3008500933647156\n",
      "Epoch: 29, Train_Loss: 0.18309883773326874, Test_Loss: 0.367758572101593\n",
      "Epoch: 29, Train_Loss: 0.2187739908695221, Test_Loss: 0.25030797719955444 *\n",
      "Epoch: 29, Train_Loss: 0.18533523380756378, Test_Loss: 0.19119031727313995 *\n",
      "Epoch: 29, Train_Loss: 0.17572738230228424, Test_Loss: 0.20036998391151428\n",
      "Epoch: 29, Train_Loss: 0.17630405724048615, Test_Loss: 0.18379360437393188 *\n",
      "Epoch: 29, Train_Loss: 0.18435828387737274, Test_Loss: 0.3236663341522217\n",
      "Epoch: 29, Train_Loss: 0.18704736232757568, Test_Loss: 0.22410784661769867 *\n",
      "Epoch: 29, Train_Loss: 0.21000313758850098, Test_Loss: 0.5032727718353271\n",
      "Epoch: 29, Train_Loss: 0.2087896317243576, Test_Loss: 0.24055275321006775 *\n",
      "Epoch: 29, Train_Loss: 0.171123206615448, Test_Loss: 0.21139377355575562 *\n",
      "Epoch: 29, Train_Loss: 0.20186132192611694, Test_Loss: 0.20076096057891846 *\n",
      "Epoch: 29, Train_Loss: 0.2108905017375946, Test_Loss: 0.17168563604354858 *\n",
      "Epoch: 29, Train_Loss: 0.1913686990737915, Test_Loss: 0.1883280724287033\n",
      "Epoch: 29, Train_Loss: 0.17872032523155212, Test_Loss: 0.17888984084129333 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 29\n",
      "Epoch: 29, Train_Loss: 0.1968114972114563, Test_Loss: 0.20029018819332123\n",
      "Epoch: 29, Train_Loss: 0.25478595495224, Test_Loss: 0.17232294380664825 *\n",
      "Epoch: 29, Train_Loss: 0.22214898467063904, Test_Loss: 0.2308499962091446\n",
      "Epoch: 29, Train_Loss: 0.19924624264240265, Test_Loss: 0.35448503494262695\n",
      "Epoch: 29, Train_Loss: 0.19217239320278168, Test_Loss: 0.37738120555877686\n",
      "Epoch: 29, Train_Loss: 0.17884032428264618, Test_Loss: 0.4423115849494934\n",
      "Epoch: 29, Train_Loss: 0.18794164061546326, Test_Loss: 0.23058375716209412 *\n",
      "Epoch: 29, Train_Loss: 0.17032082378864288, Test_Loss: 0.21471668779850006 *\n",
      "Epoch: 29, Train_Loss: 0.17692340910434723, Test_Loss: 0.21443697810173035 *\n",
      "Epoch: 29, Train_Loss: 0.1743553876876831, Test_Loss: 0.21462476253509521\n",
      "Epoch: 29, Train_Loss: 0.17829814553260803, Test_Loss: 0.2050098329782486 *\n",
      "Epoch: 29, Train_Loss: 0.23978719115257263, Test_Loss: 1.6533432006835938\n",
      "Epoch: 29, Train_Loss: 0.17623737454414368, Test_Loss: 4.292103290557861\n",
      "Epoch: 29, Train_Loss: 0.2291073501110077, Test_Loss: 0.18491363525390625 *\n",
      "Epoch: 29, Train_Loss: 0.17629221081733704, Test_Loss: 0.1810622364282608 *\n",
      "Epoch: 29, Train_Loss: 0.18846437335014343, Test_Loss: 0.1840144246816635\n",
      "Epoch: 29, Train_Loss: 0.18185865879058838, Test_Loss: 0.17863816022872925 *\n",
      "Epoch: 29, Train_Loss: 0.4759520888328552, Test_Loss: 0.1726127713918686 *\n",
      "Epoch: 29, Train_Loss: 0.21063093841075897, Test_Loss: 0.21961238980293274\n",
      "Epoch: 29, Train_Loss: 0.18561694025993347, Test_Loss: 0.18965105712413788 *\n",
      "Epoch: 29, Train_Loss: 0.17805321514606476, Test_Loss: 0.1734333336353302 *\n",
      "Epoch: 29, Train_Loss: 0.16981154680252075, Test_Loss: 0.1733236461877823 *\n",
      "Epoch: 29, Train_Loss: 0.17376583814620972, Test_Loss: 0.19719144701957703\n",
      "Epoch: 29, Train_Loss: 0.17196427285671234, Test_Loss: 0.39492446184158325\n",
      "Epoch: 29, Train_Loss: 0.17350241541862488, Test_Loss: 0.1943310648202896 *\n",
      "Epoch: 29, Train_Loss: 0.17174102365970612, Test_Loss: 0.20850026607513428\n",
      "Epoch: 29, Train_Loss: 0.1864495575428009, Test_Loss: 0.19421769678592682 *\n",
      "Epoch: 29, Train_Loss: 0.17559100687503815, Test_Loss: 0.17157778143882751 *\n",
      "Epoch: 29, Train_Loss: 0.1725953072309494, Test_Loss: 0.17326919734477997\n",
      "Epoch: 29, Train_Loss: 0.17806163430213928, Test_Loss: 0.1745995283126831\n",
      "Epoch: 29, Train_Loss: 0.17085200548171997, Test_Loss: 0.24948348104953766\n",
      "Epoch: 29, Train_Loss: 0.1681138277053833, Test_Loss: 0.17268061637878418 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train_Loss: 0.1861805021762848, Test_Loss: 0.17824117839336395\n",
      "Epoch: 29, Train_Loss: 0.17514197528362274, Test_Loss: 0.16813381016254425 *\n",
      "Epoch: 29, Train_Loss: 0.1891109198331833, Test_Loss: 0.18512237071990967\n",
      "Epoch: 29, Train_Loss: 0.17223891615867615, Test_Loss: 0.21790209412574768\n",
      "Epoch: 29, Train_Loss: 0.18294605612754822, Test_Loss: 0.17550066113471985 *\n",
      "Epoch: 29, Train_Loss: 0.18510349094867706, Test_Loss: 0.17286130785942078 *\n",
      "Epoch: 29, Train_Loss: 0.19247886538505554, Test_Loss: 0.1746632605791092\n",
      "Epoch: 29, Train_Loss: 0.16940318048000336, Test_Loss: 0.17870491743087769\n",
      "Epoch: 29, Train_Loss: 0.1832781434059143, Test_Loss: 0.17277951538562775 *\n",
      "Epoch: 29, Train_Loss: 0.17642338573932648, Test_Loss: 0.2355688065290451\n",
      "Epoch: 29, Train_Loss: 0.17934304475784302, Test_Loss: 0.27048903703689575\n",
      "Epoch: 29, Train_Loss: 0.1694605052471161, Test_Loss: 4.214227676391602\n",
      "Epoch: 29, Train_Loss: 0.18335959315299988, Test_Loss: 2.0417585372924805 *\n",
      "Epoch: 29, Train_Loss: 0.23134562373161316, Test_Loss: 0.1762600690126419 *\n",
      "Epoch: 29, Train_Loss: 2.860513687133789, Test_Loss: 0.1888168454170227\n",
      "Epoch: 29, Train_Loss: 2.898069143295288, Test_Loss: 0.18023714423179626 *\n",
      "Epoch: 29, Train_Loss: 0.18439117074012756, Test_Loss: 0.1926502287387848\n",
      "Epoch: 29, Train_Loss: 0.16959701478481293, Test_Loss: 0.18710064888000488 *\n",
      "Epoch: 29, Train_Loss: 0.221724271774292, Test_Loss: 0.2778906524181366\n",
      "Epoch: 29, Train_Loss: 0.26718616485595703, Test_Loss: 0.2338923215866089 *\n",
      "Epoch: 29, Train_Loss: 0.2006995528936386, Test_Loss: 0.17346051335334778 *\n",
      "Epoch: 29, Train_Loss: 0.16983336210250854, Test_Loss: 0.19234497845172882\n",
      "Epoch: 29, Train_Loss: 0.19995278120040894, Test_Loss: 0.18879292905330658 *\n",
      "Epoch: 29, Train_Loss: 0.21749237179756165, Test_Loss: 0.1965198814868927\n",
      "Epoch: 29, Train_Loss: 0.18118789792060852, Test_Loss: 0.1787828803062439 *\n",
      "Epoch: 29, Train_Loss: 0.22932502627372742, Test_Loss: 0.21677003800868988\n",
      "Epoch: 29, Train_Loss: 0.5238261818885803, Test_Loss: 0.24306735396385193\n",
      "Epoch: 29, Train_Loss: 0.5586187243461609, Test_Loss: 0.22306503355503082 *\n",
      "Epoch: 29, Train_Loss: 0.2808770537376404, Test_Loss: 0.20005987584590912 *\n",
      "Epoch: 29, Train_Loss: 0.30473777651786804, Test_Loss: 0.21858762204647064\n",
      "Epoch: 29, Train_Loss: 0.856499433517456, Test_Loss: 0.2822774052619934\n",
      "Epoch: 29, Train_Loss: 0.4888591766357422, Test_Loss: 0.3664109706878662\n",
      "Epoch: 29, Train_Loss: 0.17906683683395386, Test_Loss: 0.4183587431907654\n",
      "Epoch: 29, Train_Loss: 0.18784251809120178, Test_Loss: 0.43797558546066284\n",
      "Epoch: 29, Train_Loss: 0.47415316104888916, Test_Loss: 0.34676671028137207 *\n",
      "Epoch: 29, Train_Loss: 0.3673369288444519, Test_Loss: 0.3824971914291382\n",
      "Epoch: 29, Train_Loss: 0.6454256772994995, Test_Loss: 0.46567076444625854\n",
      "Epoch: 29, Train_Loss: 0.17387379705905914, Test_Loss: 0.44935423135757446 *\n",
      "Epoch: 29, Train_Loss: 0.1875705122947693, Test_Loss: 0.349740207195282 *\n",
      "Epoch: 29, Train_Loss: 0.3229643702507019, Test_Loss: 0.25859007239341736 *\n",
      "Epoch: 29, Train_Loss: 0.3955703377723694, Test_Loss: 0.2095099538564682 *\n",
      "Epoch: 29, Train_Loss: 0.1832333505153656, Test_Loss: 0.17383161187171936 *\n",
      "Epoch: 29, Train_Loss: 0.22830939292907715, Test_Loss: 0.252072811126709\n",
      "Epoch: 29, Train_Loss: 0.19024045765399933, Test_Loss: 0.32443082332611084\n",
      "Epoch: 29, Train_Loss: 0.2278388887643814, Test_Loss: 0.3215140700340271 *\n",
      "Epoch: 29, Train_Loss: 0.35241153836250305, Test_Loss: 0.37591150403022766\n",
      "Epoch: 29, Train_Loss: 0.2587518095970154, Test_Loss: 0.2454640418291092 *\n",
      "Epoch: 29, Train_Loss: 0.22870950400829315, Test_Loss: 0.21138277649879456 *\n",
      "Epoch: 29, Train_Loss: 0.24009448289871216, Test_Loss: 0.19399024546146393 *\n",
      "Epoch: 29, Train_Loss: 0.2723451554775238, Test_Loss: 0.1879483461380005 *\n",
      "Epoch: 29, Train_Loss: 0.21714597940444946, Test_Loss: 0.42697975039482117\n",
      "Epoch: 29, Train_Loss: 0.25412240624427795, Test_Loss: 0.18378067016601562 *\n",
      "Epoch: 29, Train_Loss: 0.30557823181152344, Test_Loss: 0.5755007863044739\n",
      "Epoch: 29, Train_Loss: 0.19769728183746338, Test_Loss: 0.24396248161792755 *\n",
      "Epoch: 29, Train_Loss: 0.21899017691612244, Test_Loss: 0.198350727558136 *\n",
      "Epoch: 29, Train_Loss: 0.22623391449451447, Test_Loss: 0.19951122999191284\n",
      "Epoch: 29, Train_Loss: 0.19487272202968597, Test_Loss: 0.1705702245235443 *\n",
      "Epoch: 29, Train_Loss: 0.17608438432216644, Test_Loss: 0.20547254383563995\n",
      "Epoch: 29, Train_Loss: 0.16853666305541992, Test_Loss: 0.17377200722694397 *\n",
      "Epoch: 29, Train_Loss: 0.1677062064409256, Test_Loss: 0.18579982221126556\n",
      "Epoch: 29, Train_Loss: 0.17014947533607483, Test_Loss: 0.17668604850769043 *\n",
      "Epoch: 29, Train_Loss: 0.1746332347393036, Test_Loss: 0.313240110874176\n",
      "Epoch: 29, Train_Loss: 0.19007372856140137, Test_Loss: 0.5624327659606934\n",
      "Epoch: 29, Train_Loss: 0.18306411802768707, Test_Loss: 0.3788338005542755 *\n",
      "Epoch: 29, Train_Loss: 0.18390537798404694, Test_Loss: 0.5447160005569458\n",
      "Epoch: 29, Train_Loss: 0.3974560499191284, Test_Loss: 0.29001569747924805 *\n",
      "Epoch: 29, Train_Loss: 0.4414289891719818, Test_Loss: 0.28207385540008545 *\n",
      "Epoch: 29, Train_Loss: 0.18440940976142883, Test_Loss: 0.2784190773963928 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 29\n",
      "Epoch: 29, Train_Loss: 0.21898679435253143, Test_Loss: 0.27184128761291504 *\n",
      "Epoch: 29, Train_Loss: 0.2101028859615326, Test_Loss: 0.26182910799980164 *\n",
      "Epoch: 29, Train_Loss: 0.22334730625152588, Test_Loss: 3.6061480045318604\n",
      "Epoch: 29, Train_Loss: 0.3553556799888611, Test_Loss: 2.5711655616760254 *\n",
      "Epoch: 29, Train_Loss: 0.21206220984458923, Test_Loss: 0.19061294198036194 *\n",
      "Epoch: 29, Train_Loss: 0.4502010643482208, Test_Loss: 0.2052241414785385\n",
      "Epoch: 29, Train_Loss: 0.23855358362197876, Test_Loss: 0.1919855922460556 *\n",
      "Epoch: 29, Train_Loss: 0.3197888135910034, Test_Loss: 0.17756512761116028 *\n",
      "Epoch: 29, Train_Loss: 0.18716630339622498, Test_Loss: 0.17247779667377472 *\n",
      "Epoch: 29, Train_Loss: 0.19160747528076172, Test_Loss: 0.27805325388908386\n",
      "Epoch: 29, Train_Loss: 0.286551833152771, Test_Loss: 0.21231845021247864 *\n",
      "Epoch: 29, Train_Loss: 0.8023476600646973, Test_Loss: 0.17027685046195984 *\n",
      "Epoch: 29, Train_Loss: 0.5914472937583923, Test_Loss: 0.20403355360031128\n",
      "Epoch: 29, Train_Loss: 0.18976344168186188, Test_Loss: 0.22251826524734497\n",
      "Epoch: 29, Train_Loss: 0.20484621822834015, Test_Loss: 0.47169673442840576\n",
      "Epoch: 29, Train_Loss: 0.17228776216506958, Test_Loss: 0.19410070776939392 *\n",
      "Epoch: 29, Train_Loss: 0.37704670429229736, Test_Loss: 0.18796348571777344 *\n",
      "Epoch: 29, Train_Loss: 0.497252494096756, Test_Loss: 0.1784881055355072 *\n",
      "Epoch: 29, Train_Loss: 0.1744031012058258, Test_Loss: 0.18174861371517181\n",
      "Epoch: 29, Train_Loss: 0.3217045068740845, Test_Loss: 0.1999332755804062\n",
      "Epoch: 29, Train_Loss: 0.17935848236083984, Test_Loss: 0.22681568562984467\n",
      "Epoch: 29, Train_Loss: 0.19100944697856903, Test_Loss: 0.21888382732868195 *\n",
      "Epoch: 29, Train_Loss: 0.20225630700588226, Test_Loss: 0.18319080770015717 *\n",
      "Epoch: 29, Train_Loss: 0.2503819763660431, Test_Loss: 0.18856146931648254\n",
      "Epoch: 29, Train_Loss: 0.2577897012233734, Test_Loss: 0.17206571996212006 *\n",
      "Epoch: 29, Train_Loss: 0.2354961484670639, Test_Loss: 0.19071988761425018\n",
      "Epoch: 29, Train_Loss: 0.17882616817951202, Test_Loss: 0.2257019579410553\n",
      "Epoch: 29, Train_Loss: 0.2701665163040161, Test_Loss: 0.16939690709114075 *\n",
      "Epoch: 29, Train_Loss: 0.20364263653755188, Test_Loss: 0.17339827120304108\n",
      "Epoch: 29, Train_Loss: 0.19315794110298157, Test_Loss: 0.18984192609786987\n",
      "Epoch: 29, Train_Loss: 0.18043383955955505, Test_Loss: 0.18978473544120789 *\n",
      "Epoch: 29, Train_Loss: 0.18619631230831146, Test_Loss: 0.17462989687919617 *\n",
      "Epoch: 29, Train_Loss: 0.24459269642829895, Test_Loss: 0.312566876411438\n",
      "Epoch: 29, Train_Loss: 0.3378511369228363, Test_Loss: 0.3076801300048828 *\n",
      "Epoch: 29, Train_Loss: 0.3408908545970917, Test_Loss: 5.185216903686523\n",
      "Epoch: 29, Train_Loss: 0.47855454683303833, Test_Loss: 0.7314983606338501 *\n",
      "Epoch: 29, Train_Loss: 0.49610230326652527, Test_Loss: 0.17660734057426453 *\n",
      "Epoch: 29, Train_Loss: 0.3649803102016449, Test_Loss: 0.19677744805812836\n",
      "Epoch: 29, Train_Loss: 0.2528516352176666, Test_Loss: 0.20930102467536926\n",
      "Epoch: 29, Train_Loss: 0.18736401200294495, Test_Loss: 0.19610542058944702 *\n",
      "Epoch: 29, Train_Loss: 0.1756923496723175, Test_Loss: 0.18548041582107544 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train_Loss: 0.18300440907478333, Test_Loss: 0.23289546370506287\n",
      "Epoch: 29, Train_Loss: 0.27705317735671997, Test_Loss: 0.207830548286438 *\n",
      "Epoch: 29, Train_Loss: 0.471055269241333, Test_Loss: 0.17535118758678436 *\n",
      "Epoch: 29, Train_Loss: 0.49939265847206116, Test_Loss: 0.18950587511062622\n",
      "Epoch: 29, Train_Loss: 0.9613703489303589, Test_Loss: 0.19664901494979858\n",
      "Epoch: 29, Train_Loss: 1.1351491212844849, Test_Loss: 0.22048786282539368\n",
      "Epoch: 29, Train_Loss: 0.279571533203125, Test_Loss: 0.21586939692497253 *\n",
      "Epoch: 29, Train_Loss: 0.3268091678619385, Test_Loss: 0.3153231739997864\n",
      "Epoch: 29, Train_Loss: 0.17377890646457672, Test_Loss: 0.2824631631374359 *\n",
      "Epoch: 29, Train_Loss: 0.2560136318206787, Test_Loss: 0.21623462438583374 *\n",
      "Epoch: 29, Train_Loss: 0.38940849900245667, Test_Loss: 0.20202650129795074 *\n",
      "Epoch: 29, Train_Loss: 0.7199670672416687, Test_Loss: 0.2932189702987671\n",
      "Epoch: 29, Train_Loss: 0.19583439826965332, Test_Loss: 0.25264954566955566 *\n",
      "Epoch: 29, Train_Loss: 0.19649001955986023, Test_Loss: 0.46524056792259216\n",
      "Epoch: 29, Train_Loss: 0.2187785506248474, Test_Loss: 0.37716177105903625 *\n",
      "Epoch: 29, Train_Loss: 0.4044678807258606, Test_Loss: 0.4694375693798065\n",
      "Epoch: 29, Train_Loss: 0.31456321477890015, Test_Loss: 0.31764453649520874 *\n",
      "Epoch: 29, Train_Loss: 0.35321909189224243, Test_Loss: 0.34051042795181274\n",
      "Epoch: 29, Train_Loss: 0.3123939633369446, Test_Loss: 0.4853708744049072\n",
      "Epoch: 29, Train_Loss: 0.3662782609462738, Test_Loss: 0.3593679666519165 *\n",
      "Epoch: 29, Train_Loss: 0.1726851910352707, Test_Loss: 0.2332499921321869 *\n",
      "Epoch: 29, Train_Loss: 0.1769765019416809, Test_Loss: 0.23500844836235046\n",
      "Epoch: 29, Train_Loss: 0.16970914602279663, Test_Loss: 0.20759736001491547 *\n",
      "Epoch: 29, Train_Loss: 0.20410971343517303, Test_Loss: 0.17289765179157257 *\n",
      "Epoch: 29, Train_Loss: 0.22790971398353577, Test_Loss: 0.21054673194885254\n",
      "Epoch: 29, Train_Loss: 0.31834983825683594, Test_Loss: 0.3313458263874054\n",
      "Epoch: 29, Train_Loss: 14.096841812133789, Test_Loss: 0.23726069927215576 *\n",
      "Epoch: 29, Train_Loss: 0.4258904457092285, Test_Loss: 0.26857680082321167\n",
      "Epoch: 29, Train_Loss: 1.0470472574234009, Test_Loss: 0.21654126048088074 *\n",
      "Epoch: 29, Train_Loss: 1.0331742763519287, Test_Loss: 0.21040557324886322 *\n",
      "Epoch: 29, Train_Loss: 0.24095189571380615, Test_Loss: 0.17833873629570007 *\n",
      "Epoch: 29, Train_Loss: 0.2981509566307068, Test_Loss: 0.19684991240501404\n",
      "Epoch: 29, Train_Loss: 2.1440768241882324, Test_Loss: 0.41524383425712585\n",
      "Epoch: 29, Train_Loss: 3.954591751098633, Test_Loss: 0.19254478812217712 *\n",
      "Epoch: 29, Train_Loss: 0.2977493703365326, Test_Loss: 0.45129209756851196\n",
      "Epoch: 29, Train_Loss: 0.40068531036376953, Test_Loss: 0.21134047210216522 *\n",
      "Epoch: 29, Train_Loss: 4.481648921966553, Test_Loss: 0.23805218935012817\n",
      "Epoch: 29, Train_Loss: 0.26821112632751465, Test_Loss: 0.34061378240585327\n",
      "Epoch: 29, Train_Loss: 0.20973195135593414, Test_Loss: 0.2628162205219269 *\n",
      "Epoch: 29, Train_Loss: 0.1773681789636612, Test_Loss: 0.3338662385940552\n",
      "Epoch: 29, Train_Loss: 0.20185089111328125, Test_Loss: 0.17788198590278625 *\n",
      "Epoch: 29, Train_Loss: 0.23346158862113953, Test_Loss: 0.19421222805976868\n",
      "Epoch: 29, Train_Loss: 0.17229405045509338, Test_Loss: 0.2091270238161087\n",
      "Epoch: 29, Train_Loss: 0.18808086216449738, Test_Loss: 0.4597109854221344\n",
      "Epoch: 29, Train_Loss: 0.16631129384040833, Test_Loss: 1.5756230354309082\n",
      "Epoch: 29, Train_Loss: 0.1665896773338318, Test_Loss: 0.5819156169891357 *\n",
      "Epoch: 29, Train_Loss: 0.21149007976055145, Test_Loss: 0.4833616018295288 *\n",
      "Epoch: 29, Train_Loss: 0.18852083384990692, Test_Loss: 0.2015870362520218 *\n",
      "Epoch: 29, Train_Loss: 0.2200600802898407, Test_Loss: 0.18579617142677307 *\n",
      "Epoch: 29, Train_Loss: 0.31602829694747925, Test_Loss: 0.19237658381462097\n",
      "Epoch: 29, Train_Loss: 0.18537043035030365, Test_Loss: 0.19366487860679626\n",
      "Epoch: 29, Train_Loss: 0.1824178844690323, Test_Loss: 0.20629410445690155\n",
      "Epoch: 29, Train_Loss: 0.19069021940231323, Test_Loss: 6.81710958480835\n",
      "Epoch: 29, Train_Loss: 0.19935862720012665, Test_Loss: 1.7674360275268555 *\n",
      "Epoch: 29, Train_Loss: 0.20129452645778656, Test_Loss: 0.27289023995399475 *\n",
      "Epoch: 29, Train_Loss: 0.16831715404987335, Test_Loss: 0.2551383376121521 *\n",
      "Epoch: 29, Train_Loss: 0.1667189598083496, Test_Loss: 0.2463889718055725 *\n",
      "Epoch: 29, Train_Loss: 0.16611474752426147, Test_Loss: 0.19237573444843292 *\n",
      "Epoch: 29, Train_Loss: 0.16632354259490967, Test_Loss: 0.27915284037590027\n",
      "Model saved at location save_new\\model.ckpt at epoch 29\n",
      "Epoch: 29, Train_Loss: 0.1674327254295349, Test_Loss: 0.6128063201904297\n",
      "Epoch: 29, Train_Loss: 0.16635149717330933, Test_Loss: 0.33964961767196655 *\n",
      "Epoch: 29, Train_Loss: 0.1666109561920166, Test_Loss: 0.2536422610282898 *\n",
      "Epoch: 29, Train_Loss: 0.17648851871490479, Test_Loss: 0.3246755003929138\n",
      "Epoch: 29, Train_Loss: 0.19513726234436035, Test_Loss: 0.4665525555610657\n",
      "Epoch: 29, Train_Loss: 0.20519647002220154, Test_Loss: 0.9933753609657288\n",
      "Epoch: 29, Train_Loss: 0.20976313948631287, Test_Loss: 0.2808994948863983 *\n",
      "Epoch: 29, Train_Loss: 0.2631089389324188, Test_Loss: 0.30429571866989136\n",
      "Epoch: 29, Train_Loss: 1.4823732376098633, Test_Loss: 0.17551963031291962 *\n",
      "Epoch: 29, Train_Loss: 4.285253524780273, Test_Loss: 0.17996147274971008\n",
      "Epoch: 29, Train_Loss: 0.1917240023612976, Test_Loss: 0.1979944109916687\n",
      "Epoch: 29, Train_Loss: 0.24469929933547974, Test_Loss: 0.3439742922782898\n",
      "Epoch: 29, Train_Loss: 0.22137323021888733, Test_Loss: 0.31209635734558105 *\n",
      "Epoch: 29, Train_Loss: 0.26235491037368774, Test_Loss: 0.292324423789978 *\n",
      "Epoch: 29, Train_Loss: 0.24395331740379333, Test_Loss: 0.4349379539489746\n",
      "Epoch: 29, Train_Loss: 0.2698671817779541, Test_Loss: 0.30091506242752075 *\n",
      "Epoch: 29, Train_Loss: 0.18455813825130463, Test_Loss: 0.37188321352005005\n",
      "Epoch: 29, Train_Loss: 0.2771393656730652, Test_Loss: 0.6652638912200928\n",
      "Epoch: 29, Train_Loss: 0.3489374816417694, Test_Loss: 0.18596720695495605 *\n",
      "Epoch: 29, Train_Loss: 0.20294129848480225, Test_Loss: 0.3064934015274048\n",
      "Epoch: 29, Train_Loss: 0.18067139387130737, Test_Loss: 0.5126066207885742\n",
      "Epoch: 29, Train_Loss: 0.181881383061409, Test_Loss: 0.4696572721004486 *\n",
      "Epoch: 29, Train_Loss: 0.21392297744750977, Test_Loss: 0.20656606554985046 *\n",
      "Epoch: 29, Train_Loss: 0.2290782928466797, Test_Loss: 0.6515430212020874\n",
      "Epoch: 29, Train_Loss: 0.18133503198623657, Test_Loss: 0.516349196434021 *\n",
      "Epoch: 29, Train_Loss: 0.21172314882278442, Test_Loss: 7.311824321746826\n",
      "Epoch: 29, Train_Loss: 0.17384447157382965, Test_Loss: 0.35858601331710815 *\n",
      "Epoch: 29, Train_Loss: 0.1882331520318985, Test_Loss: 0.16892696917057037 *\n",
      "Epoch: 29, Train_Loss: 0.22312159836292267, Test_Loss: 0.2891136407852173\n",
      "Epoch: 29, Train_Loss: 0.21740315854549408, Test_Loss: 0.5074317455291748\n",
      "Epoch: 29, Train_Loss: 0.18084855377674103, Test_Loss: 0.45365503430366516 *\n",
      "Epoch: 29, Train_Loss: 0.1665429174900055, Test_Loss: 0.21456295251846313 *\n",
      "Epoch: 29, Train_Loss: 0.17174044251441956, Test_Loss: 0.4707416296005249\n",
      "Epoch: 29, Train_Loss: 1.6000319719314575, Test_Loss: 0.2384716272354126 *\n",
      "Epoch: 29, Train_Loss: 3.7689244747161865, Test_Loss: 0.17207850515842438 *\n",
      "Epoch: 29, Train_Loss: 0.16928111016750336, Test_Loss: 0.1854238063097\n",
      "Epoch: 29, Train_Loss: 0.17431269586086273, Test_Loss: 0.19050395488739014\n",
      "Epoch: 29, Train_Loss: 0.17338235676288605, Test_Loss: 0.17112551629543304 *\n",
      "Epoch: 29, Train_Loss: 0.16821208596229553, Test_Loss: 0.32032692432403564\n",
      "Epoch: 29, Train_Loss: 0.16914866864681244, Test_Loss: 0.7673313617706299\n",
      "Epoch: 29, Train_Loss: 0.16733407974243164, Test_Loss: 0.39760830998420715 *\n",
      "Epoch: 29, Train_Loss: 0.18390455842018127, Test_Loss: 0.22082914412021637 *\n",
      "Epoch: 29, Train_Loss: 0.17729568481445312, Test_Loss: 0.20408415794372559 *\n",
      "Epoch: 29, Train_Loss: 0.20368309319019318, Test_Loss: 0.23396435379981995\n",
      "Epoch: 29, Train_Loss: 0.1673397570848465, Test_Loss: 0.2988258898258209\n",
      "Epoch: 29, Train_Loss: 0.16624128818511963, Test_Loss: 0.7041551470756531\n",
      "Epoch: 29, Train_Loss: 0.17222397029399872, Test_Loss: 0.40821775794029236 *\n",
      "Epoch: 29, Train_Loss: 0.1835544854402542, Test_Loss: 0.3998248875141144 *\n",
      "Epoch: 29, Train_Loss: 0.168023020029068, Test_Loss: 0.28768959641456604 *\n",
      "Epoch: 29, Train_Loss: 0.1706460565328598, Test_Loss: 0.31060370802879333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train_Loss: 0.17542053759098053, Test_Loss: 0.57464998960495\n",
      "Epoch: 29, Train_Loss: 0.1842150092124939, Test_Loss: 0.45652520656585693 *\n",
      "Epoch: 29, Train_Loss: 0.1686747670173645, Test_Loss: 0.3033435344696045 *\n",
      "Epoch: 29, Train_Loss: 0.16803579032421112, Test_Loss: 0.29912689328193665 *\n",
      "Epoch: 30, Train_Loss: 0.1983681470155716, Test_Loss: 0.18365678191184998 *\n",
      "Epoch: 30, Train_Loss: 0.19119903445243835, Test_Loss: 0.18144336342811584 *\n",
      "Epoch: 30, Train_Loss: 0.2028454840183258, Test_Loss: 0.29989326000213623\n",
      "Epoch: 30, Train_Loss: 0.19895575940608978, Test_Loss: 0.3488619029521942\n",
      "Epoch: 30, Train_Loss: 0.20628714561462402, Test_Loss: 0.23877039551734924 *\n",
      "Epoch: 30, Train_Loss: 0.2092587649822235, Test_Loss: 0.2623993158340454\n",
      "Epoch: 30, Train_Loss: 0.1810305118560791, Test_Loss: 0.225323885679245 *\n",
      "Epoch: 30, Train_Loss: 0.19670598208904266, Test_Loss: 0.2361173927783966\n",
      "Epoch: 30, Train_Loss: 0.1848195493221283, Test_Loss: 0.17217250168323517 *\n",
      "Epoch: 30, Train_Loss: 0.2843218743801117, Test_Loss: 0.23685096204280853\n",
      "Epoch: 30, Train_Loss: 0.1818753480911255, Test_Loss: 0.5135154724121094\n",
      "Epoch: 30, Train_Loss: 0.16741985082626343, Test_Loss: 0.384194940328598 *\n",
      "Epoch: 30, Train_Loss: 0.16583533585071564, Test_Loss: 0.2842887043952942 *\n",
      "Epoch: 30, Train_Loss: 0.16624589264392853, Test_Loss: 0.25428539514541626 *\n",
      "Epoch: 30, Train_Loss: 0.166883185505867, Test_Loss: 0.18683218955993652 *\n",
      "Epoch: 30, Train_Loss: 0.16767458617687225, Test_Loss: 0.19033047556877136\n",
      "Epoch: 30, Train_Loss: 2.1344363689422607, Test_Loss: 0.1797337383031845 *\n",
      "Epoch: 30, Train_Loss: 2.4042277336120605, Test_Loss: 0.19739310443401337\n",
      "Epoch: 30, Train_Loss: 0.1731482893228531, Test_Loss: 0.1774350255727768 *\n",
      "Epoch: 30, Train_Loss: 0.16908042132854462, Test_Loss: 0.18702752888202667\n",
      "Epoch: 30, Train_Loss: 0.16902002692222595, Test_Loss: 0.16990791261196136 *\n",
      "Epoch: 30, Train_Loss: 0.17032675445079803, Test_Loss: 0.27531763911247253\n",
      "Epoch: 30, Train_Loss: 0.16703493893146515, Test_Loss: 0.5562429428100586\n",
      "Epoch: 30, Train_Loss: 0.17350700497627258, Test_Loss: 0.27341943979263306 *\n",
      "Epoch: 30, Train_Loss: 0.16781261563301086, Test_Loss: 0.5316490530967712\n",
      "Epoch: 30, Train_Loss: 0.1704435497522354, Test_Loss: 0.18020561337471008 *\n",
      "Epoch: 30, Train_Loss: 0.1766882687807083, Test_Loss: 0.16979862749576569 *\n",
      "Epoch: 30, Train_Loss: 0.1819716989994049, Test_Loss: 0.17041485011577606\n",
      "Epoch: 30, Train_Loss: 0.18924477696418762, Test_Loss: 0.21244539320468903\n",
      "Epoch: 30, Train_Loss: 0.20597822964191437, Test_Loss: 0.23341986536979675\n",
      "Epoch: 30, Train_Loss: 0.19653743505477905, Test_Loss: 8.94408893585205\n",
      "Epoch: 30, Train_Loss: 0.1682291030883789, Test_Loss: 0.5719556212425232 *\n",
      "Epoch: 30, Train_Loss: 0.233098566532135, Test_Loss: 0.22812987864017487 *\n",
      "Epoch: 30, Train_Loss: 0.19482986629009247, Test_Loss: 0.23421579599380493\n",
      "Epoch: 30, Train_Loss: 0.23352769017219543, Test_Loss: 0.2547554671764374\n",
      "Epoch: 30, Train_Loss: 0.2626093924045563, Test_Loss: 0.17443513870239258 *\n",
      "Epoch: 30, Train_Loss: 0.1697784662246704, Test_Loss: 0.32291901111602783\n",
      "Epoch: 30, Train_Loss: 0.16589123010635376, Test_Loss: 0.34083080291748047\n",
      "Epoch: 30, Train_Loss: 0.17484290897846222, Test_Loss: 0.27506327629089355 *\n",
      "Epoch: 30, Train_Loss: 0.17042239010334015, Test_Loss: 0.392899751663208\n",
      "Epoch: 30, Train_Loss: 0.1717168390750885, Test_Loss: 0.40487316250801086\n",
      "Epoch: 30, Train_Loss: 0.17495179176330566, Test_Loss: 0.8452199101448059\n",
      "Epoch: 30, Train_Loss: 0.1660166233778, Test_Loss: 0.6126198768615723 *\n",
      "Epoch: 30, Train_Loss: 0.16676440834999084, Test_Loss: 0.23716306686401367 *\n",
      "Epoch: 30, Train_Loss: 0.1731579601764679, Test_Loss: 0.22922775149345398 *\n",
      "Epoch: 30, Train_Loss: 0.1993478238582611, Test_Loss: 0.19751854240894318 *\n",
      "Epoch: 30, Train_Loss: 0.23879504203796387, Test_Loss: 0.23557138442993164\n",
      "Epoch: 30, Train_Loss: 0.24033090472221375, Test_Loss: 0.1961616426706314 *\n",
      "Epoch: 30, Train_Loss: 0.2854267954826355, Test_Loss: 0.37454748153686523\n",
      "Epoch: 30, Train_Loss: 0.25124821066856384, Test_Loss: 0.28484559059143066 *\n",
      "Epoch: 30, Train_Loss: 0.2687540650367737, Test_Loss: 0.410981684923172\n",
      "Epoch: 30, Train_Loss: 0.20689943432807922, Test_Loss: 0.3256526589393616 *\n",
      "Epoch: 30, Train_Loss: 0.23940183222293854, Test_Loss: 0.28668805956840515 *\n",
      "Epoch: 30, Train_Loss: 0.1926020085811615, Test_Loss: 0.2699010968208313 *\n",
      "Epoch: 30, Train_Loss: 0.42375820875167847, Test_Loss: 0.3845483064651489\n",
      "Epoch: 30, Train_Loss: 0.18856263160705566, Test_Loss: 0.1718529909849167 *\n",
      "Epoch: 30, Train_Loss: 0.2510128915309906, Test_Loss: 0.2926709055900574\n",
      "Epoch: 30, Train_Loss: 2.095804452896118, Test_Loss: 0.35907459259033203\n",
      "Epoch: 30, Train_Loss: 0.2892147898674011, Test_Loss: 0.3368619680404663 *\n",
      "Epoch: 30, Train_Loss: 0.20683139562606812, Test_Loss: 0.1708475649356842 *\n",
      "Epoch: 30, Train_Loss: 0.1877344846725464, Test_Loss: 0.6608152389526367\n",
      "Epoch: 30, Train_Loss: 0.2041754424571991, Test_Loss: 1.3474617004394531\n",
      "Epoch: 30, Train_Loss: 0.17593586444854736, Test_Loss: 5.82729434967041\n",
      "Epoch: 30, Train_Loss: 0.18754985928535461, Test_Loss: 0.24181821942329407 *\n",
      "Epoch: 30, Train_Loss: 0.21569740772247314, Test_Loss: 0.1714937537908554 *\n",
      "Epoch: 30, Train_Loss: 0.2449374794960022, Test_Loss: 0.3020278215408325\n",
      "Epoch: 30, Train_Loss: 0.20118604600429535, Test_Loss: 0.7386521697044373\n",
      "Epoch: 30, Train_Loss: 0.191539466381073, Test_Loss: 0.29691243171691895 *\n",
      "Epoch: 30, Train_Loss: 0.17221599817276, Test_Loss: 0.22114068269729614 *\n",
      "Epoch: 30, Train_Loss: 0.17253345251083374, Test_Loss: 0.40056416392326355\n",
      "Epoch: 30, Train_Loss: 0.17826153337955475, Test_Loss: 0.1908475160598755 *\n",
      "Epoch: 30, Train_Loss: 0.17793413996696472, Test_Loss: 0.18084990978240967 *\n",
      "Epoch: 30, Train_Loss: 0.1957000344991684, Test_Loss: 0.19254973530769348\n",
      "Epoch: 30, Train_Loss: 0.17790359258651733, Test_Loss: 0.22626376152038574\n",
      "Epoch: 30, Train_Loss: 0.16648434102535248, Test_Loss: 0.1871723085641861 *\n",
      "Epoch: 30, Train_Loss: 0.17711025476455688, Test_Loss: 0.3958173394203186\n",
      "Epoch: 30, Train_Loss: 0.17736971378326416, Test_Loss: 0.7372245192527771\n",
      "Epoch: 30, Train_Loss: 0.17990005016326904, Test_Loss: 0.3360508680343628 *\n",
      "Epoch: 30, Train_Loss: 0.1666545271873474, Test_Loss: 0.174529030919075 *\n",
      "Epoch: 30, Train_Loss: 0.1657141000032425, Test_Loss: 0.20350775122642517\n",
      "Epoch: 30, Train_Loss: 0.1651061773300171, Test_Loss: 0.30818891525268555\n",
      "Epoch: 30, Train_Loss: 0.16771747171878815, Test_Loss: 0.39150524139404297\n",
      "Epoch: 30, Train_Loss: 0.17561110854148865, Test_Loss: 0.9443651437759399\n",
      "Epoch: 30, Train_Loss: 0.1673244833946228, Test_Loss: 0.6033788323402405 *\n",
      "Epoch: 30, Train_Loss: 0.17741170525550842, Test_Loss: 0.5491549372673035 *\n",
      "Epoch: 30, Train_Loss: 0.17009025812149048, Test_Loss: 0.5215448141098022 *\n",
      "Epoch: 30, Train_Loss: 0.16735848784446716, Test_Loss: 0.410170316696167 *\n",
      "Epoch: 30, Train_Loss: 0.16673435270786285, Test_Loss: 0.7015663981437683\n",
      "Epoch: 30, Train_Loss: 0.17281968891620636, Test_Loss: 0.4862150549888611 *\n",
      "Epoch: 30, Train_Loss: 0.17534852027893066, Test_Loss: 0.26904383301734924 *\n",
      "Epoch: 30, Train_Loss: 0.18248185515403748, Test_Loss: 0.2725914418697357\n",
      "Epoch: 30, Train_Loss: 0.18231996893882751, Test_Loss: 0.17088524997234344 *\n",
      "Epoch: 30, Train_Loss: 0.18519550561904907, Test_Loss: 0.1965365707874298\n",
      "Epoch: 30, Train_Loss: 0.19709865748882294, Test_Loss: 0.26812124252319336\n",
      "Epoch: 30, Train_Loss: 0.18299265205860138, Test_Loss: 0.4210202097892761\n",
      "Epoch: 30, Train_Loss: 0.17719915509223938, Test_Loss: 0.3047954738140106 *\n",
      "Epoch: 30, Train_Loss: 0.1979949027299881, Test_Loss: 0.24924784898757935 *\n",
      "Epoch: 30, Train_Loss: 0.1944996565580368, Test_Loss: 0.24351215362548828 *\n",
      "Epoch: 30, Train_Loss: 0.17026865482330322, Test_Loss: 0.19493578374385834 *\n",
      "Epoch: 30, Train_Loss: 0.17293661832809448, Test_Loss: 0.1696797013282776 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 30\n",
      "Epoch: 30, Train_Loss: 0.17465633153915405, Test_Loss: 0.2031330168247223\n",
      "Epoch: 30, Train_Loss: 0.1837848722934723, Test_Loss: 0.29303956031799316\n",
      "Epoch: 30, Train_Loss: 0.21688081324100494, Test_Loss: 0.3395838141441345\n",
      "Epoch: 30, Train_Loss: 0.21509407460689545, Test_Loss: 0.2571524977684021 *\n",
      "Epoch: 30, Train_Loss: 0.1738995760679245, Test_Loss: 0.267061710357666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train_Loss: 0.18052493035793304, Test_Loss: 0.19172222912311554 *\n",
      "Epoch: 30, Train_Loss: 0.21899767220020294, Test_Loss: 0.1679646074771881 *\n",
      "Epoch: 30, Train_Loss: 0.23547063767910004, Test_Loss: 0.17697469890117645\n",
      "Epoch: 30, Train_Loss: 0.1918168067932129, Test_Loss: 0.18129819631576538\n",
      "Epoch: 30, Train_Loss: 0.2659996747970581, Test_Loss: 0.18205799162387848\n",
      "Epoch: 30, Train_Loss: 0.2078261524438858, Test_Loss: 0.1793883740901947 *\n",
      "Epoch: 30, Train_Loss: 0.21714559197425842, Test_Loss: 0.17948727309703827\n",
      "Epoch: 30, Train_Loss: 0.18616549670696259, Test_Loss: 0.27740705013275146\n",
      "Epoch: 30, Train_Loss: 0.20231878757476807, Test_Loss: 0.5015621185302734\n",
      "Epoch: 30, Train_Loss: 0.2099706381559372, Test_Loss: 0.42409101128578186 *\n",
      "Epoch: 30, Train_Loss: 0.19135528802871704, Test_Loss: 0.37289148569107056 *\n",
      "Epoch: 30, Train_Loss: 0.17119915783405304, Test_Loss: 0.28465932607650757 *\n",
      "Epoch: 30, Train_Loss: 0.1682995706796646, Test_Loss: 0.28427761793136597 *\n",
      "Epoch: 30, Train_Loss: 0.17353783547878265, Test_Loss: 0.2800610065460205 *\n",
      "Epoch: 30, Train_Loss: 0.17279715836048126, Test_Loss: 0.234090656042099 *\n",
      "Epoch: 30, Train_Loss: 0.21078407764434814, Test_Loss: 0.3605705797672272\n",
      "Epoch: 30, Train_Loss: 0.22827944159507751, Test_Loss: 7.482308864593506\n",
      "Epoch: 30, Train_Loss: 0.19704732298851013, Test_Loss: 0.2636938989162445 *\n",
      "Epoch: 30, Train_Loss: 0.19032974541187286, Test_Loss: 0.19937664270401 *\n",
      "Epoch: 30, Train_Loss: 0.1819581687450409, Test_Loss: 0.21294209361076355\n",
      "Epoch: 30, Train_Loss: 0.17180082201957703, Test_Loss: 0.18833859264850616 *\n",
      "Epoch: 30, Train_Loss: 0.32840800285339355, Test_Loss: 0.17152833938598633 *\n",
      "Epoch: 30, Train_Loss: 0.3012857437133789, Test_Loss: 0.25190073251724243\n",
      "Epoch: 30, Train_Loss: 0.16530638933181763, Test_Loss: 0.2523609697818756\n",
      "Epoch: 30, Train_Loss: 0.2027614414691925, Test_Loss: 0.1887841820716858 *\n",
      "Epoch: 30, Train_Loss: 0.16642750799655914, Test_Loss: 0.20084652304649353\n",
      "Epoch: 30, Train_Loss: 0.17449384927749634, Test_Loss: 0.25902459025382996\n",
      "Epoch: 30, Train_Loss: 0.17690180242061615, Test_Loss: 0.6942416429519653\n",
      "Epoch: 30, Train_Loss: 0.17393016815185547, Test_Loss: 0.2409304827451706 *\n",
      "Epoch: 30, Train_Loss: 0.16650374233722687, Test_Loss: 0.20615032315254211 *\n",
      "Epoch: 30, Train_Loss: 0.17803479731082916, Test_Loss: 0.2269323766231537\n",
      "Epoch: 30, Train_Loss: 0.16842295229434967, Test_Loss: 0.19193291664123535 *\n",
      "Epoch: 30, Train_Loss: 0.17168524861335754, Test_Loss: 0.1797705441713333 *\n",
      "Epoch: 30, Train_Loss: 0.17342925071716309, Test_Loss: 0.17477981746196747 *\n",
      "Epoch: 30, Train_Loss: 0.1654588282108307, Test_Loss: 0.33556830883026123\n",
      "Epoch: 30, Train_Loss: 0.16596461832523346, Test_Loss: 0.19378244876861572 *\n",
      "Epoch: 30, Train_Loss: 0.1716614067554474, Test_Loss: 0.2735588550567627\n",
      "Epoch: 30, Train_Loss: 0.1736503690481186, Test_Loss: 0.16906026005744934 *\n",
      "Epoch: 30, Train_Loss: 0.17808863520622253, Test_Loss: 0.17002998292446136\n",
      "Epoch: 30, Train_Loss: 0.17886069416999817, Test_Loss: 0.1777513325214386\n",
      "Epoch: 30, Train_Loss: 0.17577414214611053, Test_Loss: 0.17091818153858185 *\n",
      "Epoch: 30, Train_Loss: 0.17548958957195282, Test_Loss: 0.1658983826637268 *\n",
      "Epoch: 30, Train_Loss: 0.17920885980129242, Test_Loss: 0.18876858055591583\n",
      "Epoch: 30, Train_Loss: 0.17728252708911896, Test_Loss: 0.1967875063419342\n",
      "Epoch: 30, Train_Loss: 0.1764070987701416, Test_Loss: 0.17541715502738953 *\n",
      "Epoch: 30, Train_Loss: 0.18447011709213257, Test_Loss: 0.1674586832523346 *\n",
      "Epoch: 30, Train_Loss: 0.16988950967788696, Test_Loss: 0.3769519329071045\n",
      "Epoch: 30, Train_Loss: 0.171525776386261, Test_Loss: 2.5946035385131836\n",
      "Epoch: 30, Train_Loss: 0.18497119843959808, Test_Loss: 3.830310344696045\n",
      "Epoch: 30, Train_Loss: 0.20355692505836487, Test_Loss: 0.18868787586688995 *\n",
      "Epoch: 30, Train_Loss: 2.451214075088501, Test_Loss: 0.16673220694065094 *\n",
      "Epoch: 30, Train_Loss: 2.6808972358703613, Test_Loss: 0.18700352311134338\n",
      "Epoch: 30, Train_Loss: 0.17448654770851135, Test_Loss: 0.30231642723083496\n",
      "Epoch: 30, Train_Loss: 0.16864025592803955, Test_Loss: 0.18395954370498657 *\n",
      "Epoch: 30, Train_Loss: 0.19739866256713867, Test_Loss: 0.2677735984325409\n",
      "Epoch: 30, Train_Loss: 0.2711837887763977, Test_Loss: 0.34700873494148254\n",
      "Epoch: 30, Train_Loss: 0.1871582269668579, Test_Loss: 0.17367041110992432 *\n",
      "Epoch: 30, Train_Loss: 0.16892406344413757, Test_Loss: 0.18217802047729492\n",
      "Epoch: 30, Train_Loss: 0.17371368408203125, Test_Loss: 0.18829073011875153\n",
      "Epoch: 30, Train_Loss: 0.22926118969917297, Test_Loss: 0.2053993195295334\n",
      "Epoch: 30, Train_Loss: 0.1810789555311203, Test_Loss: 0.1761409342288971 *\n",
      "Epoch: 30, Train_Loss: 0.17651057243347168, Test_Loss: 0.29994237422943115\n",
      "Epoch: 30, Train_Loss: 0.5606396198272705, Test_Loss: 0.3285052478313446\n",
      "Epoch: 30, Train_Loss: 0.36777469515800476, Test_Loss: 0.23973125219345093 *\n",
      "Epoch: 30, Train_Loss: 0.5397500991821289, Test_Loss: 0.18259447813034058 *\n",
      "Epoch: 30, Train_Loss: 0.26175352931022644, Test_Loss: 0.1908920258283615\n",
      "Epoch: 30, Train_Loss: 0.6788680553436279, Test_Loss: 0.29178282618522644\n",
      "Epoch: 30, Train_Loss: 0.4416983723640442, Test_Loss: 0.3421748876571655\n",
      "Epoch: 30, Train_Loss: 0.4188835024833679, Test_Loss: 0.6895438432693481\n",
      "Epoch: 30, Train_Loss: 0.17148037254810333, Test_Loss: 0.5084456205368042 *\n",
      "Epoch: 30, Train_Loss: 0.22911018133163452, Test_Loss: 0.4253436326980591 *\n",
      "Epoch: 30, Train_Loss: 0.4305195212364197, Test_Loss: 0.4802898168563843\n",
      "Epoch: 30, Train_Loss: 0.409014493227005, Test_Loss: 0.3756030797958374 *\n",
      "Epoch: 30, Train_Loss: 0.2064308524131775, Test_Loss: 0.5473940372467041\n",
      "Epoch: 30, Train_Loss: 0.17527982592582703, Test_Loss: 0.39445000886917114 *\n",
      "Epoch: 30, Train_Loss: 0.18948900699615479, Test_Loss: 0.2590993344783783 *\n",
      "Epoch: 30, Train_Loss: 0.2640571892261505, Test_Loss: 0.22111211717128754 *\n",
      "Epoch: 30, Train_Loss: 0.2430412769317627, Test_Loss: 0.16867293417453766 *\n",
      "Epoch: 30, Train_Loss: 0.24868889153003693, Test_Loss: 0.22125251591205597\n",
      "Epoch: 30, Train_Loss: 0.17906434834003448, Test_Loss: 0.2509114146232605\n",
      "Epoch: 30, Train_Loss: 0.22195127606391907, Test_Loss: 0.4184841513633728\n",
      "Epoch: 30, Train_Loss: 0.30574434995651245, Test_Loss: 0.3442806601524353 *\n",
      "Epoch: 30, Train_Loss: 0.30379003286361694, Test_Loss: 0.23332847654819489 *\n",
      "Epoch: 30, Train_Loss: 0.19755125045776367, Test_Loss: 0.24483250081539154\n",
      "Epoch: 30, Train_Loss: 0.23221135139465332, Test_Loss: 0.1977940946817398 *\n",
      "Epoch: 30, Train_Loss: 0.2530725300312042, Test_Loss: 0.1741858869791031 *\n",
      "Epoch: 30, Train_Loss: 0.21663959324359894, Test_Loss: 0.2501866817474365\n",
      "Epoch: 30, Train_Loss: 0.20745530724525452, Test_Loss: 0.2390260398387909 *\n",
      "Epoch: 30, Train_Loss: 0.24646040797233582, Test_Loss: 0.5260303616523743\n",
      "Epoch: 30, Train_Loss: 0.21891601383686066, Test_Loss: 0.25893813371658325 *\n",
      "Epoch: 30, Train_Loss: 0.2118416279554367, Test_Loss: 0.2209770679473877 *\n",
      "Epoch: 30, Train_Loss: 0.24986906349658966, Test_Loss: 0.18681149184703827 *\n",
      "Epoch: 30, Train_Loss: 0.19223707914352417, Test_Loss: 0.16691114008426666 *\n",
      "Epoch: 30, Train_Loss: 0.1753770262002945, Test_Loss: 0.17889483273029327\n",
      "Epoch: 30, Train_Loss: 0.1645394265651703, Test_Loss: 0.1804993450641632\n",
      "Model saved at location save_new\\model.ckpt at epoch 30\n",
      "Epoch: 30, Train_Loss: 0.1719420850276947, Test_Loss: 0.1812763661146164\n",
      "Epoch: 30, Train_Loss: 0.16467750072479248, Test_Loss: 0.17854148149490356 *\n",
      "Epoch: 30, Train_Loss: 0.17157293856143951, Test_Loss: 0.2120831161737442\n",
      "Epoch: 30, Train_Loss: 0.1787205934524536, Test_Loss: 0.37048906087875366\n",
      "Epoch: 30, Train_Loss: 0.17236216366291046, Test_Loss: 0.44159895181655884\n",
      "Epoch: 30, Train_Loss: 0.1723668873310089, Test_Loss: 0.5188797116279602\n",
      "Epoch: 30, Train_Loss: 0.36033719778060913, Test_Loss: 0.3224714994430542 *\n",
      "Epoch: 30, Train_Loss: 0.34969598054885864, Test_Loss: 0.285922110080719 *\n",
      "Epoch: 30, Train_Loss: 0.17421464622020721, Test_Loss: 0.2827013432979584 *\n",
      "Epoch: 30, Train_Loss: 0.1998370736837387, Test_Loss: 0.2798505127429962 *\n",
      "Epoch: 30, Train_Loss: 0.21083077788352966, Test_Loss: 0.26035815477371216 *\n",
      "Epoch: 30, Train_Loss: 0.20664912462234497, Test_Loss: 1.032226324081421\n",
      "Epoch: 30, Train_Loss: 0.4127088785171509, Test_Loss: 6.447025299072266\n",
      "Epoch: 30, Train_Loss: 0.20948706567287445, Test_Loss: 0.19775664806365967 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train_Loss: 0.42658787965774536, Test_Loss: 0.1943650096654892 *\n",
      "Epoch: 30, Train_Loss: 0.2232356071472168, Test_Loss: 0.21479153633117676\n",
      "Epoch: 30, Train_Loss: 0.26833662390708923, Test_Loss: 0.17590142786502838 *\n",
      "Epoch: 30, Train_Loss: 0.1965903490781784, Test_Loss: 0.16959148645401 *\n",
      "Epoch: 30, Train_Loss: 0.17995749413967133, Test_Loss: 0.2666711211204529\n",
      "Epoch: 30, Train_Loss: 0.18386265635490417, Test_Loss: 0.23457106947898865 *\n",
      "Epoch: 30, Train_Loss: 0.6746522188186646, Test_Loss: 0.16888213157653809 *\n",
      "Epoch: 30, Train_Loss: 0.6113145351409912, Test_Loss: 0.18006397783756256\n",
      "Epoch: 30, Train_Loss: 0.17172379791736603, Test_Loss: 0.2165868878364563\n",
      "Epoch: 30, Train_Loss: 0.20411726832389832, Test_Loss: 0.4514092803001404\n",
      "Epoch: 30, Train_Loss: 0.16859503090381622, Test_Loss: 0.1893720030784607 *\n",
      "Epoch: 30, Train_Loss: 0.23755532503128052, Test_Loss: 0.175893634557724 *\n",
      "Epoch: 30, Train_Loss: 0.5202940702438354, Test_Loss: 0.1956251859664917\n",
      "Epoch: 30, Train_Loss: 0.16841557621955872, Test_Loss: 0.18615928292274475 *\n",
      "Epoch: 30, Train_Loss: 0.2827915549278259, Test_Loss: 0.18847410380840302\n",
      "Epoch: 30, Train_Loss: 0.20871314406394958, Test_Loss: 0.2164222002029419\n",
      "Epoch: 30, Train_Loss: 0.19726824760437012, Test_Loss: 0.25089502334594727\n",
      "Epoch: 30, Train_Loss: 0.2099493145942688, Test_Loss: 0.1980002522468567 *\n",
      "Epoch: 30, Train_Loss: 0.2713831663131714, Test_Loss: 0.1914699673652649 *\n",
      "Epoch: 30, Train_Loss: 0.29443785548210144, Test_Loss: 0.16908401250839233 *\n",
      "Epoch: 30, Train_Loss: 0.19451257586479187, Test_Loss: 0.17798222601413727\n",
      "Epoch: 30, Train_Loss: 0.19027622044086456, Test_Loss: 0.19969043135643005\n",
      "Epoch: 30, Train_Loss: 0.24322742223739624, Test_Loss: 0.17115294933319092 *\n",
      "Epoch: 30, Train_Loss: 0.20370979607105255, Test_Loss: 0.16659829020500183 *\n",
      "Epoch: 30, Train_Loss: 0.1893312931060791, Test_Loss: 0.1892891675233841\n",
      "Epoch: 30, Train_Loss: 0.17608904838562012, Test_Loss: 0.19279815256595612\n",
      "Epoch: 30, Train_Loss: 0.17691169679164886, Test_Loss: 0.1708187758922577 *\n",
      "Epoch: 30, Train_Loss: 0.22916853427886963, Test_Loss: 0.1835618019104004\n",
      "Epoch: 30, Train_Loss: 0.3512778580188751, Test_Loss: 0.315941721200943\n",
      "Epoch: 30, Train_Loss: 0.42809268832206726, Test_Loss: 3.276425361633301\n",
      "Epoch: 30, Train_Loss: 0.5994425415992737, Test_Loss: 2.258256673812866 *\n",
      "Epoch: 30, Train_Loss: 0.4448026418685913, Test_Loss: 0.18113282322883606 *\n",
      "Epoch: 30, Train_Loss: 0.3736928701400757, Test_Loss: 0.1712847650051117 *\n",
      "Epoch: 30, Train_Loss: 0.2463802546262741, Test_Loss: 0.17009203135967255 *\n",
      "Epoch: 30, Train_Loss: 0.20220477879047394, Test_Loss: 0.2143142819404602\n",
      "Epoch: 30, Train_Loss: 0.17776615917682648, Test_Loss: 0.18163302540779114 *\n",
      "Epoch: 30, Train_Loss: 0.17518281936645508, Test_Loss: 0.22851979732513428\n",
      "Epoch: 30, Train_Loss: 0.25118494033813477, Test_Loss: 0.20085649192333221 *\n",
      "Epoch: 30, Train_Loss: 0.46544650197029114, Test_Loss: 0.17202435433864594 *\n",
      "Epoch: 30, Train_Loss: 0.5238727927207947, Test_Loss: 0.19216927886009216\n",
      "Epoch: 30, Train_Loss: 0.854799211025238, Test_Loss: 0.19020292162895203 *\n",
      "Epoch: 30, Train_Loss: 1.072004795074463, Test_Loss: 0.22250604629516602\n",
      "Epoch: 30, Train_Loss: 0.32358741760253906, Test_Loss: 0.1725749522447586 *\n",
      "Epoch: 30, Train_Loss: 0.38753706216812134, Test_Loss: 0.3230610489845276\n",
      "Epoch: 30, Train_Loss: 0.16514426469802856, Test_Loss: 0.33399146795272827\n",
      "Epoch: 30, Train_Loss: 0.18652983009815216, Test_Loss: 0.2303789108991623 *\n",
      "Epoch: 30, Train_Loss: 0.35069942474365234, Test_Loss: 0.17959628999233246 *\n",
      "Epoch: 30, Train_Loss: 0.6169167757034302, Test_Loss: 0.20893925428390503\n",
      "Epoch: 30, Train_Loss: 0.22453878819942474, Test_Loss: 0.2489355504512787\n",
      "Epoch: 30, Train_Loss: 0.1954999566078186, Test_Loss: 0.3792421221733093\n",
      "Epoch: 30, Train_Loss: 0.17952083051204681, Test_Loss: 0.4603797495365143\n",
      "Epoch: 30, Train_Loss: 0.31122034788131714, Test_Loss: 0.5084129571914673\n",
      "Epoch: 30, Train_Loss: 0.4673185348510742, Test_Loss: 0.3441447913646698 *\n",
      "Epoch: 30, Train_Loss: 0.4090462028980255, Test_Loss: 0.35298025608062744\n",
      "Epoch: 30, Train_Loss: 0.2975122332572937, Test_Loss: 0.45562586188316345\n",
      "Epoch: 30, Train_Loss: 0.4279620051383972, Test_Loss: 0.4467751979827881 *\n",
      "Epoch: 30, Train_Loss: 0.17040567100048065, Test_Loss: 0.2790476381778717 *\n",
      "Epoch: 30, Train_Loss: 0.1710466742515564, Test_Loss: 0.20728975534439087 *\n",
      "Epoch: 30, Train_Loss: 0.171017587184906, Test_Loss: 0.21272718906402588\n",
      "Epoch: 30, Train_Loss: 0.20281535387039185, Test_Loss: 0.16824868321418762 *\n",
      "Epoch: 30, Train_Loss: 0.21416817605495453, Test_Loss: 0.1984691321849823\n",
      "Epoch: 30, Train_Loss: 0.2852431535720825, Test_Loss: 0.23553261160850525\n",
      "Epoch: 30, Train_Loss: 14.327794075012207, Test_Loss: 0.2361442744731903\n",
      "Epoch: 30, Train_Loss: 0.48630014061927795, Test_Loss: 0.2864028513431549\n",
      "Epoch: 30, Train_Loss: 1.1189398765563965, Test_Loss: 0.21324515342712402 *\n",
      "Epoch: 30, Train_Loss: 0.8916329145431519, Test_Loss: 0.2096732258796692 *\n",
      "Epoch: 30, Train_Loss: 0.22404548525810242, Test_Loss: 0.20018574595451355 *\n",
      "Epoch: 30, Train_Loss: 0.21730516850948334, Test_Loss: 0.1819465607404709 *\n",
      "Epoch: 30, Train_Loss: 1.791811466217041, Test_Loss: 0.3430544435977936\n",
      "Epoch: 30, Train_Loss: 3.093536376953125, Test_Loss: 0.21347659826278687 *\n",
      "Epoch: 30, Train_Loss: 0.3979201912879944, Test_Loss: 0.5377019643783569\n",
      "Epoch: 30, Train_Loss: 0.327239066362381, Test_Loss: 0.22039464116096497 *\n",
      "Epoch: 30, Train_Loss: 4.419915676116943, Test_Loss: 0.23004025220870972\n",
      "Epoch: 30, Train_Loss: 0.551246166229248, Test_Loss: 0.28540804982185364\n",
      "Epoch: 30, Train_Loss: 0.22179076075553894, Test_Loss: 0.20417442917823792 *\n",
      "Epoch: 30, Train_Loss: 0.1743873506784439, Test_Loss: 0.298106849193573\n",
      "Epoch: 30, Train_Loss: 0.1823223978281021, Test_Loss: 0.1722850203514099 *\n",
      "Epoch: 30, Train_Loss: 0.22065891325473785, Test_Loss: 0.18807843327522278\n",
      "Epoch: 30, Train_Loss: 0.1687602996826172, Test_Loss: 0.1736583113670349 *\n",
      "Epoch: 30, Train_Loss: 0.18848192691802979, Test_Loss: 0.3423943519592285\n",
      "Epoch: 30, Train_Loss: 0.16264083981513977, Test_Loss: 1.504836916923523\n",
      "Epoch: 30, Train_Loss: 0.16254949569702148, Test_Loss: 0.8413395881652832 *\n",
      "Epoch: 30, Train_Loss: 0.16380783915519714, Test_Loss: 0.49169501662254333 *\n",
      "Epoch: 30, Train_Loss: 0.18070954084396362, Test_Loss: 0.22433793544769287 *\n",
      "Epoch: 30, Train_Loss: 0.22873127460479736, Test_Loss: 0.1639101356267929 *\n",
      "Epoch: 30, Train_Loss: 0.2307979166507721, Test_Loss: 0.16437876224517822\n",
      "Model saved at location save_new\\model.ckpt at epoch 30\n",
      "Epoch: 30, Train_Loss: 0.19802196323871613, Test_Loss: 0.16490861773490906\n",
      "Epoch: 30, Train_Loss: 0.18136218190193176, Test_Loss: 0.20884492993354797\n",
      "Epoch: 30, Train_Loss: 0.17836157977581024, Test_Loss: 3.5129637718200684\n",
      "Epoch: 30, Train_Loss: 0.16767042875289917, Test_Loss: 4.444746494293213\n",
      "Epoch: 30, Train_Loss: 0.2009565383195877, Test_Loss: 0.36798834800720215 *\n",
      "Epoch: 30, Train_Loss: 0.1677224487066269, Test_Loss: 0.33549562096595764 *\n",
      "Epoch: 30, Train_Loss: 0.16423743963241577, Test_Loss: 0.37071293592453003\n",
      "Epoch: 30, Train_Loss: 0.16349086165428162, Test_Loss: 0.21186494827270508 *\n",
      "Epoch: 30, Train_Loss: 0.16427074372768402, Test_Loss: 0.22399219870567322\n",
      "Epoch: 30, Train_Loss: 0.163185253739357, Test_Loss: 0.9628112316131592\n",
      "Epoch: 30, Train_Loss: 0.16248029470443726, Test_Loss: 0.6239346265792847 *\n",
      "Epoch: 30, Train_Loss: 0.16240155696868896, Test_Loss: 0.27961239218711853 *\n",
      "Epoch: 30, Train_Loss: 0.1674216389656067, Test_Loss: 0.7011489868164062\n",
      "Epoch: 30, Train_Loss: 0.1876625418663025, Test_Loss: 0.46658819913864136 *\n",
      "Epoch: 30, Train_Loss: 0.21264921128749847, Test_Loss: 1.6477634906768799\n",
      "Epoch: 30, Train_Loss: 0.23302076756954193, Test_Loss: 0.5382053852081299 *\n",
      "Epoch: 30, Train_Loss: 0.203713059425354, Test_Loss: 0.6592070460319519\n",
      "Epoch: 30, Train_Loss: 0.6114152073860168, Test_Loss: 0.3446095585823059 *\n",
      "Epoch: 30, Train_Loss: 3.9970130920410156, Test_Loss: 0.19527336955070496 *\n",
      "Epoch: 30, Train_Loss: 0.27032333612442017, Test_Loss: 0.23795127868652344\n",
      "Epoch: 30, Train_Loss: 0.18900498747825623, Test_Loss: 0.24803775548934937\n",
      "Epoch: 30, Train_Loss: 0.19660557806491852, Test_Loss: 0.6327897310256958\n",
      "Epoch: 30, Train_Loss: 0.2544058859348297, Test_Loss: 0.2591477930545807 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train_Loss: 0.216903418302536, Test_Loss: 0.7089095115661621\n",
      "Epoch: 30, Train_Loss: 0.2546236515045166, Test_Loss: 0.1780296266078949 *\n",
      "Epoch: 30, Train_Loss: 0.18677213788032532, Test_Loss: 0.5564401745796204\n",
      "Epoch: 30, Train_Loss: 0.25974977016448975, Test_Loss: 0.6361671090126038\n",
      "Epoch: 30, Train_Loss: 0.2568493187427521, Test_Loss: 0.20396246016025543 *\n",
      "Epoch: 30, Train_Loss: 0.2037164866924286, Test_Loss: 0.19216744601726532 *\n",
      "Epoch: 30, Train_Loss: 0.17796677350997925, Test_Loss: 0.49023503065109253\n",
      "Epoch: 30, Train_Loss: 0.17492325603961945, Test_Loss: 0.5182623267173767\n",
      "Epoch: 30, Train_Loss: 0.1651957929134369, Test_Loss: 0.25907036662101746 *\n",
      "Epoch: 30, Train_Loss: 0.18557702004909515, Test_Loss: 0.3964483141899109\n",
      "Epoch: 30, Train_Loss: 0.1663065403699875, Test_Loss: 0.5212777853012085\n",
      "Epoch: 30, Train_Loss: 0.19337581098079681, Test_Loss: 5.599178314208984\n",
      "Epoch: 30, Train_Loss: 0.1935988813638687, Test_Loss: 1.9635462760925293 *\n",
      "Epoch: 30, Train_Loss: 0.2214920073747635, Test_Loss: 0.18395833671092987 *\n",
      "Epoch: 30, Train_Loss: 0.20814980566501617, Test_Loss: 0.1925775408744812\n",
      "Epoch: 30, Train_Loss: 0.23438765108585358, Test_Loss: 0.2545251250267029\n",
      "Epoch: 30, Train_Loss: 0.1852876842021942, Test_Loss: 0.40042197704315186\n",
      "Epoch: 30, Train_Loss: 0.1628439724445343, Test_Loss: 0.17718751728534698 *\n",
      "Epoch: 30, Train_Loss: 0.1629679948091507, Test_Loss: 0.32849931716918945\n",
      "Epoch: 30, Train_Loss: 0.6752329468727112, Test_Loss: 0.3542151153087616\n",
      "Epoch: 30, Train_Loss: 3.8483290672302246, Test_Loss: 0.17225870490074158 *\n",
      "Epoch: 30, Train_Loss: 0.16820833086967468, Test_Loss: 0.18438899517059326\n",
      "Epoch: 30, Train_Loss: 0.1647615283727646, Test_Loss: 0.19024810194969177\n",
      "Epoch: 30, Train_Loss: 0.16809549927711487, Test_Loss: 0.18474648892879486 *\n",
      "Epoch: 30, Train_Loss: 0.16546902060508728, Test_Loss: 0.20056521892547607\n",
      "Epoch: 30, Train_Loss: 0.1641722172498703, Test_Loss: 0.5310827493667603\n",
      "Epoch: 30, Train_Loss: 0.16965612769126892, Test_Loss: 0.49102699756622314 *\n",
      "Epoch: 30, Train_Loss: 0.17341840267181396, Test_Loss: 0.49444150924682617\n",
      "Epoch: 30, Train_Loss: 0.17282307147979736, Test_Loss: 0.28406718373298645 *\n",
      "Epoch: 30, Train_Loss: 0.207597553730011, Test_Loss: 0.20013709366321564 *\n",
      "Epoch: 30, Train_Loss: 0.1681639403104782, Test_Loss: 0.3377276062965393\n",
      "Epoch: 30, Train_Loss: 0.16285257041454315, Test_Loss: 0.8026188015937805\n",
      "Epoch: 30, Train_Loss: 0.16419681906700134, Test_Loss: 0.5963582396507263 *\n",
      "Epoch: 30, Train_Loss: 0.18052643537521362, Test_Loss: 0.5152157545089722 *\n",
      "Epoch: 30, Train_Loss: 0.1667802929878235, Test_Loss: 0.34110376238822937 *\n",
      "Epoch: 30, Train_Loss: 0.16740338504314423, Test_Loss: 0.37260815501213074\n",
      "Epoch: 30, Train_Loss: 0.17173074185848236, Test_Loss: 0.5688640475273132\n",
      "Epoch: 30, Train_Loss: 0.17877361178398132, Test_Loss: 0.59199458360672\n",
      "Epoch: 30, Train_Loss: 0.17099513113498688, Test_Loss: 0.3716990351676941 *\n",
      "Epoch: 30, Train_Loss: 0.1624900847673416, Test_Loss: 0.2752506136894226 *\n",
      "Epoch: 30, Train_Loss: 0.17287686467170715, Test_Loss: 0.19245657324790955 *\n",
      "Epoch: 30, Train_Loss: 0.20771268010139465, Test_Loss: 0.18513143062591553 *\n",
      "Epoch: 30, Train_Loss: 0.22491872310638428, Test_Loss: 0.19821086525917053\n",
      "Epoch: 30, Train_Loss: 0.18668152391910553, Test_Loss: 0.4161902070045471\n",
      "Epoch: 30, Train_Loss: 0.19436556100845337, Test_Loss: 0.22364646196365356 *\n",
      "Epoch: 30, Train_Loss: 0.19420205056667328, Test_Loss: 0.3083941638469696\n",
      "Epoch: 30, Train_Loss: 0.17851220071315765, Test_Loss: 0.2228149175643921 *\n",
      "Epoch: 30, Train_Loss: 0.18844543397426605, Test_Loss: 0.2965296804904938\n",
      "Epoch: 30, Train_Loss: 0.16682876646518707, Test_Loss: 0.18257200717926025 *\n",
      "Epoch: 30, Train_Loss: 0.3263509273529053, Test_Loss: 0.20910978317260742\n",
      "Epoch: 30, Train_Loss: 0.17968881130218506, Test_Loss: 0.4664528965950012\n",
      "Epoch: 30, Train_Loss: 0.16589346528053284, Test_Loss: 0.3519408106803894 *\n",
      "Epoch: 30, Train_Loss: 0.1629946231842041, Test_Loss: 0.4791504144668579\n",
      "Epoch: 30, Train_Loss: 0.16485154628753662, Test_Loss: 0.2692006230354309 *\n",
      "Epoch: 30, Train_Loss: 0.16535568237304688, Test_Loss: 0.21055613458156586 *\n",
      "Epoch: 30, Train_Loss: 0.1632215529680252, Test_Loss: 0.2025466412305832 *\n",
      "Epoch: 30, Train_Loss: 0.7341721653938293, Test_Loss: 0.1783766895532608 *\n",
      "Epoch: 30, Train_Loss: 3.5446250438690186, Test_Loss: 0.18670205771923065\n",
      "Epoch: 30, Train_Loss: 0.17396721243858337, Test_Loss: 0.172805055975914 *\n",
      "Epoch: 30, Train_Loss: 0.16709338128566742, Test_Loss: 0.18760989606380463\n",
      "Epoch: 30, Train_Loss: 0.16772323846817017, Test_Loss: 0.16547220945358276 *\n",
      "Epoch: 30, Train_Loss: 0.16465476155281067, Test_Loss: 0.28371113538742065\n",
      "Epoch: 30, Train_Loss: 0.1642197072505951, Test_Loss: 0.5159368515014648\n",
      "Epoch: 30, Train_Loss: 0.16696597635746002, Test_Loss: 0.2990536689758301 *\n",
      "Epoch: 30, Train_Loss: 0.16334307193756104, Test_Loss: 0.46197375655174255\n",
      "Epoch: 30, Train_Loss: 0.16416305303573608, Test_Loss: 0.22094909846782684 *\n",
      "Epoch: 30, Train_Loss: 0.16568304598331451, Test_Loss: 0.2183123528957367 *\n",
      "Epoch: 30, Train_Loss: 0.19565215706825256, Test_Loss: 0.2175389677286148 *\n",
      "Epoch: 30, Train_Loss: 0.1974974423646927, Test_Loss: 0.21504361927509308 *\n",
      "Epoch: 30, Train_Loss: 0.2148127555847168, Test_Loss: 0.22107696533203125\n",
      "Epoch: 30, Train_Loss: 0.19579553604125977, Test_Loss: 4.744286060333252\n",
      "Epoch: 30, Train_Loss: 0.1640893816947937, Test_Loss: 2.051560163497925 *\n",
      "Epoch: 30, Train_Loss: 0.26600319147109985, Test_Loss: 0.19418610632419586 *\n",
      "Epoch: 30, Train_Loss: 0.19807842373847961, Test_Loss: 0.19923177361488342\n",
      "Epoch: 30, Train_Loss: 0.2470274567604065, Test_Loss: 0.19175627827644348 *\n",
      "Epoch: 30, Train_Loss: 0.2756497859954834, Test_Loss: 0.18395808339118958 *\n",
      "Epoch: 30, Train_Loss: 0.1671324521303177, Test_Loss: 0.16982315480709076 *\n",
      "Model saved at location save_new\\model.ckpt at epoch 30\n",
      "Epoch: 30, Train_Loss: 0.1624320149421692, Test_Loss: 0.2496580183506012\n",
      "Epoch: 30, Train_Loss: 0.17501047253608704, Test_Loss: 0.20506668090820312 *\n",
      "Epoch: 30, Train_Loss: 0.16466617584228516, Test_Loss: 0.16690191626548767 *\n",
      "Epoch: 30, Train_Loss: 0.16526883840560913, Test_Loss: 0.1894916445016861\n",
      "Epoch: 30, Train_Loss: 0.17857173085212708, Test_Loss: 0.19195380806922913\n",
      "Epoch: 30, Train_Loss: 0.16748107969760895, Test_Loss: 0.46455150842666626\n",
      "Epoch: 30, Train_Loss: 0.1621796190738678, Test_Loss: 0.1843872219324112 *\n",
      "Epoch: 30, Train_Loss: 0.1732747107744217, Test_Loss: 0.18488021194934845\n",
      "Epoch: 30, Train_Loss: 0.18836137652397156, Test_Loss: 0.1826770156621933 *\n",
      "Epoch: 30, Train_Loss: 0.22029633820056915, Test_Loss: 0.1870807558298111\n",
      "Epoch: 30, Train_Loss: 0.20200683176517487, Test_Loss: 0.1999167501926422\n",
      "Epoch: 30, Train_Loss: 0.2295607030391693, Test_Loss: 0.2916909456253052\n",
      "Epoch: 30, Train_Loss: 0.2035304456949234, Test_Loss: 0.21466563642024994 *\n",
      "Epoch: 30, Train_Loss: 0.28016558289527893, Test_Loss: 0.1723746508359909 *\n",
      "Epoch: 30, Train_Loss: 0.22511424124240875, Test_Loss: 0.1894412785768509\n",
      "Epoch: 30, Train_Loss: 0.22527199983596802, Test_Loss: 0.17457309365272522 *\n",
      "Epoch: 30, Train_Loss: 0.1899127960205078, Test_Loss: 0.1832117736339569\n",
      "Epoch: 30, Train_Loss: 0.3191679120063782, Test_Loss: 0.3258749544620514\n",
      "Epoch: 30, Train_Loss: 0.18039056658744812, Test_Loss: 0.16441835463047028 *\n",
      "Epoch: 30, Train_Loss: 0.17847470939159393, Test_Loss: 0.16726936399936676\n",
      "Epoch: 30, Train_Loss: 2.0266528129577637, Test_Loss: 0.21242758631706238\n",
      "Epoch: 30, Train_Loss: 0.4650731682777405, Test_Loss: 0.2449909746646881\n",
      "Epoch: 30, Train_Loss: 0.1878451704978943, Test_Loss: 0.16704736649990082 *\n",
      "Epoch: 30, Train_Loss: 0.17949220538139343, Test_Loss: 0.4518844485282898\n",
      "Epoch: 30, Train_Loss: 0.2012409269809723, Test_Loss: 0.29536640644073486 *\n",
      "Epoch: 30, Train_Loss: 0.19645999372005463, Test_Loss: 4.9416327476501465\n",
      "Epoch: 30, Train_Loss: 0.18124720454216003, Test_Loss: 0.3899242877960205 *\n",
      "Epoch: 30, Train_Loss: 0.2024897038936615, Test_Loss: 0.16868305206298828 *\n",
      "Epoch: 30, Train_Loss: 0.2581486105918884, Test_Loss: 0.21280086040496826\n",
      "Epoch: 30, Train_Loss: 0.19536860287189484, Test_Loss: 0.2892225384712219\n",
      "Epoch: 30, Train_Loss: 0.18462476134300232, Test_Loss: 0.21728238463401794 *\n",
      "Epoch: 30, Train_Loss: 0.1683134138584137, Test_Loss: 0.16897626221179962 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train_Loss: 0.1715930700302124, Test_Loss: 0.2346566766500473\n",
      "Epoch: 30, Train_Loss: 0.16569876670837402, Test_Loss: 0.20226138830184937 *\n",
      "Epoch: 30, Train_Loss: 0.17313040792942047, Test_Loss: 0.17326638102531433 *\n",
      "Epoch: 30, Train_Loss: 0.2237595170736313, Test_Loss: 0.17758920788764954\n",
      "Epoch: 30, Train_Loss: 0.18556572496891022, Test_Loss: 0.20248210430145264\n",
      "Epoch: 30, Train_Loss: 0.16576167941093445, Test_Loss: 0.18466104567050934 *\n",
      "Epoch: 30, Train_Loss: 0.17012184858322144, Test_Loss: 0.22905607521533966\n",
      "Epoch: 30, Train_Loss: 0.1683998852968216, Test_Loss: 0.3769432306289673\n",
      "Epoch: 30, Train_Loss: 0.1711798757314682, Test_Loss: 0.21361181139945984 *\n",
      "Epoch: 30, Train_Loss: 0.16770616173744202, Test_Loss: 0.1869291365146637 *\n",
      "Epoch: 30, Train_Loss: 0.16357940435409546, Test_Loss: 0.19919468462467194\n",
      "Epoch: 30, Train_Loss: 0.16391554474830627, Test_Loss: 0.27829042077064514\n",
      "Epoch: 30, Train_Loss: 0.16369673609733582, Test_Loss: 0.23656624555587769 *\n",
      "Epoch: 30, Train_Loss: 0.17029473185539246, Test_Loss: 0.5004621148109436\n",
      "Epoch: 30, Train_Loss: 0.16472096741199493, Test_Loss: 0.48147451877593994 *\n",
      "Epoch: 30, Train_Loss: 0.1785506010055542, Test_Loss: 0.4935879111289978\n",
      "Epoch: 30, Train_Loss: 0.17376336455345154, Test_Loss: 0.33091500401496887 *\n",
      "Epoch: 30, Train_Loss: 0.16341927647590637, Test_Loss: 0.3202337920665741 *\n",
      "Epoch: 30, Train_Loss: 0.16418227553367615, Test_Loss: 0.506584644317627\n",
      "Epoch: 30, Train_Loss: 0.17441362142562866, Test_Loss: 0.2764706015586853 *\n",
      "Epoch: 30, Train_Loss: 0.1755775660276413, Test_Loss: 0.17647284269332886 *\n",
      "Epoch: 30, Train_Loss: 0.17928314208984375, Test_Loss: 0.18397267162799835\n"
     ]
    }
   ],
   "source": [
    "SAVEDIR = \"save_new\"\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "L2NormConst = 0.001\n",
    "train_vars = tf.trainable_variables() #it will return all the variables. Here, all the weights and biases are variables which\n",
    "#are trainable.\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(y_true, y_predicted))) + tf.add_n([tf.nn.l2_loss(w) for w in train_vars]) * L2NormConst\n",
    "#since this is a regression problem so above loss is mean-squared-error loss\n",
    "# HYPERPARAMETER OPtimizer - Adam\n",
    "train_step = tf.train.AdamOptimizer(learning_rate = 10**-4).minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "epoch_number, train_loss, test_loss,  = [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_avg_loss = 0\n",
    "    test_avg_loss = 0\n",
    "    te_loss_old = 10000  #any big number can be given\n",
    "    \n",
    "    for i in range(int(len(x)/batch_size)):\n",
    "        train_batch_x, train_batch_y = loadTrainBatch(batch_size)\n",
    "        train_step.run(feed_dict = {x_input: train_batch_x, y_true: train_batch_y, keep_prob: 0.8})\n",
    "        tr_loss = loss.eval(feed_dict = {x_input: train_batch_x, y_true: train_batch_y, keep_prob: 1.0})\n",
    "        train_avg_loss += tr_loss / batch_size\n",
    "    \n",
    "        test_batch_x, test_batch_y = loadTestBatch(batch_size)\n",
    "        te_loss_new = loss.eval(feed_dict = {x_input: test_batch_x, y_true: test_batch_y, keep_prob: 1.0})\n",
    "        test_avg_loss += te_loss_new / batch_size\n",
    "        \n",
    "        if te_loss_new < te_loss_old:\n",
    "            print(\"Epoch: {}, Train_Loss: {}, Test_Loss: {} *\".format(epoch+1, tr_loss, te_loss_new))\n",
    "        else:\n",
    "            print(\"Epoch: {}, Train_Loss: {}, Test_Loss: {}\".format(epoch+1, tr_loss, te_loss_new))\n",
    "        te_loss_old = te_loss_new\n",
    "        \n",
    "        if (i+1) % batch_size == 0:\n",
    "            if not os.path.exists(SAVEDIR):\n",
    "                os.makedirs(SAVEDIR)\n",
    "            save_path = os.path.join(SAVEDIR, \"model.ckpt\")\n",
    "            saver.save(sess = sess, save_path = save_path)\n",
    "            print(\"Model saved at location {} at epoch {}\".format(save_path, epoch + 1))\n",
    "        \n",
    "    epoch_number.append(epoch)\n",
    "    train_loss.append(train_avg_loss)\n",
    "    test_loss.append(test_avg_loss)\n",
    "    \n",
    "#creating dataframe and record all the losses and accuracies at each epoch\n",
    "log_frame = pd.DataFrame(columns = [\"Epoch\", \"Train Loss\", \"Test Loss\"])\n",
    "log_frame[\"Epoch\"] = epoch_number\n",
    "log_frame[\"Train Loss\"] = train_loss\n",
    "log_frame[\"Test Loss\"] = test_loss\n",
    "log_frame.to_csv(os.path.join(SAVEDIR, \"log.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Test Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>24.600900</td>\n",
       "      <td>23.765289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16.771786</td>\n",
       "      <td>16.386271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12.390103</td>\n",
       "      <td>11.972845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9.714837</td>\n",
       "      <td>9.312085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8.343564</td>\n",
       "      <td>7.587736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6.549875</td>\n",
       "      <td>6.296506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5.644486</td>\n",
       "      <td>5.398565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4.905185</td>\n",
       "      <td>4.633310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>4.685084</td>\n",
       "      <td>4.188541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3.801439</td>\n",
       "      <td>3.625611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>3.482774</td>\n",
       "      <td>3.186119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>3.135356</td>\n",
       "      <td>2.915707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>3.213356</td>\n",
       "      <td>2.898915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2.671429</td>\n",
       "      <td>2.646253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2.544372</td>\n",
       "      <td>2.396472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2.376251</td>\n",
       "      <td>2.320761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>2.596073</td>\n",
       "      <td>2.552147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>2.166161</td>\n",
       "      <td>2.080837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2.090612</td>\n",
       "      <td>2.043052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>2.008416</td>\n",
       "      <td>2.143866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>2.260359</td>\n",
       "      <td>2.205205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>1.883247</td>\n",
       "      <td>1.867211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1.812041</td>\n",
       "      <td>1.923577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1.752669</td>\n",
       "      <td>1.907356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2.033294</td>\n",
       "      <td>1.928481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1.658224</td>\n",
       "      <td>1.881536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1.607653</td>\n",
       "      <td>1.845024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1.585167</td>\n",
       "      <td>1.772650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1.877461</td>\n",
       "      <td>1.835105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1.482526</td>\n",
       "      <td>1.979244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch  Train Loss  Test Loss\n",
       "0       0   24.600900  23.765289\n",
       "1       1   16.771786  16.386271\n",
       "2       2   12.390103  11.972845\n",
       "3       3    9.714837   9.312085\n",
       "4       4    8.343564   7.587736\n",
       "5       5    6.549875   6.296506\n",
       "6       6    5.644486   5.398565\n",
       "7       7    4.905185   4.633310\n",
       "8       8    4.685084   4.188541\n",
       "9       9    3.801439   3.625611\n",
       "10     10    3.482774   3.186119\n",
       "11     11    3.135356   2.915707\n",
       "12     12    3.213356   2.898915\n",
       "13     13    2.671429   2.646253\n",
       "14     14    2.544372   2.396472\n",
       "15     15    2.376251   2.320761\n",
       "16     16    2.596073   2.552147\n",
       "17     17    2.166161   2.080837\n",
       "18     18    2.090612   2.043052\n",
       "19     19    2.008416   2.143866\n",
       "20     20    2.260359   2.205205\n",
       "21     21    1.883247   1.867211\n",
       "22     22    1.812041   1.923577\n",
       "23     23    1.752669   1.907356\n",
       "24     24    2.033294   1.928481\n",
       "25     25    1.658224   1.881536\n",
       "26     26    1.607653   1.845024\n",
       "27     27    1.585167   1.772650\n",
       "28     28    1.877461   1.835105\n",
       "29     29    1.482526   1.979244"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = pd.read_csv(os.path.join(SAVEDIR, \"log.csv\"))\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Study\\\\Applied_AI\\\\SELF_DRIVING_CAR\\\\Autopilot-TensorFlow-master'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save_new\\model.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"save_new\\model.ckpt\")\n",
    "\n",
    "img = cv2.imread('steering_wheel_image.jpg', 0) #here, second parameter '0' specifies that img.shape will return only height and\n",
    "#width of the image and not the number of channels. It is a colored image so number of channels = 3, which it will not return.\n",
    "rows, cols = img.shape\n",
    "\n",
    "i = 0\n",
    "while(cv2.waitKey(60) != ord(\"q\")):\n",
    "    full_image = cv2.imread(test_x[i])\n",
    "    cv2.imshow('Frame Window', full_image)\n",
    "    image = ((cv2.resize(full_image[-150:], (200, 66)) / 255.0).reshape((1, 66, 200, 3)))\n",
    "    degrees = sess.run(y_predicted, feed_dict = {x_input: image, keep_prob: 1.0})[0][0] *180 / pi #here, we have converted the\n",
    "    #predicted degrees from radians to degrees.\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2), -degrees, 1) #this function rotate the image by a given degrees.\n",
    "    dst = cv2.warpAffine(src = img, M = M, dsize = (cols, rows)) #warpAffine function applies rotation to the image\n",
    "    cv2.imshow(\"Steering Wheel\", dst)\n",
    "    i += 1\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Self_driving_car.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
